Title,ID,Published,Updated,Summary,Author,Comments,Journal_Ref,Link,Primary_Category,Categories,DOI,License,Affiliation
"MATRYOSHKA: Non-Exclusive Memory Tiering via Transactional Page
  Migration",http://arxiv.org/abs/2401.13154v1,2024-01-24T00:19:19Z,2024-01-24T00:19:19Z,"  With the advent of byte-addressable memory devices, such as CXL memory,
persistent memory, and storage-class memory, tiered memory systems have become
a reality. Page migration is the de facto method within operating systems for
managing tiered memory. It aims to bring hot data whenever possible into fast
memory to optimize the performance of data accesses while using slow memory to
accommodate data spilled from fast memory. While the existing research has
demonstrated the effectiveness of various optimizations on page migration, it
falls short of addressing a fundamental question: Is exclusive memory tiering,
in which a page is either present in fast memory or slow memory, but not both
simultaneously, the optimal strategy for tiered memory management?
  We demonstrate that page migration-based exclusive memory tiering suffers
significant performance degradation when fast memory is under pressure. In this
paper, we propose non-exclusive memory tiering, a page management strategy that
retains a copy of pages recently promoted from slow memory to fast memory to
mitigate memory thrashing. To enable non-exclusive memory tiering, we develop
MATRYOSHKA, a new mechanism that features transactional page migration and page
shadowing. MATRYOSHKA removes page migration off the program's critical path
and makes migration asynchronous. Evaluations with microbenchmarks and
realworld applications show that MATRYOSHKA achieves 6x performance improvement
over the state-of-the-art transparent page placement (TPP) approach under
memory pressure. We also compare MATRYOSHKA with a recently proposed
sampling-based migration approach and demonstrate MATRYOSHKA's strengths and
potential weaknesses in various scenarios. Through the evaluations, we discover
a serious issue facing all tested approaches, unfortunately including
MATRYOSHKA, and call for further research on tiered memory-aware memory
allocation.
","['\nLingfeng Xiang\n', '\nZhen Lin\n', '\nWeishu Deng\n', '\nHui Lu\n', '\nJia Rao\n', '\nYifan Yuan\n', '\nRen Wang\n']",,,http://arxiv.org/abs/2401.13154v1,cs.OS,['cs.OS'],,,[]
numaPTE: Managing Page-Tables and TLBs on NUMA Systems,http://arxiv.org/abs/2401.15558v1,2024-01-28T03:50:14Z,2024-01-28T03:50:14Z,"  Memory management operations that modify page-tables, typically performed
during memory allocation/deallocation, are infamous for their poor performance
in highly threaded applications, largely due to process-wide TLB shootdowns
that the OS must issue due to the lack of hardware support for TLB coherence.
We study these operations in NUMA settings, where we observe up to 40x overhead
for basic operations such as munmap or mprotect. The overhead further increases
if page-table replication is used, where complete coherent copies of the
page-tables are maintained across all NUMA nodes. While eager system-wide
replication is extremely effective at localizing page-table reads during
address translation, we find that it creates additional penalties upon any
page-table changes due to the need to maintain all replicas coherent.
  In this paper, we propose a novel page-table management mechanism, called
numaPTE, to enable transparent, on-demand, and partial page-table replication
across NUMA nodes in order to perform address translation locally, while
avoiding the overheads and scalability issues of system-wide full page-table
replication. We then show that numaPTE's precise knowledge of page-table
sharers can be leveraged to significantly reduce the number of TLB shootdowns
issued upon any memory-management operation. As a result, numaPTE not only
avoids replication-related slowdowns, but also provides significant speedup
over the baseline on memory allocation/deallocation and access control
operations. We implement numaPTEin Linux on x86_64, evaluate it on 4- and
8-socket systems, and show that numaPTE achieves the full benefits of eager
page-table replication on a wide range of applications, while also achieving a
12% and 36% runtime improvement on Webserver and Memcached respectively due to
a significant reduction in TLB shootdowns.
","['\nBin Gao\n', '\nQingxuan Kang\n', '\nHao-Wei Tee\n', '\nKyle Timothy Ng Chu\n', '\nAlireza Sanaee\n', '\nDjordje Jevdjic\n']",,,http://arxiv.org/abs/2401.15558v1,cs.OS,['cs.OS'],,,[]
"Characterizing Network Requirements for GPU API Remoting in AI
  Applications",http://arxiv.org/abs/2401.13354v1,2024-01-24T10:30:39Z,2024-01-24T10:30:39Z,"  GPU remoting is a promising technique for supporting AI applications.
Networking plays a key role in enabling remoting. However, for efficient
remoting, the network requirements in terms of latency and bandwidth are
unknown. In this paper, we take a GPU-centric approach to derive the minimum
latency and bandwidth requirements for GPU remoting, while ensuring no (or
little) performance degradation for AI applications. Our study including
theoretical model demonstrates that, with careful remoting design, unmodified
AI applications can run on the remoting setup using commodity networking
hardware without any overhead or even with better performance, with low network
demands.
","['\nTianxia Wang\n', '\nZhuofu Chen\n', '\nXingda Wei\n', '\nJinyu Gu\n', '\nRong Chen\n', '\nHaibo Chen\n']",,,http://arxiv.org/abs/2401.13354v1,cs.OS,"['cs.OS', 'cs.NI']",,,[]
"Beyond Control: Exploring Novel File System Objects for Data-Only
  Attacks on Linux Systems",http://arxiv.org/abs/2401.17618v1,2024-01-31T06:16:00Z,2024-01-31T06:16:00Z,"  The widespread deployment of control-flow integrity has propelled non-control
data attacks into the mainstream. In the domain of OS kernel exploits, by
corrupting critical non-control data, local attackers can directly gain root
access or privilege escalation without hijacking the control flow. As a result,
OS kernels have been restricting the availability of such non-control data.
This forces attackers to continue to search for more exploitable non-control
data in OS kernels. However, discovering unknown non-control data can be
daunting because they are often tied heavily to semantics and lack universal
patterns.
  We make two contributions in this paper: (1) discover critical non-control
objects in the file subsystem and (2) analyze their exploitability. This work
represents the first study, with minimal domain knowledge, to
semi-automatically discover and evaluate exploitable non-control data within
the file subsystem of the Linux kernel. Our solution utilizes a custom analysis
and testing framework that statically and dynamically identifies promising
candidate objects. Furthermore, we categorize these discovered objects into
types that are suitable for various exploit strategies, including a novel
strategy necessary to overcome the defense that isolates many of these objects.
These objects have the advantage of being exploitable without requiring KASLR,
thus making the exploits simpler and more reliable. We use 18 real-world CVEs
to evaluate the exploitability of the file system objects using various exploit
strategies. We develop 10 end-to-end exploits using a subset of CVEs against
the kernel with all state-of-the-art mitigations enabled.
","['\nJinmeng Zhou\n', '\nJiayi Hu\n', '\nZiyue Pan\n', '\nJiaxun Zhu\n', '\nGuoren Li\n', '\nWenbo Shen\n', '\nYulei Sui\n', '\nZhiyun Qian\n']","14 pages, in submission of the 31th ACM Conference on Computer and
  Communications Security (CCS), 2024",,http://arxiv.org/abs/2401.17618v1,cs.CR,"['cs.CR', 'cs.OS']",,,[]
SyzRetrospector: A Large-Scale Retrospective Study of Syzbot,http://arxiv.org/abs/2401.11642v1,2024-01-22T01:06:55Z,2024-01-22T01:06:55Z,"  Over the past 6 years, Syzbot has fuzzed the Linux kernel day and night to
report over 5570 bugs, of which 4604 have been patched [11]. While this is
impressive, we have found the average time to find a bug is over 405 days.
Moreover, we have found that current metrics commonly used, such as
time-to-find and number of bugs found, are inaccurate in evaluating Syzbot
since bugs often spend the majority of their lives hidden from the fuzzer. In
this paper, we set out to better understand and quantify Syzbot's performance
and improvement in finding bugs. Our tool, SyzRetrospector, takes a different
approach to evaluating Syzbot by finding the earliest that Syzbot was capable
of finding a bug, and why that bug was revealed. We use SyzRetrospector on a
large scale to analyze 559 bugs and find that bugs are hidden for an average of
331.17 days before Syzbot is even able to find them. We further present
findings on the behaviors of revealing factors, how some bugs are harder to
reveal than others, the trends in delays over the past 6 years, and how bug
location relates to delays. We also provide key takeaways for improving
Syzbot's delays.
","['\nJoseph Bursey\n', '\nArdalan Amiri Sani\n', '\nZhiyun Qian\n']",,,http://arxiv.org/abs/2401.11642v1,cs.SE,"['cs.SE', 'cs.CR', 'cs.OS']",,,[]
File System Aging,http://arxiv.org/abs/2401.08858v1,2024-01-16T22:18:51Z,2024-01-16T22:18:51Z,"  File systems must allocate space for files without knowing what will be added
or removed in the future. Over the life of a file system, this may cause
suboptimal file placement decisions that eventually lead to slower performance,
or aging. Conventional wisdom suggests that file system aging is a solved
problem in the common case; heuristics to avoid aging, such as colocating
related files and data blocks, are effective until a storage device fills up,
at which point space pressure exacerbates fragmentation-based aging. However,
this article describes both realistic and synthetic workloads that can cause
these heuristics to fail, inducing large performance declines due to aging,
even when the storage device is nearly empty.
  We argue that these slowdowns are caused by poor layout. We demonstrate a
correlation between the read performance of a directory scan and the locality
within a file system's access patterns, using a dynamic layout score. We
complement these results with microbenchmarks that show that space pressure can
cause a substantial amount of inter-file and intra-file fragmentation. However,
our results suggest that the effect of free-space fragmentation on read
performance is best described as accelerating the file system aging process.
The effect on write performance is non-existent in some cases, and, in most
cases, an order of magnitude smaller than the read degradation from
fragmentation caused by normal usage.
  In short, many file systems are exquisitely prone to read aging after a
variety of write patterns. We show, however, that aging is not inevitable.
BetrFS, a file system based on write-optimized dictionaries, exhibits almost no
aging in our experiments. We present a framework for understanding and
predicting aging, and identify the key features of BetrFS that avoid aging.
","['\nAlex Conway\n', '\nAinesh Bakshi\n', '\nArghya Bhattacharya\n', '\nRory Bennett\n', '\nYizheng Jiao\n', '\nEric Knorr\n', '\nYang Zhan\n', '\nMichael A. Bender\n', '\nWilliam Jannen\n', '\nRob Johnson\n', '\nBradley C. Kuszmaul\n', '\nDonald E. Porter\n', '\nJun Yuan\n', '\nMartin Farach-Colton\n']","36 pages, 12 figures. Article is an extension of Conway et al. FAST
  17. (see
  https://www.usenix.org/conference/fast17/technical-sessions/presentation/conway)
  and Conway et al. HotStorage 19. (see
  https://www.usenix.org/conference/hotstorage19/presentation/conway)",,http://arxiv.org/abs/2401.08858v1,cs.OS,"['cs.OS', 'H.3.2; D.4.3; D.4.2; D.4.8; E.1; E.5; H.3.4']",,,[]
Dynamic Voltage and Frequency Scaling for Intermittent Computing,http://arxiv.org/abs/2401.08710v1,2024-01-15T12:04:04Z,2024-01-15T12:04:04Z,"  We present hardware/software techniques to intelligently regulate supply
voltage and clock frequency of intermittently-computing devices. These devices
rely on ambient energy harvesting to power their operation and small capacitors
as energy buffers. Statically setting their clock frequency fails to capture
the unique relations these devices expose between capacitor voltage, energy
efficiency at a given operating frequency, and the corresponding operating
range. Existing dynamic voltage and frequency scaling techniques are also
largely inapplicable due to extreme energy scarcity and peculiar hardware
features. We introduce two hardware/software co-designs that accommodate the
distinct hardware features and function within a constrained energy envelope,
offering varied trade-offs and functionalities. Our experimental evaluation
combines tests on custom-manufactured hardware and detailed emulation
experiments. The data gathered indicate that our approaches result in up to
3.75x reduced energy consumption and 12x swifter execution times compared to
the considered baselines, all while utilizing smaller capacitors to accomplish
identical workloads.
","['\nAndrea Maioli\n', '\nKevin A. Quinones\n', '\nSaad Ahmed\n', '\nMuhammad H. Alizai\n', '\nLuca Mottola\n']",,,http://arxiv.org/abs/2401.08710v1,cs.AR,"['cs.AR', 'cs.OS']",,,[]
Herding LLaMaS: Using LLMs as an OS Module,http://arxiv.org/abs/2401.08908v1,2024-01-17T01:32:45Z,2024-01-17T01:32:45Z,"  Computer systems are becoming increasingly heterogeneous with the emergence
of new memory technologies and compute devices. GPUs alongside CPUs have become
commonplace and CXL is poised to be a mainstay of cloud systems. The operating
system is responsible for managing these hardware resources, requiring
modification every time a new device is released. Years of research and
development are sunk into tuning the OS for high performance with each new
heterogeneous device. With the recent explosion in memory technologies and
domain-specific accelerators, it would be beneficial to have an OS that could
provide high performance for new devices without significant effort.
  We propose LLaMaS which can adapt to new devices easily. LLaMaS uses Large
Language Models (LLMs) to extract the useful features of new devices from their
textual description and uses these features to make operating system decisions
at runtime. Adding support to LLaMaS for a new device is as simple as
describing the system and new device properties in plaintext.
  LLaMaS reduces the burden on system administrators to enable easy integration
of new devices into production systems.
  Preliminary evaluation using ChatGPT shows that LLMs are capable of
extracting device features from text and make correct OS decisions based on
those features.
","['\nAditya K Kamath\n', '\nSujay Yadalam\n']","ASPLOS 2023, Wild and Crazy Ideas session",,http://arxiv.org/abs/2401.08908v1,cs.OS,"['cs.OS', 'cs.LG']",,,[]
"When eBPF Meets Machine Learning: On-the-fly OS Kernel
  Compartmentalization",http://arxiv.org/abs/2401.05641v1,2024-01-11T03:30:50Z,2024-01-11T03:30:50Z,"  Compartmentalization effectively prevents initial corruption from turning
into a successful attack. This paper presents O2C, a pioneering system designed
to enforce OS kernel compartmentalization on the fly. It not only provides
immediate remediation for sudden threats but also maintains consistent system
availability through the enforcement process.
  O2C is empowered by the newest advancements of the eBPF ecosystem which
allows to instrument eBPF programs that perform enforcement actions into the
kernel at runtime. O2C takes the lead in embedding a machine learning model
into eBPF programs, addressing unique challenges in on-the-fly
compartmentalization. Our comprehensive evaluation shows that O2C effectively
confines damage within the compartment. Further, we validate that decision tree
is optimally suited for O2C owing to its advantages in processing tabular data,
its explainable nature, and its compliance with the eBPF ecosystem. Last but
not least, O2C is lightweight, showing negligible overhead and excellent
sacalability system-wide.
","['\nZicheng Wang\n', '\nTiejin Chen\n', '\nQinrun Dai\n', '\nYueqi Chen\n', '\nHua Wei\n', '\nQingkai Zeng\n']",,,http://arxiv.org/abs/2401.05641v1,cs.OS,"['cs.OS', 'cs.CR', 'cs.LG']",,,[]
"Data-Driven Power Modeling and Monitoring via Hardware Performance
  Counters Tracking",http://arxiv.org/abs/2401.01826v1,2024-01-03T16:43:09Z,2024-01-03T16:43:09Z,"  In the current high-performance and embedded computing era, full-stack
energy-centric design is paramount. Use cases require increasingly high
performance at an affordable power budget, often under real-time constraints.
Extreme heterogeneity and parallelism address these issues but greatly
complicate online power consumption assessment, which is essential for dynamic
hardware and software stack adaptations. We introduce a novel
architecture-agnostic power modeling methodology with state-of-the-art
accuracy, low overhead, and high responsiveness. Our methodology identifies the
best Performance Monitoring Counters (PMCs) to model the power consumption of
each hardware sub-system at each Dynamic Voltage and Frequency Scaling (DVFS)
state. The individual linear models are combined into a complete model that
effectively describes the power consumption of the whole system, achieving high
accuracy and low overhead. Our evaluation reports an average estimation error
of 7.5 % for power consumption and 1.3 % for energy. Furthermore, we propose
Runmeter, an open-source, PMC-based monitoring framework integrated into the
Linux kernel. Runmeter manages PMC samples collection and manipulation,
efficiently evaluating our power models at runtime. With a time overhead of
only 0.7 % in the worst case, Runmeter provides responsive and accurate power
measurements directly in the kernel, which can be employed for actuation
policies such as Dynamic Power Management (DPM) and power-aware task
scheduling.
","['\nSergio Mazzola\n', '\nGabriele Ara\n', '\nThomas Benz\n', '\nBjörn Forsberg\n', '\nTommaso Cucinotta\n', '\nLuca Benini\n']","13 pages, 5 figures, submitted to the IEEE for possible publication",,http://arxiv.org/abs/2401.01826v1,cs.PF,"['cs.PF', 'cs.OS']",,,[]
Characterizing Physical Memory Fragmentation,http://arxiv.org/abs/2401.03523v1,2024-01-07T15:50:17Z,2024-01-07T15:50:17Z,"  External fragmentation of physical memory occurs when adjacent differently
sized regions of allocated physical memory are freed at different times,
causing free memory to be physically discontiguous. It can significantly
degrade system performance and efficiency, such as reducing the ability to use
huge pages, a critical optimization on modern large-memory system. For decades
system developers have sought to avoid and mitigate fragmentation, but few
prior studies quantify and characterize it in production settings.
  Moreover, prior work often artificially fragments physical memory to create
more realistic performance evaluations, but their fragmentation methodologies
are ad hoc and unvalidated. Out of 13 papers, we found 11 different
methodologies, some of which were subsequently found inadequate. The importance
of addressing fragmentation necessitates a validated and principled
methodology.
  Our work fills these gaps in knowledge and methodology. We conduct a study of
memory fragmentation in production by observing 248 machines in the Computer
Sciences Department at University of Wisconsin - Madison for a week. We
identify six key memory usage patterns, and find that Linux's file cache and
page reclamation systems are major contributors to fragmentation because they
often obliviously break up contiguous memory. Finally, we create and\'uril, a
tool to artificially fragment memory during experimental research evaluations.
While and\'uril ultimately fails as a scientific tool, we discuss its design
ideas, merits, and failings in hope that they may inspire future research.
","['\nMark Mansi\n', '\nMichael M. Swift\n']","23 pages, 9 figures",,http://arxiv.org/abs/2401.03523v1,cs.OS,"['cs.OS', 'cs.PF', 'D.4.2']",,,[]
"RAID Organizations for Improved Reliability and Performance: A Not
  Entirely Unbiased Tutorial (1st revision)",http://arxiv.org/abs/2401.03235v1,2024-01-06T15:08:45Z,2024-01-06T15:08:45Z,"  RAID proposal advocated replacing large disks with arrays of PC disks, but as
the capacity of small disks increased 100-fold in 1990s the production of large
disks was discontinued. Storage dependability is increased via replication or
erasure coding. Cloud storage providers store multiple copies of data obviating
for need for further redundancy. Varitaions of RAID based on local recovery
codes, partial MDS reduce recovery cost. NAND flash Solid State Disks - SSDs
have low latency and high bandwidth, are more reliable, consume less power and
have a lower TCO than Hard Disk Drives, which are more viable for hyperscalers.
",['\nAlexander Thomasian\n'],"Submitted to ACM Computing Surveys. arXiv admin note: substantial
  text overlap with arXiv:2306.08763",,http://arxiv.org/abs/2401.03235v1,cs.DC,"['cs.DC', 'cs.OS', 'cs.PF']",,,[]
"Design and Implementation Considerations for a Virtual File System Using
  an Inode Data Structure",http://arxiv.org/abs/2312.15153v1,2023-12-23T03:44:17Z,2023-12-23T03:44:17Z,"  Virtual file systems are a tool to centralize and mobilize a file system that
could otherwise be complex and consist of multiple hierarchies, hard disks, and
more. In this paper, we discuss the design of Unix-based file systems and how
this type of file system layout using inode data structures and a disk emulator
can be implemented as a single-file virtual file system in Linux. We explore
the ways that virtual file systems are vulnerable to security attacks and
introduce straightforward solutions that can be implemented to help prevent or
mitigate the consequences of such attacks.
","['\nQin Sun\n', '\nGrace McKenzie\n', '\nGuanqun Song\n', '\nTing Zhu\n']",,,http://arxiv.org/abs/2312.15153v1,cs.OS,"['cs.OS', 'cs.CR', 'cs.SY', 'eess.SY']",,,[]
"Attention, Distillation, and Tabularization: Towards Practical Neural
  Network-Based Prefetching",http://arxiv.org/abs/2401.06362v3,2023-12-23T05:46:05Z,2024-02-22T04:15:45Z,"  Attention-based Neural Networks (NN) have demonstrated their effectiveness in
accurate memory access prediction, an essential step in data prefetching.
However, the substantial computational overheads associated with these models
result in high inference latency, limiting their feasibility as practical
prefetchers. To close the gap, we propose a new approach based on
tabularization that significantly reduces model complexity and inference
latency without sacrificing prediction accuracy. Our novel tabularization
methodology takes as input a distilled, yet highly accurate attention-based
model for memory access prediction and efficiently converts its expensive
matrix multiplications into a hierarchy of fast table lookups. As an exemplar
of the above approach, we develop DART, a prefetcher comprised of a simple
hierarchy of tables. With a modest 0.09 drop in F1-score, DART reduces 99.99%
of arithmetic operations from the large attention-based model and 91.83% from
the distilled model. DART accelerates the large model inference by 170x and the
distilled model by 9.4x. DART has comparable latency and storage costs as
state-of-the-art rule-based prefetcher BO but surpasses it by 6.1% in IPC
improvement. DART outperforms state-of-the-art NN-based prefetchers TransFetch
by 33.1% and Voyager by 37.2% in terms of IPC improvement, primarily due to its
low prefetching latency.
","['\nPengmiao Zhang\n', '\nNeelesh Gupta\n', '\nRajgopal Kannan\n', '\nViktor K. Prasanna\n']",,,http://arxiv.org/abs/2401.06362v3,cs.NE,"['cs.NE', 'cs.AR', 'cs.LG', 'cs.OS']",,,[]
BPF-oF: Storage Function Pushdown Over the Network,http://arxiv.org/abs/2312.06808v1,2023-12-11T19:45:01Z,2023-12-11T19:45:01Z,"  Storage disaggregation, wherein storage is accessed over the network, is
popular because it allows applications to independently scale storage capacity
and bandwidth based on dynamic application demand. However, the added network
processing introduced by disaggregation can consume significant CPU resources.
In many storage systems, logical storage operations (e.g., lookups,
aggregations) involve a series of simple but dependent I/O access patterns.
Therefore, one way to reduce the network processing overhead is to execute
dependent series of I/O accesses at the remote storage server, reducing the
back-and-forth communication between the storage layer and the application. We
refer to this approach as \emph{remote-storage pushdown}. We present BPF-oF, a
new remote-storage pushdown protocol built on top of NVMe-oF, which enables
applications to safely push custom eBPF storage functions to a remote storage
server.
  The main challenge in integrating BPF-oF with storage systems is preserving
the benefits of their client-based in-memory caches. We address this challenge
by designing novel caching techniques for storage pushdown, including splitting
queries into separate in-memory and remote-storage phases and periodically
refreshing the client cache with sampled accesses from the remote storage
device. We demonstrate the utility of BPF-oF by integrating it with three
storage systems, including RocksDB, a popular persistent key-value store that
has no existing storage pushdown capability. We show BPF-oF provides
significant speedups in all three systems when accessed over the network, for
example improving RocksDB's throughput by up to 2.8$\times$ and tail latency by
up to 2.6$\times$.
","['\nIoannis Zarkadas\n', '\nTal Zussman\n', '\nJeremy Carin\n', '\nSheng Jiang\n', '\nYuhong Zhong\n', '\nJonas Pfefferle\n', '\nHubertus Franke\n', '\nJunfeng Yang\n', '\nKostis Kaffes\n', '\nRyan Stutsman\n', '\nAsaf Cidon\n']",,,http://arxiv.org/abs/2312.06808v1,cs.OS,['cs.OS'],,,[]
KEN: Kernel Extensions using Natural Language,http://arxiv.org/abs/2312.05531v1,2023-12-09T10:45:54Z,2023-12-09T10:45:54Z,"  The ability to modify and extend an operating system is an important feature
for improving a system's security, reliability, and performance. The extended
Berkeley Packet Filters (eBPF) ecosystem has emerged as the standard mechanism
for extending the Linux kernel and has recently been ported to Windows. eBPF
programs inject new logic into the kernel that the system will execute before
or after existing logic. While the eBPF ecosystem provides a flexible mechanism
for kernel extension, it is difficult for developers to write eBPF programs
today. An eBPF developer must have deep knowledge of the internals of the
operating system to determine where to place logic and cope with programming
limitations on the control flow and data accesses of their eBPF program
enforced by the eBPF verifier. This paper presents KEN, an alternative
framework that alleviates the difficulty of writing an eBPF program by allowing
Kernel Extensions to be written in Natural language. KEN uses recent advances
in large language models (LLMs) to synthesize an eBPF program given a user's
English language prompt. To ensure that LLM's output is semantically equivalent
to the user's prompt, KEN employs a combination of LLM-empowered program
comprehension, symbolic execution, and a series of feedback loops. KEN's key
novelty is the combination of these techniques. In particular, the system uses
symbolic execution in a novel structure that allows it to combine the results
of program synthesis and program comprehension and build on the recent success
that LLMs have shown for each of these tasks individually. To evaluate KEN, we
developed a new corpus of natural language prompts for eBPF programs. We show
that KEN produces correct eBPF programs on 80% which is an improvement of a
factor of 2.67 compared to an LLM-empowered program synthesis baseline.
","['\nYusheng Zheng\n', '\nYiwei Yang\n', '\nMaolin Chen\n', '\nAndrew Quinn\n']",,,http://arxiv.org/abs/2312.05531v1,cs.AI,"['cs.AI', 'cs.OS']",,,[]
On a Foundation Model for Operating Systems,http://arxiv.org/abs/2312.07813v1,2023-12-13T00:23:22Z,2023-12-13T00:23:22Z,"  This paper lays down the research agenda for a domain-specific foundation
model for operating systems (OSes). Our case for a foundation model revolves
around the observations that several OS components such as CPU, memory, and
network subsystems are interrelated and that OS traces offer the ideal dataset
for a foundation model to grasp the intricacies of diverse OS components and
their behavior in varying environments and workloads. We discuss a wide range
of possibilities that then arise, from employing foundation models as policy
agents to utilizing them as generators and predictors to assist traditional OS
control algorithms. Our hope is that this paper spurs further research into OS
foundation models and creating the next generation of operating systems for the
evolving computing landscape.
","['\nDivyanshu Saxena\n', '\nNihal Sharma\n', '\nDonghyun Kim\n', '\nRohit Dwivedula\n', '\nJiayi Chen\n', '\nChenxi Yang\n', '\nSriram Ravula\n', '\nZichao Hu\n', '\nAditya Akella\n', '\nSebastian Angel\n', '\nJoydeep Biswas\n', '\nSwarat Chaudhuri\n', '\nIsil Dillig\n', '\nAlex Dimakis\n', '\nP. Brighten Godfrey\n', '\nDaehyeok Kim\n', '\nChris Rossbach\n', '\nGang Wang\n']","Machine Learning for Systems Workshop at 37th NeurIPS Conference,
  2023, New Orleans, LA, USA",,http://arxiv.org/abs/2312.07813v1,cs.OS,"['cs.OS', 'cs.LG']",,,[]
PowerInfer: Fast Large Language Model Serving with a Consumer-grade GPU,http://arxiv.org/abs/2312.12456v1,2023-12-16T02:27:00Z,2023-12-16T02:27:00Z,"  This paper introduces PowerInfer, a high-speed Large Language Model (LLM)
inference engine on a personal computer (PC) equipped with a single
consumer-grade GPU. The key underlying the design of PowerInfer is exploiting
the high locality inherent in LLM inference, characterized by a power-law
distribution in neuron activation. This distribution indicates that a small
subset of neurons, termed hot neurons, are consistently activated across
inputs, while the majority, cold neurons, vary based on specific inputs.
PowerInfer exploits such an insight to design a GPU-CPU hybrid inference
engine: hot-activated neurons are preloaded onto the GPU for fast access, while
cold-activated neurons are computed on the CPU, thus significantly reducing GPU
memory demands and CPU-GPU data transfers. PowerInfer further integrates
adaptive predictors and neuron-aware sparse operators, optimizing the
efficiency of neuron activation and computational sparsity. Evaluation shows
that PowerInfer attains an average token generation rate of 13.20 tokens/s,
with a peak of 29.08 tokens/s, across various LLMs (including OPT-175B) on a
single NVIDIA RTX 4090 GPU, only 18% lower than that achieved by a top-tier
server-grade A100 GPU. This significantly outperforms llama.cpp by up to 11.69x
while retaining model accuracy.
","['\nYixin Song\n', '\nZeyu Mi\n', '\nHaotong Xie\n', '\nHaibo Chen\n']","15 pages, 18 figures",,http://arxiv.org/abs/2312.12456v1,cs.LG,"['cs.LG', 'cs.OS']",,,[]
"Security, extensibility, and redundancy in the Metabolic Operating
  System",http://arxiv.org/abs/2401.01357v1,2023-12-11T21:26:55Z,2023-12-11T21:26:55Z,"  People living with Type 1 Diabetes (T1D) lose the ability to produce insulin
naturally. To compensate, they inject synthetic insulin. One common way to
inject insulin is through automated insulin delivery systems, which use sensors
to monitor their metabolic state and an insulin pump device to adjust insulin
to adapt.
  In this paper, we present the Metabolic Operating System, a new automated
insulin delivery system that we designed from the ground up using security
first principles. From an architecture perspective, we apply separation
principles to simplify the core system and isolate non-critical functionality
from the core closed-loop algorithm. From an algorithmic perspective, we
evaluate trends in insulin technology and formulate a simple, but effective,
algorithm given the state-of-the-art. From a safety perspective, we build in
multiple layers of redundancy to ensure that the person using our system
remains safe.
  Fundamentally, this paper is a paper on real-world experiences building and
running an automated insulin delivery system. We report on the design
iterations we make based on experiences working with one individual using our
system. Our evaluation shows that an automated insulin delivery system built
from the ground up using security first principles can still help manage T1D
effectively.
  Our source code is open source and available on GitHub (link omitted).
",['\nSamuel T. King\n'],,,http://arxiv.org/abs/2401.01357v1,cs.CR,"['cs.CR', 'cs.OS']",,,[]
"MaxMem: Colocation and Performance for Big Data Applications on Tiered
  Main Memory Servers",http://arxiv.org/abs/2312.00647v1,2023-12-01T15:18:35Z,2023-12-01T15:18:35Z,"  We present MaxMem, a tiered main memory management system that aims to
maximize Big Data application colocation and performance. MaxMem uses an
application-agnostic and lightweight memory occupancy control mechanism based
on fast memory miss ratios to provide application QoS under increasing
colocation. By relying on memory access sampling and binning to quickly
identify per-process memory heat gradients, MaxMem maximizes performance for
many applications sharing tiered main memory simultaneously. MaxMem is designed
as a user-space memory manager to be easily modifiable and extensible, without
complex kernel code development. On a system with tiered main memory consisting
of DRAM and Intel Optane persistent memory modules, our evaluation confirms
that MaxMem provides 11% and 38% better throughput and up to 80% and an order
of magnitude lower 99th percentile latency than HeMem and Linux AutoNUMA,
respectively, with a Big Data key-value store in dynamic colocation scenarios.
","['\nAmanda Raybuck\nThe University of Texas at Austin\n', '\nWei Zhang\nMicrosoft\n', '\nKayvan Mansoorshahi\nThe University of Texas at Austin\n', '\nAditya K. Kamath\nUniversity of Washington\n', '\nMattan Erez\nThe University of Texas at Austin\n', '\nSimon Peter\nUniversity of Washington\n']","12 pages, 10 figures",,http://arxiv.org/abs/2312.00647v1,cs.OS,['cs.OS'],,,"['The University of Texas at Austin', 'Microsoft', 'The University of Texas at Austin', 'University of Washington', 'The University of Texas at Austin', 'University of Washington']"
Robust Resource Partitioning Approach for ARINC 653 RTOS,http://arxiv.org/abs/2312.01436v1,2023-12-03T16:03:08Z,2023-12-03T16:03:08Z,"  Modern airborne operating systems implement the concept of robust time and
resource partitioning imposed by the standards for aerospace and
airborne-embedded software systems, such as ARINC 653. While these standards do
provide a considerable amount of design choices in regards to resource
partitioning on the architectural and API levels, such as isolated memory
spaces between the application partitions, predefined resource configuration,
and unidirectional ports with limited queue and message sizes for
inter-partition communication, they do not specify how an operating system
should implement them in software. Furthermore, they often tend to set the
minimal level of the required guarantees, for example, in terms of memory
permissions, and disregard the hardware state of the art, which presently can
provide considerably stronger guarantees at no extra cost. In the paper we
present an architecture of robust resource partitioning for ARINC 653 real-time
operating systems based on completely static MMU configuration. The
architecture was implemented on different types of airborne hardware, including
platforms with TLB-based and page table-based MMU. Key benefits of the proposed
approach include minimised run-time overhead and simpler verification of the
memory subsystem.
","['\nVitaly Cheptsov\n', '\nAlexey Khoroshilov\n']","7 pages, 3 figures, submitted to Ivannikov ISP RAS Open Conference
  2023",,http://arxiv.org/abs/2312.01436v1,cs.OS,['cs.OS'],,,[]
Cascade: A Platform for Delay-Sensitive Edge Intelligence,http://arxiv.org/abs/2311.17329v1,2023-11-29T03:03:21Z,2023-11-29T03:03:21Z,"  Interactive intelligent computing applications are increasingly prevalent,
creating a need for AI/ML platforms optimized to reduce per-event latency while
maintaining high throughput and efficient resource management. Yet many
intelligent applications run on AI/ML platforms that optimize for high
throughput even at the cost of high tail-latency. Cascade is a new AI/ML
hosting platform intended to untangle this puzzle. Innovations include a
legacy-friendly storage layer that moves data with minimal copying and a ""fast
path"" that collocates data and computation to maximize responsiveness. Our
evaluation shows that Cascade reduces latency by orders of magnitude with no
loss of throughput.
","['\nWeijia Song\n', '\nThiago Garrett\n', '\nYuting Yang\n', '\nMingzhao Liu\n', '\nEdward Tremel\n', '\nLorenzo Rosa\n', '\nAndrea Merlina\n', '\nRoman Vitenberg\n', '\nKen Birman\n']","14 pages, 12 Figures",,http://arxiv.org/abs/2311.17329v1,cs.OS,"['cs.OS', 'cs.AI']",,,[]
Stop Hiding The Sharp Knives: The WebAssembly Linux Interface,http://arxiv.org/abs/2312.03858v1,2023-12-06T19:11:15Z,2023-12-06T19:11:15Z,"  WebAssembly is gaining popularity as a portable binary format targetable from
many programming languages. With a well-specified low-level virtual instruction
set, minimal memory footprint and many high-performance implementations, it has
been successfully adopted for lightweight in-process memory sandboxing in many
contexts. Despite these advantages, WebAssembly lacks many standard system
interfaces, making it difficult to reuse existing applications.
  This paper proposes WALI: The WebAssembly Linux Interface, a thin layer over
Linux's userspace system calls, creating a new class of virtualization where
WebAssembly seamlessly interacts with native processes and the underlying
operating system. By virtualizing the lowest level of userspace, WALI offers
application portability with little effort and reuses existing compiler
backends. With WebAssembly's control flow integrity guarantees, these modules
gain an additional level of protection against remote code injection attacks.
Furthermore, capability-based APIs can themselves be virtualized and
implemented in terms of WALI, improving reuse and robustness through better
layering. We present an implementation of WALI in a modern WebAssembly engine
and evaluate its performance on a number of applications which we can now
compile with mostly trivial effort.
","['\nArjun Ramesh\n', '\nTianshu Huang\n', '\nBen L. Titzer\n', '\nAnthony Rowe\n']","12 pages, 8 figures",,http://arxiv.org/abs/2312.03858v1,cs.OS,"['cs.OS', 'cs.SE']",,,[]
"LLM as OS, Agents as Apps: Envisioning AIOS, Agents and the AIOS-Agent
  Ecosystem",http://arxiv.org/abs/2312.03815v2,2023-12-06T18:50:26Z,2023-12-09T18:10:39Z,"  This paper envisions a revolutionary AIOS-Agent ecosystem, where Large
Language Model (LLM) serves as the (Artificial) Intelligent Operating System
(IOS, or AIOS)--an operating system ""with soul"". Upon this foundation, a
diverse range of LLM-based AI Agent Applications (Agents, or AAPs) are
developed, enriching the AIOS-Agent ecosystem and signaling a paradigm shift
from the traditional OS-APP ecosystem. We envision that LLM's impact will not
be limited to the AI application level, instead, it will in turn revolutionize
the design and implementation of computer system, architecture, software, and
programming language, featured by several main concepts: LLM as OS
(system-level), Agents as Applications (application-level), Natural Language as
Programming Interface (user-level), and Tools as Devices/Libraries
(hardware/middleware-level). We begin by introducing the architecture of
traditional OS. Then we formalize a conceptual framework for AIOS through ""LLM
as OS (LLMOS)"", drawing analogies between AIOS and traditional OS: LLM is
likened to OS kernel, context window to memory, external storage to file
system, hardware tools to peripheral devices, software tools to programming
libraries, and user prompts to user commands. Subsequently, we introduce the
new AIOS-Agent Ecosystem, where users can easily program Agent Applications
(AAPs) using natural language, democratizing the development of software, which
is different from the traditional OS-APP ecosystem. Following this, we explore
the diverse scope of Agent Applications. We delve into both single-agent and
multi-agent systems, as well as human-agent interaction. Lastly, drawing on the
insights from traditional OS-APP ecosystem, we propose a roadmap for the
evolution of the AIOS-Agent ecosystem. This roadmap is designed to guide the
future research and development, suggesting systematic progresses of AIOS and
its Agent applications.
","['\nYingqiang Ge\n', '\nYujie Ren\n', '\nWenyue Hua\n', '\nShuyuan Xu\n', '\nJuntao Tan\n', '\nYongfeng Zhang\n']","35 pages, 4 figures",,http://arxiv.org/abs/2312.03815v2,cs.OS,"['cs.OS', 'cs.AI', 'cs.CL', 'cs.LG']",,,[]
"Trace-enabled Timing Model Synthesis for ROS2-based Autonomous
  Applications",http://arxiv.org/abs/2311.13333v2,2023-11-22T11:54:42Z,2023-11-23T10:09:31Z,"  Autonomous applications are typically developed over Robot Operating System
2.0 (ROS2) even in time-critical systems like automotive. Recent years have
seen increased interest in developing model-based timing analysis and schedule
optimization approaches for ROS2-based applications. To complement these
approaches, we propose a tracing and measurement framework to obtain timing
models of ROS2-based applications. It offers a tracer based on extended
Berkeley Packet Filter (eBPF) that probes different functions in ROS2
middleware and reads their arguments or return values to reason about the data
flow in applications. It combines event traces from ROS2 and the operating
system to generate a directed acyclic graph showing ROS2 callbacks, precedence
relations between them, and their timing attributes. While being compatible
with existing analyses, we also show how to model (i)~message synchronization,
e.g., in sensor fusion, and (ii)~service requests from multiple clients, e.g.,
in motion planning. Considering that, in real-world scenarios, the application
code might be confidential and formal models are unavailable, our framework
still enables the application of existing analysis and optimization techniques.
","['\nHazem Abaza\n', '\nDebayan Roy\n', '\nShiqing Fan\n', '\nSelma Saidi\n', '\nAntonios Motakis\n']",,,http://arxiv.org/abs/2311.13333v2,cs.OS,['cs.OS'],,,[]
Memory Management Strategies for an Internet of Things System,http://arxiv.org/abs/2311.10458v1,2023-11-17T11:26:56Z,2023-11-17T11:26:56Z,"  The rise of the Internet has brought about significant changes in our lives,
and the rapid expansion of the Internet of Things (IoT) is poised to have an
even more substantial impact by connecting a wide range of devices across
various application domains. IoT devices, especially low-end ones, are
constrained by limited memory and processing capabilities, necessitating
efficient memory management within IoT operating systems. This paper delves
into the importance of memory management in IoT systems, with a primary focus
on the design and configuration of such systems, as well as the scalability and
performance of scene management. Effective memory management is critical for
optimizing resource usage, responsiveness, and adaptability as the IoT
ecosystem continues to grow. The study offers insights into memory allocation,
scene execution, memory reduction, and system scalability within the context of
an IoT system, ultimately highlighting the vital role that memory management
plays in facilitating a seamless and efficient IoT experience.
","['\nAna-Maria Comeagă\n', '\nIuliana Marin\n']","International Symposium on Fundamentals of Electrical Engineering
  2023","International Symposium on Fundamentals of Electrical Engineering
  2023",http://arxiv.org/abs/2311.10458v1,cs.SE,"['cs.SE', 'cs.OS']",,,[]
Telescope: Telemetry at Terabyte Scale,http://arxiv.org/abs/2311.10275v2,2023-11-17T01:44:14Z,2023-11-30T04:14:30Z,"  Data-hungry applications that require terabytes of memory have become
widespread in recent years. To meet the memory needs of these applications,
data centers are embracing tiered memory architectures with near and far memory
tiers. Precise, efficient, and timely identification of hot and cold data and
their placement in appropriate tiers is critical for performance in such
systems. Unfortunately, the existing state-of-the-art telemetry techniques for
hot and cold data detection are ineffective at the terabyte scale.
  We propose Telescope, a novel technique that profiles different levels of the
application's page table tree for fast and efficient identification of hot and
cold data. Telescope is based on the observation that, for a memory- and
TLB-intensive workload, higher levels of a page table tree are also frequently
accessed during a hardware page table walk. Hence, the hotness of the higher
levels of the page table tree essentially captures the hotness of its subtrees
or address space sub-regions at a coarser granularity. We exploit this insight
to quickly converge on even a few megabytes of hot data and efficiently
identify several gigabytes of cold data in terabyte-scale applications.
Importantly, such a technique can seamlessly scale to petabyte-scale
applications.
  Telescope's telemetry achieves 90%+ precision and recall at just 0.009%
single CPU utilization for microbenchmarks with a 5 TB memory footprint. Memory
tiering based on Telescope results in 5.6% to 34% throughput improvement for
real-world benchmarks with a 1-2 TB memory footprint compared to other
state-of-the-art telemetry techniques.
","['\nAlan Nair\n', '\nSandeep Kumar\n', '\nAravinda Prasad\n', '\nAndy Rudoff\n', '\nSreenivas Subramoney\n']",,,http://arxiv.org/abs/2311.10275v2,cs.OS,"['cs.OS', 'cs.AR', 'cs.DB', 'cs.DC']",,,[]
"bpftime: userspace eBPF Runtime for Uprobe, Syscall and Kernel-User
  Interactions",http://arxiv.org/abs/2311.07923v2,2023-11-14T05:49:16Z,2023-12-08T00:39:26Z,"  In kernel-centric operations, the uprobe component of eBPF frequently
encounters performance bottlenecks, largely attributed to the overheads borne
by context switches. Transitioning eBPF operations to user space bypasses these
hindrances, thereby optimizing performance. This also enhances configurability
and obviates the necessity for root access or privileges for kernel eBPF,
subsequently minimizing the kernel attack surface. This paper introduces
bpftime, a novel user-space eBPF runtime, which leverages binary rewriting to
implement uprobe and syscall hook capabilities. Through bpftime, userspace
uprobes achieve a 10x speed enhancement compared to their kernel counterparts
without requiring dual context switches. Additionally, this runtime facilitates
the programmatic hooking of syscalls within a process, both safely and
efficiently. Bpftime can be seamlessly attached to any running process,
limiting the need for either a restart or manual recompilation. Our
implementation also extends to interprocess eBPF Maps within shared memory,
catering to summary aggregation or control plane communication requirements.
Compatibility with existing eBPF toolchains such as clang and libbpf is
maintained, not only simplifying the development of user-space eBPF without
necessitating any modifications but also supporting CO-RE through BTF. Through
bpftime, we not only enhance uprobe performance but also extend the versatility
and user-friendliness of eBPF runtime in user space, paving the way for more
efficient and secure kernel operations.
","['\nYusheng Zheng\n', '\nTong Yu\n', '\nYiwei Yang\n', '\nYanpeng Hu\n', '\nXiaozheng Lai\n', '\nAndrew Quinn\n']",,,http://arxiv.org/abs/2311.07923v2,cs.OS,['cs.OS'],,,[]
Nahida: In-Band Distributed Tracing with eBPF,http://arxiv.org/abs/2311.09032v1,2023-11-15T15:25:08Z,2023-11-15T15:25:08Z,"  Microservices are commonly used in modern cloud-native applications to
achieve agility. However, the complexity of service dependencies in large-scale
microservices systems can lead to anomaly propagation, making fault
troubleshooting a challenge. To address this issue, distributed tracing systems
have been proposed to trace complete request execution paths, enabling
developers to troubleshoot anomalous services. However, existing distributed
tracing systems have limitations such as invasive instrumentation, trace loss,
or inaccurate trace correlation. To overcome these limitations, we propose a
new tracing system based on eBPF (extended Berkeley Packet Filter), named
Nahida, that can track complete requests in the kernel without intrusion,
regardless of programming language or implementation. Our evaluation results
show that Nahida can track over 92% of requests with stable accuracy, even
under the high concurrency of user requests, while the state-of-the-art
non-invasive approaches can not track any of the requests. Importantly, Nahida
can track requests served by a multi-threaded application that none of the
existing invasive tracing systems can handle by instrumenting tracing codes
into libraries. Moreover, the overhead introduced by Nahida is negligible,
increasing service latency by only 1.55%-2.1%. Overall, Nahida provides an
effective and non-invasive solution for distributed tracing.
","['\nWanqi Yang\n', '\nPengfei Chen\n', '\nKai Liu\n', '\nHuxing Zhang\n']",,,http://arxiv.org/abs/2311.09032v1,cs.OS,['cs.OS'],,,[]
Laccolith: Hypervisor-Based Adversary Emulation with Anti-Detection,http://arxiv.org/abs/2311.08274v2,2023-11-14T16:11:35Z,2023-11-17T08:38:51Z,"  Advanced Persistent Threats (APTs) represent the most threatening form of
attack nowadays since they can stay undetected for a long time. Adversary
emulation is a proactive approach for preparing against these attacks. However,
adversary emulation tools lack the anti-detection abilities of APTs. We
introduce Laccolith, a hypervisor-based solution for adversary emulation with
anti-detection to fill this gap. We also present an experimental study to
compare Laccolith with MITRE CALDERA, a state-of-the-art solution for adversary
emulation, against five popular anti-virus products. We found that CALDERA
cannot evade detection, limiting the realism of emulated attacks, even when
combined with a state-of-the-art anti-detection framework. Our experiments show
that Laccolith can hide its activities from all the tested anti-virus products,
thus making it suitable for realistic emulations.
","['\nVittorio Orbinato\n', '\nMarco Carlo Feliciano\n', '\nDomenico Cotroneo\n', '\nRoberto Natella\n']",,,http://arxiv.org/abs/2311.08274v2,cs.CR,"['cs.CR', 'cs.OS']",,,[]
OpenBSD formal driver verification with SeL4,http://arxiv.org/abs/2311.03585v1,2023-11-06T22:35:53Z,2023-11-06T22:35:53Z,"  The seL4 microkernel is currently the only kernel that has been fully
formally verified. In general, the increased interest in ensuring the security
of a kernel's code results from its important role in the entire operating
system. One of the basic features of an operating system is that it abstracts
the handling of devices. This abstraction is represented by device drivers -
the software that manages the hardware. A proper verification of the software
component could ensure that the device would work properly unless there is a
hardware failure.In this paper, we choose to model the behavior of a device
driver and build the proof that the code implementation matches the expected
behavior. The proof was written in Isabelle/HOL, the code translation from C to
Isabelle was done automatically by the use of the C-to-Isabelle Parser and
AutoCorres tools. We choose Isabelle theorem prover because its efficiency was
already shown through the verification of seL4 microkernel.
","['\nAdriana Nicolae\n', '\nPaul Irofti\n', '\nIoana Leustean\n']",,,http://arxiv.org/abs/2311.03585v1,cs.CR,"['cs.CR', 'cs.LO', 'cs.OS']",,,[]
Pinky: A Modern Malware-oriented Dynamic Information Retrieval Tool,http://arxiv.org/abs/2311.03588v1,2023-11-06T22:43:46Z,2023-11-06T22:43:46Z,"  We present here a reverse engineering tool that can be used for information
retrieval and anti-malware techniques. Our main contribution is the design and
implementation of an instrumentation framework aimed at providing insight on
the emulation process. Sample emulation is achieved via translation of the
binary code to an intermediate representation followed by compilation and
execution. The design makes this a versatile tool that can be used for multiple
task such as information retrieval, reverse engineering, debugging, and
integration with anti-malware products.
",['\nPaul Irofti\n'],,,http://arxiv.org/abs/2311.03588v1,cs.CR,"['cs.CR', 'cs.OS', 'cs.SE']",,,[]
"CARTOS: A Charging-Aware Real-Time Operating System for Intermittent
  Batteryless Devices",http://arxiv.org/abs/2311.07227v1,2023-11-13T10:48:36Z,2023-11-13T10:48:36Z,"  This paper presents CARTOS, a charging-aware real-time operating system
designed to enhance the functionality of intermittently-powered batteryless
devices (IPDs) for various Internet of Things (IoT) applications. While IPDs
offer significant advantages such as extended lifespan and operability in
extreme environments, they pose unique challenges, including the need to ensure
forward progress of program execution amidst variable energy availability and
maintaining reliable real-time time behavior during power disruptions. To
address these challenges, CARTOS introduces a mixed-preemption scheduling model
that classifies tasks into computational and peripheral tasks, and ensures
their efficient and timely execution by adopting just-in-time checkpointing for
divisible computation tasks and uninterrupted execution for indivisible
peripheral tasks. CARTOS also supports processing chains of tasks with
precedence constraints and adapts its scheduling in response to environmental
changes to offer continuous execution under diverse conditions. CARTOS is
implemented with new APIs and components added to FreeRTOS but is designed for
portability to other embedded RTOSs. Through real hardware experiments and
simulations, CARTOS exhibits superior performance over state-of-the-art
methods, demonstrating that it can serve as a practical platform for developing
resilient, real-time sensing applications on IPDs.
","['\nMohsen Karimi\n', '\nYidi Wang\n', '\nYoungbin Kim\n', '\nYoojin Lim\n', '\nHyoseung Kim\n']",,,http://arxiv.org/abs/2311.07227v1,cs.OS,"['cs.OS', 'cs.SY', 'eess.SY']",,,[]
HAL 9000: Skynet's Risk Manager,http://arxiv.org/abs/2311.09449v1,2023-11-15T23:36:14Z,2023-11-15T23:36:14Z,"  Intrusion Tolerant Systems (ITSs) are a necessary component for
cyber-services/infrastructures. Additionally, as cyberattacks follow a
multi-domain attack surface, a similar defensive approach should be applied,
namely, the use of an evolving multi-disciplinary solution that combines ITS,
cybersecurity and Artificial Intelligence (AI). With the increased popularity
of AI solutions, due to Big Data use-case scenarios and decision support and
automation scenarios, new opportunities to apply Machine Learning (ML)
algorithms have emerged, namely ITS empowerment. Using ML algorithms, an ITS
can augment its intrusion tolerance capability, by learning from previous
attacks and from known vulnerabilities. As such, this work's contribution is
twofold: (1) an ITS architecture (Skynet) based on the state-of-the-art and
incorporates new components to increase its intrusion tolerance capability and
its adaptability to new adversaries; (2) an improved Risk Manager design that
leverages AI to improve ITSs by automatically assessing OS risks to intrusions,
and advise with safer configurations. One of the reasons that intrusions are
successful is due to bad configurations or slow adaptability to new threats.
This can be caused by the dependency that systems have for human intervention.
One of the characteristics in Skynet and HAL 9000 design is the removal of
human intervention. Being fully automatized lowers the chance of successful
intrusions caused by human error. Our experiments using Skynet, shows that HAL
is able to choose 15% safer configurations than the state-of-the-art risk
manager.
","['\nTadeu Freitas\n', '\nMário Neto\n', '\nInês Dutra\n', '\nJoão Soares\n', '\nManuel Correia\n', '\nRolando Martins\n']","18 pages, 9 figures",,http://arxiv.org/abs/2311.09449v1,cs.CR,"['cs.CR', 'cs.AI', 'cs.OS']",,,[]
"A Survey of the Security Challenges and Requirements for IoT Operating
  Systems",http://arxiv.org/abs/2310.19825v1,2023-10-27T19:19:07Z,2023-10-27T19:19:07Z,"  The Internet of Things (IoT) is becoming an integral part of our modern lives
as we converge towards a world surrounded by ubiquitous connectivity. The
inherent complexity presented by the vast IoT ecosystem ends up in an
insufficient understanding of individual system components and their
interactions, leading to numerous security challenges. In order to create a
secure IoT platform from the ground up, there is a need for a unifying
operating system (OS) that can act as a cornerstone regulating the development
of stable and secure solutions. In this paper, we present a classification of
the security challenges stemming from the manifold aspects of IoT development.
We also specify security requirements to direct the secure development of an
unifying IoT OS to resolve many of those ensuing challenges. Survey of several
modern IoT OSs confirm that while the developers of the OSs have taken many
alternative approaches to implement security, we are far from engineering an
adequately secure and unified architecture. More broadly, the study presented
in this paper can help address the growing need for a secure and unified
platform to base IoT development on and assure the safe, secure, and reliable
operation of IoT in critical domains.
",['\nAlvi Jawad\n'],"13 pages, 2 figures",,http://arxiv.org/abs/2310.19825v1,cs.OS,"['cs.OS', 'cs.CR']",,,[]
MOSEL: Inference Serving Using Dynamic Modality Selection,http://arxiv.org/abs/2310.18481v1,2023-10-27T20:50:56Z,2023-10-27T20:50:56Z,"  Rapid advancements over the years have helped machine learning models reach
previously hard-to-achieve goals, sometimes even exceeding human capabilities.
However, to attain the desired accuracy, the model sizes and in turn their
computational requirements have increased drastically. Thus, serving
predictions from these models to meet any target latency and cost requirements
of applications remains a key challenge, despite recent work in building
inference-serving systems as well as algorithmic approaches that dynamically
adapt models based on inputs. In this paper, we introduce a form of dynamism,
modality selection, where we adaptively choose modalities from inference inputs
while maintaining the model quality. We introduce MOSEL, an automated inference
serving system for multi-modal ML models that carefully picks input modalities
per request based on user-defined performance and accuracy requirements. MOSEL
exploits modality configurations extensively, improving system throughput by
3.6$\times$ with an accuracy guarantee and shortening job completion times by
11$\times$.
","['\nBodun Hu\n', '\nLe Xu\n', '\nJeongyoon Moon\n', '\nNeeraja J. Yadwadkar\n', '\nAditya Akella\n']",,,http://arxiv.org/abs/2310.18481v1,cs.LG,"['cs.LG', 'cs.AI', 'cs.OS']",,,[]
"Optimizing Logical Execution Time Model for Both Determinism and Low
  Latency",http://arxiv.org/abs/2310.19699v2,2023-10-30T16:21:49Z,2024-01-21T21:25:43Z,"  The Logical Execution Time (LET) programming model has recently received
considerable attention, particularly because of its timing and dataflow
determinism. In LET, task computation appears always to take the same amount of
time (called the task's LET interval), and the task reads (resp. writes) at the
beginning (resp. end) of the interval. Compared to other communication
mechanisms, such as implicit communication and Dynamic Buffer Protocol (DBP),
LET performs worse on many metrics, such as end-to-end latency (including
reaction time and data age) and time disparity jitter. Compared with the
default LET setting, the flexible LET (fLET) model shrinks the LET interval
while still guaranteeing schedulability by introducing the virtual offset to
defer the read operation and using the virtual deadline to move up the write
operation. Therefore, fLET has the potential to significantly improve the
end-to-end timing performance while keeping the benefits of deterministic
behavior on timing and dataflow.
  To fully realize the potential of fLET, we consider the problem of optimizing
the assignments of its virtual offsets and deadlines. We propose new
abstractions to describe the task communication pattern and new optimization
algorithms to explore the solution space efficiently. The algorithms leverage
the linearizability of communication patterns and utilize symbolic operations
to achieve efficient optimization while providing a theoretical guarantee. The
framework supports optimizing multiple performance metrics and guarantees
bounded suboptimality when optimizing end-to-end latency. Experimental results
show that our optimization algorithms improve upon the default LET and its
existing extensions and significantly outperform implicit communication and DBP
in terms of various metrics, such as end-to-end latency, time disparity, and
its jitter.
","['\nSen Wang\n', '\nDong Li\n', '\nAshrarul H. Sifat\n', '\nShao-Yu Huang\n', '\nXuanliang Deng\n', '\nChanghee Jung\n', '\nRyan Williams\n', '\nHaibo Zeng\n']",Under Review,,http://arxiv.org/abs/2310.19699v2,eess.SY,"['eess.SY', 'cs.OS', 'cs.SC', 'cs.SY']",,,[]
GMEM: Generalized Memory Management for Peripheral Devices,http://arxiv.org/abs/2310.12554v1,2023-10-19T08:03:09Z,2023-10-19T08:03:09Z,"  This paper presents GMEM, generalized memory management, for peripheral
devices. GMEM provides OS support for centralized memory management of both CPU
and devices. GMEM provides a high-level interface that decouples MMU-specific
functions. Device drivers can thus attach themselves to a process's address
space and let the OS take charge of their memory management. This eliminates
the need for device drivers to ""reinvent the wheel"" and allows them to benefit
from general memory optimizations integrated by GMEM. Furthermore, GMEM
internally coordinates all attached devices within each virtual address space.
This drastically improves user-level programmability, since programmers can use
a single address space within their program, even when operating across the CPU
and multiple devices. A case study on device drivers demonstrates these
benefits. A GMEM-based IOMMU driver eliminates around seven hundred lines of
code and obtains 54% higher network receive throughput utilizing 32% less CPU
compared to the state-of-the-art. In addition, the GMEM-based driver of a
simulated GPU takes less than 70 lines of code, excluding its MMU functions.
","['\nWeixi Zhu\n', '\nAlan L. Cox\n', '\nScott Rixner\n']",Finished before Weixi left Rice and submitted to ASPLOS'23,,http://arxiv.org/abs/2310.12554v1,cs.OS,['cs.OS'],,,[]
"Adaptive CPU Resource Allocation for Emulator in Kernel-based Virtual
  Machine",http://arxiv.org/abs/2310.14741v1,2023-10-23T09:20:51Z,2023-10-23T09:20:51Z,"  The technologies of heterogeneous multi-core architectures, co-location, and
virtualization can be used to reduce server power consumption and improve
system utilization, which are three important technologies for data centers.
This article explores the scheduling strategy of Emulator threads within
virtual machine processes in a scenario of co-location of multiple virtual
machines on heterogeneous multi-core architectures. In this co-location
scenario, the scheduling strategy for Emulator threads significantly affects
the performance of virtual machines. This article focuses on this thread for
the first time in the relevant field. This article found that the scheduling
latency metric can well indicate the running status of the vCPU threads and
Emulator threads in the virtualization environment, and applied this metric to
the design of the scheduling strategy. This article designed an Emulator thread
scheduler based on heuristic rules, which, in coordination with the host
operating system's scheduler, dynamically adjusts the scheduling scope of
Emulator threads to improve the overall performance of virtual machines. The
article found that in real application scenarios, the scheduler effectively
improved the performance of applications within virtual machines, with a
maximum performance improvement of 40.7%.
","['\nYecheng Yang\n', '\nPu Pang\n', '\nJiawen Wang\n', '\nQuan Chen\n', '\nMinyi Guo\n']",,,http://arxiv.org/abs/2310.14741v1,cs.OS,['cs.OS'],,,[]
Configuration Validation with Large Language Models,http://arxiv.org/abs/2310.09690v1,2023-10-15T00:50:27Z,2023-10-15T00:50:27Z,"  Misconfigurations are the major causes of software failures. Existing
configuration validation techniques rely on manually written rules or test
cases, which are expensive to implement and maintain, and are hard to be
comprehensive. Leveraging machine learning (ML) and natural language processing
(NLP) for configuration validation is considered a promising direction, but has
been facing challenges such as the need of not only large-scale configuration
data, but also system-specific features and models which are hard to
generalize. Recent advances in Large Language Models (LLMs) show the promises
to address some of the long-lasting limitations of ML/NLP-based configuration
validation techniques. In this paper, we present an exploratory analysis on the
feasibility and effectiveness of using LLMs like GPT and Codex for
configuration validation. Specifically, we take a first step to empirically
evaluate LLMs as configuration validators without additional fine-tuning or
code generation. We develop a generic LLM-based validation framework, named
Ciri, which integrates different LLMs. Ciri devises effective prompt
engineering with few-shot learning based on both valid configuration and
misconfiguration data. Ciri also validates and aggregates the outputs of LLMs
to generate validation results, coping with known hallucination and
nondeterminism of LLMs. We evaluate the validation effectiveness of Ciri on
five popular LLMs using configuration data of six mature, widely deployed
open-source systems. Our analysis (1) confirms the potential of using LLMs for
configuration validation, (2) understands the design space of LLMbased
validators like Ciri, especially in terms of prompt engineering with few-shot
learning, and (3) reveals open challenges such as ineffectiveness in detecting
certain types of misconfigurations and biases to popular configuration
parameters.
","['\nXinyu Lian\n', '\nYinfang Chen\n', '\nRunxiang Cheng\n', '\nJie Huang\n', '\nParth Thakkar\n', '\nTianyin Xu\n']",,,http://arxiv.org/abs/2310.09690v1,cs.SE,"['cs.SE', 'cs.AI', 'cs.OS']",,,[]
Persistent Memory File Systems: A Survey,http://arxiv.org/abs/2310.02880v1,2023-10-04T15:17:30Z,2023-10-04T15:17:30Z,"  Persistent Memory (PM) is non-volatile byte-addressable memory that offers
read and write latencies in the order of magnitude smaller than flash storage,
such as SSDs. This survey discusses how file systems address the most prominent
challenges in the implementation of file systems for Persistent Memory. First,
we discuss how the properties of Persistent Memory change file system design.
Second, we discuss work that aims to optimize small file I/O and the associated
meta-data resolution. Third, we address how existing Persistent Memory file
systems achieve (meta) data persistence and consistency.
","['\nWiebe van Breukelen\n', '\nAnimesh Trivedi\n']",,,http://arxiv.org/abs/2310.02880v1,cs.OS,['cs.OS'],,,[]
"Motivating Next-Generation OS Physical Memory Management for
  Terabyte-Scale NVMMs",http://arxiv.org/abs/2310.03370v1,2023-10-05T08:04:15Z,2023-10-05T08:04:15Z,"  Software managed byte-addressable hybrid memory systems consisting of DRAMs
and NVMMs offer a lot of flexibility to design efficient large scale data
processing applications. Operating systems (OS) play an important role in
enabling the applications to realize the integrated benefits of DRAMs' low
access latency and NVMMs' large capacity along with its persistent
characteristics. In this paper, we comprehensively analyze the performance of
conventional OS physical memory management subsystems that were designed only
based on the DRAM memory characteristics in the context of modern hybrid
byte-addressable memory systems.
  To study the impact of high access latency and large capacity of NVMMs on
physical memory management, we perform an extensive evaluation on Linux with
Intel's Optane NVMM. We observe that the core memory management functionalities
such as page allocation are negatively impacted by high NVMM media latency,
while functionalities such as conventional fragmentation management are
rendered inadequate. We also demonstrate that certain traditional memory
management functionalities are affected by neither aspects of modern NVMMs. We
conclusively motivate the need to overhaul fundamental aspects of traditional
OS physical memory management in order to fully exploit terabyte-scale NVMMs.
","['\nShivank Garg\n', '\nAravinda Prasad\n', '\nDebadatta Mishra\n', '\nSreenivas Subramoney\n']","14 pages, 24 figures, 2 tables",,http://arxiv.org/abs/2310.03370v1,cs.OS,"['cs.OS', 'D.4.8']",,,[]
Towards a debuggable kernel design,http://arxiv.org/abs/2310.05399v1,2023-10-09T04:38:51Z,2023-10-09T04:38:51Z,"  This paper describes what it means for a kernel to be debuggable and proposes
a kernel design with debuggability in mind. We evaluate the proposed kernel
design by comparing the iterations required in cyclic debugging for different
classes of bugs in a vanilla monolithic kernel to a variant enhanced with our
design rules for debuggability. We discuss the trade offs involved in designing
a debuggable kernel.
","['\nChandrika Parimoo\n', '\nAshish Gupta\n']","7 pages, 3 figures, 1 table",,http://arxiv.org/abs/2310.05399v1,cs.OS,['cs.OS'],,,[]
"Victima: Drastically Increasing Address Translation Reach by Leveraging
  Underutilized Cache Resources",http://arxiv.org/abs/2310.04158v3,2023-10-06T11:15:20Z,2024-01-05T12:37:22Z,"  Address translation is a performance bottleneck in data-intensive workloads
due to large datasets and irregular access patterns that lead to frequent
high-latency page table walks (PTWs). PTWs can be reduced by using (i) large
hardware TLBs or (ii) large software-managed TLBs. Unfortunately, both
solutions have significant drawbacks: increased access latency, power and area
(for hardware TLBs), and costly memory accesses, the need for large contiguous
memory blocks, and complex OS modifications (for software-managed TLBs). We
present Victima, a new software-transparent mechanism that drastically
increases the translation reach of the processor by leveraging the
underutilized resources of the cache hierarchy. The key idea of Victima is to
repurpose L2 cache blocks to store clusters of TLB entries, thereby providing
an additional low-latency and high-capacity component that backs up the
last-level TLB and thus reduces PTWs. Victima has two main components. First, a
PTW cost predictor (PTW-CP) identifies costly-to-translate addresses based on
the frequency and cost of the PTWs they lead to. Second, a TLB-aware cache
replacement policy prioritizes keeping TLB entries in the cache hierarchy by
considering (i) the translation pressure (e.g., last-level TLB miss rate) and
(ii) the reuse characteristics of the TLB entries. Our evaluation results show
that in native (virtualized) execution environments Victima improves average
end-to-end application performance by 7.4% (28.7%) over the baseline four-level
radix-tree-based page table design and by 6.2% (20.1%) over a state-of-the-art
software-managed TLB, across 11 diverse data-intensive workloads. Victima (i)
is effective in both native and virtualized environments, (ii) is completely
transparent to application and system software, and (iii) incurs very small
area and power overheads on a modern high-end CPU.
","['\nKonstantinos Kanellopoulos\n', '\nHong Chul Nam\n', '\nF. Nisa Bostanci\n', '\nRahul Bera\n', '\nMohammad Sadrosadati\n', '\nRakesh Kumar\n', '\nDavide-Basilio Bartolini\n', '\nOnur Mutlu\n']","To appear in 56th IEEE/ACM International Symposium on
  Microarchitecture (MICRO), 2023",,http://dx.doi.org/10.1145/3613424.3614276,cs.AR,"['cs.AR', 'cs.OS', 'C.0']",10.1145/3613424.3614276,,[]
"Taking the Shortcut: Actively Incorporating the Virtual Memory Index of
  the OS to Hardware-Accelerate Database Indexing",http://arxiv.org/abs/2310.09124v1,2023-10-13T14:13:46Z,2023-10-13T14:13:46Z,"  Index structures often materialize one or multiple levels of explicit
indirections (aka pointers) to allow for a quick traversal to the data of
interest. Unfortunately, dereferencing a pointer to go from one level to the
other is costly since additionally to following the address, it involves two
address translations from virtual memory to physical memory under the hood. In
the worst case, such an address translation is resolved by an index access
itself, namely by a lookup into the page table, a central hardware-accelerated
index structure of the OS. However, if the page table is anyways constantly
queried, it raises the question whether we can actively incorporate it into our
database indexes and make it work for us. Precisely, instead of materializing
indirections in form of pointers, we propose to express these indirections
directly in the page table wherever possible. By introducing such shortcuts, we
(a) effectively reduce the height of traversal during lookups and (b) exploit
the hardware-acceleration of lookups in the page table. In this work, we
analyze the strengths and considerations of this approach and showcase its
effectiveness at the case of the real-world indexing scheme extendible hashing.
",['\nFelix Schuhknecht\n'],,,http://arxiv.org/abs/2310.09124v1,cs.DB,"['cs.DB', 'cs.OS']",,,[]
"Co-Optimizing Cache Partitioning and Multi-Core Task Scheduling: Exploit
  Cache Sensitivity or Not?",http://arxiv.org/abs/2310.02959v1,2023-10-04T16:50:23Z,2023-10-04T16:50:23Z,"  Cache partitioning techniques have been successfully adopted to mitigate
interference among concurrently executing real-time tasks on multi-core
processors. Considering that the execution time of a cache-sensitive task
strongly depends on the cache available for it to use, co-optimizing cache
partitioning and task allocation improves the system's schedulability. In this
paper, we propose a hybrid multi-layer design space exploration technique to
solve this multi-resource management problem. We explore the interplay between
cache partitioning and schedulability by systematically interleaving three
optimization layers, viz., (i) in the outer layer, we perform a breadth-first
search combined with proactive pruning for cache partitioning; (ii) in the
middle layer, we exploit a first-fit heuristic for allocating tasks to cores;
and (iii) in the inner layer, we use the well-known recurrence relation for the
schedulability analysis of non-preemptive fixed-priority (NP-FP) tasks in a
uniprocessor setting. Although our focus is on NP-FP scheduling, we evaluate
the flexibility of our framework in supporting different scheduling policies
(NP-EDF, P-EDF) by plugging in appropriate analysis methods in the inner layer.
Experiments show that, compared to the state-of-the-art techniques, the
proposed framework can improve the real-time schedulability of NP-FP task sets
by an average of 15.2% with a maximum improvement of 233.6% (when tasks are
highly cache-sensitive) and a minimum of 1.6% (when cache sensitivity is low).
For such task sets, we found that clustering similar-period (or mutually
compatible) tasks often leads to higher schedulability (on average 7.6%) than
clustering by cache sensitivity. In our evaluation, the framework also achieves
good results for preemptive and dynamic-priority scheduling policies.
","['\nBinqi Sun\n', '\nDebayan Roy\n', '\nTomasz Kloda\n', '\nAndrea Bastoni\n', '\nRodolfo Pellizzoni\n', '\nMarco Caccamo\n']","to be published in IEEE Real-Time Systems Symposium (RTSS), 2023",,http://arxiv.org/abs/2310.02959v1,cs.AR,"['cs.AR', 'cs.DC', 'cs.OS']",,,[]
"Prompt-to-OS (P2OS): Revolutionizing Operating Systems and
  Human-Computer Interaction with Integrated AI Generative Models",http://arxiv.org/abs/2310.04875v1,2023-10-07T17:16:34Z,2023-10-07T17:16:34Z,"  In this paper, we present a groundbreaking paradigm for human-computer
interaction that revolutionizes the traditional notion of an operating system.
  Within this innovative framework, user requests issued to the machine are
handled by an interconnected ecosystem of generative AI models that seamlessly
integrate with or even replace traditional software applications. At the core
of this paradigm shift are large generative models, such as language and
diffusion models, which serve as the central interface between users and
computers. This pioneering approach leverages the abilities of advanced
language models, empowering users to engage in natural language conversations
with their computing devices. Users can articulate their intentions, tasks, and
inquiries directly to the system, eliminating the need for explicit commands or
complex navigation. The language model comprehends and interprets the user's
prompts, generating and displaying contextual and meaningful responses that
facilitate seamless and intuitive interactions.
  This paradigm shift not only streamlines user interactions but also opens up
new possibilities for personalized experiences. Generative models can adapt to
individual preferences, learning from user input and continuously improving
their understanding and response generation. Furthermore, it enables enhanced
accessibility, as users can interact with the system using speech or text,
accommodating diverse communication preferences.
  However, this visionary concept raises significant challenges, including
privacy, security, trustability, and the ethical use of generative models.
Robust safeguards must be in place to protect user data and prevent potential
misuse or manipulation of the language model.
  While the full realization of this paradigm is still far from being achieved,
this paper serves as a starting point for envisioning this transformative
potential.
","['\nGabriele Tolomei\n', '\nCesare Campagnano\n', '\nFabrizio Silvestri\n', '\nGiovanni Trappolini\n']","5 pages, 1 figure. Accepted at IEEE CogMI 2023 (IEEE International
  Conference on Cognitive Machine Intelligence)",,http://arxiv.org/abs/2310.04875v1,cs.LG,"['cs.LG', 'cs.CL', 'cs.CY', 'cs.HC', 'cs.OS']",,,[]
Loupe: Driving the Development of OS Compatibility Layers,http://arxiv.org/abs/2309.15996v1,2023-09-27T20:21:37Z,2023-09-27T20:21:37Z,"  Supporting mainstream applications is fundamental for a new OS to have
impact. It is generally achieved by developing a layer of compatibility
allowing applications developed for a mainstream OS like Linux to run
unmodified on the new OS. Building such a layer, as we show, results in large
engineering inefficiencies due to the lack of efficient methods to precisely
measure the OS features required by a set of applications.
  We propose Loupe, a novel method based on dynamic analysis that determines
the OS features that need to be implemented in a prototype OS to bring support
for a target set of applications and workloads. Loupe guides and boosts OS
developers as they build compatibility layers, prioritizing which features to
implement in order to quickly support many applications as early as possible.
We apply our methodology to 100+ applications and several OSes currently under
development, demonstrating high engineering effort savings vs. existing
approaches: for example, for the 62 applications supported by the OSv kernel,
we show that using Loupe, would have required implementing only 37 system calls
vs. 92 for the non-systematic process followed by OSv developers.
  We study our measurements and extract novel key insights. Overall, we show
that the burden of building compatibility layers is significantly less than
what previous works suggest: in some cases, only as few as 20% of system calls
reported by static analysis, and 50% of those reported by naive dynamic
analysis need an implementation for an application to successfully run standard
benchmarks.
","['\nHugo Lefeuvre\n', '\nGaulthier Gain\n', '\nVlad-Andrei Bădoiu\n', '\nDaniel Dinca\n', '\nVlad-Radu Schiller\n', '\nCostin Raiciu\n', '\nFelipe Huici\n', '\nPierre Olivier\n']","Accepted to appear at ASPLOS'24
  (https://www.asplos-conference.org/asplos2024/)",,http://arxiv.org/abs/2309.15996v1,cs.OS,['cs.OS'],,,[]
First Principles of Big Memory Systems,http://arxiv.org/abs/2310.00428v2,2023-09-30T16:40:16Z,2023-12-09T13:43:39Z,"  In this paper, we comprehensively analyze the vertical and horizontal
extensions of existing memory hierarchy. The difference between memory and big
memory is well reported. We present the state-of-the-art studies upon the big
memory systems, together with design methodology and implementations.
Persistence is the first principle of big memory systems. We further show the
full-stack and moving persistence.
",['\nYu Hua\n'],,,http://arxiv.org/abs/2310.00428v2,cs.OS,['cs.OS'],,,[]
"Virtuoso: High Resource Utilization and μs-scale Performance
  Isolation in a Shared Virtual Machine TCP Network Stack",http://arxiv.org/abs/2309.14016v2,2023-09-25T10:29:06Z,2024-02-05T14:45:30Z,"  Virtualization improves resource efficiency and ensures security and
performance isolation for cloud applications. Today, operators use a layered
architecture with separate network stack instances in each VM and container
connected to a virtual switch. Decoupling through layering reduces complexity,
but induces performance and resource overheads at odds with increasing demands
for network bandwidth, connection scalability, and low latency.
  We present Virtuoso, a new software network stack for VMs and containers.
Virtuoso re-organizes the network stack to maximize CPU utilization, enforce
isolation, and minimize processing overheads. We maximize utilization by
running one elastically shared network stack instance on dedicated cores; we
enforce isolation by performing central and fine-grained per-packet resource
accounting and scheduling; we reduce overheads by building a single-layer data
path with a one-shot fast-path incorporating all processing from the TCP
transport layer through network virtualization and virtual switching. Virtuoso
improves resource efficiency by up to 82%, latencies by up to 58% compared to
other virtualized network stacks without sacrificing isolation, and keeps
processing overhead within 6.7% of unvirtualized stacks.
","['\nMatheus Stolet\n', '\nLiam Arzola\n', '\nSimon Peter\n', '\nAntoine Kaufmann\n']",Under submission for conference peer review,,http://arxiv.org/abs/2309.14016v2,cs.NI,"['cs.NI', 'cs.OS']",,,[]
Expedited Data Transfers for Serverless Clouds,http://arxiv.org/abs/2309.14821v1,2023-09-26T10:39:59Z,2023-09-26T10:39:59Z,"  Serverless computing has emerged as a popular cloud deployment paradigm. In
serverless, the developers implement their application as a set of chained
functions that form a workflow in which functions invoke each other. The cloud
providers are responsible for automatically scaling the number of instances for
each function on demand and forwarding the requests in a workflow to the
appropriate function instance. Problematically, today's serverless clouds lack
efficient support for cross-function data transfers in a workflow, preventing
the efficient execution of data-intensive serverless applications. In
production clouds, functions transmit intermediate, i.e., ephemeral, data to
other functions either as part of invocation HTTP requests (i.e., inline) or
via third-party services, such as AWS S3 storage or AWS ElastiCache in-memory
cache. The former approach is restricted to small transfer sizes, while the
latter supports arbitrary transfers but suffers from performance and cost
overheads. This work introduces Expedited Data Transfers (XDT), an
API-preserving high-performance data communication method for serverless that
enables direct function-to-function transfers. With XDT, a trusted component of
the sender function buffers the payload in its memory and sends a secure
reference to the receiver, which is picked by the load balancer and autoscaler
based on the current load. Using the reference, the receiver instance pulls the
transmitted data directly from the sender's memory. XDT is natively compatible
with existing autoscaling infrastructure, preserves function invocation
semantics, is secure, and avoids the cost and performance overheads of using an
intermediate service for data transfers. We prototype our system in
vHive/Knative deployed on a cluster of AWS EC2 nodes, showing that XDT improves
latency, bandwidth, and cost over AWS S3 and ElasticCache.
","['\nDmitrii Ustiugov\n', '\nShyam Jesalpura\n', '\nMert Bora Alper\n', '\nMichal Baczun\n', '\nRustem Feyzkhanov\n', '\nEdouard Bugnion\n', '\nBoris Grot\n', '\nMarios Kogias\n']",latest version,,http://arxiv.org/abs/2309.14821v1,cs.DC,"['cs.DC', 'cs.OS', '68', 'D.4.4']",,,[]
Case Study: Securing MMU-less Linux Using CHERI,http://arxiv.org/abs/2310.00933v2,2023-10-02T06:56:29Z,2024-01-18T15:16:32Z,"  MMU-less Linux variant lacks security because it does not have protection or
isolation mechanisms. It also does not use MPUs as they do not fit with its
software model because of the design drawbacks of MPUs (\ie coarse-grained
protection with fixed number of protected regions). We secure the existing
MMU-less Linux version of the RISC-V port using CHERI. CHERI is a
hardware-software capability-based system that extends the ISA, toolchain,
programming languages, operating systems, and applications in order to provide
complete pointer and memory safety. We believe that CHERI could provide
significant security guarantees for high-end dynamic MMU-less embedded systems
at lower costs, compared to MMUs and MPUs, by: 1) building the entire software
stack in pure-capability CHERI C mode which provides complete spatial memory
safety at the kernel and user-level, 2) isolating user programs as separate
ELFs, each with its own CHERI-based capability table; this provides spatial
memory safety similar to what the MMU offers (\ie user programs cannot access
each other's memory), 3) isolating user programs from the kernel as the kernel
has its own capability table from the users and vice versa, and 4)
compartmentalising kernel modules using CompartOS' linkage-based
compartmentalisation. This offers a new security front that is not possible
using the current MMU-based Linux, where vulnerable/malicious kernel modules
(\eg device drivers) executing in the kernel space would not compromise or take
down the entire system. These are the four main contributions of this paper,
presenting novel CHERI-based mechanisms to secure MMU-less embedded Linux.
","['\nHesham Almatary\n', '\nAlfredo Mazzinghi\n', '\nRobert N. M. Watson\n']",,,http://arxiv.org/abs/2310.00933v2,cs.OS,"['cs.OS', 'cs.CR']",,,[]
"Carbon Containers: A System-level Facility for Managing
  Application-level Carbon Emissions",http://arxiv.org/abs/2309.14477v1,2023-09-25T19:22:25Z,2023-09-25T19:22:25Z,"  To reduce their environmental impact, cloud datacenters' are increasingly
focused on optimizing applications' carbon-efficiency, or work done per mass of
carbon emitted. To facilitate such optimizations, we present Carbon Containers,
a simple system-level facility, which extends prior work on power containers,
that automatically regulates applications' carbon emissions in response to
variations in both their workload's intensity and their energy's
carbon-intensity. Specifically, \carbonContainerS enable applications to
specify a maximum carbon emissions rate (in g$\cdot$CO$_2$e/hr), and then
transparently enforce this rate via a combination of vertical scaling,
container migration, and suspend/resume while maximizing either
energy-efficiency or performance.
  Carbon Containers are especially useful for applications that i) must
continue running even during high-carbon periods, and ii) execute in regions
with few variations in carbon-intensity. These low-variability regions also
tend to have high average carbon-intensity, which increases the importance of
regulating carbon emissions. We implement a Carbon Containers prototype by
extending Linux Containers to incorporate the mechanisms above and evaluate it
using real workload traces and carbon-intensity data from multiple regions. We
compare Carbon Containers with prior work that regulates carbon emissions by
suspending/resuming applications during high/low carbon periods. We show that
Carbon Containers are more carbon-efficient and improve performance while
maintaining similar carbon emissions.
","['\nJohn Thiede\n', '\nNoman Bashir\n', '\nDavid Irwin\n', '\nPrashant Shenoy\n']",ACM Symposium on Cloud Computing (SoCC),,http://dx.doi.org/10.1145/3620678.3624644,cs.DC,"['cs.DC', 'cs.ET', 'cs.OS', 'cs.PF', 'cs.SY', 'eess.SY']",10.1145/3620678.3624644,,[]
"RackBlox: A Software-Defined Rack-Scale Storage System with
  Network-Storage Co-Design",http://arxiv.org/abs/2309.06513v1,2023-09-12T18:42:08Z,2023-09-12T18:42:08Z,"  Software-defined networking (SDN) and software-defined flash (SDF) have been
serving as the backbone of modern data centers. They are managed separately to
handle I/O requests. At first glance, this is a reasonable design by following
the rack-scale hierarchical design principles. However, it suffers from
suboptimal end-to-end performance, due to the lack of coordination between SDN
and SDF.
  In this paper, we co-design the SDN and SDF stack by redefining the functions
of their control plane and data plane, and splitting up them within a new
architecture named RackBlox. RackBlox decouples the storage management
functions of flash-based solid-state drives (SSDs), and allow the SDN to track
and manage the states of SSDs in a rack. Therefore, we can enable the state
sharing between SDN and SDF, and facilitate global storage resource management.
RackBlox has three major components: (1) coordinated I/O scheduling, in which
it dynamically adjusts the I/O scheduling in the storage stack with the
measured and predicted network latency, such that it can coordinate the effort
of I/O scheduling across the network and storage stack for achieving
predictable end-to-end performance; (2) coordinated garbage collection (GC), in
which it will coordinate the GC activities across the SSDs in a rack to
minimize their impact on incoming I/O requests; (3) rack-scale wear leveling,
in which it enables global wear leveling among SSDs in a rack by periodically
swapping data, for achieving improved device lifetime for the entire rack. We
implement RackBlox using programmable SSDs and switch. Our experiments
demonstrate that RackBlox can reduce the tail latency of I/O requests by up to
5.8x over state-of-the-art rack-scale storage systems.
","['\nBenjamin Reidys\n', '\nYuqi Xue\n', '\nDaixuan Li\n', '\nBharat Sukhwani\n', '\nWen-mei Hwu\n', '\nDeming Chen\n', '\nSameh Asaad\n', '\nJian Huang\n']","14 pages. Published in published in ACM SIGOPS 29th Symposium on
  Operating Systems Principles (SOSP'23)",,http://dx.doi.org/10.1145/3600006.3613170,cs.OS,"['cs.OS', 'cs.NI']",10.1145/3600006.3613170,,[]
Sync+Sync: A Covert Channel Built on fsync with Storage,http://arxiv.org/abs/2309.07657v1,2023-09-14T12:22:29Z,2023-09-14T12:22:29Z,"  Scientists have built a variety of covert channels for secretive information
transmission with CPU cache and main memory. In this paper, we turn to a lower
level in the memory hierarchy, i.e., persistent storage. Most programs store
intermediate or eventual results in the form of files and some of them call
fsync to synchronously persist a file with storage device for orderly
persistence. Our quantitative study shows that one program would undergo
significantly longer response time for fsync call if the other program is
concurrently calling fsync, although they do not share any data. We further
find that, concurrent fsync calls contend at multiple levels of storage stack
due to sharing software structures (e.g., Ext4's journal) and hardware
resources (e.g., disk's I/O dispatch queue).
  We accordingly build a covert channel named Sync+Sync. Sync+Sync delivers a
transmission bandwidth of 20,000 bits per second at an error rate of about
0.40% with an ordinary solid-state drive. Sync+Sync can be conducted in
cross-disk partition, cross-file system, cross-container, cross-virtual
machine, and even cross-disk drive fashions, without sharing data between
programs. Next, we launch side-channel attacks with Sync+Sync and manage to
precisely detect operations of a victim database (e.g., insert/update and
B-Tree node split). We also leverage Sync+Sync to distinguish applications and
websites with high accuracy by detecting and analyzing their fsync frequencies
and flushed data volumes. These attacks are useful to support further
fine-grained information leakage.
","['\nQisheng Jiang\n', '\nChundong Wang\n']","A full version for the paper with the same title accepted by the 33rd
  USENIX Security Symposium (USENIX Security 2024)",,http://arxiv.org/abs/2309.07657v1,cs.CR,"['cs.CR', 'cs.OS']",,,[]
OSmosis: No more Déjà vu in OS isolation,http://arxiv.org/abs/2309.09291v1,2023-09-17T14:58:33Z,2023-09-17T14:58:33Z,"  Operating systems provide an abstraction layer between the hardware and
higher-level software. Many abstractions, such as threads, processes,
containers, and virtual machines, are mechanisms to provide isolation. New
application scenarios frequently introduce new isolation mechanisms.
Implementing each isolation mechanism as an independent abstraction makes it
difficult to reason about the state and resources shared among different tasks,
leading to security vulnerabilities and performance interference. We present
OSmosis, an isolation model that expresses the precise level of resource
sharing, a framework in which to implement isolation mechanisms based on the
model, and an implementation of the framework on seL4. The OSmosis model lets
the user determine the degree of isolation guarantee that they need from the
system. This determination empowers developers to make informed decisions about
isolation and performance trade-offs, and the framework enables them to create
mechanisms with the desired degree of isolation.
","['\nSidhartha Agrawal\nUniversity of British Columbia\n', '\nReto Achermann\nUniversity of British Columbia\n', '\nMargo Seltzer\nUniversity of British Columbia\n']","6 pages, 1 figure",,http://arxiv.org/abs/2309.09291v1,cs.CR,"['cs.CR', 'cs.OS', 'D.4.6; D.4.7']",,,"['University of British Columbia', 'University of British Columbia', 'University of British Columbia']"
A Discussion on Generalization in Next-Activity Prediction,http://arxiv.org/abs/2309.09618v1,2023-09-18T09:42:36Z,2023-09-18T09:42:36Z,"  Next activity prediction aims to forecast the future behavior of running
process instances. Recent publications in this field predominantly employ deep
learning techniques and evaluate their prediction performance using publicly
available event logs. This paper presents empirical evidence that calls into
question the effectiveness of these current evaluation approaches. We show that
there is an enormous amount of example leakage in all of the commonly used
event logs, so that rather trivial prediction approaches perform almost as well
as ones that leverage deep learning. We further argue that designing robust
evaluations requires a more profound conceptual engagement with the topic of
next-activity prediction, and specifically with the notion of generalization to
new data. To this end, we present various prediction scenarios that necessitate
different types of generalization to guide future research.
","['\nLuka Abb\n', '\nPeter Pfeiffer\n', '\nPeter Fettke\n', '\nJana-Rebecca Rehse\n']","Pre-print, published at the AI4BPM workshop at BPM 2023",,http://arxiv.org/abs/2309.09618v1,cs.LG,"['cs.LG', 'cs.OS']",,,[]
Software Compartmentalization Trade-Offs with Hardware Capabilities,http://arxiv.org/abs/2309.11332v2,2023-09-20T14:07:20Z,2023-09-21T08:14:29Z,"  Compartmentalization is a form of defensive software design in which an
application is broken down into isolated but communicating components.
Retrofitting compartmentalization into existing applications is often thought
to be expensive from the engineering effort and performance overhead points of
view. Still, recent years have seen proposals of compartmentalization methods
with promises of low engineering efforts and reduced performance impact. ARM
Morello combines a modern ARM processor with an implementation of Capability
Hardware Enhanced RISC Instructions (CHERI) aiming to provide efficient and
secure compartmentalization. Past works exploring CHERI-based
compartmentalization were restricted to emulated/FPGA prototypes.
  In this paper, we explore possible compartmentalization schemes with CHERI on
the Morello chip. We propose two approaches representing different trade-offs
in terms of engineering effort, security, scalability, and performance impact.
We describe and implement these approaches on a prototype OS running bare metal
on the Morello chip, compartmentalize two popular applications, and investigate
the performance overheads. Furthermore, we show that compartmentalization can
be achieved with an engineering cost that can be quite low if one is willing to
trade off on scalability and security, and that performance overheads are
similar to other intra-address space isolation mechanisms.
","['\nJohn Alistair Kressel\n', '\nHugo Lefeuvre\n', '\nPierre Olivier\n']","12th Workshop on Programming Languages and Operating Systems (PLOS
  2023)",,http://arxiv.org/abs/2309.11332v2,cs.CR,"['cs.CR', 'cs.OS']",,,[]
CoRD: Converged RDMA Dataplane for High-Performance Clouds,http://arxiv.org/abs/2309.00898v1,2023-09-02T10:25:34Z,2023-09-02T10:25:34Z,"  High-performance networking is often characterized by kernel bypass which is
considered mandatory in high-performance parallel and distributed applications.
But kernel bypass comes at a price because it breaks the traditional OS
architecture, requiring applications to use special APIs and limiting the OS
control over existing network connections. We make the case, that kernel bypass
is not mandatory. Rather, high-performance networking relies on multiple
performance-improving techniques, with kernel bypass being the least effective.
CoRD removes kernel bypass from RDMA networks, enabling efficient OS-level
control over RDMA dataplane.
","['\nMaksym Planeta\n', '\nJan Bierbaum\n', '\nMichael Roitzsch\n', '\nHermann Härtig\n']",11 pages,,http://arxiv.org/abs/2309.00898v1,cs.OS,['cs.OS'],,,[]
Language Models for Novelty Detection in System Call Traces,http://arxiv.org/abs/2309.02206v1,2023-09-05T13:11:40Z,2023-09-05T13:11:40Z,"  Due to the complexity of modern computer systems, novel and unexpected
behaviors frequently occur. Such deviations are either normal occurrences, such
as software updates and new user activities, or abnormalities, such as
misconfigurations, latency issues, intrusions, and software bugs. Regardless,
novel behaviors are of great interest to developers, and there is a genuine
need for efficient and effective methods to detect them. Nowadays, researchers
consider system calls to be the most fine-grained and accurate source of
information to investigate the behavior of computer systems. Accordingly, this
paper introduces a novelty detection methodology that relies on a probability
distribution over sequences of system calls, which can be seen as a language
model. Language models estimate the likelihood of sequences, and since
novelties deviate from previously observed behaviors by definition, they would
be unlikely under the model. Following the success of neural networks for
language models, three architectures are evaluated in this work: the widespread
LSTM, the state-of-the-art Transformer, and the lower-complexity Longformer.
However, large neural networks typically require an enormous amount of data to
be trained effectively, and to the best of our knowledge, no massive modern
datasets of kernel traces are publicly available. This paper addresses this
limitation by introducing a new open-source dataset of kernel traces comprising
over 2 million web requests with seven distinct behaviors. The proposed
methodology requires minimal expert hand-crafting and achieves an F-score and
AuROC greater than 95% on most novelties while being data- and task-agnostic.
The source code and trained models are publicly available on GitHub while the
datasets are available on Zenodo.
","['\nQuentin Fournier\n', '\nDaniel Aloise\n', '\nLeandro R. Costa\n']","12 pages, 7 figures, 3 tables",,http://arxiv.org/abs/2309.02206v1,cs.LG,"['cs.LG', 'cs.OS', 'cs.SE']",,,[]
OSMOSIS: Enabling Multi-Tenancy in Datacenter SmartNICs,http://arxiv.org/abs/2309.03628v2,2023-09-07T10:50:32Z,2023-09-08T09:36:13Z,"  Multi-tenancy is essential for unleashing SmartNIC's potential in
datacenters. Our systematic analysis in this work shows that existing on-path
SmartNICs have resource multiplexing limitations. For example, existing
solutions lack multi-tenancy capabilities such as performance isolation and QoS
provisioning for compute and IO resources. Compared to standard NIC data paths
with a well-defined set of offloaded functions, unpredictable execution times
of SmartNIC kernels make conventional approaches for multi-tenancy and QoS
insufficient. We fill this gap with OSMOSIS, a SmartNICs resource manager
co-design. OSMOSIS extends existing OS mechanisms to enable dynamic hardware
resource multiplexing on top of the on-path packet processing data plane. We
implement OSMOSIS within an open-source RISC-V-based 400Gbit/s SmartNIC. Our
performance results demonstrate that OSMOSIS fully supports multi-tenancy and
enables broader adoption of SmartNICs in datacenters with low overhead.
","['\nMikhail Khalilov\n', '\nMarcin Chrapek\n', '\nSiyuan Shen\n', '\nAlessandro Vezzu\n', '\nThomas Benz\n', '\nSalvatore Di Girolamo\n', '\nTimo Schneider\n', '\nDaniele De Sensi\n', '\nLuca Benini\n', '\nTorsten Hoefler\n']","12 pages, 14 figures, 103 references",,http://arxiv.org/abs/2309.03628v2,cs.NI,"['cs.NI', 'cs.DC', 'cs.OS', 'cs.SY', 'eess.SY']",,,[]
"CPU frequency scheduling of real-time applications on embedded devices
  with temporal encoding-based deep reinforcement learning",http://arxiv.org/abs/2309.03779v1,2023-09-07T15:28:03Z,2023-09-07T15:28:03Z,"  Small devices are frequently used in IoT and smart-city applications to
perform periodic dedicated tasks with soft deadlines. This work focuses on
developing methods to derive efficient power-management methods for periodic
tasks on small devices. We first study the limitations of the existing Linux
built-in methods used in small devices. We illustrate three typical
workload/system patterns that are challenging to manage with Linux's built-in
solutions. We develop a reinforcement-learning-based technique with temporal
encoding to derive an effective DVFS governor even with the presence of the
three system patterns. The derived governor uses only one performance counter,
the same as the built-in Linux mechanism, and does not require an explicit task
model for the workload. We implemented a prototype system on the Nvidia Jetson
Nano Board and experimented with it with six applications, including two
self-designed and four benchmark applications. Under different deadline
constraints, our approach can quickly derive a DVFS governor that can adapt to
performance requirements and outperform the built-in Linux approach in energy
saving. On Mibench workloads, with performance slack ranging from 0.04 s to 0.4
s, the proposed method can save 3% - 11% more energy compared to Ondemand.
AudioReg and FaceReg applications tested have 5%- 14% energy-saving
improvement. We have open-sourced the implementation of our in-kernel quantized
neural network engine. The codebase can be found at:
https://github.com/coladog/tinyagent.
","['\nTi Zhou\n', '\nMan Lin\n']",Accepted to Journal of Systems Architecture,"Journal of Systems Architecture, 2023",http://dx.doi.org/10.1016/j.sysarc.2023.102955,cs.LG,"['cs.LG', 'cs.AI', 'cs.AR', 'cs.OS', 'cs.SY', 'eess.SY']",10.1016/j.sysarc.2023.102955,,[]
"Revitalising the Single Batch Environment: A 'Quest' to Achieve Fairness
  and Efficiency",http://arxiv.org/abs/2308.10062v2,2023-08-19T15:55:37Z,2024-01-05T12:17:03Z,"  In the realm of computer systems, efficient utilisation of the CPU (Central
Processing Unit) has always been a paramount concern. Researchers and engineers
have long sought ways to optimise process execution on the CPU, leading to the
emergence of CPU scheduling as a field of study. This research proposes a novel
algorithm for batch processing that operates on a preemptive model, dynamically
assigning priorities based on a robust ratio, employing a dynamic time slice,
and utilising periodic sorting technique to achieve fairness. By engineering
this responsive and fair model, the proposed algorithm strikes a delicate
balance between efficiency and fairness, providing an optimised solution for
batch scheduling while ensuring system responsiveness.
","['\nSupriya Manna\n', '\nKrishna Siva Prasad Mudigonda\n']",,,http://arxiv.org/abs/2308.10062v2,cs.OS,['cs.OS'],,,[]
"HotOS XIX Panel Report: Panel on Future of Reproduction and Replication
  of Systems Research",http://arxiv.org/abs/2308.05762v1,2023-08-08T20:47:09Z,2023-08-08T20:47:09Z,"  At HotOS XIX (2023), we organized a panel to discuss the future of
reproducibility and replication in systems research. In this document, we
highlight the key points and themes that were discussed in the panel and
summarize the various opinions shared by both the panelists as well as the
HotOS attendees.
","['\nRoberta De Viti\n', '\nSolal Pirelli\n', '\nVaastav Anand\n']",,,http://arxiv.org/abs/2308.05762v1,cs.OS,['cs.OS'],,,[]
Unleashing Unprivileged eBPF Potential with Dynamic Sandboxing,http://arxiv.org/abs/2308.01983v2,2023-08-03T18:33:56Z,2023-08-15T04:31:28Z,"  For safety reasons, unprivileged users today have only limited ways to
customize the kernel through the extended Berkeley Packet Filter (eBPF). This
is unfortunate, especially since the eBPF framework itself has seen an increase
in scope over the years. We propose SandBPF, a software-based kernel isolation
technique that dynamically sandboxes eBPF programs to allow unprivileged users
to safely extend the kernel, unleashing eBPF's full potential. Our early
proof-of-concept shows that SandBPF can effectively prevent exploits missed by
eBPF's native safety mechanism (i.e., static verification) while incurring
0%-10% overhead on web server benchmarks.
","['\nSoo Yee Lim\n', '\nXueyuan Han\n', '\nThomas Pasquier\n']","8 pages, 5 figures, to appear in the 1st SIGCOMM Workshop on eBPF and
  Kernel Extensions",,http://arxiv.org/abs/2308.01983v2,cs.OS,"['cs.OS', 'cs.CR']",,,[]
Towards Learned Predictability of Storage Systems,http://arxiv.org/abs/2307.16288v1,2023-07-30T17:53:08Z,2023-07-30T17:53:08Z,"  With the rapid development of cloud computing and big data technologies,
storage systems have become a fundamental building block of datacenters,
incorporating hardware innovations such as flash solid state drives and
non-volatile memories, as well as software infrastructures such as RAID and
distributed file systems. Despite the growing popularity and interests in
storage, designing and implementing reliable storage systems remains
challenging, due to their performance instability and prevailing hardware
failures.
  Proactive prediction greatly strengthens the reliability of storage systems.
There are two dimensions of prediction: performance and failure. Ideally,
through detecting in advance the slow IO requests, and predicting device
failures before they really happen, we can build storage systems with
especially low tail latency and high availability. While its importance is well
recognized, such proactive prediction in storage systems, on the other hand, is
particularly difficult. To move towards predictability of storage systems,
various mechanisms and field studies have been proposed in the past few years.
In this report, we present a survey of these mechanisms and field studies,
focusing on machine learning based black-box approaches. Based on three
representative research works, we discuss where and how machine learning should
be applied in this field. The strengths and limitations of each research work
are also evaluated in detail.
",['\nChenyuan Wu\n'],,,http://arxiv.org/abs/2307.16288v1,cs.DB,"['cs.DB', 'cs.AI', 'cs.OS']",,,[]
"Towards Fast, Adaptive, and Hardware-Assisted User-Space Scheduling",http://arxiv.org/abs/2308.02896v2,2023-08-05T15:09:12Z,2023-11-11T16:22:45Z,"  Modern datacenter applications are prone to high tail latencies since their
requests typically follow highly-dispersive distributions. Delivering fast
interrupts is essential to reducing tail latency. Prior work has proposed both
OS- and system-level solutions to reduce tail latencies for microsecond-scale
workloads through better scheduling. Unfortunately, existing approaches like
customized dataplane OSes, require significant OS changes, experience
scalability limitations, or do not reach the full performance capabilities
hardware offers.
  The emergence of new hardware features like UINTR exposed new opportunities
to rethink the design paradigms and abstractions of traditional scheduling
systems. We propose LibPreemptible, a preemptive user-level threading library
that is flexible, lightweight, and adaptive. LibPreemptible was built with a
set of optimizations like LibUtimer for scalability, and deadline-oriented API
for flexible policies, time-quantum controller for adaptiveness. Compared to
the prior state-of-the-art scheduling system Shinjuku, our system achieves
significant tail latency and throughput improvements for various workloads
without modifying the kernel. We also demonstrate the flexibility of
LibPreemptible across scheduling policies for real applications experiencing
varying load levels and characteristics.
","['\n Lisa\nYueying\n', '\n Li\n', '\nNikita Lazarev\n', '\nDavid Koufaty\n', '\nYijun Yin\n', '\nAndy Anderson\n', '\nZhiru Zhang\n', '\nEdward Suh\n', '\nKostis Kaffes\n', '\nChristina Delimitrou\n']",Accepted by HPCA2024,,http://arxiv.org/abs/2308.02896v2,cs.DC,"['cs.DC', 'cs.AR', 'cs.NI', 'cs.OS']",,,['Yueying']
FHPM: Fine-grained Huge Page Management For Virtualization,http://arxiv.org/abs/2307.10618v1,2023-07-20T06:36:16Z,2023-07-20T06:36:16Z,"  As more data-intensive tasks with large footprints are deployed in virtual
machines (VMs), huge pages are widely used to eliminate the increasing address
translation overhead. However, once the huge page mapping is established, all
the base page regions in the huge page share a single extended page table (EPT)
entry, so that the hypervisor loses awareness of accesses to base page regions.
None of the state-of-the-art solutions can obtain access information at base
page granularity for huge pages. We observe that this can lead to incorrect
decisions by the hypervisor, such as incorrect data placement in a tiered
memory system and unshared base page regions when sharing pages.
  This paper proposes FHPM, a fine-grained huge page management for
virtualization without hardware and guest OS modification. FHPM can identify
access information at base page granularity, and dynamically promote and demote
pages. A key insight of FHPM is to redirect the EPT huge page directory entries
(PDEs) to new companion pages so that the MMU can track access information
within huge pages. Then, FHPM can promote and demote pages according to the
current hot page pressure to balance address translation overhead and memory
usage. At the same time, FHPM proposes a VM-friendly page splitting and
collapsing mechanism to avoid extra VM-exits. In combination, FHPM minimizes
the monitoring and management overhead and ensures that the hypervisor gets
fine-grained VM memory accesses to make the proper decision. We apply FHPM to
improve tiered memory management (FHPM-TMM) and to promote page sharing
(FHPM-Share). FHPM-TMM achieves a performance improvement of up to 33% and 61%
over the pure huge page and base page management. FHPM-Share can save 41% more
memory than Ingens, a state-of-the-art page sharing solution, with comparable
performance.
","['\nChuandong Li\nPeking University\n', '\nSai Sha\nPeking University\n', '\nYangqing Zeng\nPeking University\n', '\nXiran Yang\nPeking University\n', '\nYingwei Luo\nPeking University\n', '\nXiaolin Wang\nPeking University\n', '\nZhenlin Wang\nMichigan Technological University\n']",,,http://arxiv.org/abs/2307.10618v1,cs.OS,['cs.OS'],,,"['Peking University', 'Peking University', 'Peking University', 'Peking University', 'Peking University', 'Peking University', 'Michigan Technological University']"
Understanding (Un)Written Contracts of NVMe ZNS Devices with zns-tools,http://arxiv.org/abs/2307.11860v1,2023-07-21T19:00:25Z,2023-07-21T19:00:25Z,"  Operational and performance characteristics of flash SSDs have long been
associated with a set of Unwritten Contracts due to their hidden, complex
internals and lack of control from the host software stack. These unwritten
contracts govern how data should be stored, accessed, and garbage collected.
The emergence of Zoned Namespace (ZNS) flash devices with their open and
standardized interface allows us to write these unwritten contracts for the
storage stack. However, even with a standardized storage-host interface, due to
the lack of appropriate end-to-end operational data collection tools, the
quantification and reasoning of such contracts remain a challenge. In this
paper, we propose zns.tools, an open-source framework for end-to-end event and
metadata collection, analysis, and visualization for the ZNS SSDs contract
analysis. We showcase how zns.tools can be used to understand how the
combination of RocksDB with the F2FS file system interacts with the underlying
storage. Our tools are available openly at
\url{https://github.com/stonet-research/zns-tools}.
","['\nNick Tehrany\n', '\nKrijn Doekemeijer\n', '\nAnimesh Trivedi\n']",,,http://arxiv.org/abs/2307.11860v1,cs.OS,['cs.OS'],,,[]
"A Survey on the Integration of NAND Flash Storage in the Design of File
  Systems and the Host Storage Software Stack",http://arxiv.org/abs/2307.11866v1,2023-07-21T19:14:40Z,2023-07-21T19:14:40Z,"  With the ever-increasing amount of data generate in the world, estimated to
reach over 200 Zettabytes by 2025, pressure on efficient data storage systems
is intensifying. The shift from HDD to flash-based SSD provides one of the most
fundamental shifts in storage technology, increasing performance capabilities
significantly. However, flash storage comes with different characteristics than
prior HDD storage technology. Therefore, storage software was unsuitable for
leveraging the capabilities of flash storage. As a result, a plethora of
storage applications have been design to better integrate with flash storage
and align with flash characteristics.
  In this literature study we evaluate the effect the introduction of flash
storage has had on the design of file systems, which providing one of the most
essential mechanisms for managing persistent storage. We analyze the mechanisms
for effectively managing flash storage, managing overheads of introduced design
requirements, and leverage the capabilities of flash storage. Numerous methods
have been adopted in file systems, however prominently revolve around similar
design decisions, adhering to the flash hardware constrains, and limiting
software intervention. Future design of storage software remains prominent with
the constant growth in flash-based storage devices and interfaces, providing an
increasing possibility to enhance flash integration in the host storage
software stack.
","['\nNick Tehrany\n', '\nKrijn Doekemeijer\n', '\nAnimesh Trivedi\n']",,,http://arxiv.org/abs/2307.11866v1,cs.OS,['cs.OS'],,,[]
Verifiable Sustainability in Data Centers,http://arxiv.org/abs/2307.11993v3,2023-07-22T06:39:20Z,2024-01-12T05:30:32Z,"  Data centers have significant energy needs, both embodied and operational,
affecting sustainability adversely. The current techniques and tools for
collecting, aggregating, and reporting verifiable sustainability data are
vulnerable to cyberattacks and misuse, requiring new security and
privacy-preserving solutions. This paper outlines security challenges and
research directions for addressing these pressing requirements.
","['\nSyed Rafiul Hussain\n', '\nPatrick McDaniel\n', '\nAnshul Gandhi\n', '\nKanad Ghose\n', '\nKartik Gopalan\n', '\nDongyoon Lee\n', '\nYu David Liu\n', '\nZhenhua Liu\n', '\nShuai Mu\n', '\nErez Zadok\n']",,,http://arxiv.org/abs/2307.11993v3,cs.CR,"['cs.CR', 'cs.CY', 'cs.DC', 'cs.OS', 'cs.SY', 'eess.SY']",,,[]
Understanding Persistent-Memory Related Issues in the Linux Kernel,http://arxiv.org/abs/2307.04095v1,2023-07-09T04:35:20Z,2023-07-09T04:35:20Z,"  Persistent memory (PM) technologies have inspired a wide range of PM-based
system optimizations. However, building correct PM-based systems is difficult
due to the unique characteristics of PM hardware. To better understand the
challenges as well as the opportunities to address them, this paper presents a
comprehensive study of PM-related issues in the Linux kernel. By analyzing
1,553 PM-related kernel patches in-depth and conducting experiments on
reproducibility and tool extension, we derive multiple insights in terms of PM
patch categories, PM bug patterns, consequences, fix strategies, triggering
conditions, and remedy solutions. We hope our results could contribute to the
development of robust PM-based storage systems
","['\nOm Rameshwar Gatla\n', '\nDuo Zhang\n', '\nWei Xu\n', '\nMai Zheng\n']",ACM TRANSACTIONS ON STORAGE(TOS'23),,http://arxiv.org/abs/2307.04095v1,cs.OS,['cs.OS'],,,[]
Joint Time-and Event-Triggered Scheduling in the Linux Kernel,http://arxiv.org/abs/2306.16271v2,2023-06-28T14:52:51Z,2023-07-27T13:10:40Z,"  There is increasing interest in using Linux in the real-time domain due to
the emergence of cloud and edge computing, the need to decrease costs, and the
growing number of complex functional and non-functional requirements of
real-time applications. Linux presents a valuable opportunity as it has rich
hardware support, an open-source development model, a well-established
programming environment, and avoids vendor lock-in. Although Linux was
initially developed as a general-purpose operating system, some real-time
capabilities have been added to the kernel over many years to increase its
predictability and reduce its scheduling latency. Unfortunately, Linux
currently has no support for time-triggered (TT) scheduling, which is widely
used in the safety-critical domain for its determinism, low run-time scheduling
latency, and strong isolation properties. We present an enhancement of the
Linux scheduler as a new low-overhead TT scheduling class to support offline
table-driven scheduling of tasks on multicore Linux nodes. Inspired by the Slot
shifting algorithm, we complement the new scheduling class with a low overhead
slot shifting manager running on a non-time-triggered core to provide
guaranteed execution time to real-time aperiodic tasks by using the slack of
the time-triggered tasks and avoiding high-overhead table regeneration for
adding new periodic tasks. Furthermore, we evaluate our implementation on
server-grade hardware with Intel Xeon Scalable Processor.
","['\nGautam Gala\n', '\nIsser Kadusale\n', '\nGerhard Fohler\n']","Accepted at the 17th annual workshop on Operating Systems Platforms
  for Embedded Real-Time applications (OSPERT) workshop 2023 co-hosted with
  35th Euromicro conference on Real-time systems. OSPERT proceedings:
  https://www.ecrts.org/wp-content/uploads/2023/07/ospert23-proceedings.pdf",,http://arxiv.org/abs/2306.16271v2,cs.OS,"['cs.OS', '68N25', 'D.4.1']",,,[]
Energy-aware Time- and Event-triggered KVM Nodes,http://arxiv.org/abs/2307.00950v1,2023-07-03T11:49:08Z,2023-07-03T11:49:08Z,"  Industries are considering the adoption of cloud and edge computing for
real-time applications due to current improvements in network latencies and the
advent of Fog and Edge computing. Current cloud paradigms are not designed for
real-time applications, as they neither provide low latencies/jitter nor the
guarantees and determinism required by real-time applications. Experts estimate
that data centers use 1% of global electricity for powering the equipment, and
in turn, for dealing with the produced heat. Hence, energy consumption is a
crucial metric in cloud technologies. Applying energy conservation techniques
is not straightforward due to the increased scheduling overheads and
application execution times. Inspired by slot shifting, we propose an algorithm
to support energy-aware time-triggered execution of periodic real-time VMs
while still providing the ability to execute aperiodic real-time and
best-effort VMs in the slack of the time-triggered ones. The algorithm
considers energy reduction techniques based on dynamic power management and
dynamic voltage and frequency scaling. We implement our algorithm as an
extension to the Linux kernel scheduler (for use with the KVM hypervisor) and
evaluate it on a server-grade Intel Xeon node.
","['\nIsser Kadusale\n', '\nGautam Gala\n', '\nGerhard Fohler\n']","to appear in Real-time Cloud (RT-Cloud) 2023 workshop co-hosted with
  35th Euromicro conference on Real-time systems (ECRTS)",,http://arxiv.org/abs/2307.00950v1,cs.OS,"['cs.OS', '68N25', 'D.4.1']",,,[]
A Survey on User-Space Storage and Its Implementations,http://arxiv.org/abs/2306.10503v1,2023-06-18T08:55:36Z,2023-06-18T08:55:36Z,"  The storage stack in the traditional operating system is primarily optimized
towards improving the CPU utilization and hiding the long I/O latency imposed
by the slow I/O devices such as hard disk drivers (HDDs). However, the emerging
storage media experience significant technique shifts in the past decade, which
exhibit high bandwidth and low latency. These high-performance storage devices,
unfortunately, suffer from the huge overheads imposed by the system software
including the long storage stack and the frequent context switch between the
user and kernel modes. Many researchers have investigated huge efforts in
addressing this challenge by constructing a direct software path between a user
process and the underlying storage devices. We revisit such novel designs in
the prior work and present a survey in this paper. Specifically, we classify
the former research into three categories according to their commonalities. We
then present the designs of each category based on the timeline and analyze
their uniqueness and contributions. This paper also reviews the applications
that exploit the characteristics of theses designs. Given that the user-space
storage is a growing research field, we believe this paper can be an
inspiration for future researchers, who are interested in the user-space
storage system designs.
","['\nJunzhe Li\n', '\nXiurui Pan\n', '\nShushu Yi\n', '\nJie Zhang\n']",,,http://arxiv.org/abs/2306.10503v1,cs.OS,['cs.OS'],,,[]
"DFlow: Efficient Dataflow-based Invocation Workflow Execution for
  Function-as-a-Service",http://arxiv.org/abs/2306.11043v2,2023-06-19T16:09:07Z,2023-07-05T02:31:46Z,"  The Serverless Computing is becoming increasingly popular due to its ease of
use and fine-grained billing. These features make it appealing for stateful
application or serverless workflow. However, current serverless workflow
systems utilize a controlflow-based invocation pattern to invoke functions. In
this execution pattern, the function invocation depends on the state of the
function. A function can only begin executing once all its precursor functions
have completed. As a result, this pattern may potentially lead to longer
end-to-end execution time. We design and implement the DFlow, a novel
dataflow-based serverless workflow system that achieves high performance for
serverless workflow. DFlow introduces a distributed scheduler (DScheduler) by
using the dataflow-based invocation pattern to invoke functions. In this
pattern, the function invocation depends on the data dependency between
functions. The function can start to execute even its precursor functions are
still running. DFlow further features a distributed store (DStore) that
utilizes effective fine-grained optimization techniques to eliminate function
interaction, thereby enabling efficient data exchange. With the support of
DScheduler and DStore, DFlow can achieving an average improvement of 60% over
CFlow, 40% over FaaSFlow, 25% over FaasFlowRedis, and 40% over KNIX on 99%-ile
latency respectively. Further, it can improve network bandwidth utilization by
2x-4x over CFlow and 1.5x-3x over FaaSFlow, FaaSFlowRedis and KNIX,
respectively. DFlow effectively reduces the cold startup latency, achieving an
average improvement of 5.6x over CFlow and 1.1x over FaaSFlow
","['\nXiaoxiang Shi\n', '\nChao Li\n', '\nZijun Li\n', '\nZihan Liu\n', '\nDianmo Sheng\n', '\nQuan Chen\n', '\nJingwen Leng\n', '\nMinyi Guo\n']","22 pages, 13 figures",,http://arxiv.org/abs/2306.11043v2,cs.DC,"['cs.DC', 'cs.OS']",,,[]
An Introduction to the Compute Express Link (CXL) Interconnect,http://arxiv.org/abs/2306.11227v1,2023-06-20T01:30:08Z,2023-06-20T01:30:08Z,"  The Compute Express Link (CXL) is an open industry-standard interconnect
between processors and devices such as accelerators, memory buffers, smart
network interfaces, persistent memory, and solid-state drives. CXL offers
coherency and memory semantics with bandwidth that scales with PCIe bandwidth
while achieving significantly lower latency than PCIe. All major CPU vendors,
device vendors, and datacenter operators have adopted CXL as a common standard.
This enables an inter-operable ecosystem that supports key computing use cases
including highly efficient accelerators, server memory bandwidth and capacity
expansion, multi-server resource pooling and sharing, and efficient
peer-to-peer communication. This survey provides an introduction to CXL
covering the standards CXL 1.0, CXL 2.0, and CXL 3.0. We further survey CXL
implementations, discuss CXL's impact on the datacenter landscape, and future
directions.
","['\nDebendra Das Sharma\n', '\nRobert Blankenship\n', '\nDaniel S. Berger\n']",,,http://arxiv.org/abs/2306.11227v1,cs.AR,"['cs.AR', 'cs.OS']",,,[]
"Enabling Lightweight Privilege Separation in Applications with
  MicroGuards",http://arxiv.org/abs/2306.14202v1,2023-06-25T10:50:15Z,2023-06-25T10:50:15Z,"  Application compartmentalization and privilege separation are our primary
weapons against ever-increasing security threats and privacy concerns on
connected devices. Despite significant progress, it is still challenging to
privilege separate inside an application address space and in multithreaded
environments, particularly on resource-constrained and mobile devices. We
propose MicroGuards, a lightweight kernel modification and set of security
primitives and APIs aimed at flexible and fine-grained in-process memory
protection and privilege separation in multithreaded applications. MicroGuards
take advantage of hardware support in modern CPUs and are high-level enough to
be adaptable to various architectures. This paper focuses on enabling
MicroGuards on embedded and mobile devices running Linux kernel and utilizes
tagged memory support to achieve good performance. Our evaluation show that
MicroGuards add small runtime overhead (less than 3.5\%), minimal memory
footprint, and are practical to get integrated with existing applications to
enable fine-grained privilege separation.
","['\nZahra Tarkhani\n', '\nAnil Madhavapeddy\n']",arXiv admin note: substantial text overlap with arXiv:2004.04846,,http://arxiv.org/abs/2306.14202v1,cs.CR,"['cs.CR', 'cs.OS']",,,[]
Semi-online Scheduling with Lookahead,http://arxiv.org/abs/2306.06003v1,2023-06-09T16:18:19Z,2023-06-09T16:18:19Z,"  The knowledge of future partial information in the form of a lookahead to
design efficient online algorithms is a theoretically-efficient and realistic
approach to solving computational problems. Design and analysis of semi-online
algorithms with extra-piece-of-information (EPI) as a new input parameter has
gained the attention of the theoretical computer science community in the last
couple of decades. Though competitive analysis is a pessimistic worst-case
performance measure to analyze online algorithms, it has immense theoretical
value in developing the foundation and advancing the state-of-the-art
contributions in online and semi-online scheduling. In this paper, we study and
explore the impact of lookahead as an EPI in the context of online scheduling
in identical machine frameworks. We introduce a $k$-lookahead model and design
improved competitive semi-online algorithms. For a $2$-identical machine
setting, we prove a lower bound of $\frac{4}{3}$ and design an optimal
algorithm with a matching upper bound of $\frac{4}{3}$ on the competitive
ratio. For a $3$-identical machine setting, we show a lower bound of
$\frac{15}{11}$ and design a $\frac{16}{11}$-competitive improved semi-online
algorithm.
","['\nDebasis Dwibedy\n', '\nRakesh Mohanty\n']","14 pages, 1 figure",,http://arxiv.org/abs/2306.06003v1,cs.DS,"['cs.DS', 'cs.OS']",,,[]
"SWAM: Revisiting Swap and OOMK for Improving Application Responsiveness
  on Mobile Devices",http://arxiv.org/abs/2306.08345v1,2023-06-14T08:28:41Z,2023-06-14T08:28:41Z,"  Existing memory reclamation policies on mobile devices may be no longer valid
because they have negative effects on the response time of running
applications. In this paper, we propose SWAM, a new integrated memory
management technique that complements the shortcomings of both the swapping and
killing mechanism in mobile devices and improves the application
responsiveness. SWAM consists of (1) Adaptive Swap that performs swapping
adaptively into memory or storage device while managing the swap space
dynamically, (2) OOM Cleaner that reclaims shared object pages in the swap
space to secure available memory and storage space, and (3) EOOM Killer that
terminates processes in the worst case while prioritizing the lowest
initialization cost applications as victim processes first. Experimental
results demonstrate that SWAM significantly reduces the number of applications
killed by OOMK (6.5x lower), and improves application launch time (36% faster)
and response time (41% faster), compared to the conventional schemes.
","['\nGeunsik Lim\n', '\nDonghyun Kang\n', '\nMyungJoo Ham\n', '\nYoung Ik Eom\n']","15 pages, 13 figures, MobiCom 2023 (29th Annual International
  Conference On Mobile Computing And Networking)",,http://dx.doi.org/10.1145/3570361.3592518,cs.OS,"['cs.OS', 'cs.PF']",10.1145/3570361.3592518,,[]
CAWL: A Cache-aware Write Performance Model of Linux Systems,http://arxiv.org/abs/2306.05701v1,2023-06-09T06:41:57Z,2023-06-09T06:41:57Z,"  The performance of data intensive applications is often dominated by their
input/output (I/O) operations but the I/O stack of systems is complex and
severely depends on system specific settings and hardware components. This
situation makes generic performance optimisation challenging and costly for
developers as they would have to run their application on a large variety of
systems to evaluate their improvements. Here, simulation frameworks can help
reducing the experimental overhead but they typically handle the topic of I/O
rather coarse-grained, which leads to significant inaccuracies in performance
predictions. Here, we propose a more accurate model of the write performance of
Linux-based systems that takes different I/O methods and levels (via system
calls, library calls, direct or indirect, etc.), the page cache, background
writing, and the I/O throttling capabilities of the Linux kernel into account.
With our model, we reduce, for example, the relative prediction error compared
to a standard I/O model included in SimGrid for a random I/O scenario from 67 %
down to 10 % relative error against real measurements of the simulated
workload. In other scenarios the differences are even more pronounced.
","['\nMasoud Gholami\n', '\nFlorian Schintke\n']","22 pages, 9 figures, 1 table",,http://arxiv.org/abs/2306.05701v1,cs.PF,"['cs.PF', 'cs.DC', 'cs.OS']",,,[]
Karma: Resource Allocation for Dynamic Demands,http://arxiv.org/abs/2305.17222v2,2023-05-26T19:30:48Z,2023-07-07T15:23:13Z,"  We consider the problem of fair resource allocation in a system where user
demands are dynamic, that is, where user demands vary over time. Our key
observation is that the classical max-min fairness algorithm for resource
allocation provides many desirable properties (e.g., Pareto efficiency,
strategy-proofness, and fairness), but only under the strong assumption of user
demands being static over time. For the realistic case of dynamic user demands,
the max-min fairness algorithm loses one or more of these properties.
  We present Karma, a new resource allocation mechanism for dynamic user
demands. The key technical contribution in Karma is a credit-based resource
allocation algorithm: in each quantum, users donate their unused resources and
are assigned credits when other users borrow these resources; Karma carefully
orchestrates the exchange of credits across users (based on their instantaneous
demands, donated resources and borrowed resources), and performs prioritized
resource allocation based on users' credits. We theoretically establish Karma
guarantees related to Pareto efficiency, strategy-proofness, and fairness for
dynamic user demands. Empirical evaluations over production workloads show that
these properties translate well into practice: Karma is able to reduce
disparity in performance across users to a bare minimum while maintaining
Pareto-optimal system-wide performance.
","['\nMidhul Vuppalapati\n', '\nGiannis Fikioris\n', '\nRachit Agarwal\n', '\nAsaf Cidon\n', '\nAnurag Khandelwal\n', '\nEva Tardos\n']","Full version of paper accepted to USENIX OSDI 2023 with proofs of
  theoretical guarantees",,http://arxiv.org/abs/2305.17222v2,cs.OS,['cs.OS'],,,[]
Securing Cloud File Systems using Shielded Execution,http://arxiv.org/abs/2305.18639v1,2023-05-29T22:27:37Z,2023-05-29T22:27:37Z,"  Cloud file systems offer organizations a scalable and reliable file storage
solution. However, cloud file systems have become prime targets for
adversaries, and traditional designs are not equipped to protect organizations
against the myriad of attacks that may be initiated by a malicious cloud
provider, co-tenant, or end-client. Recently proposed designs leveraging
cryptographic techniques and trusted execution environments (TEEs) still force
organizations to make undesirable trade-offs, consequently leading to either
security, functional, or performance limitations. In this paper, we introduce
TFS, a cloud file system that leverages the security capabilities provided by
TEEs to bootstrap new security protocols that meet real-world security,
functional, and performance requirements. Through extensive security and
performance analyses, we show that TFS can ensure stronger security guarantees
while still providing practical utility and performance w.r.t. state-of-the-art
systems; compared to the widely-used NFS, TFS achieves up to 2.1X speedups
across micro-benchmarks and incurs <1X overhead for most macro-benchmark
workloads. TFS demonstrates that organizations need not sacrifice file system
security to embrace the functional and performance advantages of outsourcing.
","['\nQuinn Burke\n', '\nYohan Beugin\n', '\nBlaine Hoak\n', '\nRachel King\n', '\nEric Pauley\n', '\nRyan Sheatsley\n', '\nMingli Yu\n', '\nTing He\n', '\nThomas La Porta\n', '\nPatrick McDaniel\n']",,,http://arxiv.org/abs/2305.18639v1,cs.CR,"['cs.CR', 'cs.OS']",,,[]
"DVFO: Learning-Based DVFS for Energy-Efficient Edge-Cloud Collaborative
  Inference",http://arxiv.org/abs/2306.01811v3,2023-06-02T07:00:42Z,2023-06-23T07:34:40Z,"  Due to limited resources on edge and different characteristics of deep neural
network (DNN) models, it is a big challenge to optimize DNN inference
performance in terms of energy consumption and end-to-end latency on edge
devices. In addition to the dynamic voltage frequency scaling (DVFS) technique,
the edge-cloud architecture provides a collaborative approach for efficient DNN
inference. However, current edge-cloud collaborative inference methods have not
optimized various compute resources on edge devices. Thus, we propose DVFO, a
novel DVFS-enabled edge-cloud collaborative inference framework, which
co-optimizes DVFS and offloading parameters via deep reinforcement learning
(DRL). Specifically, DVFO automatically co-optimizes 1) the CPU, GPU and memory
frequencies of edge devices, and 2) the feature maps to be offloaded to cloud
servers. In addition, it leverages a thinking-while-moving concurrent mechanism
to accelerate the DRL learning process, and a spatial-channel attention
mechanism to extract DNN feature maps of secondary importance for workload
offloading. This approach improves inference performance for different DNN
models under various edge-cloud network conditions. Extensive evaluations using
two datasets and six widely-deployed DNN models on three heterogeneous edge
devices show that DVFO significantly reduces the energy consumption by 33% on
average, compared to state-of-the-art schemes. Moreover, DVFO achieves up to
28.6%-59.1% end-to-end latency reduction, while maintaining accuracy within 1%
loss on average.
","['\nZiyang Zhang\n', '\nYang Zhao\n', '\nHuan Li\n', '\nChangyao Lin\n', '\nJie Liu\n']",,,http://arxiv.org/abs/2306.01811v3,cs.LG,"['cs.LG', 'cs.DC', 'cs.OS']",,,[]
"Quiver: Supporting GPUs for Low-Latency, High-Throughput GNN Serving
  with Workload Awareness",http://arxiv.org/abs/2305.10863v1,2023-05-18T10:34:23Z,2023-05-18T10:34:23Z,"  Systems for serving inference requests on graph neural networks (GNN) must
combine low latency with high throughout, but they face irregular computation
due to skew in the number of sampled graph nodes and aggregated GNN features.
This makes it challenging to exploit GPUs effectively: using GPUs to sample
only a few graph nodes yields lower performance than CPU-based sampling; and
aggregating many features exhibits high data movement costs between GPUs and
CPUs. Therefore, current GNN serving systems use CPUs for graph sampling and
feature aggregation, limiting throughput.
  We describe Quiver, a distributed GPU-based GNN serving system with
low-latency and high-throughput. Quiver's key idea is to exploit workload
metrics for predicting the irregular computation of GNN requests, and governing
the use of GPUs for graph sampling and feature aggregation: (1) for graph
sampling, Quiver calculates the probabilistic sampled graph size, a metric that
predicts the degree of parallelism in graph sampling. Quiver uses this metric
to assign sampling tasks to GPUs only when the performance gains surpass
CPU-based sampling; and (2) for feature aggregation, Quiver relies on the
feature access probability to decide which features to partition and replicate
across a distributed GPU NUMA topology. We show that Quiver achieves up to 35
times lower latency with an 8 times higher throughput compared to
state-of-the-art GNN approaches (DGL and PyG).
","['\nZeyuan Tan\n', '\nXiulong Yuan\n', '\nCongjie He\n', '\nMan-Kit Sit\n', '\nGuo Li\n', '\nXiaoze Liu\n', '\nBaole Ai\n', '\nKai Zeng\n', '\nPeter Pietzuch\n', '\nLuo Mai\n']",,,http://arxiv.org/abs/2305.10863v1,cs.DC,"['cs.DC', 'cs.AI', 'cs.LG', 'cs.OS']",,,[]
NVMM cache design: Logging vs. Paging,http://arxiv.org/abs/2305.02244v1,2023-05-03T16:29:39Z,2023-05-03T16:29:39Z,"  Modern NVMM is closing the gap between DRAM and persistent storage, both in
terms of performance and features. Having both byte addressability and
persistence on the same device gives NVMM an unprecedented set of features,
leading to the following question: How should we design an NVMM-based caching
system to fully exploit its potential? We build two caching mechanisms, NVPages
and NVLog, based on two radically different design approaches. NVPages stores
memory pages in NVMM, similar to the Linux page cache (LPC). NVLog uses NVMM to
store a log of pending write operations to be submitted to the LPC, while it
ensures reads with a small DRAM cache. Our study shows and quantifies
advantages and flaws for both designs.
","['\nRémi Dulong\n', '\nQuentin Acher\n', '\nBaptiste Lepers\n', '\nValerio Schiavoni\n', '\nPascal Felber\n', '\nGaël Thomas\n']","3 pages, 4 figures, presented for NVMW'23: 14th Annual Non-Volatile
  Memories Workshop",,http://arxiv.org/abs/2305.02244v1,cs.OS,"['cs.OS', 'cs.AR', '68M15']",,,[]
ONCache: A Cache-Based Low-Overhead Container Overlay Network,http://arxiv.org/abs/2305.05455v2,2023-05-04T10:15:06Z,2023-10-07T07:43:59Z,"  Recent years have witnessed a widespread adoption of containers. While
containers simplify and accelerate application development, existing container
network technologies either incur significant overhead, which hurts performance
for distributed applications, or lose flexibility or compatibility, which
hinders the widespread deployment in production.
  We design and implement ONCache (\textbf{O}verlay \textbf{N}etwork
\textbf{Cache}), a cache-based container overlay network, to eliminate the
overhead while keeping flexibility and compatibility. We carefully analyze the
difference between an overlay network and a host network, and find that an
overlay network incurs extra packet processing, including encapsulating,
intra-host routing, namespace traversing and packet filtering. Fortunately, the
extra processing exhibits an \emph{invariance property}, e.g., most packets of
the same flow have the same processing results. This property motivates us to
cache the extra processing results. With the proposed cache, ONCache
significantly reduces the extra overhead while maintaining the same flexibility
and compatibility as standard overlay networks. We implement ONCache using eBPF
with only 524 lines of code, and deploy ONCache as a plugin of Antrea.
  With ONCache, container communication achieves similar performance as host
communication. Compared to the standard overlay network, ONCache improves the
throughput and request-response transaction rate by 12\% and 36\% for TCP (20\%
and 34\% for UDP), while significant reduces per-packet CPU overhead. Many
distributed applications also benefit from ONCache.
","['\nShengkai Lin\n', '\nShizhen Zhao\n', '\nPeirui Cao\n', '\nXinchi Han\n', '\nQuan Tian\n', '\nWenfeng Liu\n', '\nQi Wu\n', '\nDonghai Han\n', '\nXinbing Wang\n', '\nChenghu Zhou\n']",,,http://arxiv.org/abs/2305.05455v2,cs.NI,"['cs.NI', 'cs.OS']",,,[]
"COLA: Characterizing and Optimizing the Tail Latency for Safe Level-4
  Autonomous Vehicle Systems",http://arxiv.org/abs/2305.07147v1,2023-05-11T21:26:22Z,2023-05-11T21:26:22Z,"  Autonomous vehicles (AVs) are envisioned to revolutionize our life by
providing safe, relaxing, and convenient ground transportation. The computing
systems in such vehicles are required to interpret various sensor data and
generate responses to the environment in a timely manner to ensure driving
safety. However, such timing-related safety requirements are largely unexplored
in prior works.
  In this paper, we conduct a systematic study to understand the timing
requirements of AV systems. We focus on investigating and mitigating the
sources of tail latency in Level-4 AV computing systems. We observe that the
performance of AV algorithms is not uniformly distributed -- instead, the
latency is susceptible to vehicle environment fluctuations, such as traffic
density. This contributes to burst computation and memory access in response to
the traffic, and further leads to tail latency in the system. Furthermore, we
observe that tail latency also comes from a mismatch between the pre-configured
AV computation pipeline and the dynamic latency requirements in real-world
driving scenarios.
  Based on these observations, we propose a set of system designs to mitigate
AV tail latency. We demonstrate our design on widely-used industrial Level-4 AV
systems, Baidu Apollo and Autoware. The evaluation shows that our design
achieves 1.65 X improvement over the worst-case latency and 1.3 X over the
average latency, and avoids 93% of accidents on Apollo.
","['\nHaolan Liu\n', '\nZixuan Wang\n', '\nJishen Zhao\n']",,,http://arxiv.org/abs/2305.07147v1,cs.RO,"['cs.RO', 'cs.OS', 'cs.PF']",,,[]
"BCEdge: SLO-Aware DNN Inference Services with Adaptive Batching on Edge
  Platforms",http://arxiv.org/abs/2305.01519v1,2023-05-01T02:56:43Z,2023-05-01T02:56:43Z,"  As deep neural networks (DNNs) are being applied to a wide range of edge
intelligent applications, it is critical for edge inference platforms to have
both high-throughput and low-latency at the same time. Such edge platforms with
multiple DNN models pose new challenges for scheduler designs. First, each
request may have different service level objectives (SLOs) to improve quality
of service (QoS). Second, the edge platforms should be able to efficiently
schedule multiple heterogeneous DNN models so that system utilization can be
improved. To meet these two goals, this paper proposes BCEdge, a novel
learning-based scheduling framework that takes adaptive batching and concurrent
execution of DNN inference services on edge platforms. We define a utility
function to evaluate the trade-off between throughput and latency. The
scheduler in BCEdge leverages maximum entropy-based deep reinforcement learning
(DRL) to maximize utility by 1) co-optimizing batch size and 2) the number of
concurrent models automatically. Our prototype implemented on different edge
platforms shows that the proposed BCEdge enhances utility by up to 37.6% on
average, compared to state-of-the-art solutions, while satisfying SLOs.
","['\nZiyang Zhang\n', '\nHuan Li\n', '\nYang Zhao\n', '\nChangyao Lin\n', '\nJie Liu\n']",,,http://arxiv.org/abs/2305.01519v1,cs.LG,"['cs.LG', 'cs.DC', 'cs.OS']",,,[]
"High-performance and Scalable Software-based NVMe Virtualization
  Mechanism with I/O Queues Passthrough",http://arxiv.org/abs/2304.05148v1,2023-04-11T11:29:51Z,2023-04-11T11:29:51Z,"  NVMe(Non-Volatile Memory Express) is an industry standard for solid-state
drives (SSDs) that has been widely adopted in data centers. NVMe virtualization
is crucial in cloud computing as it allows for virtualized NVMe devices to be
used by virtual machines (VMs), thereby improving the utilization of storage
resources. However, traditional software-based solutions have flexibility
benefits but often come at the cost of performance degradation or high CPU
overhead. On the other hand, hardware-assisted solutions offer high performance
and low CPU usage, but their adoption is often limited by the need for special
hardware support or the requirement for new hardware development.
  In this paper, we propose LightIOV, a novel software-based NVMe
virtualization mechanism that achieves high performance and scalability without
consuming valuable CPU resources and without requiring special hardware
support. LightIOV can support thousands of VMs on each server. The key idea
behind LightIOV is NVMe hardware I/O queues passthrough, which enables VMs to
directly access I/O queues of NVMe devices, thus eliminating virtualization
overhead and providing near-native performance. Results from our experiments
show that LightIOV can provide comparable performance to VFIO, with an IOPS of
97.6%-100.2% of VFIO. Furthermore, in high-density VMs environments, LightIOV
achieves 31.4% lower latency than SPDK-Vhost when running 200 VMs, and an
improvement of 27.1% in OPS performance in real-world applications.
","['\nYiquan Chen\n', '\nZhen Jin\n', '\nYijing Wang\n', '\nYi Chen\n', '\nHao Yu\n', '\nJiexiong Xu\n', '\nJinlong Chen\n', '\nWenhai Lin\n', '\nKanghua Fang\n', '\nChengkun Wei\n', '\nQiang Liu\n', '\nYuan Xie\n', '\nWenzhi Chen\n']",,,http://arxiv.org/abs/2304.05148v1,cs.AR,"['cs.AR', 'cs.OS']",,,[]
Remote Procedure Call as a Managed System Service,http://arxiv.org/abs/2304.07349v1,2023-04-14T18:47:55Z,2023-04-14T18:47:55Z,"  Remote Procedure Call (RPC) is a widely used abstraction for cloud computing.
The programmer specifies type information for each remote procedure, and a
compiler generates stub code linked into each application to marshal and
unmarshal arguments into message buffers. Increasingly, however, application
and service operations teams need a high degree of visibility and control over
the flow of RPCs between services, leading many installations to use sidecars
or service mesh proxies for manageability and policy flexibility. These
sidecars typically involve inspection and modification of RPC data that the
stub compiler had just carefully assembled, adding needless overhead. Further,
upgrading diverse application RPC stubs to use advanced hardware capabilities
such as RDMA or DPDK is a long and involved process, and often incompatible
with sidecar policy control.
  In this paper, we propose, implement, and evaluate a novel approach, where
RPC marshalling and policy enforcement are done as a system service rather than
as a library linked into each application. Applications specify type
information to the RPC system as before, while the RPC service executes policy
engines and arbitrates resource use, and then marshals data customized to the
underlying network hardware capabilities. Our system, mRPC, also supports live
upgrades so that both policy and marshalling code can be updated transparently
to application code. Compared with using a sidecar, mRPC speeds up a standard
microservice benchmark, DeathStarBench, by up to 2.5$\times$ while having a
higher level of policy flexibility and availability.
","['\nJingrong Chen\n', '\nYongji Wu\n', '\nShihan Lin\n', '\nYechen Xu\n', '\nXinhao Kong\n', '\nThomas Anderson\n', '\nMatthew Lentz\n', '\nXiaowei Yang\n', '\nDanyang Zhuo\n']",NSDI 2023,,http://arxiv.org/abs/2304.07349v1,cs.NI,"['cs.NI', 'cs.OS']",,,[]
"InversOS: Efficient Control-Flow Protection for AArch64 Applications
  with Privilege Inversion",http://arxiv.org/abs/2304.08717v2,2023-04-18T03:49:17Z,2023-07-19T17:30:33Z,"  With the increasing popularity of AArch64 processors in general-purpose
computing, securing software running on AArch64 systems against control-flow
hijacking attacks has become a critical part toward secure computation. Shadow
stacks keep shadow copies of function return addresses and, when protected from
illegal modifications and coupled with forward-edge control-flow integrity,
form an effective and proven defense against such attacks. However, AArch64
lacks native support for write-protected shadow stacks, while software
alternatives either incur prohibitive performance overhead or provide weak
security guarantees.
  We present InversOS, the first hardware-assisted write-protected shadow
stacks for AArch64 user-space applications, utilizing commonly available
features of AArch64 to achieve efficient intra-address space isolation (called
Privilege Inversion) required to protect shadow stacks. Privilege Inversion
adopts unconventional design choices that run protected applications in the
kernel mode and mark operating system (OS) kernel memory as user-accessible;
InversOS therefore uses a novel combination of OS kernel modifications,
compiler transformations, and another AArch64 feature to ensure the safety of
doing so and to support legacy applications. We show that InversOS is secure by
design, effective against various control-flow hijacking attacks, and
performant on selected benchmarks and applications (incurring overhead of 7.0%
on LMBench, 7.1% on SPEC CPU 2017, and 3.0% on Nginx web server).
","['\nZhuojia Shen\n', '\nJohn Criswell\n']","18 pages, 9 figures, 4 tables",,http://arxiv.org/abs/2304.08717v2,cs.CR,"['cs.CR', 'cs.OS']",,,[]
Diagnosing applications' I/O behavior through system call observability,http://arxiv.org/abs/2304.08569v1,2023-04-17T19:14:37Z,2023-04-17T19:14:37Z,"  We present DIO, a generic tool for observing inefficient and erroneous I/O
interactions between applications and in-kernel storage systems that lead to
performance, dependability, and correctness issues. DIO facilitates the
analysis and enables near real-time visualization of complex I/O patterns for
data-intensive applications generating millions of storage requests. This is
achieved by non-intrusively intercepting system calls, enriching collected data
with relevant context, and providing timely analysis and visualization for
traced events. We demonstrate its usefulness by analyzing two production-level
applications. Results show that DIO enables diagnosing resource contention in
multi-threaded I/O that leads to high tail latency and erroneous file accesses
that cause data loss.
","['\nTânia Esteves\n', '\nRicardo Macedo\n', '\nRui Oliveira\n', '\nJoão Paulo\n']",,,http://arxiv.org/abs/2304.08569v1,cs.DC,"['cs.DC', 'cs.OS', 'cs.PF']",,,[]
Virtio-FPGA: a virtualization solution for SoC-attached FPGAs,http://arxiv.org/abs/2304.01721v1,2023-04-04T11:30:24Z,2023-04-04T11:30:24Z,"  Recently, FPGA accelerators have risen in popularity as they present a
suitable way of satisfying the high-computation and low-power demands of real
time applications. The modern electric transportation systems (such as
aircraft, road vehicles) can greatly profit from embedded FPGAs, which
incorporate both high-performance and flexibility features into a single SoC.
At the same time, the virtualization of FPGA resources aims to reinforce these
systems with strong isolation, consolidation and security. In this paper, we
present a novel virtualization framework aimed for SoC-attached FPGA devices,
in a Linux and QEMU/KVM setup. We use Virtio as a means to enable the
configuration of FPGA resources from guest systems in an efficient way. Also,
we employ the Linux VFIO and Device Tree Overlays technologies in order to
render the FPGA resources dynamically accessible to guest systems. The ability
to dynamically configure and utilize the FPGA resources from a virtualization
environment is described in details. The evaluation procedure of the solution
is presented and the virtualization overhead is benchmarked as minimal (around
10%) when accessing the FPGA devices from guest systems.
","['\nAnna Panagopoulou\n', '\nMichele Paolino\n', '\nDaniel Raho\n']",,,http://arxiv.org/abs/2304.01721v1,cs.OS,['cs.OS'],,,[]
SGDP: A Stream-Graph Neural Network Based Data Prefetcher,http://arxiv.org/abs/2304.03864v2,2023-04-07T23:25:48Z,2023-10-11T07:50:28Z,"  Data prefetching is important for storage system optimization and access
performance improvement. Traditional prefetchers work well for mining access
patterns of sequential logical block address (LBA) but cannot handle complex
non-sequential patterns that commonly exist in real-world applications. The
state-of-the-art (SOTA) learning-based prefetchers cover more LBA accesses.
However, they do not adequately consider the spatial interdependencies between
LBA deltas, which leads to limited performance and robustness. This paper
proposes a novel Stream-Graph neural network-based Data Prefetcher (SGDP).
Specifically, SGDP models LBA delta streams using a weighted directed graph
structure to represent interactive relations among LBA deltas and further
extracts hybrid features by graph neural networks for data prefetching. We
conduct extensive experiments on eight real-world datasets. Empirical results
verify that SGDP outperforms the SOTA methods in terms of the hit ratio by
6.21%, the effective prefetching ratio by 7.00%, and speeds up inference time
by 3.13X on average. Besides, we generalize SGDP to different variants by
different stream constructions, further expanding its application scenarios and
demonstrating its robustness. SGDP offers a novel data prefetching solution and
has been verified in commercial hybrid storage systems in the experimental
phase. Our codes and appendix are available at
https://github.com/yyysjz1997/SGDP/.
","['\nYiyuan Yang\n', '\nRongshang Li\n', '\nQiquan Shi\n', '\nXijun Li\n', '\nGang Hu\n', '\nXing Li\n', '\nMingxuan Yuan\n']","Accepted by International Joint Conference on Neural Networks (IJCNN
  2023)",,http://arxiv.org/abs/2304.03864v2,cs.OS,"['cs.OS', 'cs.DC', 'cs.LG']",,,[]
"Shedding Light on Static Partitioning Hypervisors for Arm-based
  Mixed-Criticality Systems",http://arxiv.org/abs/2303.11186v2,2023-03-20T15:17:21Z,2023-03-23T20:27:09Z,"  In this paper, we aim to understand the properties and guarantees of static
partitioning hypervisors (SPH) for Arm-based mixed-criticality systems (MCS).
To this end, we performed a comprehensive empirical evaluation of popular
open-source SPH, i.e., Jailhouse, Xen (Dom0-less), Bao, and seL4 CAmkES VMM,
focusing on two key requirements of modern MCS: real-time and safety. The goal
of this study is twofold. Firstly, to empower industrial practitioners with
hard data to reason about the different trade-offs of SPH. Secondly, we aim to
raise awareness of the research and open-source communities to the still open
problems in SPH by unveiling new insights regarding lingering weaknesses. All
artifacts will be open-sourced to enable independent validation of results and
encourage further exploration on SPH.
","['\nJosé Martins\n', '\nSandro Pinto\n']",,,http://arxiv.org/abs/2303.11186v2,cs.OS,['cs.OS'],,,[]
"IRIS: a Record and Replay Framework to Enable Hardware-assisted
  Virtualization Fuzzing",http://arxiv.org/abs/2303.12817v1,2023-03-22T12:16:00Z,2023-03-22T12:16:00Z,"  Nowadays, industries are looking into virtualization as an effective means to
build safe applications, thanks to the isolation it can provide among virtual
machines (VMs) running on the same hardware. In this context, a fundamental
issue is understanding to what extent the isolation is guaranteed, despite
possible (or induced) problems in the virtualization mechanisms. Uncovering
such isolation issues is still an open challenge, especially for
hardware-assisted virtualization, since the search space should include all the
possible VM states (and the linked hypervisor state), which is prohibitive. In
this paper, we propose IRIS, a framework to record (learn) sequences of inputs
(i.e., VM seeds) from the real guest execution (e.g., OS boot), replay them
as-is to reach valid and complex VM states, and finally use them as valid seed
to be mutated for enabling fuzzing solutions for hardware-assisted hypervisors.
We demonstrate the accuracy and efficiency of IRIS in automatically reproducing
valid VM behaviors, with no need to execute guest workloads. We also provide a
proof-of-concept fuzzer, based on the proposed architecture, showing its
potential on the Xen hypervisor.
","['\nCarmine Cesarano\n', '\nMarcello Cinque\n', '\nDomenico Cotroneo\n', '\nLuigi De Simone\n', '\nGiorgio Farina\n']","13 pages, Accepted for publication at The 53rd Annual IEEE/IFIP
  International Conference on Dependable Systems and Networks (DSN)",,http://arxiv.org/abs/2303.12817v1,cs.CR,"['cs.CR', 'cs.OS']",,,[]
"LearnedFTL: A Learning-based Page-level FTL for Improving Random Reads
  in Flash-based SSDs",http://arxiv.org/abs/2303.13226v1,2023-03-23T12:43:57Z,2023-03-23T12:43:57Z,"  We present LearnedFTL, which applies learned indexes to on-demand page-level
flash translation layer (FTL) designs to improve the random read performance of
flash-based solid-state drives (SSDs). The first of its kind, it minimizes the
number of double reads induced by address translation in random read accesses.
To apply the learned indexes to address translation, LearnedFTL proposes
dynamic piece-wise regression to efficiently build learned indexes. LearnedFTL
also exploits the unique feature of page relocation in SSD internal garbage
collection (GC), and embeds the learned index training in GC, which can
minimize additional delay on normal read and write operations. Additionally,
LearnedFTL employs a bitmap prediction filter to guarantee the accuracy of
learned indexes' predictions. With these designs, LearnedFTL considerably
speeds up address translation while reducing the number of flash read accesses
caused by the demand-based page-level FTL. Our benchmark-driven experiments on
a FEMU-based prototype show that LearnedFTL reduces the 99th percentile tail
latency by 4.8$\times$, on average, compared to the state-of-the-art TPFTL
scheme.
","['\nShengzhe Wang\n', '\nZihang Lin\n', '\nSuzhen Wu\n', '\nHong Jiang\n', '\nJie Zhang\n', '\nBo Mao\n']","This paper was first submitted to EuroSys 23 Spring Deadline (May 18,
  2022)",,http://arxiv.org/abs/2303.13226v1,cs.AR,"['cs.AR', 'cs.OS']",,,[]
Intel TDX Demystified: A Top-Down Approach,http://arxiv.org/abs/2303.15540v1,2023-03-27T18:38:28Z,2023-03-27T18:38:28Z,"  Intel Trust Domain Extensions (TDX) is a new architectural extension in the
4th Generation Intel Xeon Scalable Processor that supports confidential
computing. TDX allows the deployment of virtual machines in the
Secure-Arbitration Mode (SEAM) with encrypted CPU state and memory, integrity
protection, and remote attestation. TDX aims to enforce hardware-assisted
isolation for virtual machines and minimize the attack surface exposed to host
platforms, which are considered to be untrustworthy or adversarial in the
confidential computing's new threat model. TDX can be leveraged by regulated
industries or sensitive data holders to outsource their computations and data
with end-to-end protection in public cloud infrastructure.
  This paper aims to provide a comprehensive understanding of TDX to potential
adopters, domain experts, and security researchers looking to leverage the
technology for their own purposes. We adopt a top-down approach, starting with
high-level security principles and moving to low-level technical details of
TDX. Our analysis is based on publicly available documentation and source code,
offering insights from security researchers outside of Intel.
","['\nPau-Chen Cheng\n', '\nWojciech Ozga\n', '\nEnriquillo Valdez\n', '\nSalman Ahmed\n', '\nZhongshu Gu\n', '\nHani Jamjoom\n', '\nHubertus Franke\n', '\nJames Bottomley\n']",,,http://arxiv.org/abs/2303.15540v1,cs.CR,"['cs.CR', 'cs.OS']",,,[]
Remote attestation of SEV-SNP confidential VMs using e-vTPMs,http://arxiv.org/abs/2303.16463v2,2023-03-29T05:33:11Z,2023-06-25T23:41:55Z,"  Trying to address the security challenges of a cloud-centric software
deployment paradigm, silicon and cloud vendors are introducing confidential
computing - an umbrella term aimed at providing hardware and software
mechanisms for protecting cloud workloads from the cloud provider and its
software stack. Today, Intel SGX, AMD SEV, Intel TDX, etc., provide a way to
shield cloud applications from the cloud provider through encryption of the
application's memory below the hardware boundary of the CPU, hence requiring
trust only in the CPU vendor. Unfortunately, existing hardware mechanisms do
not automatically enable the guarantee that a protected system was not tampered
with during configuration and boot time. Such a guarantee relies on a hardware
RoT, i.e., an integrity-protected location that can store measurements in a
trustworthy manner, extend them, and authenticate the measurement logs to the
user.
  In this work, we design and implement a virtual TPM that virtualizes the
hardware RoT without requiring trust in the cloud provider. To ensure the
security of a vTPM in a provider-controlled environment, we leverage unique
isolation properties of the SEV-SNP hardware that allows us to execute secure
services as part of the enclave environment protected from the cloud provider.
We further develop a novel approach to vTPM state management where the vTPM
state is not preserved across reboots. Specifically, we develop a stateless
ephemeral vTPM that supports remote attestation without any persistent state on
the host. This allows us to pair each confidential VM with a private instance
of a vTPM completely isolated from the provider-controlled environment and
other VMs. We built our prototype entirely on open-source components. Though
our work is AMD-specific, a similar approach could be used to build remote
attestation protocols on other trusted execution environments.
","['\nVikram Narayanan\n', '\nClaudio Carvalho\n', '\nAngelo Ruocco\n', '\nGheorghe Almási\n', '\nJames Bottomley\n', '\nMengmei Ye\n', '\nTobin Feldman-Fitzthum\n', '\nDaniele Buono\n', '\nHubertus Franke\n', '\nAnton Burtsev\n']","12 pages, 4 figures","In Proceedings of the 39th Annual Computer Security Applications
  Conference (ACSAC 2023). 732-743",http://dx.doi.org/10.1145/3627106.3627112,cs.CR,"['cs.CR', 'cs.OS']",10.1145/3627106.3627112,,[]
"E2C: A Visual Simulator to Reinforce Education of Heterogeneous
  Computing Systems",http://arxiv.org/abs/2303.10901v1,2023-03-20T06:38:10Z,2023-03-20T06:38:10Z,"  With the increasing popularity of accelerator technologies (e.g., GPUs and
TPUs) and the emergence of domain-specific computing via ASICs and FPGA, the
matter of heterogeneity and understanding its ramifications on the performance
has become more critical than ever before. However, it is challenging to
effectively educate students about the potential impacts of heterogeneity on
the performance of distributed systems; and on the logic of resource allocation
methods to efficiently utilize the resources. Making use of the real
infrastructure for benchmarking the performance of heterogeneous machines, for
different applications, with respect to different objectives, and under various
workload intensities is cost- and time-prohibitive. To reinforce the quality of
learning about various dimensions of heterogeneity, and to decrease the
widening gap in education, we develop an open-source simulation tool, called
E2C, that can help students researchers to study any type of heterogeneous (or
homogeneous) computing system and measure its performance under various
configurations. E2C is equipped with an intuitive graphical user interface
(GUI) that enables its users to easily examine system-level solutions
(scheduling, load balancing, scalability, etc.) in a controlled environment
within a short time. E2C is a discrete event simulator that offers the
following features: (i) simulating a heterogeneous computing system; (ii)
implementing a newly developed scheduling method and plugging it into the
system, (iii) measuring energy consumption and other output-related metrics;
and (iv) powerful visual aspects to ease the learning curve for students. We
used E2C as an assignment in the Distributed and Cloud Computing course. Our
anonymous survey study indicates that students rated E2C with the score of 8.7
out of 10 for its usefulness in understanding the concepts of scheduling in
heterogeneous computing.
","['\nAli Mokhtari\n', '\nDrake Rawls\n', '\nTony Huynh\n', '\nJeremiah Green\n', '\nMohsen Amini Salehi\n']","Accepted in Edupar '23, as part of IPDPS '23 Conference. arXiv admin
  note: text overlap with arXiv:2212.11333",,http://arxiv.org/abs/2303.10901v1,cs.DC,"['cs.DC', 'cs.AR', 'cs.OS']",,,[]
ESP32: QEMU Emulation within a Docker Container,http://arxiv.org/abs/2303.10204v2,2023-03-17T18:48:50Z,2023-03-25T22:22:19Z,"  The ESP32 is a popular microcontroller from Espressif that can be used in
many embedded applications. Robotic joints, smart car chargers, beer vat
agitators and automated bread mixers are a few examples where this
system-on-a-chip excels. It is cheap to buy and has a number of vendors
providing low-cost development board kits that come with the microcontroller
and many external connection points with peripherals. There is a large software
ecosystem for the ESP32. Espressif maintains an SDK containing many C-language
sample projects providing a starting point for a huge variety of software
services and I/O needs. Third party projects provide additional sample code as
well as support for other programming languages. For example, MicroPython is a
mature project with sample code and officially supported by Espressif. The SDK
provides tools to not just build an application but also merge a flash image,
flash to the microcontroller and monitor the output. Is it possible to build
the ESP32 load and emulate on another host OS? This paper explores the QEMU
emulator and its ability to emulate the ethernet interface for the guest OS.
Additionally, we look into the concept of containerizing the entire emulator
and ESP32 load package such that a microcontroller flash image can successfully
run with a one-step deployment of a Docker container.
","['\nMichael Howard\n', '\nR. Bruce Irvin\n']",7 pages and 9 figures,,http://arxiv.org/abs/2303.10204v2,cs.OS,"['cs.OS', 'F.2.2; I.2.7']",,,[]
Policy/mechanism separation in the Warehouse-Scale OS,http://arxiv.org/abs/2303.09725v1,2023-03-17T01:52:02Z,2023-03-17T01:52:02Z,"  ""As many of us know from bitter experience, the policies provided in extant
operating systems, which are claimed to work well and behave fairly 'on the
average', often fail to do so in the special cases important to us"" [Wulf et
al. 1974]. Written in 1974, these words motivated moving policy decisions into
user-space. Today, as warehouse-scale computers (WSCs) have become ubiquitous,
it is time to move policy decisions away from individual servers altogether.
Built-in policies are complex and often exhibit bad performance at scale.
Meanwhile, the highly-controlled WSC setting presents opportunities to improve
performance and predictability.
  We propose moving all policy decisions from the OS kernel to the cluster
manager (CM), in a new paradigm we call Grape CM. In this design, the role of
the kernel is reduced to monitoring, sending metrics to the CM, and executing
policy decisions made by the CM. The CM uses metrics from all kernels across
the WSC to make informed policy choices, sending commands back to each kernel
in the cluster. We claim that Grape CM will improve performance, transparency,
and simplicity. Our initial experiments show how the CM can identify the
optimal set of huge pages for any workload or improve memcached latency by 15%.
","['\nMark Mansi\nUniversity of Wisconsin-Madison\n', '\nMichael M. Swift\nUniversity of Wisconsin-Madison\n']","10 pages, 5 figures, 2 code listings",,http://arxiv.org/abs/2303.09725v1,cs.OS,"['cs.OS', 'cs.DC', 'D.4.7; D.4.2; D.4.8; C.2.4']",,,"['University of Wisconsin-Madison', 'University of Wisconsin-Madison']"
"CXLMemSim: A pure software simulated CXL.mem for performance
  characterization",http://arxiv.org/abs/2303.06153v1,2023-03-10T04:37:07Z,2023-03-10T04:37:07Z,"  The emerging CXL.mem standard provides a new type of byte-addressable remote
memory with a variety of memory types and hierarchies. With CXL.mem, multiple
layers of memory -- e.g., local DRAM and CXL-attached remote memory at
different locations -- are exposed to operating systems and user applications,
bringing new challenges and research opportunities. Unfortunately, since
CXL.mem devices are not commercially available, it is difficult for researchers
to conduct systems research that uses CXL.mem. In this paper, we present our
ongoing work, CXLMemSim, a fast and lightweight CXL.mem simulator for
performance characterization. CXLMemSim uses a performance model driven using
performance monitoring events, which are supported by most commodity
processors. Specifically, CXLMemSim attaches to an existing, unmodified
program, and divides the execution of the program into multiple epochs; once an
epoch finishes, CXLMemSim collects performance monitoring events and calculates
the simulated execution time of the epoch based on these events. Through this
method, CXLMemSim avoids the performance overhead of a full-system simulator
(e.g., Gem5) and allows the memory hierarchy and latency to be easily adjusted,
enabling research such as memory scheduling for complex applications. Our
preliminary evaluation shows that CXLMemSim slows down the execution of the
attached program by 4.41x on average for real-world applications.
","['\nYiwei Yang\n', '\nPooneh Safayenikoo\n', '\nJiacheng Ma\n', '\nTanvir Ahmed Khan\n', '\nAndrew Quinn\n']",,,http://arxiv.org/abs/2303.06153v1,cs.PF,"['cs.PF', 'cs.AR', 'cs.OS']",,,[]
"Protected Data Plane OS Using Memory Protection Keys and Lightweight
  Activation",http://arxiv.org/abs/2302.14417v2,2023-02-28T08:50:03Z,2023-03-01T03:40:50Z,"  Increasing data center network speed coupled with application requirements
for high throughput and low latencies have raised the efficiency bar for
network stacks. To reduce substantial kernel overhead in network processing,
recent proposals bypass the kernel or implement the stack as user space OS
service -- both with performance isolation, security, and resource efficiency
trade-offs. We present Tardis, a new network stack architecture that combines
the performance and resource efficiency benefits of kernel-bypass and the
security and performance enforcement of in-kernel stacks. Tardis runs the OS
I/O stack in user-level threads that share both address spaces and kernel
threads with applications, avoiding almost all kernel context switch and
cross-core communication overheads. To provide sufficient protection, Tardis
leverages x86 protection keys (MPK) extension to isolate the I/O stack from
application code. And to enforce timely scheduling of network processing and
fine-grained performance isolation, Tardis implements lightweight scheduler
activations with preemption timers.
","['\nYihan Yang\n', '\nZhuobin Huang\n', '\nAntoine Kaufmann\n', '\nJialin Li\n']",,,http://arxiv.org/abs/2302.14417v2,cs.OS,['cs.OS'],,,[]
"EavesDroid: Eavesdropping User Behaviors via OS Side-Channels on
  Smartphones",http://arxiv.org/abs/2303.03700v2,2023-03-07T07:31:46Z,2023-07-14T03:11:45Z,"  As the Internet of Things (IoT) continues to evolve, smartphones have become
essential components of IoT systems. However, with the increasing amount of
personal information stored on smartphones, user privacy is at risk of being
compromised by malicious attackers. Although malware detection engines are
commonly installed on smartphones against these attacks, attacks that can evade
these defenses may still emerge. In this paper, we analyze the return values of
system calls on Android smartphones and find two never-disclosed vulnerable
return values that can leak fine-grained user behaviors. Based on this
observation, we present EavesDroid, an application-embedded side-channel attack
on Android smartphones that allows unprivileged attackers to accurately
identify fine-grained user behaviors (e.g., viewing messages and playing
videos) via on-screen operations. Our attack relies on the correlation between
user behaviors and the return values associated with hardware and system
resources. While this attack is challenging since these return values are
susceptible to fluctuation and misalignment caused by many factors, we show
that attackers can eavesdrop on fine-grained user behaviors using a CNN-GRU
classification model that adopts min-max normalization and multiple return
value fusion. Our experiments on different models and versions of Android
smartphones demonstrate that EavesDroid can achieve 98% and 86% inference
accuracy for 17 classes of user behaviors in the test set and real-world
settings, highlighting the risk of our attack on user privacy. Finally, we
recommend effective malware detection, carefully designed obfuscation methods,
or restrictions on reading vulnerable return values to mitigate this attack.
","['\nQuancheng Wang\n', '\nMing Tang\n', '\nJianming Fu\n']","15 pages, 25 figures",,http://dx.doi.org/10.1109/JIOT.2023.3298992,cs.CR,"['cs.CR', 'cs.OS']",10.1109/JIOT.2023.3298992,,[]
"Capstone: A Capability-based Foundation for Trustless Secure Memory
  Access (Extended Version)",http://arxiv.org/abs/2302.13863v2,2023-02-27T15:03:15Z,2023-03-09T07:23:25Z,"  Capability-based memory isolation is a promising new architectural primitive.
Software can access low-level memory only via capability handles rather than
raw pointers, which provides a natural interface to enforce security
restrictions. Existing architectural capability designs such as CHERI provide
spatial safety, but fail to extend to other memory models that
security-sensitive software designs may desire. In this paper, we propose
Capstone, a more expressive architectural capability design that supports
multiple existing memory isolation models in a trustless setup, i.e., without
relying on trusted software components. We show how Capstone is well-suited for
environments where privilege boundaries are fluid (dynamically extensible),
memory sharing/delegation are desired both temporally and spatially, and where
such needs are to be balanced with availability concerns. Capstone can also be
implemented efficiently. We present an implementation sketch and through
evaluation show that its overhead is below 50% in common use cases. We also
prototype a functional emulator for Capstone and use it to demonstrate the
runnable implementations of six real-world memory models without trusted
software components: three types of enclave-based TEEs, a thread scheduler, a
memory allocator, and Rust-style memory safety -- all within the interface of
Capstone.
","['\nJason Zhijingcheng Yu\n', '\nConrad Watt\n', '\nAditya Badole\n', '\nTrevor E. Carlson\n', '\nPrateek Saxena\n']","31 pages, 10 figures. This is an extended version of a paper to
  appear at 32nd USENIX Security Symposium, August 2023; acknowledgments
  updated",,http://arxiv.org/abs/2302.13863v2,cs.CR,"['cs.CR', 'cs.AR', 'cs.OS']",,,[]
"Rethinking Memory Profiling and Migration for Multi-Tiered Large Memory
  Systems",http://arxiv.org/abs/2302.09468v2,2023-02-19T03:49:02Z,2023-05-01T18:43:34Z,"  Multi-tiered large memory systems call for rethinking of memory profiling and
migration because of the unique problems unseen in the traditional memory
systems with smaller capacity and fewer tiers. We develop MTM, an
application-transparent page management system based on three principles: (1)
connecting the control of profiling overhead with the profiling mechanism for
high-quality profiling; (2) building a universal page migration policy on the
complex multi-tiered memory for high performance; and (3) introducing huge page
awareness. We evaluate MTM using common big-data applications with realistic
working sets (hundreds of GB to 1 TB). MTM outperforms seven state-of-the-art
solutions by up to 42% (17% on average)
","['\nJie Ren\n', '\nDong Xu\n', '\nIvy Peng\n', '\nJunhee Ryu\n', '\nKwangsik Shin\n', '\nDaewoo Kim\n', '\nDong Li\n']",,,http://arxiv.org/abs/2302.09468v2,cs.PF,"['cs.PF', 'cs.OS']",,,[]
Programmable System Call Security with eBPF,http://arxiv.org/abs/2302.10366v1,2023-02-20T23:54:04Z,2023-02-20T23:54:04Z,"  System call filtering is a widely used security mechanism for protecting a
shared OS kernel against untrusted user applications. However, existing system
call filtering techniques either are too expensive due to the context switch
overhead imposed by userspace agents, or lack sufficient programmability to
express advanced policies. Seccomp, Linux's system call filtering module, is
widely used by modern container technologies, mobile apps, and system
management services. Despite the adoption of the classic BPF language (cBPF),
security policies in Seccomp are mostly limited to static allow lists,
primarily because cBPF does not support stateful policies. Consequently, many
essential security features cannot be expressed precisely and/or require kernel
modifications.
  In this paper, we present a programmable system call filtering mechanism,
which enables more advanced security policies to be expressed by leveraging the
extended BPF language (eBPF). More specifically, we create a new Seccomp eBPF
program type, exposing, modifying or creating new eBPF helper functions to
safely manage filter state, access kernel and user state, and utilize
synchronization primitives. Importantly, our system integrates with existing
kernel privilege and capability mechanisms, enabling unprivileged users to
install advanced filters safely. Our evaluation shows that our eBPF-based
filtering can enhance existing policies (e.g., reducing the attack surface of
early execution phase by up to 55.4% for temporal specialization), mitigate
real-world vulnerabilities, and accelerate filters.
","['\nJinghao Jia\n', '\nYiFei Zhu\n', '\nDan Williams\n', '\nAndrea Arcangeli\n', '\nClaudio Canella\n', '\nHubertus Franke\n', '\nTobin Feldman-Fitzthum\n', '\nDimitrios Skarlatos\n', '\nDaniel Gruss\n', '\nTianyin Xu\n']",,,http://arxiv.org/abs/2302.10366v1,cs.OS,"['cs.OS', 'cs.CR']",,,[]
Faabric: Fine-Grained Distribution of Scientific Workloads in the Cloud,http://arxiv.org/abs/2302.11358v1,2023-02-22T13:10:07Z,2023-02-22T13:10:07Z,"  With their high parallelism and resource needs, many scientific applications
benefit from cloud deployments. Today, scientific applications are executed on
dedicated pools of VMs, resulting in resource fragmentation: users pay for
underutilised resources, and providers cannot reallocate unused resources
between applications. While serverless cloud computing could address these
issues, its programming model is incompatible with the use of shared memory and
message passing in scientific applications: serverless functions do not share
memory directly on the same VM or support message passing semantics when
scheduling functions dynamically.
  We describe Faabric, a new serverless cloud runtime that transparently
distributes applications with shared memory and message passing across VMs.
Faabric achieves this by scheduling computation in a fine-grained
(thread/process) fashion through a new execution abstraction called Granules.
To support shared memory, Granules are isolated using WebAssembly but share
memory directly; to support message passing, Granules offer asynchronous
point-to-point communication. Faabric schedules Granules to meet an
application's parallelism needs. It also synchronises changes to Granule's
shared memory, and migrates Granules to improve locality.
","['\nSimon Shillaker\n', '\nCarlos Segarra\n', '\nEleftheria Mappoura\n', '\nMayeul Fournial\n', '\nLluis Vilanova\n', '\nPeter Pietzuch\n']",12 pages,,http://arxiv.org/abs/2302.11358v1,cs.DC,"['cs.DC', 'cs.OS']",,,[]
"Virtualization of Tiny Embedded Systems with a robust real-time capable
  and extensible Stack Virtual Machine REXAVM supporting Material-integrated
  Intelligent Systems and Tiny Machine Learning",http://arxiv.org/abs/2302.09002v1,2023-02-17T17:13:35Z,2023-02-17T17:13:35Z,"  In the past decades, there has been a significant increase in sensor density
and sensor deployment, driven by a significant miniaturization and decrease in
size down to the chip level, addressing ubiquitous computing, edge computing,
as well as distributed sensor networks. Material-integrated and intelligent
systems (MIIS) provide the next integration and application level, but they
create new challenges and introduce hard constraints (resources, energy supply,
communication, resilience, and security). Commonly, low-resource systems are
statically programmed processors with application-specific software or
application-specific hardware (FPGA). This work demonstrates the need for and
solution to virtualization in such low-resource and constrained systems towards
resilient distributed sensor and cyber-physical networks using a unified
low-resource, customizable, and real-time capable embedded and extensible stack
virtual machine (REXAVM) that can be implemented and cooperate in both software
and hardware. In a holistic architecture approach, the VM specifically
addresses digital signal processing and tiny machine learning. The REXAVM is
highly customizable through the use of VM program code generators at compile
time and incremental code processing at run time. The VM uses an integrated,
highly efficient just-in-time compiler to create Bytecode from text code. This
paper shows and evaluates the suitability of the proposed VM architecture for
operationally equivalent software and hardware (FPGA) implementations. Specific
components supporting tiny ML and DSP using fixed-point arithmetic with respect
to efficiency and accuracy are discussed. An extended use-case section
demonstrates the usability of the introduced VM architecture for a broad range
of applications.
","['\nStefan Bosse\n', '\nSarah Bornemann\n', '\nBjörn Lüssem\n']",,,http://arxiv.org/abs/2302.09002v1,cs.OS,"['cs.OS', 'cs.AR', 'cs.LG']",,,[]
Parma: Confidential Containers via Attested Execution Policies,http://arxiv.org/abs/2302.03976v3,2023-02-08T10:15:07Z,2023-03-07T16:16:33Z,"  Container-based technologies empower cloud tenants to develop highly portable
software and deploy services in the cloud at a rapid pace. Cloud privacy,
meanwhile, is important as a large number of container deployments operate on
privacy-sensitive data, but challenging due to the increasing frequency and
sophistication of attacks. State-of-the-art confidential container-based
designs leverage process-based trusted execution environments (TEEs), but face
security and compatibility issues that limits their practical deployment. We
propose Parma, an architecture that provides lift-and-shift deployment of
unmodified containers while providing strong security protection against a
powerful attacker who controls the untrusted host and hypervisor. Parma
leverages VM-level isolation to execute a container group within a unique
VM-based TEE. Besides container integrity and user data confidentiality and
integrity, Parma also offers container attestation and execution integrity
based on an attested execution policy. Parma execution policies provide an
inductive proof over all future states of the container group. This proof,
which is established during initialization, forms a root of trust that can be
used for secure operations within the container group without requiring any
modifications of the containerized workflow itself (aside from the inclusion of
the execution policy.) We evaluate Parma on AMD SEV-SNP processors by running a
diverse set of workloads demonstrating that workflows exhibit 0-26% additional
overhead in performance over running outside the enclave, with a mean 13%
overhead on SPEC2017, while requiring no modifications to their program code.
Adding execution policies introduces less than 1% additional overhead.
Furthermore, we have deployed Parma as the underlying technology driving
Confidential Containers on Azure Container Instances.
","['\nMatthew A. Johnson\n', '\nStavros Volos\n', '\nKen Gordon\n', '\nSean T. Allen\n', '\nChristoph M. Wintersteiger\n', '\nSylvan Clebsch\n', '\nJohn Starks\n', '\nManuel Costa\n']","12 pages, 6 figures, 2 tables",,http://arxiv.org/abs/2302.03976v3,cs.CR,"['cs.CR', 'cs.NI', 'cs.OS']",,,[]
"PADLL: Taming Metadata-intensive HPC Jobs Through Dynamic,
  Application-agnostic QoS Control",http://arxiv.org/abs/2302.06418v2,2023-02-13T14:55:06Z,2023-03-24T00:25:31Z,"  Modern I/O applications that run on HPC infrastructures are increasingly
becoming read and metadata intensive. However, having multiple concurrent
applications submitting large amounts of metadata operations can easily
saturate the shared parallel file system's metadata resources, leading to
overall performance degradation and I/O unfairness. We present PADLL, an
application and file system agnostic storage middleware that enables QoS
control of data and metadata workflows in HPC storage systems. It adopts ideas
from Software-Defined Storage, building data plane stages that mediate and rate
limit POSIX requests submitted to the shared file system, and a control plane
that holistically coordinates how all I/O workflows are handled. We demonstrate
its performance and feasibility under multiple QoS policies using synthetic
benchmarks, real-world applications, and traces collected from a production
file system. Results show that PADLL can enforce complex storage QoS policies
over concurrent metadata-aggressive jobs, ensuring fairness and prioritization.
","['\nRicardo Macedo\n', '\nMariana Miranda\n', '\nYusuke Tanimura\n', '\nJason Haga\n', '\nAmit Ruhela\n', '\nStephen Lien Harrell\n', '\nRichard Todd Evans\n', '\nJosé Pereira\n', '\nJoão Paulo\n']","To appear at 23rd IEEE/ACM International Symposium on Cluster, Cloud
  and Internet Computing (CCGrid'23)",,http://arxiv.org/abs/2302.06418v2,cs.DC,"['cs.DC', 'cs.OS', 'cs.PF']",,,[]
A Learned Cache Eviction Framework with Minimal Overhead,http://arxiv.org/abs/2301.11886v1,2023-01-27T17:57:37Z,2023-01-27T17:57:37Z,"  Recent work shows the effectiveness of Machine Learning (ML) to reduce cache
miss ratios by making better eviction decisions than heuristics. However,
state-of-the-art ML caches require many predictions to make an eviction
decision, making them impractical for high-throughput caching systems. This
paper introduces Machine learning At the Tail (MAT), a framework to build
efficient ML-based caching systems by integrating an ML module with a
traditional cache system based on a heuristic algorithm. MAT treats the
heuristic algorithm as a filter to receive high-quality samples to train an ML
model and likely candidate objects for evictions. We evaluate MAT on 8
production workloads, spanning storage, in-memory caching, and CDNs. The
simulation experiments show MAT reduces the number of costly ML
predictions-per-eviction from 63 to 2, while achieving comparable miss ratios
to the state-of-the-art ML cache system. We compare a MAT prototype system with
an LRU-based caching system in the same setting and show that they achieve
similar request rates.
","['\nDongsheng Yang\n', '\nDaniel S. Berger\n', '\nKai Li\n', '\nWyatt Lloyd\n']",,,http://arxiv.org/abs/2301.11886v1,cs.OS,"['cs.OS', 'cs.DC']",,,[]
MOAT: Towards Safe BPF Kernel Extension,http://arxiv.org/abs/2301.13421v2,2023-01-31T05:31:45Z,2023-03-04T13:36:36Z,"  The Linux kernel makes considerable use of Berkeley Packet Filter (BPF) to
allow user-written BPF applications to execute in the kernel space. BPF employs
a verifier to statically check the security of user-supplied BPF code. Recent
attacks show that BPF programs can evade security checks and gain unauthorized
access to kernel memory, indicating that the verification process is not
flawless. In this paper, we present MOAT, a system that isolates potentially
malicious BPF programs using Intel Memory Protection Keys (MPK). Enforcing BPF
program isolation with MPK is not straightforward; MOAT is carefully designed
to alleviate technical obstacles, such as limited hardware keys and supporting
a wide variety of kernel BPF helper functions. We have implemented MOAT in a
prototype kernel module, and our evaluation shows that MOAT delivers low-cost
isolation of BPF programs under various real-world usage scenarios, such as the
isolation of a packet-forwarding BPF program for the memcached database with an
average throughput loss of 6%.
","['\nHongyi Lu\n', '\nShuai Wang\n', '\nYechang Wu\n', '\nWanning He\n', '\nFengwei Zhang\n']",,,http://arxiv.org/abs/2301.13421v2,cs.CR,"['cs.CR', 'cs.OS']",,,[]
"Extensions for Shared Resource Orchestration in Kubernetes to Support
  RT-Cloud Containers",http://arxiv.org/abs/2301.07479v1,2023-01-18T12:47:14Z,2023-01-18T12:47:14Z,"  Industries are considering the adoption of cloud computing for real-time
applications due to current improvements in network latencies and the advent of
Fog and Edge computing. To create an RT-cloud capable of hosting real-time
applications, it is increasingly significant to improve the entire stack,
including the containerization of applications, and their deployment and
orchestration across nodes. However, state-of-the-art orchestrators (e.g.,
Kubernetes) and underlying container engines are designed for general-purpose
applications. They ignore orchestration and management of shared resources
(e.g. memory bandwidth, cache, shared interconnect) making them unsuitable for
use with an RT-cloud. Taking inspiration from existing resource management
architectures for multicore nodes, such as ACTORS, and for distributed
mixed-criticality systems, such as the DREAMS, we propose a series of
extensions in the way shared resources are orchestrated by Kubernetes and
managed by the underlying Linux layers. Our approach allows fine-grained
monitoring and allocation of low-level shared resources on nodes to provide
better isolation to real-time containers and supports dynamic orchestration and
balancing of containers across the nodes based on the availability and demand
of shared resources.
","['\nGabriele Monaco\n', '\nGautam Gala\n', '\nGerhard Fohler\n']","6 pages, 2 figures, Accepted at the Real-time Cloud Workshop of the
  34th Euromicro Conference on Real-time Systems",,http://arxiv.org/abs/2301.07479v1,cs.DC,"['cs.DC', 'cs.OS', 'C.3; D.4.7; C.2.4; D.2.11']",,,[]
From MMU to MPU: adaptation of the Pip kernel to constrained devices,http://arxiv.org/abs/2301.04546v1,2023-01-11T16:15:17Z,2023-01-11T16:15:17Z,"  This article presents a hardware-based memory isolation solution for
constrained devices. Existing solutions target high-end embedded systems
(typically ARM Cortex-A with a Memory Management Unit, MMU) such as seL4 or Pip
(formally verified kernels) or target low-end devices such as ACES, MINION,
TrustLite, EwoK but with limited flexibility by proposing a single level of
isolation. Our approach consists in adapting Pip to inherit its flexibility
(multiple levels of isolation) but using the Memory Protection Unit (MPU)
instead of the MMU since the MPU is commonly available on constrained embedded
systems (typically ARMv7 Cortex-M4 or ARMv8 Cortex-M33 and similar devices).
This paper describes our design of Pip-MPU (Pip's variant based on the MPU) and
the rationale behind our choices. We validate our proposal with an
implementation on an nRF52840 development kit and we perform various
evaluations such as memory footprint, CPU cycles and energy consumption. We
demonstrate that although our prototyped Pip-MPU causes a 16% overhead on both
performance and energy consumption, it can reduce the attack surface of the
accessible application memory from 100% down to 2% and the privileged
operations by 99%. Pip-MPU takes less than 10 kB of Flash (6 kB for its core
components) and 550 B of RAM.
","['\nNicolas Dejon\n', '\nChrystel Gaber\n', '\nGilles Grimaud\n']","3rd International Conference on Internet of Things & Embedded Systems
  (IoTE 2022), 23-23 December 2022, Sydney (Australia)",,http://arxiv.org/abs/2301.04546v1,cs.OS,['cs.OS'],,,[]
"SFP: Providing System Call Flow Protection against Software and Fault
  Attacks",http://arxiv.org/abs/2301.02915v2,2023-01-07T18:35:08Z,2023-01-12T12:10:34Z,"  With the improvements in computing technologies, edge devices in the
Internet-of-Things have become more complex. The enabler technology for these
complex systems are powerful application core processors with operating system
support, such as Linux. While the isolation of applications through the
operating system increases the security, the interface to the kernel poses a
new threat. Different attack vectors, including fault attacks and memory
vulnerabilities, exploit the kernel interface to escalate privileges and take
over the system.
  In this work, we present SFP, a mechanism to protect the execution of system
calls against software and fault attacks providing integrity to user-kernel
transitions. SFP provides system call flow integrity by a two-step linking
approach, which links the system call and its origin to the state of
control-flow integrity. A second linking step within the kernel ensures that
the right system call is executed in the kernel. Combining both linking steps
ensures that only the correct system call is executed at the right location in
the program and cannot be skipped. Furthermore, SFP provides dynamic CFI
instrumentation and a new CFI checking policy at the edge of the kernel to
verify the control-flow state of user programs before entering the kernel. We
integrated SFP into FIPAC, a CFI protection scheme exploiting ARM pointer
authentication. Our prototype is based on a custom LLVM-based toolchain with an
instrumented runtime library combined with a custom Linux kernel to protect
system calls. The evaluation of micro- and macrobenchmarks based on SPEC 2017
show an average runtime overhead of 1.9 % and 20.6 %, which is only an increase
of 1.8 % over plain control-flow protection. This small impact on the
performance shows the efficiency of SFP for protecting all system calls and
providing integrity for the user-kernel transitions.
","['\nRobert Schilling\n', '\nPascal Nasahl\n', '\nMartin Unterguggenberger\n', '\nStefan Mangard\n']",Published at HASP22,,http://arxiv.org/abs/2301.02915v2,cs.CR,"['cs.CR', 'cs.OS']",,,[]
Exoshuffle-CloudSort,http://arxiv.org/abs/2301.03734v1,2023-01-10T00:43:32Z,2023-01-10T00:43:32Z,"  We present Exoshuffle-CloudSort, a sorting application running on top of Ray
using the Exoshuffle architecture. Exoshuffle-CloudSort runs on Amazon EC2,
with input and output data stored on Amazon S3. Using 40 i4i.4xlarge workers,
Exoshuffle-CloudSort completes the 100 TB CloudSort Benchmark (Indy category)
in 5378 seconds, with an average total cost of $97.
","['\nFrank Sifei Luan\n', '\nStephanie Wang\n', '\nSamyukta Yagati\n', '\nSean Kim\n', '\nKenneth Lien\n', '\nIsaac Ong\n', '\nTony Hong\n', '\nSangBin Cho\n', '\nEric Liang\n', '\nIon Stoica\n']",,,http://arxiv.org/abs/2301.03734v1,cs.DC,"['cs.DC', 'cs.OS']",,,[]
"Age-Optimal Multi-Channel-Scheduling under Energy and Tolerance
  Constraints",http://arxiv.org/abs/2301.00562v1,2023-01-02T08:53:44Z,2023-01-02T08:53:44Z,"  We study the optimal scheduling problem where n source nodes attempt to
transmit updates over L shared wireless on/off fading channels to optimize
their age performance under energy and age-violation tolerance constraints.
Specifically, we provide a generic formulation of age-optimization in the form
of a constrained Markov Decision Processes (CMDP), and obtain the optimal
scheduler as the solution of an associated Linear Programming problem. We
investigate the characteristics of the optimal single-user multi-channel
scheduler for the important special cases of average-age and violation-rate
minimization. This leads to several key insights on the nature of the optimal
allocation of the limited energy, where a usual threshold-based policy does not
apply and will be useful in guiding scheduler designers. We then investigate
the stability region of the optimal scheduler for the multi-user case. We also
develop an online scheduler using Lyapunov-drift-minimization methods that do
not require the knowledge of channel statistics. Our numerical studies compare
the stability region of our online scheduler to the optimal scheduler to reveal
that it performs closely with unknown channel statistics.
","['\nXujin Zhou\n', '\nIrem Koprulu\n', '\nAtilla Eryilmaz\n']",,,http://arxiv.org/abs/2301.00562v1,cs.IT,"['cs.IT', 'cs.NI', 'cs.OS', 'math.IT']",,,[]
LeaFTL: A Learning-Based Flash Translation Layer for Solid-State Drives,http://arxiv.org/abs/2301.00072v1,2022-12-30T23:37:39Z,2022-12-30T23:37:39Z,"  In modern solid-state drives (SSDs), the indexing of flash pages is a
critical component in their storage controllers. It not only affects the data
access performance, but also determines the efficiency of the precious
in-device DRAM resource. A variety of address mapping schemes and optimization
techniques have been proposed. However, most of them were developed with
human-driven heuristics. They cannot automatically capture diverse data access
patterns at runtime in SSD controllers, which leaves a large room for
improvement. In this paper, we present a learning-based flash translation layer
(FTL), named LeaFTL, which learns the address mapping to tolerate dynamic data
access patterns via linear regression at runtime. By grouping a large set of
mapping entries into a learned segment, it significantly reduces the memory
footprint of the address mapping table, which further benefits the data caching
in SSD controllers. LeaFTL also employs various optimization techniques,
including out-of-band metadata verification to tolerate mispredictions,
optimized flash allocation, and dynamic compaction of learned index segments.
We implement LeaFTL with an SSD simulator and evaluate it with various storage
workloads. LeaFTL saves the memory consumption of the mapping table by 2.9x on
average and improves the storage performance by 1.4x on average, in comparison
with state-of-the-art FTL schemes.
","['\nJinghan Sun\n', '\nShaobo Li\n', '\nYunxin Sun\n', '\nChao Sun\n', '\nDejan Vucinic\n', '\nJian Huang\n']","This paper is accepted at the 28th Conference on Architectural
  Support for Programming Languages and Operating Systems (ASPLOS 2023)",,http://dx.doi.org/10.1145/3575693.3575744,cs.OS,['cs.OS'],10.1145/3575693.3575744,,[]
MProtect: Operating System Memory Management without Access,http://arxiv.org/abs/2212.12671v1,2022-12-24T06:36:36Z,2022-12-24T06:36:36Z,"  Modern operating systems (OSes) have unfettered access to application data,
assuming that applications trust them. This assumption, however, is problematic
under many scenarios where either the OS provider is not trustworthy or the OS
can be compromised due to its large attack surface. Our investigation began
with the hypothesis that unfettered access to memory is not fundamentally
necessary for the OS to perform its own job, including managing the memory. The
result is a system called MProtect that leverages a small piece of software
running at a higher privilege level than the OS. MProtect protects the entire
user space of a process, requires only a small modification to the OS, and
supports major architectures such as ARM, x86 and RISC-V. Unlike prior works
that resorted to nested virtualization, which is often undesirable in mobile
and embedded systems, MProtect mediates how the OS accesses the memory and
handles exceptions. We report an implementation of MProtect called MGuard with
ARMv8/Linux and evaluate its performance with both macro and microbenchmarks.
We show MGuard has a runtime TCB 2~3 times smaller than related systems and
enjoys competitive performance while supporting legitimate OS access to the
user space.
","['\nCaihua Li\n', '\nSeung-seob Lee\n', '\nMin Hong Yun\n', '\nLin Zhong\n']",,,http://arxiv.org/abs/2212.12671v1,cs.OS,"['cs.OS', 'cs.CR']",,,[]
Automated Cache for Container Executables,http://arxiv.org/abs/2212.07376v1,2022-12-13T04:09:44Z,2022-12-13T04:09:44Z,"  Linux container technologies such as Docker and Singularity offer
encapsulated environments for easy execution of software. In high performance
computing, this is especially important for evolving and complex software
stacks with conflicting dependencies that must co-exist. Singularity Registry
HPC (""shpc"") was created as an effort to install containers in this environment
as modules, seamlessly allowing for typically hidden executables inside
containers to be presented to the user as commands, and as such significantly
simplifying the user experience. A remaining challenge, however, is deriving
the list of important executables in the container. In this work, we present
new automation and methods that allow for not only discovering new containers
in large community sets, but also deriving container entries with important
executables. With this work we have added over 8,000 containers from the
BioContainers community that can be maintained and updated by the software
automation over time. All software is publicly available on the GitHub
platform, and can be beneficial to container registries and infrastructure
providers for automatically generating container modules to lower the usage
entry barrier and improve user experience.
","['\nVanessa Sochat\n', '\nMatthieu Muffato\n', '\nAudrey Stott\n', '\nMarco De La Pierre\n', '\nGeorgia Stuart\n']",6 pages,,http://arxiv.org/abs/2212.07376v1,cs.OS,"['cs.OS', 'cs.CR']",,,[]
Unsafe at Any Copy: Name Collisions from Mixing Case Sensitivities,http://arxiv.org/abs/2211.16735v1,2022-11-30T04:48:07Z,2022-11-30T04:48:07Z,"  File name confusion attacks, such as malicious symbolic links and file
squatting, have long been studied as sources of security vulnerabilities.
However, a recently emerged type, i.e., case-sensitivity-induced name
collisions, has not been scrutinized. These collisions are introduced by
differences in name resolution under case-sensitive and case-insensitive file
systems or directories. A prominent example is the recent Git vulnerability
(CVE-2021-21300) which can lead to code execution on a victim client when it
clones a maliciously crafted repository onto a case-insensitive file system.
With trends including ext4 adding support for per-directory case-insensitivity
and the broad deployment of the Windows Subsystem for Linux, the prerequisites
for such vulnerabilities are increasingly likely to exist even in a single
system.
  In this paper, we make a first effort to investigate how and where the lack
of any uniform approach to handling name collisions leads to a diffusion of
responsibility and resultant vulnerabilities. Interestingly, we demonstrate the
existence of a range of novel security challenges arising from name collisions
and their inconsistent handling by low-level utilities and applications.
Specifically, our experiments show that utilities handle many name collision
scenarios unsafely, leaving the responsibility to applications whose developers
are unfortunately not yet aware of the threats. We examine three case studies
as a first step towards systematically understanding the emerging type of name
collision vulnerability.
","['\nAditya Basu\nThe Pennsylvania State University\n', '\nJohn Sampson\nThe Pennsylvania State University\n', '\nZhiyun Qian\nUniversity of California, Riverside\n', '\nTrent Jaeger\nThe Pennsylvania State University\n']","15 pages, 1 appendix, 2 tables, 12 figures",,http://arxiv.org/abs/2211.16735v1,cs.CR,"['cs.CR', 'cs.OS']",,,"['The Pennsylvania State University', 'The Pennsylvania State University', 'University of California, Riverside', 'The Pennsylvania State University']"
"DisTRaC: Accelerating High Performance Compute Processing for Temporary
  Data Storage",http://arxiv.org/abs/2212.03054v1,2022-12-06T15:31:20Z,2022-12-06T15:31:20Z,"  High Performance Compute (HPC) clusters often produce intermediate files as
part of code execution and message passing is not always possible to supply
data to these cluster jobs. In these cases, I/O goes back to central
distributed storage to allow cross node data sharing. These systems are often
high performance and characterised by their high cost per TB and sensitivity to
workload type such as being tuned to small or large file I/O. However, compute
nodes often have large amounts of RAM, so when dealing with intermediate files
where longevity or reliability of the system is not as important, local RAM
disks can be used to obtain performance benefits. In this paper we show how
this problem was tackled by creating a RAM block that could interact with the
object storage system Ceph, as well as creating a deployment tool to deploy
Ceph on HPC infrastructure effectively. This work resulted in a system that was
more performant than the central high performance distributed storage system
used at Diamond reducing I/O overhead and processing time for Savu, a
tomography data processing application, by 81.04% and 8.32% respectively.
","['\nGabryel Mason-Williams\n', '\nDave Bond\n', '\nMark Basham\n']","5 pages, 4 figures",,http://arxiv.org/abs/2212.03054v1,cs.DC,"['cs.DC', 'cs.OS']",,,[]
HyperEnclave: An Open and Cross-platform Trusted Execution Environment,http://arxiv.org/abs/2212.04197v1,2022-12-08T11:23:48Z,2022-12-08T11:23:48Z,"  A number of trusted execution environments (TEEs) have been proposed by both
academia and industry. However, most of them require specific hardware or
firmware changes and are bound to specific hardware vendors (such as Intel,
AMD, ARM, and IBM). In this paper, we propose HyperEnclave, an open and
cross-platform process-based TEE that relies on the widely-available
virtualization extension to create the isolated execution environment. In
particular, HyperEnclave is designed to support the flexible enclave operation
modes to fulfill the security and performance demands under various enclave
workloads. We provide the enclave SDK to run existing SGX programs on
HyperEnclave with little or no source code changes. We have implemented
HyperEnclave on commodity AMD servers and deployed the system in a
world-leading FinTech company to support real-world privacy-preserving
computations. The evaluation on both micro-benchmarks and application
benchmarks shows the design of HyperEnclave introduces only a small overhead.
","['\nYuekai Jia\n', '\nShuang Liu\n', '\nWenhao Wang\n', '\nYu Chen\n', '\nZhengde Zhai\n', '\nShoumeng Yan\n', '\nZhengyu He\n']",,"In 2022 USENIX Annual Technical Conference (USENIX ATC 22), pages
  437-454, Carlsbad, CA, July 2022. USENIX Association",http://arxiv.org/abs/2212.04197v1,cs.CR,"['cs.CR', 'cs.AR', 'cs.OS']",,,[]
"MeSHwA: The case for a Memory-Safe Software and Hardware Architecture
  for Serverless Computing",http://arxiv.org/abs/2211.08056v1,2022-11-15T11:15:46Z,2022-11-15T11:15:46Z,"  Motivated by developer productivity, serverless computing, and microservices
have become the de facto development model in the cloud. Microservices
decompose monolithic applications into separate functional units deployed
individually. This deployment model, however, costs CSPs a large infrastructure
tax of more than 25%. To overcome these limitations, CSPs shift workloads to
Infrastructure Processing Units (IPUs) like Amazon's Nitro or, complementary,
innovate by building on memory-safe languages and novel software abstractions.
  Based on these trends, we hypothesize a \arch providing a general-purpose
runtime environment to specialize functionality when needed and strongly
isolate components. To achieve this goal, we investigate building a single
address space OS or a multi-application library OS, possible hardware
implications, and demonstrate their capabilities, drawbacks and requirements.
The goal is to bring the advantages to all application workloads including
legacy and memory-unsafe applications, and analyze how hardware may improve the
efficiency and security.
","['\nAnjo Vahldiek-Oberwagner\n', '\nMona Vij\n']",Workshop On Resource Disaggregation and Serverless Computing (WORDS),,http://arxiv.org/abs/2211.08056v1,cs.OS,"['cs.OS', 'cs.CR']",,,[]
MUSTACHE: Multi-Step-Ahead Predictions for Cache Eviction,http://arxiv.org/abs/2211.02177v1,2022-11-03T23:10:21Z,2022-11-03T23:10:21Z,"  In this work, we propose MUSTACHE, a new page cache replacement algorithm
whose logic is learned from observed memory access requests rather than fixed
like existing policies. We formulate the page request prediction problem as a
categorical time series forecasting task. Then, our method queries the learned
page request forecaster to obtain the next $k$ predicted page memory references
to better approximate the optimal B\'el\'ady's replacement algorithm. We
implement several forecasting techniques using advanced deep learning
architectures and integrate the best-performing one into an existing
open-source cache simulator. Experiments run on benchmark datasets show that
MUSTACHE outperforms the best page replacement heuristic (i.e., exact LRU),
improving the cache hit ratio by 1.9% and reducing the number of reads/writes
required to handle cache misses by 18.4% and 10.3%.
","['\nGabriele Tolomei\n', '\nLorenzo Takanen\n', '\nFabio Pinelli\n']",,,http://arxiv.org/abs/2211.02177v1,cs.OS,"['cs.OS', 'cs.LG']",,,[]
"Rescuing the End-user systems from Vulnerable Applications using
  Virtualization Techniques",http://arxiv.org/abs/2211.02266v1,2022-11-04T05:31:48Z,2022-11-04T05:31:48Z,"  In systems owned by normal end-users, many times security attacks are mounted
by sneaking in malicious applications or exploiting existing software
vulnerabilities through security non-conforming actions of users.
Virtualization approaches can address this problem by providing a quarantine
environment for applications, malicious devices, and device drivers, which are
mostly used as entry points for security attacks. However, the existing methods
to provide quarantine environments using virtualization are not transparent to
the user, both in terms of application interface transparency and file system
transparency. Further, software configuration level solutions like remote
desktops and remote application access mechanisms combined with shared file
systems do not meet the user transparency and security requirements. We propose
qOS, a VM-based solution combined with certain OS extensions to meet the
security requirements of end-point systems owned by normal users, in a
transparent and efficient manner. We demonstrate the efficacy of qOS by
empirically evaluating the prototype implementation in the Linux+KVM system in
terms of efficiency, security, and user transparency.
","['\nVinayak Trivedi\n', '\nTushar Gurjar\n', '\nSumaiya Shaikh\n', '\nSaketh Maddamsetty\n', '\nDebadatta Mishra\n']","14 pages, 9 figures",,http://arxiv.org/abs/2211.02266v1,cs.CR,"['cs.CR', 'cs.OS']",,,[]
"The Digital Foundation Platform -- A Multi-layered SOA Architecture for
  Intelligent Connected Vehicle Operating System",http://arxiv.org/abs/2210.08818v1,2022-10-17T08:08:09Z,2022-10-17T08:08:09Z,"  Legacy AD/ADAS development from OEMs centers around developing functions on
ECUs using services provided by AUTOSAR Classic Platform (CP) to meet
automotive-grade and mass-production requirements. The AUTOSAR CP couples
hardware and software components statically and encounters challenges to
provide sufficient capacities for the processing of high-level intelligent
driving functions, whereas the new platform, AUTOSAR Adaptive Platform (AP) is
designed to support dynamically communication and provide richer services and
function abstractions for those resource-intensive (memory, CPU) applications.
Yet for both platforms, application development and the supporting system
software are still closely coupled together, and this makes application
development and the enhancement less scalable and flexible, resulting in longer
development cycles and slower time-to-market. This paper presents a
multi-layered, service-oriented intelligent driving operating system foundation
(we named it as Digital Foundation Platform) that provides abstractions for
easier adoption of heterogeneous computing hardware. It features a multi-layer
SOA software architecture with each layer providing adaptive service API at
north-bound for application developers. The proposed Digital Foundation
Platform (DFP) has significant advantages of decoupling hardware, operating
system core, middle-ware, functional software and application software
development. It provides SOA at multiple layers and enables application
developers from OEMs, to customize and develop new applications or enhance
existing applications with new features, either in autonomous domain or
intelligent cockpit domain, with great agility, and less code through
re-usability, and thus reduce the time-to-market.
","['\nDavid Yu\n', '\nAndy Xiao\n']",WCX SAE World Congress Experience 2022,,http://dx.doi.org/10.4271/2022-01-0107,cs.OS,['cs.OS'],10.4271/2022-01-0107,,[]
RIO: Order-Preserving and CPU-Efficient Remote Storage Access,http://arxiv.org/abs/2210.08934v1,2022-10-17T10:55:15Z,2022-10-17T10:55:15Z,"  Modern NVMe SSDs and RDMA networks provide dramatically higher bandwidth and
concurrency. Existing networked storage systems (e.g., NVMe over Fabrics) fail
to fully exploit these new devices due to inefficient storage ordering
guarantees. Severe synchronous execution for storage order in these systems
stalls the CPU and I/O devices and lowers the CPU and I/O performance
efficiency of the storage system.
  We present Rio, a new approach to the storage order of remote storage access.
The key insight in Rio is that the layered design of the software stack, along
with the concurrent and asynchronous network and storage devices, makes the
storage stack conceptually similar to the CPU pipeline. Inspired by the CPU
pipeline that executes out-of-order and commits in-order, Rio introduces the
I/O pipeline that allows internal out-of-order and asynchronous execution for
ordered write requests while offering intact external storage order to
applications. Together with merging consecutive ordered requests, these design
decisions make for write throughput and CPU efficiency close to that of
orderless requests.
  We implement Rio in Linux NVMe over RDMA stack, and further build a file
system named RioFS atop Rio. Evaluations show that Rio outperforms Linux NVMe
over RDMA and a state-of-the-art storage stack named Horae by two orders of
magnitude and 4.9 times on average in terms of throughput of ordered write
requests, respectively. RioFS increases the throughput of RocksDB by 1.9 times
and 1.5 times on average, against Ext4 and HoraeFS, respectively.
","['\nXiaojian Liao\n', '\nZhe Yang\n', '\nJiwu Shu\n']",,,http://dx.doi.org/10.1145/3552326.3567495,cs.OS,['cs.OS'],10.1145/3552326.3567495,,[]
"Cutting-plane algorithms for preemptive uniprocessor real-time
  scheduling problems",http://arxiv.org/abs/2210.11185v5,2022-10-20T11:57:36Z,2023-08-18T16:44:47Z,"  Fixed-point iteration algorithms like RTA (response time analysis) and QPA
(quick processor-demand analysis) are arguably the most popular ways of solving
schedulability problems for preemptive uniprocessor FP (fixed-priority) and EDF
(earliest-deadline-first) systems. Several IP (integer program) formulations
have also been proposed for these problems, but it is unclear whether the
algorithms for solving these formulations are related to RTA and QPA. By
discovering connections between the problems and the algorithms, we show that
RTA and QPA are, in fact, suboptimal cutting-plane algorithms for specific IP
formulations of FP and EDF schedulability, where optimality is defined with
respect to convergence rate. We propose optimal cutting-plane algorithms for
these IP formulations. We compare the new algorithms with RTA and QPA on large
collections of synthetic systems to gauge the improvement in convergence rates
and running times.
",['\nAbhishek Singh\n'],"45 pages, 5 figures. Changes in v2: new terms like CP-KERN are added
  to explain ideas more clearly; models include release jitter. Changes in v3:
  typos are fixed; evaluation section is modified so that it is in sync with
  public code. Changes in v4: algorithm, theoretical and empirical analyses are
  improved. Changes in v5: minor structural changes, acknowledgements added",,http://dx.doi.org/10.1007/s11241-023-09408-y,cs.OS,"['cs.OS', 'cs.DS', 'cs.SY', 'eess.SY', 'C.3; F.2.1; G.1.6; I.1.2']",10.1007/s11241-023-09408-y,,[]
"Femto-Containers: Lightweight Virtualization and Fault Isolation For
  Small Software Functions on Low-Power IoT Microcontrollers",http://arxiv.org/abs/2210.03432v1,2022-10-07T10:03:55Z,2022-10-07T10:03:55Z,"  Low-power operating system runtimes used on IoT microcontrollers typically
provide rudimentary APIs, basic connectivity and, sometimes, a (secure)
firmware update mechanism. In contrast, on less constrained hardware, networked
software has entered the age of serverless, microservices and agility. With a
view to bridge this gap, in the paper we design Femto-Containers, a new
middleware runtime which can be embedded on heterogeneous low-power IoT
devices. Femto-Containers enable the secure deployment, execution and isolation
of small virtual software functions on low-power IoT devices, over the network.
We implement Femto-Containers, and provide integration in RIOT, a popular open
source IoT operating system. We then evaluate the performance of our
implementation, which was formally verified for fault-isolation, guaranteeing
that RIOT is shielded from logic loaded and executed in a Femto-Container. Our
experiments on various popular microcontroller architectures (Arm Cortex-M,
ESP32 and RISC-V) show that Femto-Containers offer an attractive trade-off in
terms of memory footprint overhead, energy consumption, and security
","['\nKoen Zandberg\n', '\nEmmanuel Baccelli\n', '\nShenghao Yuan\n', '\nFrédéric Besson\n', '\nJean-Pierre Talpin\n']",arXiv admin note: text overlap with arXiv:2106.12553,"23rd ACM/IFIP International Middleware Conference (MIDDLEWARE
  2022)",http://arxiv.org/abs/2210.03432v1,cs.OS,['cs.OS'],,,[]
"Microsoft Defender Will Be Defended: MemoryRanger Prevents Blinding
  Windows AV",http://arxiv.org/abs/2210.02821v1,2022-10-06T11:25:05Z,2022-10-06T11:25:05Z,"  Windows OS is facing a huge rise in kernel attacks. An overview of popular
techniques that result in loading kernel drivers will be presented. One of the
key targets of modern threats is disabling and blinding Microsoft Defender, a
default Windows AV. The analysis of recent driver-based attacks will be given,
the challenge is to block them. The survey of user- and kernel-level attacks on
Microsoft Defender will be given. One of the recently published attackers
techniques abuses Mandatory Integrity Control (MIC) and Security Reference
Monitor (SRM) by modifying Integrity Level and Debug Privileges for the
Microsoft Defender via syscalls. However, this user-mode attack can be blocked
via the Windows 'trust labels' mechanism. The presented paper discovered the
internals of MIC and SRM, including the analysis of Microsoft Defender during
malware detection. We show how attackers can attack Microsoft Defender using a
kernel-mode driver. This driver modifies the fields of the Token structure
allocated for the Microsoft Defender application. The presented attack resulted
in disabling Microsoft Defender, without terminating any of its processes and
without triggering any Windows security features, such as PatchGuard. The
customized hypervisor-based solution named MemoryRanger was used to protect the
Windows Defender kernel structures. The experiments show that MemoryRanger
successfully restricts access to the sensitive kernel data from illegal access
attempts with affordable performance degradation.
","['\nDenis Pogonin\n', '\nIgor Korkin\n']","29 pages, 17 figures, 1 table, In Proceedings of the ADFSL 2022, USA",,http://arxiv.org/abs/2210.02821v1,cs.CR,"['cs.CR', 'cs.OS']",,,[]
"seL4 Microkernel for virtualization use-cases: Potential directions
  towards a standard VMM",http://arxiv.org/abs/2210.04328v2,2022-10-09T19:26:27Z,2022-12-28T06:53:56Z,"  Virtualization plays an essential role in providing security to computational
systems by isolating execution environments. Many software solutions, called
hypervisors, have been proposed to provide virtualization capabilities.
However, only a few were designed for being deployed at the edge of the
network, in devices with fewer computation resources when compared with servers
in the Cloud. Among the few lightweight software that can play the hypervisor
role, seL4 stands out by providing a small Trusted Computing Base and formally
verified components, enhancing its security. Despite today being more than a
decade with seL4 microkernel technology, its existing userland and tools are
still scarce and not very mature. Over the last few years, the main effort has
been put into increasing the maturity of the kernel itself and not the tools
and applications that can be hosted on top. Therefore, it currently lacks
proper support for a full-featured userland Virtual Machine Monitor, and the
existing one is quite fragmented. This article discusses the potential
directions to a standard VMM by presenting our view of design principles and
feature set needed. This article does not intend to define a standard VMM, we
intend to instigate this discussion through the seL4 community.
","['\nEverton de Matos\n', '\nMarkku Ahvenjärvi\n']",,,http://dx.doi.org/10.3390/electronics11244201,cs.CR,"['cs.CR', 'cs.OS']",10.3390/electronics11244201,,[]
Paging with Succinct Predictions,http://arxiv.org/abs/2210.02775v1,2022-10-06T09:26:34Z,2022-10-06T09:26:34Z,"  Paging is a prototypical problem in the area of online algorithms. It has
also played a central role in the development of learning-augmented algorithms
-- a recent line of research that aims to ameliorate the shortcomings of
classical worst-case analysis by giving algorithms access to predictions. Such
predictions can typically be generated using a machine learning approach, but
they are inherently imperfect. Previous work on learning-augmented paging has
investigated predictions on (i) when the current page will be requested again
(reoccurrence predictions), (ii) the current state of the cache in an optimal
algorithm (state predictions), (iii) all requests until the current page gets
requested again, and (iv) the relative order in which pages are requested.
  We study learning-augmented paging from the new perspective of requiring the
least possible amount of predicted information. More specifically, the
predictions obtained alongside each page request are limited to one bit only.
We consider two natural such setups: (i) discard predictions, in which the
predicted bit denotes whether or not it is ``safe'' to evict this page, and
(ii) phase predictions, where the bit denotes whether the current page will be
requested in the next phase (for an appropriate partitioning of the input into
phases). We develop algorithms for each of the two setups that satisfy all
three desirable properties of learning-augmented algorithms -- that is, they
are consistent, robust and smooth -- despite being limited to a one-bit
prediction per request. We also present lower bounds establishing that our
algorithms are essentially best possible.
","['\nAntonios Antoniadis\n', '\nJoan Boyar\n', '\nMarek Eliáš\n', '\nLene M. Favrholdt\n', '\nRuben Hoeksma\n', '\nKim S. Larsen\n', '\nAdam Polak\n', '\nBertrand Simon\n']",,,http://arxiv.org/abs/2210.02775v1,cs.LG,"['cs.LG', 'cs.DS', 'cs.OS']",,,[]
Ecovisor: A Virtual Energy System for Carbon-Efficient Applications,http://arxiv.org/abs/2210.04951v1,2022-10-10T18:41:56Z,2022-10-10T18:41:56Z,"  Cloud platforms' rapid growth is raising significant concerns about their
carbon emissions. To reduce emissions, future cloud platforms will need to
increase their reliance on renewable energy sources, such as solar and wind,
which have zero emissions but are highly unreliable. Unfortunately, today's
energy systems effectively mask this unreliability in hardware, which prevents
applications from optimizing their carbon-efficiency, or work done per kilogram
of carbon emitted. To address this problem, we design an ""ecovisor"", which
virtualizes the energy system and exposes software-defined control of it to
applications. An ecovisor enables each application to handle clean energy's
unreliability in software based on its own specific requirements. We implement
a small-scale ecovisor prototype that virtualizes a physical energy system to
enable software-based application-level i) visibility into variable grid
carbon-intensity and renewable generation and ii) control of server power usage
and battery charging/discharging. We evaluate the ecovisor approach by showing
how multiple applications can concurrently exercise their virtual energy system
in different ways to better optimize carbon-efficiency based on their specific
requirements compared to a general system-wide policy.
","['\nAbel Souza\n', '\nNoman Bashir\n', '\nJorge Murillo\n', '\nWalid Hanafy\n', '\nQianlin Liang\n', '\nDavid Irwin\n', '\nPrashant Shenoy\n']",,,http://arxiv.org/abs/2210.04951v1,cs.OS,"['cs.OS', 'cs.DC', 'cs.SE']",,,[]
HMM-V: Heterogeneous Memory Management for Virtualization,http://arxiv.org/abs/2209.13111v1,2022-09-27T01:56:19Z,2022-09-27T01:56:19Z,"  The memory demand of virtual machines (VMs) is increasing, while DRAM has
limited capacity and high power consumption. Non-volatile memory (NVM) is an
alternative to DRAM, but it has high latency and low bandwidth. We observe that
the VM with heterogeneous memory may incur up to a $1.5\times$ slowdown
compared to a DRAM VM, if not managed well. However, none of the
state-of-the-art heterogeneous memory management designs are customized for
virtualization on a real system.
  In this paper, we propose HMM-V, a Heterogeneous Memory Management system for
Virtualization. HMM-V automatically determines page hotness and migrates pages
between DRAM and NVM to achieve performance close to the DRAM system. First,
HMM-V tracks memory accesses through page table manipulation, but reduces the
cost by leveraging Intel page-modification logging (PML) and a multi-level
queue. Second, HMM-V quantifies the ``temperature'' of page and determines the
hot set with bucket-sorting. HMM-V then efficiently migrates pages with minimal
access pause and handles dirty pages with the assistance of PML. Finally, HMM-V
provides pooling management to balance precious DRAM across multiple VMs to
maximize utilization and overall performance. HMM-V is implemented on a real
system with Intel Optane DC persistent memory. The four-VM co-running results
show that HMM-V outperforms NUMA balancing and hardware management (Intel
Optane memory mode) by $51\%$ and $31\%$, respectively.
","['\nSai sha\nPeking University\n', '\nChuandong Li\nPeking University\n', '\nYingwei Luo\nPeking University\n', '\nXiaolin Wang\nPeking University\n', '\nZhenlin Wang\nMichigan Technological University\n']",,,http://arxiv.org/abs/2209.13111v1,cs.OS,['cs.OS'],,,"['Peking University', 'Peking University', 'Peking University', 'Peking University', 'Michigan Technological University']"
"Rapid Recovery of Program Execution Under Power Failures for Embedded
  Systems with NVM",http://arxiv.org/abs/2209.08826v1,2022-09-19T08:16:09Z,2022-09-19T08:16:09Z,"  After power is switched on, recovering the interrupted program from the
initial state can cause negative impact. Some programs are even unrecoverable.
To rapid recovery of program execution under power failures, the execution
states of checkpoints are backed up by NVM under power failures for embedded
systems with NVM. However, frequent checkpoints will shorten the lifetime of
the NVM and incur significant write overhead. In this paper, the technique of
checkpoint setting triggered by function calls is proposed to reduce the write
on NVM. The evaluation results show an average of 99.8% and 80.5$% reduction on
NVM backup size for stack backup, compared to the log-based method and
step-based method. In order to better achieve this, we also propose
pseudo-function calls to increase backup points to reduce recovery costs, and
exponential incremental call-based backup methods to reduce backup costs in the
loop. To further avoid the content on NVM is cluttered and out of NVM, a method
to clean the contents on the NVM that are useless for restoration is proposed.
Based on aforementioned problems and techniques, the recovery technology is
proposed, and the case is used to analyze how to recover rapidly under
different power failures.
","['\nMin Jia\nEast China Normal University School of Computer Science and Technology, China\n', '\nEdwin Hsing. -M. Sha\nEast China Normal University School of Computer Science and Technology, China\n', '\nQingfeng Zhuge\nEast China Normal University School of Computer Science and Technology, China\n', '\nRui Xu\nEast China Normal University School of Computer Science and Technology, China\n', '\nShouzhen Gu\nEast China Normal University Software Engineering Institute, China\n']","This paper has been accepted for publication to Microprocessors and
  Microsystems in March 15, 2021",,http://arxiv.org/abs/2209.08826v1,cs.OS,"['cs.OS', 'cs.SE']",,,"['East China Normal University School of Computer Science and Technology, China', 'East China Normal University School of Computer Science and Technology, China', 'East China Normal University School of Computer Science and Technology, China', 'East China Normal University School of Computer Science and Technology, China', 'East China Normal University Software Engineering Institute, China']"
Multi-level Explanation of Deep Reinforcement Learning-based Scheduling,http://arxiv.org/abs/2209.09645v1,2022-09-18T13:22:53Z,2022-09-18T13:22:53Z,"  Dependency-aware job scheduling in the cluster is NP-hard. Recent work shows
that Deep Reinforcement Learning (DRL) is capable of solving it. It is
difficult for the administrator to understand the DRL-based policy even though
it achieves remarkable performance gain. Therefore the complex model-based
scheduler is not easy to gain trust in the system where simplicity is favored.
In this paper, we give the multi-level explanation framework to interpret the
policy of DRL-based scheduling. We dissect its decision-making process to job
level and task level and approximate each level with interpretable models and
rules, which align with operational practices. We show that the framework gives
the system administrator insights into the state-of-the-art scheduler and
reveals the robustness issue in regards to its behavior pattern.
","['\nShaojun Zhang\n', '\nChen Wang\n', '\nAlbert Zomaya\n']",Accepted in the MLSys'22 Workshop on Cloud Intelligence / AIOps,,http://arxiv.org/abs/2209.09645v1,cs.DC,"['cs.DC', 'cs.AI', 'cs.OS']",,,[]
SFS: Smart OS Scheduling for Serverless Functions,http://arxiv.org/abs/2209.01709v2,2022-09-04T23:38:25Z,2022-09-07T03:57:32Z,"  Serverless computing enables a new way of building and scaling cloud
applications by allowing developers to write fine-grained serverless or cloud
functions. The execution duration of a cloud function is typically
short-ranging from a few milliseconds to hundreds of seconds. However, due to
resource contentions caused by public clouds' deep consolidation, the function
execution duration may get significantly prolonged and fail to accurately
account for the function's true resource usage. We observe that the function
duration can be highly unpredictable with huge amplification of more than 50x
for an open-source FaaS platform (OpenLambda). Our experiments show that the OS
scheduling policy of cloud functions' host server can have a crucial impact on
performance. The default Linux scheduler, CFS (Completely Fair Scheduler),
being oblivious to workloads, frequently context-switches short functions,
causing a turnaround time that is much longer than their service time.
  We propose SFS (Smart Function Scheduler),which works entirely in the user
space and carefully orchestrates existing Linux FIFO and CFS schedulers to
approximate Shortest Remaining Time First (SRTF). SFS uses two-level scheduling
that seamlessly combines a new FILTER policy with Linux CFS, to trade off
increased duration of long functions for significant performance improvement
for short functions. We implement {\proj} in the Linux user space and port it
to OpenLambda. Evaluation results show that SFS significantly improves short
functions' duration with a small impact on relatively longer functions,
compared to CFS.
","['\nYuqi Fu\n', '\nLi Liu\n', '\nHaoliang Wang\n', '\nYue Cheng\n', '\nSongqing Chen\n']","SFS has been accepted to appear in SC'22 and nominated as a best
  student paper award finalist",,http://arxiv.org/abs/2209.01709v2,cs.OS,['cs.OS'],,,[]
Towards Adaptive Storage Views in Virtual Memory,http://arxiv.org/abs/2209.01635v2,2022-09-04T14:55:41Z,2022-12-06T15:27:44Z,"  Traditionally, DBMSs separate their storage layer from their indexing layer.
While the storage layer physically materializes the database and provides
low-level access methods to it, the indexing layer on top enables a faster
locating of searched-for entries. While this clearly separates concerns, it
also adds a level of indirection to the already complex execution path. In this
work, we propose an alternative design: Instead of conservatively separating
both layers, we naturally fuse them by integrating an adaptive coarse-granular
indexing scheme directly into the storage layer. We do so by utilizing tools of
the virtual memory management subsystem provided by the OS: On the lowest
level, we materialize the database content in form of physical main memory. On
top of that, we allow the creation of arbitrarily many virtual memory storage
views that map to subsets of the database having certain properties of
interest. This creation happens fully adaptively as a side-product of query
processing. To speed up query answering, we route each query automatically to
the most fitting virtual view(s). By this, we naturally index the storage layer
in its core and gradually improve the provided scan performance.
","['\nFelix Schuhknecht\n', '\nJustus Henneberg\n']",,,http://arxiv.org/abs/2209.01635v2,cs.DB,"['cs.DB', 'cs.OS']",,,[]
"RunPHI: Enabling Mixed-criticality Containers via Partitioning
  Hypervisors in Industry 4.0",http://arxiv.org/abs/2209.01843v1,2022-09-05T09:06:39Z,2022-09-05T09:06:39Z,"  Orchestration systems are becoming a key component to automatically manage
distributed computing resources in many fields with criticality requirements
like Industry 4.0 (I4.0). However, they are mainly linked to OS-level
virtualization, which is known to suffer from reduced isolation. In this paper,
we propose RunPHI with the aim of integrating partitioning hypervisors, as a
solution for assuring strong isolation, with OS-level orchestration systems.
The purpose is to enable container orchestration in mixed-criticality systems
with isolation requirements through partitioned containers.
","['\nMarco Barletta\n', '\nMarcello Cinque\n', '\nLuigi De Simone\n', '\nRaffaele Della Corte\n', '\nGiorgio Farina\n', '\nDaniele Ottaviano\n']","2 pages, accepted for publication in Proc. ISSREW, 2022",,http://arxiv.org/abs/2209.01843v1,cs.DC,"['cs.DC', 'cs.OS']",,,[]
Ellipsis: Towards Efficient System Auditing for Real-Time Systems,http://arxiv.org/abs/2208.02699v1,2022-08-04T14:54:09Z,2022-08-04T14:54:09Z,"  System auditing is a powerful tool that provides insight into the nature of
suspicious events in computing systems, allowing machine operators to detect
and subsequently investigate security incidents. While auditing has proven
invaluable to the security of traditional computers, existing audit frameworks
are rarely designed with consideration for Real-Time Systems (RTS). The
transparency provided by system auditing would be of tremendous benefit in a
variety of security-critical RTS domains, (e.g., autonomous vehicles); however,
if audit mechanisms are not carefully integrated into RTS, auditing can be
rendered ineffectual and violate the real-world temporal requirements of the
RTS.
  In this paper, we demonstrate how to adapt commodity audit frameworks to RTS.
Using Linux Audit as a case study, we first demonstrate that the volume of
audit events generated by commodity frameworks is unsustainable within the
temporal and resource constraints of real-time (RT) applications. To address
this, we present Ellipsis, a set of kernel-based reduction techniques that
leverage the periodic repetitive nature of RT applications to aggressively
reduce the costs of system-level auditing. Ellipsis generates succinct
descriptions of RT applications' expected activity while retaining a detailed
record of unexpected activities, enabling analysis of suspicious activity while
meeting temporal constraints. Our evaluation of Ellipsis, using ArduPilot (an
open-source autopilot application suite) demonstrates up to 93% reduction in
audit log generation.
","['\nAyoosh Bansal\n', '\nAnant Kandikuppa\n', '\nChien-Ying Chen\n', '\nMonowar Hasan\n', '\nAdam Bates\n', '\nSibin Mohan\n']",Extended version of a paper accepted at ESORICS 2022,,http://arxiv.org/abs/2208.02699v1,cs.CR,"['cs.CR', 'cs.OS', 'D.4.6; C.3']",,,[]
3PO: Programmed Far-Memory Prefetching for Oblivious Applications,http://arxiv.org/abs/2207.07688v1,2022-07-15T18:17:48Z,2022-07-15T18:17:48Z,"  Using memory located on remote machines, or far memory, as a swap space is a
promising approach to meet the increasing memory demands of modern datacenter
applications. Operating systems have long relied on prefetchers to mask the
increased latency of fetching pages from swap space to main memory.
Unfortunately, with traditional prefetching heuristics, performance still
degrades when applications use far memory. In this paper we propose a new
prefetching technique for far-memory applications. We focus our efforts on
memory-intensive, oblivious applications whose memory access patterns are
independent of their inputs, such as matrix multiplication. For this class of
applications we observe that we can perfectly prefetch pages without relying on
heuristics. However, prefetching perfectly without requiring significant
application modifications is challenging.
  In this paper we describe the design and implementation of 3PO, a system that
provides pre-planned prefetching for general oblivious applications. We
demonstrate that 3PO can accelerate applications, e.g., running them 30-150%
faster than with Linux's prefetcher with 20% local memory. We also use 3PO to
understand the fundamental software overheads of prefetching in a paging-based
system, and the minimum performance penalty that they impose when we run
applications under constrained local memory.
","['\nChristopher Branner-Augmon\n', '\nNarek Galstyan\n', '\nSam Kumar\n', '\nEmmanuel Amaro\n', '\nAmy Ousterhout\n', '\nAurojit Panda\n', '\nSylvia Ratnasamy\n', '\nScott Shenker\n']",14 pages,,http://arxiv.org/abs/2207.07688v1,cs.OS,['cs.OS'],,,[]
"Learnings from an Under the Hood Analysis of an Object Storage Node IO
  Stack",http://arxiv.org/abs/2207.01849v1,2022-07-05T07:27:04Z,2022-07-05T07:27:04Z,"  Conventional object-stores are built on top of traditional OS storage stack,
where I/O requests typically transfers through multiple hefty and redundant
layers. The complexity of object management has grown dramatically with the
ever increasing requirements of performance, consistency and fault-tolerance
from storage subsystems. Simply stated, more number of intermediate layers are
encountered in the I/O data path, with each passing layer adding its own syntax
and semantics. Thereby increasing the overheads of request processing. In this
paper, through comprehensive under-the-hood analysis of an object-storage node,
we characterize the impact of object-store (and user-application) workloads on
the OS I/O stack and its subsequent rippling effect on the underlying
object-storage devices (OSD). We observe that the legacy architecture of the OS
based I/O storage stack coupled with complex data management policies leads to
a performance mismatch between what an end-storage device is capable of
delivering and what it actually delivers in a production environment.
Therefore, the gains derived from developing faster storage devices is often
nullified. These issues get more pronounced in highly concurrent and
multiplexed cloud environments. Owing to the associated issues of
object-management and the vulnerabilities of the OS I/O software stacks, we
discuss the potential of a new class of storage devices, known as
Object-Drives. Samsung Key-Value SSD (KV-SSD) [1] and Seagate Kinetic Drive [2]
are classic industrial implementations of object-drives, where host data
management functionalities can be offloaded to the storage device. This leads
towards the simplification of the over-all storage stack. Based on our
analysis, we believe object-drives can alleviate object-stores from highly
taxing overheads of data management with 20-38% time-savings over traditional
Operating Systems (OS) stack.
","['\nPratik Mishra\n', '\nRekha Pitchumani\n', '\nYang Suk Kee\n']",,,http://dx.doi.org/10.48550/arXiv.2207.01849,cs.DB,"['cs.DB', 'cs.AR', 'cs.DC', 'cs.OS', 'cs.PF']",10.48550/arXiv.2207.01849,,[]
Implementation of SquashFS Support in U-Boot,http://arxiv.org/abs/2206.12751v1,2022-06-25T23:56:45Z,2022-06-25T23:56:45Z,"  U-Boot is a notorious bootloader and Open Source project. This work had as
objective adding support for the SquashFS filesystem to U-Boot and the support
developed was submitted as a contribution to the project. The bootloader is
responsible, in this context, for loading the kernel and the device tree blob
into RAM. It needs to be capable of reading a storage device's partition
formatted with a specific filesystem type. Adding this support allows U-Boot to
read from SquashFS partitions. The source code was submitted to U-Boot's
mailing list through a series of patches to be reviewed by one of the project's
maintainer. Once it gets merged, the support will be used and modified by
U-Boot's international community.
","['\nMariana Villarim\n', '\nJoão Marcos Costa\n', '\nDiomadson Belfort\n']",,,http://arxiv.org/abs/2206.12751v1,cs.OS,['cs.OS'],,,[]
Multilevel Bidirectional Cache Filter,http://arxiv.org/abs/2206.13367v1,2022-06-27T15:28:44Z,2022-06-27T15:28:44Z,"  Modern caches are often required to handle a massive amount of data, which
exceeds the amount of available memory; thus, hybrid caches, specifically
DRAM/SSD combination, become more and more prevalent. In such environments, in
addition to the classical hit-ratio target, saving writes to the second-level
cache is a dominant factor to avoid write amplification and wear out, two
notorious phenomena of SSD.
  This paper presents BiDiFilter, a novel multilevel caching scheme that
controls demotions and promotions between cache levels using a frequency sketch
filter. Further, it splits the higher cache level into two areas to keep the
most recent and the most frequent items close to the user.
  We conduct an extensive evaluation over real-world traces, comparing to
previous multilevel policies. We show that using our mechanism yields an x10
saving of writes in almost all cases and often improving latencies by up to
20%.
","['\nOhad Eytan\n', '\nRoy Friedman\n']",,,http://arxiv.org/abs/2206.13367v1,cs.OS,['cs.OS'],,,[]
Software Mitigation of RISC-V Spectre Attacks,http://arxiv.org/abs/2206.04507v2,2022-06-09T13:47:18Z,2023-11-06T22:34:46Z,"  Speculative attacks are still an active threat today that, even if initially
focused on the x86 platform, reach across all modern hardware architectures.
RISC-V is a newly proposed open instruction set architecture that has seen
traction from both the industry and academia in recent years. In this paper we
focus on the RISC-V cores where speculation is enabled and, as we show, where
Spectre attacks are as effective as on x86. Even though RISC-V hardware
mitigations were proposed in the past, they have not yet passed the prototype
phase. Instead, we propose low-overhead software mitigations for Spectre-BTI,
inspired from those used on the x86 architecture, and for Spectre-RSB, to our
knowledge the first such mitigation to be proposed. We show that these
mitigations work in practice and that they can be integrated in the LLVM
toolchain. For transparency and reproducibility, all our programs and data are
made publicly available online.
","['\nRuxandra Bălucea\n', '\nPaul Irofti\n']",,,http://arxiv.org/abs/2206.04507v2,cs.CR,"['cs.CR', 'cs.OS']",,,[]
"Is Kernel Code Different From Non-Kernel Code? A Case Study of BSD
  Family Operating Systems",http://arxiv.org/abs/2206.05616v1,2022-06-11T21:18:16Z,2022-06-11T21:18:16Z,"  Code churn and code velocity describe the evolution of a code base. Current
research quantifies and studies code churn and velocity at a high level of
abstraction, often at the overall project level or even at the level of an
entire company. We argue that such an approach ignores noticeable differences
among the subsystems of large projects. We conducted an exploratory study on
four BSD family operating systems: DragonFlyBSD, FreeBSD, NetBSD, and OpenBSD.
We mine 797,879 commits to characterize code churn in terms of the annual
growth rate, commit types, change type ratio, and size taxonomy of commits for
different subsystems (kernel, non-kernel, and mixed). We also investigate
differences among various code review periods, i.e., time-to-first-response,
time-to-accept, and time-to-merge, as indicators of code velocity. Our study
provides empirical evidence that quantifiable evolutionary code characteristics
at a global system scope fail to take into account significant individual
differences that exist at a subsystem level. We found that while there exist
similarities in the code base growth rate and distribution of commit types
(neutral, additive, and subtractive) across BSD subsystems, (a) most commits
contain kernel or non-kernel code exclusively, (b) kernel commits are larger
than non-kernel commits, and (c) code reviews for kernel code take longer than
non-kernel code.
","['\nGunnar Kudrjavets\n', '\nJeff Thomas\n', '\nNachiappan Nagappan\n', '\nAyushi Rastogi\n']","13 pages. To be published in 38th IEEE International Conference on
  Software Maintenance and Evolution (ICSME 2022), Oct 3-7, 2022, Limassol,
  Cyprus",,http://dx.doi.org/10.1109/ICSME55016.2022.00027,cs.SE,"['cs.SE', 'cs.OS']",10.1109/ICSME55016.2022.00027,,[]
Unikernel Linux (UKL),http://arxiv.org/abs/2206.00789v2,2022-06-01T22:45:12Z,2023-06-22T19:13:54Z,"  This paper presents Unikernel Linux (UKL), a path toward integrating
unikernel optimization techniques in Linux, a general purpose operating system.
UKL adds a configuration option to Linux allowing for a single, optimized
process to link with the kernel directly, and run at supervisor privilege. This
UKL process does not require application source code modification, only a
re-link with our, slightly modified, Linux kernel and glibc. Unmodified
applications show modest performance gains out of the box, and developers can
further optimize applications for more significant gains (e.g. 26% throughput
improvement for Redis). UKL retains support for co-running multiple user level
processes capable of communicating with the UKL process using standard IPC. UKL
preserves Linux's battle-tested codebase, community, and ecosystem of tools,
applications, and hardware support. UKL runs both on bare-metal and virtual
servers and supports multi-core execution. The changes to the Linux kernel are
modest (1250 LOC).
","['\nAli Raza\nBoston University\n', '\nThomas Unger\nBoston University\n', '\nMatthew Boyd\nMIT CSAIL\n', '\nEric Munson\nBoston University\n', '\nParul Sohal\nBoston University\n', '\nUlrich Drepper\nRed Hat\n', '\nRichard Jones\nRed Hat\n', '\nDaniel Bristot de Oliveira\nRed Hat\n', '\nLarry Woodman\nRed Hat\n', '\nRenato Mancuso\nBoston University\n', '\nJonathan Appavoo\nBoston University\n', '\nOrran Krieger\nBoston University\n']","Added more results in the evaluation section. Improved overall
  writing and added diagrams to explain the architecture","Proceedings of the Eighteenth European Conference on Computer
  Systems (EuroSys 23), May 2023, Pages 590 - 605",http://dx.doi.org/10.1145/3552326.3587458,cs.OS,['cs.OS'],10.1145/3552326.3587458,,"['Boston University', 'Boston University', 'MIT CSAIL', 'Boston University', 'Boston University', 'Red Hat', 'Red Hat', 'Red Hat', 'Red Hat', 'Boston University', 'Boston University', 'Boston University']"
Understanding NVMe Zoned Namespace (ZNS) Flash SSD Storage Devices,http://arxiv.org/abs/2206.01547v1,2022-06-03T12:54:55Z,2022-06-03T12:54:55Z,"  The standardization of NVMe Zoned Namespaces (ZNS) in the NVMe 2.0
specification presents a unique new addition to storage devices. Unlike
traditional SSDs, where the flash media management idiosyncrasies are hidden
behind a flash translation layer (FTL) inside the device, ZNS devices push
certain operations regarding data placement and garbage collection out from the
device to the host. This allows the host to achieve more optimal data placement
and predictable garbage collection overheads, along with lower device write
amplification. Thus, additionally increasing flash media lifetime. As a result,
ZNS devices are gaining significant attention in the research community.
  However, with the current software stack there are numerous ways of
integrating ZNS devices into a host system. In this work, we begin to
systematically analyze the integration options, report on the current software
support for ZNS devices in the Linux Kernel, and provide an initial set of
performance measurements. Our main findings show that larger I/O sizes are
required to saturate the ZNS device bandwidth, and configuration of the I/O
scheduler can provide workload dependent performance gains, requiring careful
consideration of ZNS integration and configuration depending on the application
workload and its access patterns. Our dataset and code are available at https:
//github.com/nicktehrany/ZNS-Study.
","['\nNick Tehrany\n', '\nAnimesh Trivedi\n']",,,http://arxiv.org/abs/2206.01547v1,cs.OS,"['cs.OS', 'cs.PF', 'D.4.2']",,,[]
HyperDbg: Reinventing Hardware-Assisted Debugging (Extended Version),http://arxiv.org/abs/2207.05676v2,2022-05-29T09:19:27Z,2022-09-02T15:26:13Z,"  Software analysis, debugging, and reverse engineering have a crucial impact
in today's software industry. Efficient and stealthy debuggers are especially
relevant for malware analysis. However, existing debugging platforms fail to
address a transparent, effective, and high-performance low-level debugger due
to their detectable fingerprints, complexity, and implementation restrictions.
In this paper, we present HyperDbg, a new hypervisor-assisted debugger for
high-performance and stealthy debugging of user and kernel applications. To
accomplish this, HyperDbg relies on state-of-the-art hardware features
available in today's CPUs, such as VT-x and extended page tables. In contrast
to other widely used existing debuggers, we design HyperDbg using a custom
hypervisor, making it independent of OS functionality or API. We propose
hardware-based instruction-level emulation and OS-level API hooking via
extended page tables to increase the stealthiness. Our results of the dynamic
analysis of 10,853 malware samples show that HyperDbg's stealthiness allows
debugging on average 22% and 26% more samples than WinDbg and x64dbg,
respectively. Moreover, in contrast to existing debuggers, HyperDbg is not
detected by any of the 13 tested packers and protectors. We improve the
performance over other debuggers by deploying a VMX-compatible script engine,
eliminating unnecessary context switches. Our experiment on three concrete
debugging scenarios shows that compared to WinDbg as the only kernel debugger,
HyperDbg performs step-in, conditional breaks, and syscall recording, 2.98x,
1319x, and 2018x faster, respectively. We finally show real-world applications,
such as a 0-day analysis, structure reconstruction for reverse engineering,
software performance analysis, and code-coverage analysis.
","['\nMohammad Sina Karvandi\n', '\nMohammadHossein Gholamrezaei\n', '\nSaleh Khalaj Monfared\n', '\nSoroush Meghdadizanjani\n', '\nBehrooz Abbassi\n', '\nAli Amini\n', '\nReza Mortazavi\n', '\nSaeid Gorgin\n', '\nDara Rahmati\n', '\nMichael Schwarz\n']",,,http://arxiv.org/abs/2207.05676v2,cs.CR,"['cs.CR', 'cs.AR', 'cs.OS']",,,[]
Analyzing FreeRTOS Scheduling Behaviors with the Spin Model Checker,http://arxiv.org/abs/2205.07480v2,2022-05-16T07:04:11Z,2022-10-07T02:42:58Z,"  FreeRTOS is a real-time operating system with configurable scheduling
policies. Its portability and configurability make FreeRTOS one of the most
popular real-time operating systems for embedded devices. We formally analyze
the FreeRTOS scheduler on ARM Cortex-M4 processor in this work. Specifically,
we build a formal model for the FreeRTOS ARM Cortex-M4 port and apply model
checking to find errors in our models for FreeRTOS example applications.
Intriguingly, several errors are found in our application models under
different scheduling policies. In order to confirm our findings, we modify
application programs distributed by FreeRTOS and reproduce assertion failures
on the STM32F429I-DISC1 board.
","['\nChen-Kai Lin\n', '\nBow-Yaw Wang\n']",,,http://arxiv.org/abs/2205.07480v2,cs.FL,"['cs.FL', 'cs.OS']",,,[]
rgpdOS: GDPR Enforcement By The Operating System,http://arxiv.org/abs/2205.10929v2,2022-05-22T20:50:20Z,2022-05-30T11:36:09Z,"  The General Data Protection Regulation (GDPR) forces IT companies to comply
with a number of principles when dealing with European citizens' personal data.
Non-compliant companies are exposed to penalties which may represent up to 4%
of their turnover. Currently, it is very hard for companies driven by personal
data to make their applications GDPR-compliant, especially if those
applications were developed before the GDPR was established. We present rgpdOS,
a GDPR-aware operating system that aims to bring GDPR-compliance to every
application, while requiring minimal changes to application code.
","['\nAlain Tchana\n', '\nRaphael Colin\n', '\nAdrien Le Berre\n', '\nVincent Berger\n', '\nBenoit Combemale\n', '\nNatacha Crooks\n', '\nLudovic Pailler\n']",,,http://arxiv.org/abs/2205.10929v2,cs.OS,"['cs.OS', 'cs.CR']",,,[]
Protecting File Activities via Deception for ARM TrustZone,http://arxiv.org/abs/2205.10963v2,2022-05-22T23:55:23Z,2022-05-24T18:57:20Z,"  A TrustZone TEE often invokes an external filesystem. While filedata can be
encrypted, the revealed file activities can leak secrets. To hide the file
activities from the filesystem and its OS, we propose Enigma, a deception-based
defense injecting sybil file activities as the cover of the actual file
activities.
  Enigma contributes three new designs. (1) To make the deception credible, the
TEE generates sybil calls by replaying file calls from the TEE code under
protection. (2) To make sybil activities cheap, the TEE requests the OS to run
K filesystem images simultaneously. Concealing the disk, the TEE backs only one
image with the actual disk while backing other images by only storing their
metadata. (3) To protect filesystem image identities, the TEE shuffles the
images frequently, preventing the OS from observing any image for long.
  Enigma works with unmodified filesystems shipped withLinux. On a low-cost Arm
SoC with EXT4 and F2FS, our system can concurrently run as many as 50
filesystem images with 1% of disk overhead per additional image. Compared to
common obfuscation for hiding addresses in a flat space, Enigma hides file
activities with richer semantics. Its cost is lower by one order of magnitude
while achieving the same level of probabilistic security guarantees.
","['\nLiwei Guo\n', '\nKaiyang Zhao\n', '\nYiying Zhang\n', '\nFelix Xiaozhu Lin\n']",Under submission,,http://arxiv.org/abs/2205.10963v2,cs.CR,"['cs.CR', 'cs.OS']",,,[]
The Next-Generation OS Process Abstraction,http://arxiv.org/abs/2205.12270v1,2022-05-24T15:22:16Z,2022-05-24T15:22:16Z,"  Operating Systems are built upon a set of abstractions to provide resource
management and programming APIs for common functionality, such as
synchronization, communication, protection, and I/O. The process abstraction is
the bridge across these two aspects; unsurprisingly, research efforts pay
particular attention to the process abstraction, aiming at enhancing security,
improving performance, and supporting hardware innovations. However, given the
intrinsic difficulties to implement modifications at the OS level, recent
endeavors have not yet been widely adopted in production-oriented OSes. Still,
we believe the current hardware evolution and new application requirements
provide favorable conditions to change this trend. This paper evaluates recent
research on OS process features identifying potential evolution paths. We
derive a set of relevant process characteristics, and propose how to extend
them as to benefit OSes and applications.
","['\nRodrigo Siqueira\n', '\nNelson Lago\n', '\nFabio Kon\n', '\nDejan Milojičić\n']",,,http://arxiv.org/abs/2205.12270v1,cs.OS,"['cs.OS', 'cs.AR']",,,[]
"Hyperion: A Case for Unified, Self-Hosting, Zero-CPU Data-Processing
  Units (DPUs)",http://arxiv.org/abs/2205.08882v2,2022-05-18T12:12:05Z,2022-08-31T15:42:00Z,"  Since the inception of computing, we have been reliant on CPU-powered
architectures. However, today this reliance is challenged by manufacturing
limitations (CMOS scaling), performance expectations (stalled clocks, Turing
tax), and security concerns (microarchitectural attacks). To re-imagine our
computing architecture, in this work we take a more radical but pragmatic
approach and propose to eliminate the CPU with its design baggage, and
integrate three primary pillars of computing, i.e., networking, storage, and
computing, into a single, self-hosting, unified CPU-free Data Processing Unit
(DPU) called Hyperion. In this paper, we present the case for Hyperion, its
design choices, initial work-in-progress details, and seek feedback from the
systems community.
","['\nMarco Spaziani Brunella\n', '\nMarco Bonola\n', '\nAnimesh Trivedi\n']",,,http://arxiv.org/abs/2205.08882v2,cs.AR,"['cs.AR', 'cs.DC', 'cs.OS', 'cs.PL', 'C.5; D.3; D.4; B.4']",,,[]
Private delegated computations using strong isolation,http://arxiv.org/abs/2205.03322v1,2022-05-06T15:56:40Z,2022-05-06T15:56:40Z,"  Sensitive computations are now routinely delegated to third-parties. In
response, Confidential Computing technologies are being introduced to
microprocessors, offering a protected processing environment, which we
generically call an isolate, providing confidentiality and integrity guarantees
to code and data hosted within -- even in the face of a privileged attacker.
Isolates, with an attestation protocol, permit remote third-parties to
establish a trusted ""beachhead"" containing known code and data on an otherwise
untrusted machine. Yet, the rise of these technologies introduces many new
problems, including: how to ease provisioning of computations safely into
isolates; how to develop distributed systems spanning multiple classes of
isolate; and what to do about the billions of ""legacy"" devices without support
for Confidential Computing?
  Tackling the problems above, we introduce Veracruz, a framework that eases
the design and implementation of complex privacy-preserving, collaborative,
delegated computations among a group of mutually mistrusting principals.
Veracruz supports multiple isolation technologies and provides a common
programming model and attestation protocol across all of them, smoothing
deployment of delegated computations over supported technologies. We
demonstrate Veracruz in operation, on private in-cloud object detection on
encrypted video streaming from a video camera. In addition to supporting
hardware-backed isolates -- like AWS Nitro Enclaves and Arm Confidential
Computing Architecture Realms -- Veracruz also provides pragmatic ""software
isolates"" on Armv8-A devices without hardware Confidential Computing
capability, using the high-assurance seL4 microkernel and our IceCap framework.
","['\nMathias Brossard\n', '\nGuilhem Bryant\n', '\nBasma El Gaabouri\n', '\nXinxin Fan\n', '\nAlexandre Ferreira\n', '\nEdmund Grimley-Evans\n', '\nChristopher Haster\n', '\nEvan Johnson\n', '\nDerek Miller\n', '\nFan Mo\n', '\nDominic P. Mulligan\n', '\nNick Spinale\n', '\nEric van Hensbergen\n', '\nHugo J. M. Vincent\n', '\nShale Xiong\n']",,,http://arxiv.org/abs/2205.03322v1,cs.CR,"['cs.CR', 'cs.OS', 'cs.PL']",,,[]
Towards Porting Operating Systems with Program Synthesis,http://arxiv.org/abs/2204.07167v2,2022-04-15T14:42:24Z,2022-09-22T06:33:47Z,"  The end of Moore's Law has ushered in a diversity of hardware not seen in
decades. Operating system (and system software) portability is accordingly
becoming increasingly critical. Simultaneously, there has been tremendous
progress in program synthesis. We set out to explore the feasibility of using
modern program synthesis to generate the machine-dependent parts of an
operating system. Our ultimate goal is to generate new ports automatically from
descriptions of new machines. One of the issues involved is writing
specifications, both for machine-dependent operating system functionality and
for instruction set architectures. We designed two domain-specific languages:
Alewife for machine-independent specifications of machine-dependent operating
system functionality and Cassiopea for describing instruction set architecture
semantics. Automated porting also requires an implementation. We developed a
toolchain that, given an Alewife specification and a Cassiopea machine
description, specializes the machine-independent specification to the target
instruction set architecture and synthesizes an implementation in assembly
language with a customized symbolic execution engine. Using this approach, we
demonstrate successful synthesis of a total of 140 OS components from two
pre-existing OSes for four real hardware platforms. We also developed several
optimization methods for OS-related assembly synthesis to improve scalability.
The effectiveness of our languages and ability to synthesize code for all 140
specifications is evidence of the feasibility of program synthesis for
machine-dependent OS code. However, many research challenges remain; we also
discuss the benefits and limitations of our synthesis-based approach to
automated OS porting.
","['\nJingmei Hu\n', '\nEric Lu\n', '\nDavid A. Holland\n', '\nMing Kawaguchi\n', '\nStephen Chong\n', '\nMargo I. Seltzer\n']","ACM Transactions on Programming Languages and Systems. Accepted on
  August 2022",,http://dx.doi.org/10.1145/3563943,cs.PL,"['cs.PL', 'cs.OS']",10.1145/3563943,,[]
"Differentiating Network Flows for Priority-Aware Scheduling of Incoming
  Packets in Real-Time IoT Systems",http://arxiv.org/abs/2204.08846v1,2022-04-19T12:23:21Z,2022-04-19T12:23:21Z,"  When IP-packet processing is unconditionally carried out on behalf of an
operating system kernel thread, processing systems can experience overload in
high incoming traffic scenarios. This is especially worrying for embedded
real-time devices controlling their physical environment in industrial IoT
scenarios and automotive systems. We propose an embedded real-time aware IP
stack adaption with an early demultiplexing scheme for incoming packets and
subsequent per-flow aperiodic scheduling. By instrumenting existing embedded IP
stacks, rigid prioritization with minimal latency is deployed without the need
of further task resources. Simple mitigation techniques can be applied to
individual flows, causing hardly measurable overhead while at the same time
protecting the system from overload conditions. Our IP stack adaption is able
to reduce the low-priority packet processing time by over 86% compared to an
unmodified stack. The network subsystem can thereby remain active at a 7x
higher general traffic load before disabling the receive IRQ as a last resort
to assure deadlines.
","['\nChristoph Blumschein\n', '\nIlja Behnke\n', '\nLauritz Thamsen\n', '\nOdej Kao\n']",25th International Symposium on Real-Time Distributed Computing,,http://arxiv.org/abs/2204.08846v1,cs.NI,"['cs.NI', 'cs.OS']",,,[]
"Persistent Memory Objects: Fast and Easy Crash Consistency for
  Persistent Memory",http://arxiv.org/abs/2204.03289v1,2022-04-07T08:38:37Z,2022-04-07T08:38:37Z,"  DIMM-compatible persistent memory unites memory and storage. Prior works
utilize persistent memory either by combining the filesystem with direct access
on memory mapped files or by managing it as a collection of objects while
abolishing the POSIX abstraction. In contrast, we propose retaining the POSIX
abstraction and extending it to provide support for persistent memory, using
Persistent Memory Objects (PMOs). In this work, we design and implement PMOs, a
crash-consistent abstraction for managing persistent memory. We introduce
psync, a single system call, that a programmer can use to specify crash
consistency points in their code, without needing to orchestrate durability
explicitly. When rendering data crash consistent, our design incurs a overhead
of $\approx 25\%$ and $\approx 21\%$ for parallel workloads and FileBench,
respectively, compared to a system without crash consistency. Compared to
NOVA-Fortis, our design provides a speedup of $\approx 1.67\times$ and $\approx
3\times$ for the two set of benchmarks, respectively.
","['\nDerrick Greenspan\n', '\nNaveed Ul Mustafa\n', '\nZoran Kolega\n', '\nMark Heinrich\n', '\nYan Solihin\n']","12 pages, 15 figures",,http://arxiv.org/abs/2204.03289v1,cs.OS,['cs.OS'],,,[]
"HetSched: Quality-of-Mission Aware Scheduling for Autonomous Vehicle
  SoCs",http://arxiv.org/abs/2203.13396v1,2022-03-25T00:24:44Z,2022-03-25T00:24:44Z,"  Systems-on-Chips (SoCs) that power autonomous vehicles (AVs) must meet
stringent performance and safety requirements prior to deployment. With
increasing complexity in AV applications, the system needs to meet these
real-time demands of multiple safety-critical applications simultaneously. A
typical AV-SoC is a heterogeneous multiprocessor consisting of accelerators
supported by general-purpose cores. Such heterogeneity, while needed for
power-performance efficiency, complicates the art of task scheduling.
  In this paper, we demonstrate that hardware heterogeneity impacts the
scheduler's effectiveness and that optimizing for only the real-time aspect of
applications is not sufficient in AVs. Therefore, a more holistic approach is
required -- one that considers global Quality-of-Mission (QoM) metrics, as
defined in the paper. We then propose HetSched, a multi-step scheduler that
leverages dynamic runtime information about the underlying heterogeneous
hardware platform, along with the applications' real-time constraints and the
task traffic in the system to optimize overall mission performance. HetSched
proposes two scheduling policies: MSstat and MSdyn and scheduling optimizations
like task pruning, hybrid heterogeneous ranking and rank update. HetSched
improves overall mission performance on average by 4.6x, 2.6x and 2.6x when
compared against CPATH, ADS and 2lvl-EDF (state-of-the-art real-time schedulers
built for heterogeneous systems), respectively, and achieves an average of
53.3% higher hardware utilization, while meeting 100% critical deadlines for
real-world applications of autonomous vehicles. Furthermore, when used as part
of an SoC design space exploration loop, in comparison to prior schedulers,
HetSched reduces the number of processing elements required by an SoC to safely
complete AV's missions by 35% on average while achieving 2.7x lower
energy-mission time product.
","['\nAporva Amarnath\n', '\nSubhankar Pal\n', '\nHiwot Kassa\n', '\nAugusto Vega\n', '\nAlper Buyuktosunoglu\n', '\nHubertus Franke\n', '\nJohn-David Wellman\n', '\nRonald Dreslinski\n', '\nPradip Bose\n']","14 pages, 11 figures, 4 tables",,http://arxiv.org/abs/2203.13396v1,cs.AR,"['cs.AR', 'cs.DC', 'cs.OS']",,,[]
"GraphBLAS on the Edge: Anonymized High Performance Streaming of Network
  Traffic",http://arxiv.org/abs/2203.13934v2,2022-03-25T23:28:43Z,2022-09-05T15:34:47Z,"  Long range detection is a cornerstone of defense in many operating domains
(land, sea, undersea, air, space, ..,). In the cyber domain, long range
detection requires the analysis of significant network traffic from a variety
of observatories and outposts. Construction of anonymized hypersparse traffic
matrices on edge network devices can be a key enabler by providing significant
data compression in a rapidly analyzable format that protects privacy.
GraphBLAS is ideally suited for both constructing and analyzing anonymized
hypersparse traffic matrices. The performance of GraphBLAS on an Accolade
Technologies edge network device is demonstrated on a near worse case traffic
scenario using a continuous stream of CAIDA Telescope darknet packets. The
performance for varying numbers of traffic buffers, threads, and processor
cores is explored. Anonymized hypersparse traffic matrices can be constructed
at a rate of over 50,000,000 packets per second; exceeding a typical 400
Gigabit network link. This performance demonstrates that anonymized hypersparse
traffic matrices are readily computable on edge network devices with minimal
compute resources and can be a viable data product for such devices.
","['\nMichael Jones\n', '\nJeremy Kepner\n', '\nDaniel Andersen\n', '\nAydin Buluc\n', '\nChansup Byun\n', '\nK Claffy\n', '\nTimothy Davis\n', '\nWilliam Arcand\n', '\nJonathan Bernays\n', '\nDavid Bestor\n', '\nWilliam Bergeron\n', '\nVijay Gadepally\n', '\nMicheal Houle\n', '\nMatthew Hubbell\n', '\nHayden Jananthan\n', '\nAnna Klein\n', '\nChad Meiners\n', '\nLauren Milechin\n', '\nJulie Mullen\n', '\nSandeep Pisharody\n', '\nAndrew Prout\n', '\nAlbert Reuther\n', '\nAntonio Rosa\n', '\nSiddharth Samsi\n', '\nJon Sreekanth\n', '\nDoug Stetson\n', '\nCharles Yee\n', '\nPeter Michaleas\n']","Accepted to IEEE HPEC, Outstanding Paper Award, 8 pages, 8 figures, 1
  table, 70 references. arXiv admin note: text overlap with arXiv:2108.06653,
  arXiv:2008.00307, arXiv:2203.10230",,http://dx.doi.org/10.1109/HPEC55821.2022.9926332,cs.NI,"['cs.NI', 'cs.DC', 'cs.OS', 'cs.SI']",10.1109/HPEC55821.2022.9926332,,[]
syslrn: Learning What to Monitor for Efficient Anomaly Detection,http://arxiv.org/abs/2203.15324v1,2022-03-29T08:10:06Z,2022-03-29T08:10:06Z,"  While monitoring system behavior to detect anomalies and failures is
important, existing methods based on log-analysis can only be as good as the
information contained in the logs, and other approaches that look at the
OS-level software state introduce high overheads. We tackle the problem with
syslrn, a system that first builds an understanding of a target system offline,
and then tailors the online monitoring instrumentation based on the learned
identifiers of normal behavior. While our syslrn prototype is still preliminary
and lacks many features, we show in a case study for the monitoring of
OpenStack failures that it can outperform state-of-the-art log-analysis systems
with little overhead.
","['\nDavide Sanvito\n', '\nGiuseppe Siracusano\n', '\nSharan Santhanam\n', '\nRoberto Gonzalez\n', '\nRoberto Bifulco\n']",,,http://arxiv.org/abs/2203.15324v1,cs.LG,"['cs.LG', 'cs.DC', 'cs.OS']",,,[]
"A Framework for Formal Specification and Verification of Security
  Properties of the Android Permissions System",http://arxiv.org/abs/2203.09857v1,2022-03-18T10:54:33Z,2022-03-18T10:54:33Z,"  Android is a widely deployed operating system that employs a permission-based
access control model. The Android Permissions System (APS) is responsible for
mediating resource requests from applications. APS is a critical component of
the Android security mechanism. A failure in the design of APS can potentially
lead to vulnerabilities that grant unauthorized access to resources by
malicious applications. Researchers have employed formal methods for analyzing
the security properties of APS. Since Android is constantly evolving, we intend
to design and implement a framework for formal specification and verification
of the security properties of APS. In particular, we intend to present a
behavioral model of APS that represents the non-binary, context dependent
permissions introduced in Android 10 and temporal permissions introduced in
Android 11.
",['\nAmirhosein Sayyadabdi\n'],To be presented at the EuroSys Doctoral Workshop 2022,,http://arxiv.org/abs/2203.09857v1,cs.CR,"['cs.CR', 'cs.OS']",,,[]
"No Provisioned Concurrency: Fast RDMA-codesigned Remote Fork for
  Serverless Computing",http://arxiv.org/abs/2203.10225v3,2022-03-19T02:49:55Z,2022-09-17T01:52:44Z,"  Serverless platforms essentially face a tradeoff between container startup
time and provisioned concurrency (i.e., cached instances), which is further
exaggerated by the frequent need for remote container initialization. This
paper presents MITOSIS, an operating system primitive that provides fast remote
fork, which exploits a deep codesign of the OS kernel with RDMA. By leveraging
the fast remote read capability of RDMA and partial state transfer across
serverless containers, MITOSIS bridges the performance gap between local and
remote container initialization. MITOSIS is the first to fork over 10,000 new
containers from one instance across multiple machines within a second, while
allowing the new containers to efficiently transfer the pre-materialized states
of the forked one. We have implemented MITOSIS on Linux and integrated it with
FN, a popular serverless platform. Under load spikes in real-world serverless
workloads, MITOSIS reduces the function tail latency by 89% with orders of
magnitude lower memory usage. For serverless workflow that requires state
transfer, MITOSIS improves its execution time by 86%.
","['\nXingda Wei\n', '\nFangming Lu\n', '\nTianxia Wang\n', '\nJinyu Gu\n', '\nYuhan Yang\n', '\nRong Chen\n', '\nHaibo Chen\n']",To appear in OSDI'23,,http://arxiv.org/abs/2203.10225v3,cs.OS,"['cs.OS', 'cs.DC']",,,[]
FPGA-extended General Purpose Computer Architecture,http://arxiv.org/abs/2203.10359v3,2022-03-19T17:21:01Z,2022-08-21T21:00:41Z,"  This paper introduces a computer architecture, where part of the instruction
set architecture (ISA) is implemented on small highly-integrated
field-programmable gate arrays (FPGAs). Small FPGAs inside a general-purpose
processor (CPU) can be used effectively to implement custom or standardised
instructions. Our proposed architecture directly address related challenges for
high-end CPUs, where such highly-integrated FPGAs would have the highest
impact, such as on main memory bandwidth. This also enables
software-transparent context-switching. The simulation-based evaluation of a
dynamically reconfigurable core shows promising results approaching the
performance of an equivalent core with all enabled instructions. Finally, the
feasibility of adopting the proposed architecture in today's CPUs is studied
through the prototyping of fast-reconfigurable FPGAs and studying the miss
behaviour of opcodes.
","['\nPhilippos Papaphilippou\n', '\nMyrtle Shah\n']","Accepted at the 18th International Symposium on Applied
  Reconfigurable Computing (ARC) 2022",,http://arxiv.org/abs/2203.10359v3,cs.AR,"['cs.AR', 'cs.OS']",,,[]
Minimizing Trust with Exclusively-Used Physically-Isolated Hardware,http://arxiv.org/abs/2203.08284v2,2022-03-15T22:03:23Z,2022-10-21T03:50:58Z,"  Smartphone owners often need to run security-critical programs on the same
device as other untrusted and potentially malicious programs. This requires
users to trust hardware and system software to correctly sandbox malicious
programs, trust that is often misplaced.
  Our goal is to minimize the number and complexity of hardware and software
components that a smartphone owner needs to trust to withstand adversarial
inputs. We present a multi-domain hardware design composed of
statically-partitioned, physically-isolated trust domains. We introduce a few
simple, formally-verified hardware components to enable a program to gain
provably exclusive and simultaneous access to both computation and I/O on a
temporary basis. To manage this hardware, we present OctopOS, an OS composed of
mutually distrustful subsystems.
  We present a prototype of this machine (hardware and OS) on a CPU-FPGA board
and show that it incurs a small hardware cost compared to modern SoCs. For
security-critical programs, we show that this machine significantly reduces the
required trust compared to mainstream TEEs while achieving decent performance.
For normal programs, performance is similar to a legacy machine.
","['\nZhihao Yao\n', '\nSeyed Mohammadjavad Seyed Talebi\n', '\nMingyi Chen\n', '\nArdalan Amiri Sani\n', '\nThomas Anderson\n']",,,http://arxiv.org/abs/2203.08284v2,cs.CR,"['cs.CR', 'cs.AR', 'cs.OS']",,,[]
"Canvas: Isolated and Adaptive Swapping for Multi-Applications on Remote
  Memory",http://arxiv.org/abs/2203.09615v2,2022-03-17T21:06:15Z,2022-10-12T05:50:01Z,"  Remote memory techniques for datacenter applications have recently gained a
great deal of popularity. Existing remote memory techniques focus on the
efficiency of a single application setting only. However, when multiple
applications co-run on a remote-memory system, significant interference could
occur, resulting in unexpected slowdowns even if the same amounts of physical
resources are granted to each application. This slowdown stems from massive
sharing in applications' swap data paths. Canvas is a redesigned swap system
that fully isolates swap paths for remote-memory applications. Canvas allows
each application to possess its dedicated swap partition, swap cache,
prefetcher, and RDMA bandwidth. Swap isolation lays a foundation for adaptive
optimization techniques based on each application's own access patterns and
needs. We develop three such techniques: (1) adaptive swap entry allocation,
(2) semantics-aware prefetching, and (3) two-dimensional RDMA scheduling. A
thorough evaluation with a set of widely-deployed applications demonstrates
that Canvas minimizes performance variation and dramatically reduces
performance degradation.
","['\nChenxi Wang\n', '\nYifan Qiao\n', '\nHaoran Ma\n', '\nShi Liu\n', '\nYiying Zhang\n', '\nWenguang Chen\n', '\nRavi Netravali\n', '\nMiryung Kim\n', '\nGuoqing Harry Xu\n']",,,http://arxiv.org/abs/2203.09615v2,cs.OS,"['cs.OS', 'cs.DC', 'cs.NI']",,,[]
Pond: CXL-Based Memory Pooling Systems for Cloud Platforms,http://arxiv.org/abs/2203.00241v4,2022-03-01T05:32:52Z,2022-10-21T22:02:53Z,"  Public cloud providers seek to meet stringent performance requirements and
low hardware cost. A key driver of performance and cost is main memory. Memory
pooling promises to improve DRAM utilization and thereby reduce costs. However,
pooling is challenging under cloud performance requirements. This paper
proposes Pond, the first memory pooling system that both meets cloud
performance goals and significantly reduces DRAM cost. Pond builds on the
Compute Express Link (CXL) standard for load/store access to pool memory and
two key insights. First, our analysis of cloud production traces shows that
pooling across 8-16 sockets is enough to achieve most of the benefits. This
enables a small-pool design with low access latency. Second, it is possible to
create machine learning models that can accurately predict how much local and
pool memory to allocate to a virtual machine (VM) to resemble same-NUMA-node
memory performance. Our evaluation with 158 workloads shows that Pond reduces
DRAM costs by 7% with performance within 1-5% of same-NUMA-node VM allocations.
","['\nHuaicheng Li\n', '\nDaniel S. Berger\n', '\nStanko Novakovic\n', '\nLisa Hsu\n', '\nDan Ernst\n', '\nPantea Zardoshti\n', '\nMonish Shah\n', '\nSamir Rajadnya\n', '\nScott Lee\n', '\nIshwar Agarwal\n', '\nMark D. Hill\n', '\nMarcus Fontoura\n', '\nRicardo Bianchini\n']",Update affiliations,,http://arxiv.org/abs/2203.00241v4,cs.OS,"['cs.OS', 'cs.PF']",,,[]
Limited Associativity Caching in the Data Plane,http://arxiv.org/abs/2203.04803v1,2022-03-09T15:32:40Z,2022-03-09T15:32:40Z,"  In-network caching promises to improve the performance of networked and edge
applications as it shortens the paths data need to travel. This is by storing
so-called hot items in the network switches on-route between clients who access
the data and the storage servers who maintain it. Since the data flows through
those switches in any case, it is natural to cache hot items there.
  Most software-managed caches treat the cache as a fully associative region.
Alas, a fully associative design seems to be at odds with programmable
switches' goal of handling packets in a short bounded amount of time, as well
as their restricted programming model. In this work, we present PKache, a
generic limited associativity cache implementation in the programmable
switches' domain-specific P4 language, and demonstrate its utility by realizing
multiple popular cache management schemes.
","['\nRoy Friedman\n', '\nOr Goaz\n', '\nDor Hovav\n']",,,http://arxiv.org/abs/2203.04803v1,cs.NI,"['cs.NI', 'cs.OS']",,,[]
Relaxed virtual memory in Armv8-A (extended version),http://arxiv.org/abs/2203.00642v1,2022-03-01T17:34:36Z,2022-03-01T17:34:36Z,"  Virtual memory is an essential mechanism for enforcing security boundaries,
but its relaxed-memory concurrency semantics has not previously been
investigated in detail. The concurrent systems code managing virtual memory has
been left on an entirely informal basis, and OS and hypervisor verification has
had to make major simplifying assumptions.
  We explore the design space for relaxed virtual memory semantics in the
Armv8-A architecture, to support future system-software verification. We
identify many design questions, in discussion with Arm; develop a test suite,
including use cases from the pKVM production hypervisor under development by
Google; delimit the design space with axiomatic-style concurrency models; prove
that under simple stable configurations our architectural model collapses to
previous ""user"" models; develop tooling to compute allowed behaviours in the
model integrated with the full Armv8-A ISA semantics; and develop a hardware
test harness.
  This lays out some of the main issues in relaxed virtual memory bringing
these security-critical systems phenomena into the domain of
programming-language semantics and verification with foundational architecture
semantics.
  This document is an extended version of a paper in ESOP 2022, with additional
explanation and examples in the main body, and appendices detailing our litmus
tests, models, proofs, and test results.
","['\nBen Simner\n', '\nAlasdair Armstrong\n', '\nJean Pichon-Pharabod\n', '\nChristopher Pulte\n', '\nRichard Grisenthwaite\n', '\nPeter Sewell\n']",,,http://arxiv.org/abs/2203.00642v1,cs.AR,"['cs.AR', 'cs.OS', 'cs.PL', 'C.1.2; D.3.1; F.3.2']",,,[]
"GPU-Initiated On-Demand High-Throughput Storage Access in the BaM System
  Architecture",http://arxiv.org/abs/2203.04910v3,2022-03-09T17:44:56Z,2023-02-06T20:18:16Z,"  Graphics Processing Units (GPUs) have traditionally relied on the host CPU to
initiate access to the data storage. This approach is well-suited for GPU
applications with known data access patterns that enable partitioning of their
dataset to be processed in a pipelined fashion in the GPU. However, emerging
applications such as graph and data analytics, recommender systems, or graph
neural networks, require fine-grained, data-dependent access to storage. CPU
initiation of storage access is unsuitable for these applications due to high
CPU-GPU synchronization overheads, I/O traffic amplification, and long CPU
processing latencies. GPU-initiated storage removes these overheads from the
storage control path and, thus, can potentially support these applications at
much higher speed. However, there is a lack of systems architecture and
software stack that enable efficient GPU-initiated storage access. This work
presents a novel system architecture, BaM, that fills this gap. BaM features a
fine-grained software cache to coalesce data storage requests while minimizing
I/O traffic amplification. This software cache communicates with the storage
system via high-throughput queues that enable the massive number of concurrent
threads in modern GPUs to make I/O requests at a high rate to fully utilize the
storage devices and the system interconnect. Experimental results show that BaM
delivers 1.0x and 1.49x end-to-end speed up for BFS and CC graph analytics
benchmarks while reducing hardware costs by up to 21.7x over accessing the
graph data from the host memory. Furthermore, BaM speeds up data-analytics
workloads by 5.3x over CPU-initiated storage access on the same hardware.
","['\nZaid Qureshi\n', '\nVikram Sharma Mailthody\n', '\nIsaac Gelado\n', '\nSeung Won Min\n', '\nAmna Masood\n', '\nJeongmin Park\n', '\nJinjun Xiong\n', '\nCJ Newburn\n', '\nDmitri Vainbrand\n', '\nI-Hsin Chung\n', '\nMichael Garland\n', '\nWilliam Dally\n', '\nWen-mei Hwu\n']","This is an extension to the published conference paper at ASPLOS'23:
  https://dl.acm.org/doi/abs/10.1145/3575693.3575748","ASPLOS 2023: Proceedings of the 28th ACM International Conference
  on Architectural Support for Programming Languages and Operating Systems,
  Volume 2",http://dx.doi.org/10.1145/3575693.3575748,cs.DC,"['cs.DC', 'cs.AR', 'cs.OS', 'cs.PF']",10.1145/3575693.3575748,,[]
Migration-Based Synchronization,http://arxiv.org/abs/2202.09365v1,2022-02-18T12:50:30Z,2022-02-18T12:50:30Z,"  A fundamental challenge in multi- and many-core systems is the correct
execution of concurrent access to shared data. A common drawback from existing
synchronization mechanisms is the loss of data locality as the shared data is
transferred between the accessing cores. In real-time systems, this is
especially important as knowledge about data access times is crucial to
establish bounds on execution times and guarantee the meeting of deadlines.We
propose in this paper a refinement of our previously sketched approach of
Migration-Based Synchronization (MBS) as well as its first practical
implementation. The core concept of MBS is the replacement of data migration
with control-flow migration to achieve synchronized memory accesses with
guaranteed data locality. This leads to both shorter and more predictable
execution times for critical sections. As MBS can be used as a substitute for
classical locks, it can be employed in legacy applications without code
alterations.We further examine how the gained data locality improves the
results of worst-case timing analyses and results in tighter bounds on
execution and response time. We reason about the similarity of MBS to existing
synchronization approaches and how it enables us to reuse existing analysis
techniques.Finally, we evaluate our prototype implementation, showing that MBS
can exploit data locality with similar overheads as traditional locking
mechanisms.
","['\nStefan Reif\n', '\nPhillip Raffeck\n', '\nLuis Gerhorst\n', '\nWolfgang Schröder-Preikschat\n', '\nTimo Hönig\n']",,"SBESC'21: Proceedings of the XI Brazilian Symposium on Computing
  Systems Engineering. 2021. IEEE, Pages 1-8",http://dx.doi.org/10.1109/SBESC53686.2021.9628358,cs.OS,['cs.OS'],10.1109/SBESC53686.2021.9628358,,[]
"GenStore: A High-Performance and Energy-Efficient In-Storage Computing
  System for Genome Sequence Analysis",http://arxiv.org/abs/2202.10400v2,2022-02-21T17:53:01Z,2023-04-06T16:56:04Z,"  Read mapping is a fundamental, yet computationally-expensive step in many
genomics applications. It is used to identify potential matches and differences
between fragments (called reads) of a sequenced genome and an already known
genome (called a reference genome). To address the computational challenges in
genome analysis, many prior works propose various approaches such as filters
that select the reads that must undergo expensive computation, efficient
heuristics, and hardware acceleration. While effective at reducing the
computation overhead, all such approaches still require the costly movement of
a large amount of data from storage to the rest of the system, which can
significantly lower the end-to-end performance of read mapping in conventional
and emerging genomics systems.
  We propose GenStore, the first in-storage processing system designed for
genome sequence analysis that greatly reduces both data movement and
computational overheads of genome sequence analysis by exploiting low-cost and
accurate in-storage filters. GenStore leverages hardware/software co-design to
address the challenges of in-storage processing, supporting reads with 1)
different read lengths and error rates, and 2) different degrees of genetic
variation. Through rigorous analysis of read mapping processes, we meticulously
design low-cost hardware accelerators and data/computation flows inside a NAND
flash-based SSD. Our evaluation using a wide range of real genomic datasets
shows that GenStore, when implemented in three modern SSDs, significantly
improves the read mapping performance of state-of-the-art software (hardware)
baselines by 2.07-6.05$\times$ (1.52-3.32$\times$) for read sets with high
similarity to the reference genome and 1.45-33.63$\times$ (2.70-19.2$\times$)
for read sets with low similarity to the reference genome.
","['\nNika Mansouri Ghiasi\n', '\nJisung Park\n', '\nHarun Mustafa\n', '\nJeremie Kim\n', '\nAtaberk Olgun\n', '\nArvid Gollwitzer\n', '\nDamla Senol Cali\n', '\nCan Firtina\n', '\nHaiyu Mao\n', '\nNour Almadhoun Alserr\n', '\nRachata Ausavarungnirun\n', '\nNandita Vijaykumar\n', '\nMohammed Alser\n', '\nOnur Mutlu\n']",Published at ASPLOS 2022,,http://arxiv.org/abs/2202.10400v2,cs.AR,"['cs.AR', 'cs.DC', 'cs.OS', 'q-bio.GN']",,,[]
CAP-VMs: Capability-Based Isolation and Sharing for Microservices,http://arxiv.org/abs/2202.05732v2,2022-02-11T16:08:43Z,2022-06-24T09:55:17Z,"  Cloud stacks must isolate application components, while permitting efficient
data sharing between components deployed on the same physical host.
Traditionally, the MMU enforces isolation and permits sharing at page
granularity. MMU approaches, however, lead to cloud stacks with large TCBs in
kernel space, and page granularity requires inefficient OS interfaces for data
sharing. Forthcoming CPUs with hardware support for memory capabilities offer
new opportunities to implement isolation and sharing at a finer granularity.
  We describe cVMs, a new VM-like abstraction that uses memory capabilities to
isolate application components while supporting efficient data sharing, all
without mandating application code to be capability-aware. cVMs share a single
virtual address space safely, each having only capabilities to access its own
memory. A cVM may include a library OS, thus minimizing its dependency on the
cloud environment. cVMs efficiently exchange data through two capability-based
primitives assisted by a small trusted monitor: (i) an asynchronous read-write
interface to buffers shared between cVMs; and (ii) a call interface to transfer
control between cVMs. Using these two primitives, we build more expressive
mechanisms for efficient cross-cVM communication. Our prototype implementation
using CHERI RISC-V capabilities shows that cVMs isolate services (Redis and
Python) with low overhead while improving data sharing.
","['\nVasily A. Sartakov\n', '\nLluís Vilanova\n', '\nDavid Eyers\n', '\nTakahiro Shinagawa\n', '\nPeter Pietzuch\n']",,,http://arxiv.org/abs/2202.05732v2,cs.OS,['cs.OS'],,,[]
Systems for Memory Disaggregation: Challenges & Opportunities,http://arxiv.org/abs/2202.02223v1,2022-02-03T07:05:13Z,2022-02-03T07:05:13Z,"  Memory disaggregation addresses memory imbalance in a cluster by decoupling
CPU and memory allocations of applications while also increasing the effective
memory capacity for (memory-intensive) applications beyond the local memory
limit imposed by traditional fixed-capacity servers. As the network speeds in
the tightly-knit environments like modern datacenters inch closer to the DRAM
speeds, there has been a recent proliferation of work in this space ranging
from software solutions that pool memory of traditional servers for the shared
use of the cluster to systems targeting the memory disaggregation in the
hardware. In this report, we look at some of these recent memory disaggregation
systems and study the important factors that guide their design, such as the
interface through which the memory is exposed to the application, their runtime
design and relevant optimizations to retain the near-native application
performance, various approaches they employ in managing cluster memory to
maximize utilization, etc. and we analyze the associated trade-offs. We
conclude with a discussion on some open questions and potential future
directions that can render disaggregation more amenable for adoption.
",['\nAnil Yelam\n'],11 pages,,http://arxiv.org/abs/2202.02223v1,cs.DC,"['cs.DC', 'cs.OS']",,,[]
AnyCall: Fast and Flexible System-Call Aggregation,http://arxiv.org/abs/2201.13160v1,2022-01-31T12:13:17Z,2022-01-31T12:13:17Z,"  Operating systems rely on system calls to allow the controlled communication
of isolated processes with the kernel and other processes. Every system call
includes a processor mode switch from the unprivileged user mode to the
privileged kernel mode. Although processor mode switches are the essential
isolation mechanism to guarantee the system's integrity, they induce direct and
indirect performance costs as they invalidate parts of the processor state. In
recent years, high-performance networks and storage hardware has made the
user/kernel transition overhead the bottleneck for IO-heavy applications. To
make matters worse, security vulnerabilities in modern processors (e.g.,
Meltdown) have prompted kernel mitigations that further increase the transition
overhead. To decouple system calls from user/kernel transitions we propose
AnyCall, which uses an in-kernel compiler to execute safety-checked user
bytecode in kernel mode. This allows for very fast system calls interleaved
with error checking and processing logic using only a single user/kernel
transition. We have implemented AnyCall based on the Linux kernel's eBPF
subsystem. Our evaluation demonstrates that system call bursts are up to 55
times faster using AnyCall and that real-world applications can be sped up by
24% even if only a minimal part of their code is run by AnyCall.
","['\nLuis Gerhorst\n', '\nBenedict Herzog\n', '\nStefan Reif\n', '\nWolfgang Schröder-Preikschat\n', '\nTimo Hönig\n']",,"PLOS'21: Proceedings of the 11th Workshop on Programming Languages
  and Operating Systems. 2021. Association for Computing Machinery (ACM), New
  York, NY, USA, Pages 1-8",http://dx.doi.org/10.1145/3477113.3487267,cs.CR,"['cs.CR', 'cs.OS', 'cs.PL']",10.1145/3477113.3487267,,[]
How ISO C became unusable for operating systems development,http://arxiv.org/abs/2201.07845v1,2022-01-19T20:07:24Z,2022-01-19T20:07:24Z,"  The C programming language was developed in the 1970s as a fairly
unconventional systems and operating systems development tool, but has, through
the course of the ISO Standards process, added many attributes of more
conventional programming languages and become less suitable for operating
systems development. Operating system programming continues to be done in
non-ISO dialects of C. The differences provide a glimpse of operating system
requirements for programming languages.
",['\nVictor Yodaiken\n'],"PLOS '21: Proceedings of the 11th Workshop on Programming Languages
  and Operating Systems October 2021",,http://dx.doi.org/10.1145/3477113.3487274,cs.OS,"['cs.OS', 'cs.PL', 'd.4, d.3', 'D.4; D.3']",10.1145/3477113.3487274,,[]
"Adelie: Continuous Address Space Layout Re-randomization for Linux
  Drivers",http://arxiv.org/abs/2201.08378v1,2022-01-20T18:58:44Z,2022-01-20T18:58:44Z,"  While address space layout randomization (ASLR) has been extensively studied
for user-space programs, the corresponding OS kernel's KASLR support remains
very limited, making the kernel vulnerable to just-in-time (JIT)
return-oriented programming (ROP) attacks. Furthermore, commodity OSs such as
Linux restrict their KASLR range to 32 bits due to architectural constraints
(e.g., x86-64 only supports 32-bit immediate operands for most instructions),
which makes them vulnerable to even unsophisticated brute-force ROP attacks due
to low entropy. Most in-kernel pointers remain static, exacerbating the problem
when pointers are leaked.
  Adelie, our kernel defense mechanism, overcomes KASLR limitations, increases
KASLR entropy, and makes successful ROP attacks on the Linux kernel much harder
to achieve. First, Adelie enables the position-independent code (PIC) model so
that the kernel and its modules can be placed anywhere in the 64-bit virtual
address space, at any distance apart from each other. Second, Adelie implements
stack re-randomization and address encryption on modules. Finally, Adelie
enables efficient continuous KASLR for modules by using the PIC model to make
it (almost) impossible to inject ROP gadgets through these modules regardless
of gadget's origin.
  Since device drivers (typically compiled as modules) are often developed by
third parties and are typically less tested than core OS parts, they are also
often more vulnerable. By fully re-randomizing device drivers, the last two
contributions together prevent most JIT ROP attacks since vulnerable modules
are very likely to be a starting point of an attack. Furthermore, some OS
instances in virtualized environments are specifically designated to run device
drivers, where drivers are the primary target of JIT ROP attacks. Our
evaluation shows high efficiency of Adelie's approach.
  [full abstract is in the paper]
","['\nRuslan Nikolaev\n', '\nHassan Nadeem\n', '\nCathlyn Stone\n', '\nBinoy Ravindran\n']","27th ACM International Conference on Architectural Support for
  Programming Languages and Operating Systems (ASPLOS '22), February 28 - March
  4, 2022, Lausanne, Switzerland",,http://dx.doi.org/10.1145/3503222.3507779,cs.OS,"['cs.OS', 'cs.CR']",10.1145/3503222.3507779,,[]
Enhancing the Security & Privacy of Wearable Brain-Computer Interfaces,http://arxiv.org/abs/2201.07711v1,2022-01-19T16:51:18Z,2022-01-19T16:51:18Z,"  Brain computing interfaces (BCI) are used in a plethora of
safety/privacy-critical applications, ranging from healthcare to smart
communication and control. Wearable BCI setups typically involve a head-mounted
sensor connected to a mobile device, combined with ML-based data processing.
Consequently, they are susceptible to a multiplicity of attacks across the
hardware, software, and networking stacks used that can leak users' brainwave
data or at worst relinquish control of BCI-assisted devices to remote
attackers. In this paper, we: (i) analyse the whole-system security and privacy
threats to existing wearable BCI products from an operating system and
adversarial machine learning perspective; and (ii) introduce Argus, the first
information flow control system for wearable BCI applications that mitigates
these attacks. Argus' domain-specific design leads to a lightweight
implementation on Linux ARM platforms suitable for existing BCI use-cases. Our
proof of concept attacks on real-world BCI devices (Muse, NeuroSky, and
OpenBCI) led us to discover more than 300 vulnerabilities across the stacks of
six major attack vectors. Our evaluation shows Argus is highly effective in
tracking sensitive dataflows and restricting these attacks with an acceptable
memory and performance overhead (<15%).
","['\nZahra Tarkhani\n', '\nLorena Qendro\n', ""\nMalachy O'Connor Brown\n"", '\nOscar Hill\n', '\nCecilia Mascolo\n', '\nAnil Madhavapeddy\n']",,,http://arxiv.org/abs/2201.07711v1,cs.CR,"['cs.CR', 'cs.HC', 'cs.LG', 'cs.OS']",,,[]
DuVisor: a User-level Hypervisor Through Delegated Virtualization,http://arxiv.org/abs/2201.09652v1,2022-01-24T13:17:51Z,2022-01-24T13:17:51Z,"  Today's mainstream virtualization systems comprise of two cooperative
components: a kernel-resident driver that accesses virtualization hardware and
a user-level helper process that provides VM management and I/O virtualization.
However, this virtualization architecture has intrinsic issues in both security
(a large attack surface) and performance. While there is a long thread of work
trying to minimize the kernel-resident driver by offloading functions to user
mode, they face a fundamental tradeoff between security and performance: more
offloading may reduce the kernel attack surface, yet increase the runtime ring
crossings between the helper process and the driver, and thus more performance
cost.
  This paper explores a new design called delegated virtualization, which
completely separates the control plane (the kernel driver) from the data plane
(the helper process) and thus eliminates the kernel driver from runtime
intervention. The resulting user-level hypervisor, called DuVisor, can handle
all VM operations without trapping into the kernel once the kernel driver has
done the initialization. DuVisor retrofits existing hardware virtualization
support with a new delegated virtualization extension to directly handle VM
exits, configure virtualization registers, manage the stage-2 page table and
virtual devices in user mode. We have implemented the hardware extension on an
open-source RISC-V CPU and built a Rust-based hypervisor atop the hardware.
Evaluation on FireSim shows that DuVisor outperforms KVM by up to 47.96\% in a
variety of real-world applications and significantly reduces the attack
surface.
","['\nJiahao Chen\n', '\nDingji Li\n', '\nZeyu Mi\n', '\nYuxuan Liu\n', '\nBinyu Zang\n', '\nHaibing Guan\n', '\nHaibo Chen\n']","17 pages, 9 figures",,http://arxiv.org/abs/2201.09652v1,cs.OS,"['cs.OS', 'cs.AR', 'cs.CR']",,,[]
DiOS -- An Extended Reality Operating System for the Metaverse,http://arxiv.org/abs/2201.03256v1,2022-01-10T10:19:49Z,2022-01-10T10:19:49Z,"  Driven by the recent improvements in device and networks capabilities,
Extended Reality (XR) is becoming more pervasive; industry and academia alike
envision ambitious projects such as the metaverse. However, XR is still limited
by the current architecture of mobile systems. This paper makes the case for an
XR-specific operating system (XROS). Such an XROS integrates hardware-support,
computer vision algorithms, and XR-specific networking as the primitives
supporting XR technology. These primitives represent the physical-digital world
as a single shared resource among applications. Such an XROS allows for the
development of coherent and system-wide interaction and display methods,
systematic privacy preservation on sensor data, and performance improvement
while simplifying application development.
","['\nTristan Braud\n', '\nLik-Hang Lee\n', '\nAhmad Alhilal\n', '\nCarlos Bermejo Fernández\n', '\nPan Hui\n']","6 pages, 4 figures",,http://arxiv.org/abs/2201.03256v1,cs.HC,"['cs.HC', 'cs.MM', 'cs.OS']",,,[]
FlexOS: Towards Flexible OS Isolation,http://arxiv.org/abs/2112.06566v3,2021-12-13T11:19:01Z,2022-01-14T09:12:31Z,"  At design time, modern operating systems are locked in a specific safety and
isolation strategy that mixes one or more hardware/software protection
mechanisms (e.g. user/kernel separation); revisiting these choices after
deployment requires a major refactoring effort. This rigid approach shows its
limits given the wide variety of modern applications' safety/performance
requirements, when new hardware isolation mechanisms are rolled out, or when
existing ones break.
  We present FlexOS, a novel OS allowing users to easily specialize the safety
and isolation strategy of an OS at compilation/deployment time instead of
design time. This modular LibOS is composed of fine-grained components that can
be isolated via a range of hardware protection mechanisms with various data
sharing strategies and additional software hardening. The OS ships with an
exploration technique helping the user navigate the vast safety/performance
design space it unlocks. We implement a prototype of the system and
demonstrate, for several applications (Redis/Nginx/SQLite), FlexOS' vast
configuration space as well as the efficiency of the exploration technique: we
evaluate 80 FlexOS configurations for Redis and show how that space can be
probabilistically subset to the 5 safest ones under a given performance budget.
We also show that, under equivalent configurations, FlexOS performs similarly
or better than several baselines/competitors.
","['\nHugo Lefeuvre\n', '\nVlad-Andrei Bădoiu\n', '\nAlexander Jung\n', '\nStefan Teodorescu\n', '\nSebastian Rauch\n', '\nFelipe Huici\n', '\nCostin Raiciu\n', '\nPierre Olivier\n']","Artifact Evaluation Repository:
  https://github.com/project-flexos/asplos22-ae",,http://arxiv.org/abs/2112.06566v3,cs.OS,['cs.OS'],,,[]
"Slowing Down for Performance and Energy: An OS-Centric Study in Network
  Driven Workloads",http://arxiv.org/abs/2112.07010v1,2021-12-13T21:04:26Z,2021-12-13T21:04:26Z,"  This paper studies three fundamental aspects of an OS that impact the
performance and energy efficiency of network processing: 1) batching, 2)
processor energy settings, and 3) the logic and instructions of the OS
networking paths. A network device's interrupt delay feature is used to induce
batching and processor frequency is manipulated to control the speed of
instruction execution. A baremetal library OS is used to explore OS path
specialization. This study shows how careful use of batching and interrupt
delay results in 2X energy and performance improvements across different
workloads. Surprisingly, we find polling can be made energy efficient and can
result in gains up to 11X over baseline Linux. We developed a methodology and a
set of tools to collect system data in order to understand how energy is
impacted at a fine-grained granularity. This paper identifies a number of other
novel findings that have implications in OS design for networked applications
and suggests a path forward to consider energy as a focal point of systems
research.
","['\nHan Dong\n', '\nSanjay Arora\n', '\nYara Awad\n', '\nTommy Unger\n', '\nOrran Krieger\n', '\nJonathan Appavoo\n']",,,http://arxiv.org/abs/2112.07010v1,cs.OS,['cs.OS'],,,[]
New Mechanism for Fast System Calls,http://arxiv.org/abs/2112.10106v1,2021-12-19T10:26:55Z,2021-12-19T10:26:55Z,"  System calls have no place on the fast path of microsecond-scale systems.
However, kernel bypass prevents the OS from controlling and supervising access
to the hardware. In this paper we introduce the fastcall space, a new layer in
the traditional OS architecture, that hosts fastcalls. A fastcall implements
the fast path of a traditional kernel operation and can stay on the fast path,
because the transition to the fastcall space is $\approx\times 15$ faster than
to the kernel space. This way the OS does not give up the control over device
access, whereas the applications maintain their performance.
","['\nTill Miemietz\nBarkhausen Institut, Germany\n', '\nMaksym Planeta\nTU Dresden, Germany\n', '\nViktor Laurin Reusch\nBarkhausen Institut, Germany\nTU Dresden, Germany\n']",,,http://arxiv.org/abs/2112.10106v1,cs.OS,['cs.OS'],,,"['Barkhausen Institut, Germany', 'TU Dresden, Germany', 'Barkhausen Institut, Germany', 'TU Dresden, Germany']"
Bento and the Art of Repeated Research,http://arxiv.org/abs/2112.06810v1,2021-12-13T17:13:00Z,2021-12-13T17:13:00Z,"  Bento provides a new approach to developing file systems, with safety and
high-velocity development in mind. This is achieved by using Rust, a modern and
memory-safe systems programming language, and by providing a framework to run a
single file system implementation in kernel space with the VFS or in user space
with FUSE. In this paper, the benchmarking experiments from the Bento paper are
repeated. We fail to exactly reproduce the results of the Bento paper, but more
or less find the same patterns albeit with more outlying results. Additionally
we unsuccessfully run a standardized test suite, and expand the set of
experiments with latency benchmarks and throughput benchmarks using a RAM block
device. The latency benchmarks show that ext4 with journaling consistently
outperforms Bento-fs and the RAM throughput benchmarks show no additional
consistent performance pattern. During this experimentation, a set of 12 bugs
was encountered and analyzed. We find that the ratio of memory related bugs is
lower than other systems programming projects that use C as opposed to Rust,
thus supporting the claims of the Bento framework.
","['\nPeter-Jan Gootzen\n', '\nAnimesh Trivedi\n']",,,http://arxiv.org/abs/2112.06810v1,cs.OS,"['cs.OS', 'cs.PL']",,,[]
"Virtualizing Mixed-Criticality Systems: A Survey on Industrial Trends
  and Issues",http://arxiv.org/abs/2112.06875v1,2021-12-13T18:32:53Z,2021-12-13T18:32:53Z,"  Virtualization is gaining attraction in the industry as it promises a
flexible way to integrate, manage, and re-use heterogeneous software components
with mixed-criticality levels, on a shared hardware platform, while obtaining
isolation guarantees. This work surveys the state-of-the-practice of real-time
virtualization technologies by discussing common issues in the industry. In
particular, we analyze how different virtualization approaches and solutions
can impact isolation guarantees and testing/certification activities, and how
they deal with dependability challenges. The aim is to highlight current
industry trends and support industrial practitioners to choose the most
suitable solution according to their application domains.
","['\nMarcello Cinque\n', '\nDomenico Cotroneo\n', '\nLuigi De Simone\n', '\nStefano Rosiello\n']","Accepted for publication in Elsevier Future Generation Computer
  Systems",,http://dx.doi.org/10.1016/j.future.2021.12.002,cs.SE,"['cs.SE', 'cs.OS']",10.1016/j.future.2021.12.002,,[]
"In-Kernel Control-Flow Integrity on Commodity OSes using ARM Pointer
  Authentication",http://arxiv.org/abs/2112.07213v1,2021-12-14T07:49:32Z,2021-12-14T07:49:32Z,"  This paper presents an in-kernel, hardware-based control-flow integrity (CFI)
protection, called PAL, that utilizes ARM's Pointer Authentication (PA). It
provides three important benefits over commercial, state-of-the-art PA-based
CFIs like iOS's: 1) enhancing CFI precision via automated refinement
techniques, 2) addressing hindsight problems of PA for in kernel uses such as
preemptive hijacking and brute-forcing attacks, and 3) assuring the algorithmic
or implementation correctness via post validation. PAL achieves these goals in
an OS-agnostic manner, so could be applied to commodity OSes like Linux and
FreeBSD. The precision of the CFI protection can be adjusted for better
performance or improved for better security with minimal engineering efforts if
a user opts in to. Our evaluation shows that PAL incurs negligible performance
overhead: e.g., <1% overhead for Apache benchmark and 3~5% overhead for Linux
perf benchmark on the latest Mac mini (M1). Our post-validation approach helps
us ensure the security invariant required for the safe uses of PA inside the
kernel, which also reveals new attack vectors on the iOS kernel. PAL as well as
the CFI-protected kernels will be open sourced.
","['\nSungbae Yoo\n', '\nJinbum Park\n', '\nSeolheui Kim\n', '\nYeji Kim\n', '\nTaesoo Kim\n']",,,http://arxiv.org/abs/2112.07213v1,cs.CR,"['cs.CR', 'cs.OS', 'D.4.6']",,,[]
"Toolset for Collecting Shell Commands and Its Application in Hands-on
  Cybersecurity Training",http://arxiv.org/abs/2112.11118v1,2021-12-21T11:45:13Z,2021-12-21T11:45:13Z,"  When learning cybersecurity, operating systems, or networking, students
perform practical tasks using a broad range of command-line tools. Collecting
and analyzing data about the command usage can reveal valuable insights into
how students progress and where they make mistakes. However, few learning
environments support recording and inspecting command-line inputs, and setting
up an efficient infrastructure for this purpose is challenging. To aid
engineering and computing educators, we share the design and implementation of
an open-source toolset for logging commands that students execute on Linux
machines. Compared to basic solutions, such as shell history files, the
toolset's added value is threefold. 1) Its configuration is automated so that
it can be easily used in classes on different topics. 2) It collects metadata
about the command execution, such as a timestamp, hostname, and IP address. 3)
Data are instantly forwarded to central storage in a unified, semi-structured
format. This enables automated processing, both in real-time and post hoc, to
enhance the instructors' understanding of student actions. The toolset works
independently of the teaching content, the training network's topology, or the
number of students working in parallel. We demonstrated the toolset's value in
two learning environments at four training sessions. Over two semesters, 50
students played educational cybersecurity games using a Linux command-line
interface. Each training session lasted approximately two hours, during which
we recorded 4439 shell commands. The semi-automated data analysis revealed
solution patterns, used tools, and misconceptions of students. Our insights
from creating the toolset and applying it in teaching practice are relevant for
instructors, researchers, and developers of learning environments. We provide
the software and data resulting from this work so that others can use them.
","['\nValdemar Švábenský\n', '\nJan Vykopal\n', '\nDaniel Tovarňák\n', '\nPavel Čeleda\n']","IEEE FIE 2021 conference, 9 pages, 5 figure, 3 tables",,http://dx.doi.org/10.1109/FIE49875.2021.9637052,cs.CR,"['cs.CR', 'cs.CY', 'cs.OS', 'K.3.2']",10.1109/FIE49875.2021.9637052,,[]
"Reproducing software environments: a prerequisite for reproducible
  research",http://arxiv.org/abs/2112.04384v1,2021-12-07T17:07:53Z,2021-12-07T17:07:53Z,"  As software has become an integral part of scientific workflows, reproducible
research practices must take it into account. In what way? Archiving source
code is a necessary but insufficient condition. The ability to redeploy
software environments, which at first sight may be viewed as a technical
detail, is in fact a requirement. This article explores tools and methods to
achieve this goal.
",['\nLudovic Courtès\nSED\n'],in French,"1024 : Bulletin de la Soci{\'e}t{\'e} Informatique de France,
  Soci{\'e}t{\'e} Informatique de France, 2021, pp.15-22",http://dx.doi.org/10.48556/SIF.1024.18.15,cs.SE,"['cs.SE', 'cs.OS']",10.48556/SIF.1024.18.15,,['SED']
Verifying and Optimizing Compact NUMA-Aware Locks on Weak Memory Models,http://arxiv.org/abs/2111.15240v2,2021-11-30T09:49:32Z,2022-07-09T11:21:18Z,"  Developing concurrent software is challenging, especially if it has to run on
modern architectures with Weak Memory Models (WMMs) such as ARMv8, Power, or
RISC-V. For the sake of performance, WMMs allow hardware and compilers to
aggressively reorder memory accesses. To guarantee correctness, developers have
to carefully place memory barriers in the code to enforce ordering among
critical memory operations.
  While WMM architectures are growing in popularity, identifying the necessary
and sufficient barriers of complex synchronization primitives is notoriously
difficult. Unfortunately, publications often consider barriers to be just
implementation details and omit them. In this technical note, we report our
efforts in verifying the correctness of the Compact NUMA-Aware (CNA) lock
algorithm on WMMs. The CNA lock is of special interest because it has been
proposed as a new slowpath for Linux qspinlock, the main spinlock in Linux.
Besides determining a correct and efficient set of barriers for the original
CNA algorithm on WMMs, we investigate the correctness of Linux qspinlock and
the latest Linux CNA patch (v15) on the memory models LKMM, ARMv8, and Power.
Surprisingly, we have found that Linux qspinlock and, consequently, Linux CNA
are incorrect according to LKMM, but are still correct when compiled to ARMv8
or Power.
","['\nAntonio Paolillo\n', '\nHernán Ponce-de-León\n', '\nThomas Haas\n', '\nDiogo Behrens\n', '\nRafael Chehab\n', '\nMing Fu\n', '\nRoland Meyer\n']",,,http://arxiv.org/abs/2111.15240v2,cs.OS,['cs.OS'],,,[]
KML: Using Machine Learning to Improve Storage Systems,http://arxiv.org/abs/2111.11554v2,2021-11-22T21:59:50Z,2022-01-26T01:45:24Z,"  Operating systems include many heuristic algorithms designed to improve
overall storage performance and throughput. Because such heuristics cannot work
well for all conditions and workloads, system designers resorted to exposing
numerous tunable parameters to users -- thus burdening users with continually
optimizing their own storage systems and applications. Storage systems are
usually responsible for most latency in I/O-heavy applications, so even a small
latency improvement can be significant. Machine learning (ML) techniques
promise to learn patterns, generalize from them, and enable optimal solutions
that adapt to changing workloads. We propose that ML solutions become a
first-class component in OSs and replace manual heuristics to optimize storage
systems dynamically. In this paper, we describe our proposed ML architecture,
called KML. We developed a prototype KML architecture and applied it to two
case studies: optimizing readahead and NFS read-size values. Our experiments
show that KML consumes less than 4KB of dynamic kernel memory, has a CPU
overhead smaller than 0.2%, and yet can learn patterns and improve I/O
throughput by as much as 2.3x and 15x for two case studies -- even for complex,
never-seen-before, concurrently running mixed workloads on different storage
devices.
","['\nIbrahim Umit Akgun\n', '\nAli Selman Aydin\n', '\nAndrew Burford\n', '\nMichael McNeill\n', '\nMichael Arkhangelskiy\n', '\nAadil Shaikh\n', '\nLukas Velikov\n', '\nErez Zadok\n']","17 pages, 13 figures",,http://arxiv.org/abs/2111.11554v2,cs.OS,"['cs.OS', 'cs.LG']",,,[]
EDF-Like Scheduling for Self-Suspending Real-Time Tasks,http://arxiv.org/abs/2111.09725v1,2021-11-18T14:35:14Z,2021-11-18T14:35:14Z,"  In real-time systems, schedulability tests are utilized to provide timing
guarantees. However, for self-suspending task sets, current suspension-aware
schedulability tests are limited to Task-Level Fixed-Priority~(TFP) scheduling
or Earliest-Deadline-First~(EDF) with constrained-deadline task systems. In
this work we provide a unifying schedulability test for the uniprocessor
version of Global EDF-Like (GEL) schedulers and arbitrary-deadline task sets. A
large body of existing scheduling algorithms can be considered as EDF-Like,
such as EDF, First-In-First-Out~(FIFO), Earliest-Quasi-Deadline-First~(EQDF)
and Suspension-Aware EDF~(SAEDF). Therefore, the unifying schedulability test
is applicable to those algorithms. Moreover, the schedulability test can be
applied to TFP scheduling as well.
  Our analysis is the first suspension-aware schedulability test applicable to
arbitrary-deadline sporadic real-time task systems under Job-Level
Fixed-Priority (JFP) scheduling, such as EDF. Moreover, it is the first
unifying suspension-aware schedulability test framework that covers a wide
range of scheduling algorithms. Through numerical simulations, we show that the
schedulability test outperforms the state of the art for EDF under
constrained-deadline scenarios. Moreover, we demonstrate the performance of
different configurations under EQDF and SAEDF.
","['\nMario Günzel\n', '\nKuan-Hsun Chen\n', '\nJian-Jia Chen\n']",,,http://arxiv.org/abs/2111.09725v1,cs.OS,['cs.OS'],,,[]
"VOSySmonitoRV: a mixed-criticality solution on Linux-capable RISC-V
  platforms",http://arxiv.org/abs/2111.02821v1,2021-11-03T16:34:07Z,2021-11-03T16:34:07Z,"  Embedded systems are pervasively used in many fields nowadays. In
mixed-criticality environments (automotive, industry 4.0, drones, etc.) they
need to run real-time applications with certain time and safety constraints
alongside a rich operating system (OS). This is usually possible thanks to
virtualization techniques, that leverage on hardware virtualization extensions
on the machine. However, these hardware extensions might not cope with the
security and safety requirements of the specific use case, and additionally,
they might not always be available. A notable example is the emerging RISC-V
architecture, that is today gaining a lot of traction in the mixed criticality
field, but that do not offer today hardware virtualization extensions. In this
paper VOSySmonitoRV is proposed as a mixed-criticality solution for RISC-V
systems. VOSySmonitoRVallows the co-execution of two or more operating systems
in a secure and isolated manner by running in the highest privileged machine
level. A specific benchmark, measuring the interrupt latency and context switch
time is done to assess the system performance in mixed criticality systems.
","['\nFlavia Caforio\n', '\nPierpaolo Iannicelli\n', '\nMichele Paolino\n', '\nDaniel Raho\n']","Also available at
  http://www.virtualopensystems.com/en/research/scientific-contributions/vosysmonitorv-risc-v-meco2021/",,http://dx.doi.org/10.1109/MECO52532.2021.9460246,cs.CR,"['cs.CR', 'cs.OS']",10.1109/MECO52532.2021.9460246,,[]
Safe and Practical GPU Acceleration in TrustZone,http://arxiv.org/abs/2111.03065v1,2021-11-04T19:39:11Z,2021-11-04T19:39:11Z,"  We present a holistic design for GPU-accelerated computation in TrustZone
TEE. Without pulling the complex GPU software stack into the TEE, we follow a
simple approach: record the CPU/GPU interactions ahead of time, and replay the
interactions in the TEE at run time. This paper addresses the approach's key
missing piece -- the recording environment, which needs both strong security
and access to diverse mobile GPUs. To this end, we present a novel architecture
called CODY, in which a mobile device (which possesses the GPU hardware) and a
trustworthy cloud service (which runs the GPU software) exercise the GPU
hardware/software in a collaborative, distributed fashion. To overcome numerous
network round trips and long delays, CODY contributes optimizations specific to
mobile GPUs: register access deferral, speculation, and metastate-only
synchronization. With these optimizations, recording a compute workload takes
only tens of seconds, which is up to 95% less than a naive approach; replay
incurs 25% lower delays compared to insecure, native execution.
","['\nHeejin Park\n', '\nFelix Xiaozhu Lin\n']",,,http://arxiv.org/abs/2111.03065v1,cs.DC,"['cs.DC', 'cs.CR', 'cs.OS']",,,[]
FlexTOE: Flexible TCP Offload with Fine-Grained Parallelism,http://arxiv.org/abs/2110.10919v2,2021-10-21T06:19:31Z,2022-03-13T19:04:02Z,"  FlexTOE is a flexible, yet high-performance TCP offload engine (TOE) to
SmartNICs. FlexTOE eliminates almost all host data-path TCP processing and is
fully customizable. FlexTOE interoperates well with other TCP stacks, is robust
under adverse network conditions, and supports POSIX sockets.
  FlexTOE focuses on data-path offload of established connections, avoiding
complex control logic and packet buffering in the NIC. FlexTOE leverages
fine-grained parallelization of the TCP data-path and segment reordering for
high performance on wimpy SmartNIC architectures, while remaining flexible via
a modular design. We compare FlexTOE on an Agilio-CX40 to host TCP stacks Linux
and TAS, and to the Chelsio Terminator TOE. We find that Memcached scales up to
38% better on FlexTOE versus TAS, while saving up to 81% host CPU cycles versus
Chelsio. FlexTOE provides competitive performance for RPCs, even with wimpy
SmartNICs. FlexTOE cuts 99.99th-percentile RPC RTT by 3.2$\times$ and 50%
versus Chelsio and TAS, respectively. FlexTOE's data-path parallelism
generalizes across hardware architectures, improving single connection RPC
throughput up to 2.4$\times$ on x86 and 4$\times$ on BlueField. FlexTOE
supports C and XDP programs written in eBPF. It allows us to implement popular
data center transport features, such as TCP tracing, packet filtering and
capture, VLAN stripping, flow classification, firewalling, and connection
splicing.
","['\nRajath Shashidhara\n', '\nTimothy Stamler\n', '\nAntoine Kaufmann\n', '\nSimon Peter\n']","Published in 19th USENIX Symposium on Networked Systems Design and
  Implementation (NSDI 22). See
  https://www.usenix.org/conference/nsdi22/presentation/shashidhara",,http://arxiv.org/abs/2110.10919v2,cs.NI,"['cs.NI', 'cs.OS']",,,[]
"Fast Bitmap Fit: A CPU Cache Line friendly memory allocator for single
  object allocations",http://arxiv.org/abs/2110.10357v1,2021-10-20T03:22:38Z,2021-10-20T03:22:38Z,"  Applications making excessive use of single-object based data structures
(such as linked lists, trees, etc...) can see a drop in efficiency over a
period of time due to the randomization of nodes in memory. This slow down is
due to the ineffective use of the CPU's L1/L2 cache. We present a novel
approach for mitigating this by presenting the design of a single-object memory
allocator that preserves memory locality across randomly ordered memory
allocations and deallocations.
","['\nDhruv Matani\n', '\nGaurav Menghani\n']",,,http://arxiv.org/abs/2110.10357v1,cs.DS,"['cs.DS', 'cs.OS', 'cs.PF']",,,[]
An O(1) algorithm for implementing the LFU cache eviction scheme,http://arxiv.org/abs/2110.11602v1,2021-10-22T05:36:52Z,2021-10-22T05:36:52Z,"  Cache eviction algorithms are used widely in operating systems, databases and
other systems that use caches to speed up execution by caching data that is
used by the application. There are many policies such as MRU (Most Recently
Used), MFU (Most Frequently Used), LRU (Least Recently Used) and LFU (Least
Frequently Used) which each have their advantages and drawbacks and are hence
used in specific scenarios. By far, the most widely used algorithm is LRU, both
for its $O(1)$ speed of operation as well as its close resemblance to the kind
of behaviour that is expected by most applications. The LFU algorithm also has
behaviour desirable by many real world workloads. However, in many places, the
LRU algorithm is is preferred over the LFU algorithm because of its lower run
time complexity of $O(1)$ versus $O(\log n)$. We present here an LFU cache
eviction algorithm that has a runtime complexity of $O(1)$ for all of its
operations, which include insertion, access and deletion(eviction).
","['\nDhruv Matani\n', '\nKetan Shah\n', '\nAnirban Mitra\n']",,,http://arxiv.org/abs/2110.11602v1,cs.DS,"['cs.DS', 'cs.IR', 'cs.OS']",,,[]
"An Evaluation of WebAssembly and eBPF as Offloading Mechanisms in the
  Context of Computational Storage",http://arxiv.org/abs/2111.01947v1,2021-10-22T11:05:34Z,2021-10-22T11:05:34Z,"  As the volume of data that needs to be processed continues to increase, we
also see renewed interests in near-data processing in the form of computational
storage, with eBPF (extended Berkeley Packet Filter) being proposed as a
vehicle for computation offloading. However, discussions in this regard have so
far ignored viable alternatives, and no convincing analysis has been provided.
As such, we qualitatively and quantitatively evaluated eBPF against
WebAssembly, a seemingly similar technology, in the context of computation
offloading. This report presents our findings.
","['\nWenjun Huang\n', '\nMarcus Paradies\n']",,,http://arxiv.org/abs/2111.01947v1,cs.AR,"['cs.AR', 'cs.DB', 'cs.OS']",,,[]
Minimum Viable Device Drivers for ARM TrustZone,http://arxiv.org/abs/2110.08303v2,2021-10-15T18:22:10Z,2022-03-15T15:50:04Z,"  While TrustZone can isolate IO hardware, it lacks drivers for modern IO
devices. Rather than porting drivers, we propose a novel approach to deriving
minimum viable drivers: developers exercise a full driver and record the
driver/device interactions; the processed recordings, dubbed driverlets, are
replayed in the TEE at run time to access IO devices.
  Driverlets address two key challenges: correctness and expressiveness, for
which they build on a key construct called interaction template. The
interaction template ensures faithful reproduction of recorded IO jobs (albeit
on new IO data); it accepts dynamic input values; it tolerates nondeterministic
device behaviors.
  We demonstrate driverlets on a series of sophisticated devices, making them
accessible to TrustZone for the first time to our knowledge. Our experiments
show that driverlets are secure, easy to build, and incur acceptable overhead
(1.4x -2.7x compared to native drivers). Driverlets fill a critical gap in the
TrustZone TEE, realizing its long-promised vision of secure IO.
","['\nLiwei Guo\n', '\nFelix Xiaozhu Lin\n']",Eurosys 2022,,http://dx.doi.org/10.1145/3492321.3519565,cs.OS,"['cs.OS', 'cs.CR']",10.1145/3492321.3519565,,[]
StateAFL: Greybox Fuzzing for Stateful Network Servers,http://arxiv.org/abs/2110.06253v2,2021-10-12T18:08:38Z,2022-10-04T12:18:24Z,"  Fuzzing network servers is a technical challenge, since the behavior of the
target server depends on its state over a sequence of multiple messages.
Existing solutions are costly and difficult to use, as they rely on
manually-customized artifacts such as protocol models, protocol parsers, and
learning frameworks. The aim of this work is to develop a greybox fuzzer
(StateaAFL) for network servers that only relies on lightweight analysis of the
target program, with no manual customization, in a similar way to what the AFL
fuzzer achieved for stateless programs. The proposed fuzzer instruments the
target server at compile-time, to insert probes on memory allocations and
network I/O operations. At run-time, it infers the current protocol state of
the target server by taking snapshots of long-lived memory areas, and by
applying a fuzzy hashing algorithm (Locality-Sensitive Hashing) to map memory
contents to a unique state identifier. The fuzzer incrementally builds a
protocol state machine for guiding fuzzing.
  We implemented and released StateaAFL as open-source software. As a basis for
reproducible experimentation, we integrated StateaAFL with a large set of
network servers for popular protocols, with no manual customization to
accomodate for the protocol. The experimental results show that the fuzzer can
be applied with no manual customization on a large set of network servers for
popular protocols, and that it can achieve comparable, or even better code
coverage and bug detection than customized fuzzing. Moreover, our qualitative
analysis shows that states inferred from memory better reflect the server
behavior than only using response codes from messages.
",['\nRoberto Natella\n'],The tool is available at https://github.com/stateafl/stateafl,"Empir Software Eng 27, 191 (2022)",http://dx.doi.org/10.1007/s10664-022-10233-3,cs.CR,"['cs.CR', 'cs.OS', 'cs.SE']",10.1007/s10664-022-10233-3,,[]
A Learning-based Approach Towards Automated Tuning of SSD Configurations,http://arxiv.org/abs/2110.08685v1,2021-10-17T00:25:21Z,2021-10-17T00:25:21Z,"  Thanks to the mature manufacturing techniques, solid-state drives (SSDs) are
highly customizable for applications today, which brings opportunities to
further improve their storage performance and resource utilization. However,
the SSD efficiency is usually determined by many hardware parameters, making it
hard for developers to manually tune them and determine the optimal SSD
configurations.
  In this paper, we present an automated learning-based framework, named
LearnedSSD, that utilizes both supervised and unsupervised machine learning
(ML) techniques to drive the tuning of hardware configurations for SSDs.
LearnedSSD automatically extracts the unique access patterns of a new workload
using its block I/O traces, maps the workload to previously workloads for
utilizing the learned experiences, and recommends an optimal SSD configuration
based on the validated storage performance. LearnedSSD accelerates the
development of new SSD devices by automating the hard-ware parameter
configurations and reducing the manual efforts. We develop LearnedSSD with
simple yet effective learning algorithms that can run efficiently on multi-core
CPUs. Given a target storage workload, our evaluation shows that LearnedSSD can
always deliver an optimal SSD configuration for the target workload, and this
configuration will not hurt the performance of non-target workloads.
","['\nDaixuan Li\n', '\nJian Huang\n']",,,http://arxiv.org/abs/2110.08685v1,cs.AR,"['cs.AR', 'cs.LG', 'cs.OS']",,,[]
"Report on the ""The Future of the Shell"" Panel at HotOS 2021",http://arxiv.org/abs/2109.11016v1,2021-09-22T20:09:25Z,2021-09-22T20:09:25Z,"  This document summarizes the challenges and possible research directions
around the shell and its ecosystem, collected during and after the HotOS21
Panel on the future of the shell. The goal is to create a snapshot of what a
number of researchers from various disciplines -- connected to the shell to
varying degrees -- think about its future. We hope that this document will
serve as a reference for future research on the shell and its ecosystem.
","['\nMichael Greenberg\n', '\nKonstantinos Kallas\n', '\nNikos Vasilakis\n', '\nStephen Kell\n']",,,http://arxiv.org/abs/2109.11016v1,cs.OS,"['cs.OS', 'cs.PL']",,,[]
SLO beyond the Hardware Isolation Limits,http://arxiv.org/abs/2109.11666v1,2021-09-23T22:10:26Z,2021-09-23T22:10:26Z,"  Performance isolation is a keystone for SLO guarantees with shared resources
in cloud and datacenter environments. To meet SLO requirements, the state of
the art relies on hardware QoS support (e.g., Intel RDT) to allocate shared
resources such as last-level caches and memory bandwidth for co-located
latency-critical applications. As a result, the number of latency-critical
applications that can be deployed on a physical machine is bounded by the
hardware allocation capability. Unfortunately, such hardware capability is very
limited. For example, Intel Xeon E5 v3 processors support at most four
partitions for last-level caches, i.e., at most four applications can have
dedicated resource allocation. This paper discusses the feasibility and
unexplored challenges of providing SLO guarantees beyond the limits of hardware
capability. We present CoCo to show the feasibility and the benefits. CoCo
schedules applications to time-share interference-free partitions as a
transparent software layer. Our evaluation shows that CoCo outperforms
non-partitioned and round-robin approaches by up to 9x and 1.2x.
","['\nHaoran Qiu\n', '\nYongzhou Chen\n', '\nTianyin Xu\n', '\nZbigniew T. Kalbarczyk\n', '\nRavishankar K. Iyer\n']",,,http://arxiv.org/abs/2109.11666v1,cs.OS,"['cs.OS', 'cs.PF']",,,[]
"Analytical Process Scheduling Optimization for Heterogeneous Multi-core
  Systems",http://arxiv.org/abs/2109.04605v1,2021-09-10T01:05:52Z,2021-09-10T01:05:52Z,"  In this paper, we propose the first optimum process scheduling algorithm for
an increasingly prevalent type of heterogeneous multicore (HEMC) system that
combines high-performance big cores and energy-efficient small cores with the
same instruction-set architecture (ISA). Existing algorithms are all
heuristics-based, and the well-known IPC-driven approach essentially tries to
schedule high scaling factor processes on big cores. Our analysis shows that,
for optimum solutions, it is also critical to consider placing long running
processes on big cores. Tests of SPEC 2006 cases on various big-small core
combinations show that our proposed optimum approach is up to 34% faster than
the IPC-driven heuristic approach in terms of total workload completion time.
The complexity of our algorithm is O(NlogN) where N is the number of processes.
Therefore, the proposed optimum algorithm is practical for use.
","['\nChien-Hao Chen\n', '\nRen-Song Tsay\n']",,,http://arxiv.org/abs/2109.04605v1,cs.DC,"['cs.DC', 'cs.OS', 'cs.PF']",,,[]
Understanding Fuchsia Security,http://arxiv.org/abs/2108.04183v1,2021-08-09T17:12:10Z,2021-08-09T17:12:10Z,"  Fuchsia is a new open-source operating system created at Google that is
currently under active development. The core architectural principles guiding
the design and development of the OS include high system modularity and a
specific focus on security and privacy. This paper analyzes the architecture
and the software model of Fuchsia, giving a specific focus on the core security
mechanisms of this new operating system.
","['\nFrancesco Pagano\n', '\nLuca Verderame\n', '\nAlessio Merlo\n']",,"Journal of Wireless Mobile Networks, Ubiquitous Computing, and
  Dependable Applications, September 2021",http://dx.doi.org/10.22667/JOWUA.2021.09.30.047,cs.CR,"['cs.CR', 'cs.OS']",10.22667/JOWUA.2021.09.30.047,,[]
Revisiting Swapping in User-space with Lightweight Threading,http://arxiv.org/abs/2107.13848v1,2021-07-29T09:29:37Z,2021-07-29T09:29:37Z,"  Memory-intensive applications, such as in-memory databases, caching systems
and key-value stores, are increasingly demanding larger main memory to fit
their working sets. Conventional swapping can enlarge the memory capacity by
paging out inactive pages to disks. However, the heavy I/O stack makes the
traditional kernel-based swapping suffers from several critical performance
issues.
  In this paper, we redesign the swapping system and propose LightSwap, an
high-performance user-space swapping scheme that supports paging with both
local SSDs and remote memories. First, to avoids kernel-involving, a novel page
fault handling mechanism is proposed to handle page faults in user-space and
further eliminates the heavy I/O stack with the help of user-space I/O drivers.
Second, we co-design Lightswap with light weight thread (LWT) to improve system
throughput and make it be transparent to user applications. Finally, we propose
a try-catch framework in Lightswap to deal with paging errors which are
exacerbated by the scaling in process technology.
  We implement Lightswap in our production-level system and evaluate it with
YCSB workloads running on memcached. Results show that Ligthswap reduces the
page faults handling latency by 3--5 times, and improves the throughput of
memcached by more than 40% compared with the stat-of-art swapping systems.
","['\nKan Zhong\n', '\nWenlin Cui\n', '\nYouyou Lu\n', '\nQuanzhang Liu\n', '\nXiaodan Yan\n', '\nQizhao Yuan\n', '\nSiwei Luo\n', '\nKeji Huang\n']",,,http://arxiv.org/abs/2107.13848v1,cs.OS,['cs.OS'],,,[]
"An efficient reverse-lookup table based strategy for solving the synonym
  and cache coherence problem in virtually indexed, virtually tagged caches",http://arxiv.org/abs/2108.00444v1,2021-08-01T12:36:13Z,2021-08-01T12:36:13Z,"  Virtually indexed and virtually tagged (VIVT) caches are an attractive option
for micro-processor level-1 caches, because of their fast response time and
because they are cheaper to implement than more complex caches such as
virtually-indexed physical-tagged (VIPT) caches. The level-1 VIVT cache becomes
even simpler to construct if it is implemented as a direct-mapped cache
(VIVT-DM cache). However, VIVT and VIVT-DM caches have some drawbacks. When the
number of sets in the cache is larger than the smallest page size, there is a
possibility of synonyms (two or more virtual addresses mapped to the same
physical address) existing in the cache. Further, maintenance of cache
coherence across multiple processors requires a physical to virtual translation
mechanism in the hardware. We describe a simple, efficient reverse lookup table
based approach to address the synonym and the coherence problems in VIVT (both
set associative and direct-mapped) caches. In particular, the proposed scheme
does not disturb the critical memory access paths in a typical micro-processor,
and requires a low overhead for its implementation. We have implemented and
validated the scheme in the AJIT 32-bit microprocessor core (an implementation
of the SPARC-V8 ISA) and the implementation uses approximately 2% of the gates
and 5.3% of the memory bits in the processor core.
","['\nMadhav P. Desai\n', '\nAniket Deshmukh\n']",13 pages,,http://arxiv.org/abs/2108.00444v1,cs.AR,"['cs.AR', 'cs.OS']",,,[]
"PHiLIP on the HiL: Automated Multi-platform OS Testing with External
  Reference Devices",http://arxiv.org/abs/2107.07255v1,2021-07-15T11:26:31Z,2021-07-15T11:26:31Z,"  Developing an operating system (OS) for low-end embedded devices requires
continuous adaptation to new hardware architectures and components, while
serviceability of features needs to be assured for each individual platform
under tight resource constraints. It is challenging to design a versatile and
accurate heterogeneous test environment that is agile enough to cover a
continuous evolution of the code base and platforms. This mission is even
morehallenging when organized in an agile open-source community process with
many contributors such as for the RIOT OS. Hardware in the Loop (HiL) testing
and Continuous Integration (CI) are automatable approaches to verify
functionality, prevent regressions, and improve the overall quality at
development speed in large community projects. In this paper, we present PHiLIP
(Primitive Hardware in the Loop Integration Product), an open-source external
reference device together with tools that validate the system software while it
controls hardware and interprets physical signals. Instead of focusing on a
specific test setting, PHiLIP takes the approach of a tool-assisted agile HiL
test process, designed for continuous evolution and deployment cycles. We
explain its design, describe how it supports HiL tests, evaluate performance
metrics, and report on practical experiences of employing PHiLIP in an
automated CI test infrastructure. Our initial deployment comprises 22 unique
platforms, each of which executes 98 peripheral tests every night. PHiLIP
allows for easy extension of low-cost, adaptive testing infrastructures but
serves testing techniques and tools to a much wider range of applications.
","['\nKevin Weiss\n', '\nMichel Rottleuthner\n', '\nThomas C. Schmidt\n', '\nMatthias Wählisch\n']",,"ACM Transactions on Embedded Computing Systems, Volume 20, Issue
  5s, 2021",http://dx.doi.org/10.1145/3477040,eess.SY,"['eess.SY', 'cs.OS', 'cs.SE', 'cs.SY', 'B.8.1; D.2.5; C.3; D.4']",10.1145/3477040,,[]
MAGE: Nearly Zero-Cost Virtual Memory for Secure Computation,http://arxiv.org/abs/2106.14651v2,2021-06-23T23:44:27Z,2022-10-27T22:31:58Z,"  Secure Computation (SC) is a family of cryptographic primitives for computing
on encrypted data in single-party and multi-party settings. SC is being
increasingly adopted by industry for a variety of applications. A significant
obstacle to using SC for practical applications is the memory overhead of the
underlying cryptography. We develop MAGE, an execution engine for SC that
efficiently runs SC computations that do not fit in memory. We observe that,
due to their intended security guarantees, SC schemes are inherently oblivious
-- their memory access patterns are independent of the input data. Using this
property, MAGE calculates the memory access pattern ahead of time and uses it
to produce a memory management plan. This formulation of memory management,
which we call memory programming, is a generalization of paging that allows
MAGE to provide a highly efficient virtual memory abstraction for SC. MAGE
outperforms the OS virtual memory system by up to an order of magnitude, and in
many cases, runs SC computations that do not fit in memory at nearly the same
speed as if the underlying machines had unbounded physical memory to fit the
entire computation.
","['\nSam Kumar\n', '\nDavid E. Culler\n', '\nRaluca Ada Popa\n']",19 pages; Accepted to OSDI 2021,,http://arxiv.org/abs/2106.14651v2,cs.OS,"['cs.OS', 'cs.CR']",,,[]
"Windows Kernel Hijacking Is Not an Option: MemoryRanger Comes to the
  Rescue Again",http://arxiv.org/abs/2106.06065v1,2021-06-10T21:56:49Z,2021-06-10T21:56:49Z,"  The security of a computer system depends on OS kernel protection. It is
crucial to reveal and inspect new attacks on kernel data, as these are used by
hackers. The purpose of this paper is to continue research into attacks on
dynamically allocated data in the Windows OS kernel and demonstrate the
capacity of MemoryRanger to prevent these attacks. This paper discusses three
new hijacking attacks on kernel data, which are based on bypassing OS security
mechanisms. The first two hijacking attacks result in illegal access to files
open in exclusive access. The third attack escalates process privileges,
without applying token swapping. Although Windows security experts have issued
new protection features, access attempts to the dynamically allocated data in
the kernel are not fully controlled. MemoryRanger hypervisor is designed to
fill this security gap. The updated MemoryRanger prevents these new attacks as
well as supporting the Windows 10 1903 x64.
",['\nIgor Korkin\n'],"29 pages, 7 figures. Korkin, I. (2021, June 10). Windows Kernel
  Hijacking Is Not an Option: MemoryRanger Comes to the Rescue Again. Journal
  of Digital Forensics, Security and Law, Vol 16, No.1, Article 4. Available
  at: https://commons.erau.edu/jdfsl/vol16/iss1/4",,http://arxiv.org/abs/2106.06065v1,cs.CR,"['cs.CR', 'cs.OS']",,,[]
"Femto-Containers: DevOps on Microcontrollers with Lightweight
  Virtualization & Isolation for IoT Software Modules",http://arxiv.org/abs/2106.12553v2,2021-06-10T10:54:29Z,2021-11-03T16:56:49Z,"  Development, deployment and maintenance of networked software has been
revolutionized by DevOps, which have become essential to boost system software
quality and to enable agile evolution. Meanwhile the Internet of Things (IoT)
connects more and more devices which are not covered by DevOps tools:
low-power, microcontroller-based devices. In this paper, we contribute to
bridge this gap by designing Femto-Containers, a new architecture which enables
containerization, virtualization and secure deployment of software modules
embedded on microcontrollers over low-power networks. As proof-of-concept, we
implemented and evaluated Femto-Containers on popular microcontroller
architectures (Arm Cortex-M, ESP32 and RISC-V), using eBPF virtualization, and
RIOT, a common operating system in this space. We show that Femto-Containers
can virtualize and isolate multiple software modules, executed concurrently,
with very small memory footprint overhead (below 10%) and very small startup
time (tens of microseconds) compared to native code execution. We show that
Femto-Containers can satisfy the constraints of both low-level debug logic
inserted in a hot code path, and high-level business logic coded in a variety
of common programming languages. Compared to prior work, Femto-Containers thus
offer an attractive trade-off in terms of memory footprint, energy consumption,
agility and security.
","['\nKoen Zandberg\n', '\nEmmanuel Baccelli\n']",,,http://arxiv.org/abs/2106.12553v2,cs.SE,"['cs.SE', 'cs.NI', 'cs.OS']",,,[]
PAIO: A Software-Defined Storage Data Plane Framework,http://arxiv.org/abs/2106.03617v3,2021-06-07T13:43:05Z,2021-08-12T22:35:55Z,"  We propose PAIO, the first general-purpose framework that enables system
designers to build custom-made Software-Defined Storage (SDS) data plane
stages. It provides the means to implement storage optimizations adaptable to
different workflows and user-defined policies, and allows straightforward
integration with existing applications and I/O layers. PAIO allows stages to be
integrated with modern SDS control planes to ensure holistic control and
system-wide optimal performance. We demonstrate the performance and
applicability of PAIO with two use cases. The first improves 99th percentile
latency by 4x in industry-standard LSM-based key-value stores. The second
ensures dynamic per-application bandwidth guarantees under shared storage
environments.
","['\nRicardo Macedo\n', '\nYusuke Tanimura\n', '\nJason Haga\n', '\nVijay Chidambaram\n', '\nJosé Pereira\n', '\nJoão Paulo\n']","15 pages, 8 figures",,http://arxiv.org/abs/2106.03617v3,cs.DC,"['cs.DC', 'cs.OS']",,,[]
GearV: A Two-Gear Hypervisor for Mixed-Criticality IoT Systems,http://arxiv.org/abs/2106.04514v1,2021-06-05T14:06:20Z,2021-06-05T14:06:20Z,"  This paper presents GearV, a two-gear lightweight hypervisor architecture to
address the some known challenges. By dividing hypervisor into some partitions,
and dividing scheduling policies into Gear1 and Gear2 respectively, GearV
creates a consolidated platform to run best-effort system and safety-critical
system simultaneously with managed engineering effort. The two-gears
architecture also simplifies retrofitting the virtualization systems. We
believe that GearV can serves as a reasonable hypervisor architecture for the
mix-critical IoT systems.
","['\nKaiwen Long\n', '\nChong Xing\n', '\nYuebin Qi\n', '\nPei Zhang\n', '\nChangsong Wu\n', '\nWenxiao Fang\n', '\nJing Tan\n', '\nJie Chen\n', '\nShiming Zhang\n', '\nZuosheng Wang\n', '\nZuanmin Liu\n', '\nCao Liang\n', '\nJiaxiang Xu\n']","12 pages, 8 figures, 11 tables",,http://arxiv.org/abs/2106.04514v1,cs.DC,"['cs.DC', 'cs.OS']",,,[]
Lightweight Robust Size Aware Cache Management,http://arxiv.org/abs/2105.08770v2,2021-05-18T18:35:40Z,2021-05-23T18:41:00Z,"  Modern key-value stores, object stores, Internet proxy caches, as well as
Content Delivery Networks (CDN) often manage objects of diverse sizes, e.g.,
blobs, video files of different lengths, images with varying resolution, and
small documents. In such workloads, size-aware cache policies outperform
size-oblivious algorithms. Unfortunately, existing size-aware algorithms tend
to be overly complicated and computationally~expensive.
  Our work follows a more approachable pattern; we extend the prevalent
(size-oblivious) TinyLFU cache admission policy to handle variable sized items.
Implementing our approach inside two popular caching libraries only requires
minor changes. We show that our algorithms yield competitive or better
hit-ratios and byte hit-ratios compared to the state of the art size-aware
algorithms such as AdaptSize, LHD, LRB, and GDSF. Further, a runtime comparison
indicates that our implementation is faster by up to x3 compared to the best
alternative, i.e., it imposes much lower CPU overhead.
","['\nGil Einziger\n', '\nOhad Eytan\n', '\nRoy Friedman\n', '\nBenjamin Manes\n']",,,http://arxiv.org/abs/2105.08770v2,cs.OS,"['cs.OS', 'cs.DB']",,,[]
Budget-based real-time Executor for Micro-ROS,http://arxiv.org/abs/2105.05590v2,2021-05-12T11:09:47Z,2021-05-18T10:40:07Z,"  The Robot Operating System (ROS) is a popular robotics middleware framework.
In the last years, it underwent a redesign and reimplementation under the name
ROS~2. It now features QoS-configurable communication and a flexible layered
architecture. Micro-ROS is a variant developed specifically for
resource-constrained microcontrollers (MCU). Such MCUs are commonly used in
robotics for sensors and actuators, for time-critical control functions, and
for safety. While the execution management of ROS 2 has been addressed by an
Executor concept, its lack of real-time capabilities make it unsuitable for
industrial use. Neither defining an execution order nor the assignment of
scheduling parameters to tasks is possible, despite the fact that advanced
real-time scheduling algorithms are well-known and available in modern RTOS's.
For example, the NuttX RTOS supports a variant of the reservation-based
scheduling which is very attractive for industrial applications: It allows to
assign execution time budgets to software components so that a system
integrator can thereby guarantee the real-time requirements of the entire
system. This paper presents for the first time a ROS~2 Executor design which
enables the real-time scheduling capabilities of the operating system. In
particular, we successfully demonstrate the budget-based scheduling of the
NuttX RTOS with a micro-ROS application on an STM32 microcontroller.
","['\nJan Staschulat\nRobert Bosch GmbH, Stuttgart, Germany\n', '\nRalph Lange\nRobert Bosch GmbH, Stuttgart, Germany\n', '\nDakshina Narahari Dasari\nRobert Bosch GmbH, Stuttgart, Germany\n']","4 pages, 5 figures, submitted to RTAS conference",,http://arxiv.org/abs/2105.05590v2,cs.RO,"['cs.RO', 'cs.OS']",,,"['Robert Bosch GmbH, Stuttgart, Germany', 'Robert Bosch GmbH, Stuttgart, Germany', 'Robert Bosch GmbH, Stuttgart, Germany']"
NVCache: A Plug-and-Play NVMM-based I/O Booster for Legacy Systems,http://arxiv.org/abs/2105.10397v2,2021-05-14T14:08:10Z,2021-09-03T12:47:12Z,"  This paper introduces NVCache, an approach that uses a non-volatile main
memory (NVMM) as a write cache to improve the write performance of legacy
applications. We compare NVCache against file systems tailored for NVMM
(Ext4-DAX and NOVA) and with I/O-heavy applications (SQLite, RocksDB). Our
evaluation shows that NVCache reaches the performance level of the existing
state-of-the-art systems for NVMM, but without their limitations: NVCache does
not limit the size of the stored data to the size of the NVMM, and works
transparently with unmodified legacy applications, providing additional
persistence guarantees even when their source code is not available.
","['\nRémi Dulong\n', '\nRafael Pires\n', '\nAndreia Correia\n', '\nValerio Schiavoni\n', '\nPedro Ramalhete\n', '\nPascal Felber\n', '\nGaël Thomas\n']","13 pages, 7 figures, to be published in the 51th IEEE/IFIP
  International Conference on Dependable Systems and Networks (DSN 21)",,http://dx.doi.org/10.1109/DSN48987.2021.00033,cs.DC,"['cs.DC', 'cs.AR', 'cs.OS', '68M20', 'D.4.2; D.4.3; D.4.8']",10.1109/DSN48987.2021.00033,,[]
"Unikraft: Fast, Specialized Unikernels the Easy Way",http://arxiv.org/abs/2104.12721v1,2021-04-26T17:07:07Z,2021-04-26T17:07:07Z,"  Unikernels are famous for providing excellent performance in terms of boot
times, throughput and memory consumption, to name a few metrics. However, they
are infamous for making it hard and extremely time consuming to extract such
performance, and for needing significant engineering effort in order to port
applications to them. We introduce Unikraft, a novel micro-library OS that (1)
fully modularizes OS primitives so that it is easy to customize the unikernel
and include only relevant components and (2) exposes a set of composable,
performance-oriented APIs in order to make it easy for developers to obtain
high performance.
  Our evaluation using off-the-shelf applications such as nginx, SQLite, and
Redis shows that running them on Unikraft results in a 1.7x-2.7x performance
improvement compared to Linux guests. In addition, Unikraft images for these
apps are around 1MB, require less than 10MB of RAM to run, and boot in around
1ms on top of the VMM time (total boot time 3ms-40ms). Unikraft is a Linux
Foundation open source project and can be found at www.unikraft.org.
","['\nSimon Kuenzer\n', '\nVlad-Andrei Bădoiu\n', '\nHugo Lefeuvre\n', '\nSharan Santhanam\n', '\nAlexander Jung\n', '\nGaulthier Gain\n', '\nCyril Soldani\n', '\nCostin Lupu\n', '\nŞtefan Teodorescu\n', '\nCosti Răducanu\n', '\nCristian Banu\n', '\nLaurent Mathy\n', '\nRăzvan Deaconescu\n', '\nCostin Raiciu\n', '\nFelipe Huici\n']","19 pages, 22 figures, 7 tables, conference proceedings",,http://dx.doi.org/10.1145/3447786.3456248,cs.OS,['cs.OS'],10.1145/3447786.3456248,,[]
"SoCRATES: System-on-Chip Resource Adaptive Scheduling using Deep
  Reinforcement Learning",http://arxiv.org/abs/2104.14354v3,2021-04-28T15:46:02Z,2021-10-12T01:19:49Z,"  Deep Reinforcement Learning (DRL) is being increasingly applied to the
problem of resource allocation for emerging System-on-Chip (SoC) applications,
and has shown remarkable promises. In this paper, we introduce SoCRATES (SoC
Resource AdapTivE Scheduler), an extremely efficient DRL-based SoC scheduler
which maps a wide range of hierarchical jobs to heterogeneous resources within
SoC using the Eclectic Interaction Matching (EIM) technique. It is noted that
the majority of SoC resource management approaches have been targeting makespan
minimization with fixed number of jobs in the system. In contrast, SoCRATES
aims at minimizing average latency in a steady-state condition while assigning
tasks in the ready queue to heterogeneous resources (processing elements). We
first show that the latency-minimization-driven SoC applications operate
high-frequency job workload and distributed/parallel job execution. We then
demonstrate SoCRATES successfully addresses the challenge of concurrent
observations caused by the task dependency inherent in the latency minimization
objective. Extensive tests show that SoCRATES outperforms other existing neural
and non-neural schedulers with as high as 38% gain in latency reduction under a
variety of job types and incoming rates. The resulting model is also compact in
size and has very favorable energy consumption behaviors, making it highly
practical for deployment in future SoC systems with built-in neural
accelerator.
","['\nTegg Taekyong Sung\n', '\nBo Ryu\n']","This paper has been accepted for publication by 20th IEEE
  International Conference on Machine Learning and Applications (ICMLA 2021).
  The copyright is with the IEEE",,http://arxiv.org/abs/2104.14354v3,cs.OS,"['cs.OS', 'cs.DC']",,,[]
WELES: Policy-driven Runtime Integrity Enforcement of Virtual Machines,http://arxiv.org/abs/2104.14862v1,2021-04-30T09:37:14Z,2021-04-30T09:37:14Z,"  Trust is of paramount concern for tenants to deploy their security-sensitive
services in the cloud. The integrity of VMs in which these services are
deployed needs to be ensured even in the presence of powerful adversaries with
administrative access to the cloud. Traditional approaches for solving this
challenge leverage trusted computing techniques, e.g., vTPM, or hardware CPU
extensions, e.g., AMD SEV. But, they are vulnerable to powerful adversaries, or
they provide only load time (not runtime) integrity measurements of VMs.
  We propose WELES, a protocol allowing tenants to establish and maintain trust
in VM runtime integrity of software and its configuration. WELES is transparent
to the VM configuration and setup. It performs an implicit attestation of VMs
during a secure login and binds the VM integrity state with the secure
connection. Our prototype's evaluation shows that WELES is practical and incurs
low performance overhead.
","['\nWojciech Ozga\n', '\nDo Le Quoc\n', '\nChristof Fetzer\n']",,"Proceedings of 2021 IEEE International Conference on Cloud
  Computing (IEEE CLOUD'21)",http://arxiv.org/abs/2104.14862v1,cs.CR,"['cs.CR', 'cs.OS']",,,[]
"Continual Learning Approach for Improving the Data and Computation
  Mapping in Near-Memory Processing System",http://arxiv.org/abs/2104.13671v1,2021-04-28T09:50:35Z,2021-04-28T09:50:35Z,"  The resurgence of near-memory processing (NMP) with the advent of big data
has shifted the computation paradigm from processor-centric to memory-centric
computing. To meet the bandwidth and capacity demands of memory-centric
computing, 3D memory has been adopted to form a scalable memory-cube network.
Along with NMP and memory system development, the mapping for placing data and
guiding computation in the memory-cube network has become crucial in driving
the performance improvement in NMP. However, it is very challenging to design a
universal optimal mapping for all applications due to unique application
behavior and intractable decision space. In this paper, we propose an
artificially intelligent memory mapping scheme, AIMM, that optimizes data
placement and resource utilization through page and computation remapping. Our
proposed technique involves continuously evaluating and learning the impact of
mapping decisions on system performance for any application. AIMM uses a neural
network to achieve a near-optimal mapping during execution, trained using a
reinforcement learning algorithm that is known to be effective for exploring a
vast design space. We also provide a detailed AIMM hardware design that can be
adopted as a plugin module for various NMP systems. Our experimental evaluation
shows that AIMM improves the baseline NMP performance in single and multiple
program scenario by up to 70% and 50%, respectively.
","['\nPritam Majumder\n', '\nJiayi Huang\n', '\nSungkeun Kim\n', '\nAbdullah Muzahid\n', '\nDylan Siegers\n', '\nChia-Che Tsai\n', '\nEun Jung Kim\n']",,,http://arxiv.org/abs/2104.13671v1,cs.AR,"['cs.AR', 'cs.LG', 'cs.NI', 'cs.OS']",,,[]
Android OS CASE STUDY,http://arxiv.org/abs/2104.09487v1,2021-04-19T17:46:29Z,2021-04-19T17:46:29Z,"  Android is a mobile operating system based on a modified version of the Linux
kernel and other open source software, designed primarily for touchscreen
mobile devices such as smartphones and tablets. It is an operating system for
low powered devices that run on battery and are full of hardware like Global
Positioning System (GPS) receivers, cameras, light and orientation sensors,
Wi-Fi and LTE (4G telephony) connectivity and a touch screen. Like all
operating systems, Android enables applications to make use of the hardware
features through abstraction and provide a defined environment for
applications. The study includes following topic: Background And History
Android Architecture Kernel And StartUp Process Process Management Deadlock CPU
Scheduling Memory Management Storage Management I/O Battery Optimization
","['\nMayank Goel\n', '\nGourav Singal\n']",,,http://arxiv.org/abs/2104.09487v1,cs.OS,"['cs.OS', 'cs.AR']",,,[]
WLFC: Write Less in Flash-based Cache,http://arxiv.org/abs/2104.05306v5,2021-04-12T09:26:04Z,2023-11-15T01:49:00Z,"  Flash-based disk caches, for example Bcache and Flashcache, has gained
tremendous popularity in industry in the last decade because of its low energy
consumption, non-volatile nature and high I/O speed. But these cache systems
have a worse write performance than the read performance because of the
asymmetric I/O costs and the the internal GC mechanism. In addition to the
performance issues, since the NAND flash is a type of EEPROM device, the
lifespan is also limited by the Program/Erase (P/E) cycles. So how to improve
the performance and the lifespan of flash-based caches in write-intensive
scenarios has always been a hot issue. Benefiting from Open-Channel SSDs
(OCSSDs), we propose a write-friendly flash-based disk cache system, which is
called WLFC (Write Less in the Flash-based Cache). In WLFC, a strictly
sequential writing method is used to minimize the write amplification. A new
replacement algorithm for the write buffer is designed to minimize the erase
count caused by the evicting. And a new data layout strategy is designed to
minimize the metadata size persisted in SSDs. As a result, the Over-Provisioned
(OP) space is completely removed, the erase count of the flash is greatly
reduced, and the metadata size is 1/10 or less than that in BCache. Even with a
small amount of metadata, the data consistency after the crash is still
guaranteed. Compared with the existing mechanism, WLFC brings a 7%-80%
reduction in write latency, a 1.07*-4.5* increment in write throughput, and a
50%-88.9% reduction in erase count, with a moderate overhead in read
performance.
","['\nChaos Dong\n', '\nFang Wang\n', '\nJianshun Zhang\n']",Need revision,,http://arxiv.org/abs/2104.05306v5,cs.OS,['cs.OS'],,,[]
Supporting Multiprocessor Resource Synchronization Protocols in RTEMS,http://arxiv.org/abs/2104.06366v2,2021-04-13T17:23:21Z,2022-06-20T12:42:32Z,"  When considering recurrent tasks in real-time systems, concurrent accesses to
shared resources, can cause race conditions or data corruptions. Such a problem
has been extensively studied since the 1990s, and numerous resource
synchronization protocols have been developed for both uni-processor and
multiprocessor real-time systems, with the assumption that the implementation
overheads are negligible. However, in practice, the implementation overheads
may impact the performance of different protocols depending upon the practiced
scenarios, e.g., resources are accessed locally or remotely, and tasks spin or
suspend themselves when the requested resources are not available. In this
paper, to show the applicability of different protocols in real-world systems,
we detail the implementation of several state-of-the-art multiprocessor
resource synchronization protocols in RTEMS. To study the impact of the
implementation overheads, we deploy these implemented protocols on a real
platform with synthetic task set. The measured results illustrate that the
developed resource synchronization protocols in RTEMS are comparable to the
existed protocol, i.e., MrsP.
","['\nJunjie Shi\n', '\nJan Duy Thien Pham\n', '\nMalte Münch\n', '\nJan Viktor Hafemeister\n', '\nJian-Jia Chen\n', '\nKuan-Hsun Chen\n']","6 pages, 5 figures, presented in 16th annual workshop on Operating
  Systems Platforms for Embedded Real-Time applications (OSPERT'22)",,http://arxiv.org/abs/2104.06366v2,cs.OS,['cs.OS'],,,[]
"Detecting and Mitigating Network Packet Overloads on Real-Time Devices
  in IoT Systems",http://arxiv.org/abs/2104.02393v1,2021-04-06T09:49:56Z,2021-04-06T09:49:56Z,"  Manufacturing, automotive, and aerospace environments use embedded systems
for control and automation and need to fulfill strict real-time guarantees. To
facilitate more efficient business processes and remote control, such devices
are being connected to IP networks. Due to the difficulty in predicting network
packets and the interrelated workloads of interrupt handlers and drivers,
devices controlling time critical processes stand under the risk of missing
process deadlines when under high network loads. Additionally, devices at the
edge of large networks and the internet are subject to a high risk of load
spikes and network packet overloads.
  In this paper, we investigate strategies to detect network packet overloads
in real-time and present four approaches to adaptively mitigate local deadline
misses. In addition to two strategies mitigating network bursts with and
without hysteresis, we present and discuss two novel mitigation algorithms,
called Budget and Queue Mitigation. In an experimental evaluation, all
algorithms showed mitigating effects, with the Queue Mitigation strategy
enabling most packet processing while preventing lateness of critical tasks.
","['\nRobert Danicki\n', '\nMartin Haug\n', '\nIlja Behnke\n', '\nLaurenz Mädje\n', '\nLauritz Thamsen\n']",EdgeSys '21,,http://dx.doi.org/10.1145/3434770.3459733,cs.NI,"['cs.NI', 'cs.OS']",10.1145/3434770.3459733,,[]
SchedGuard: Protecting against Schedule Leaks Using Linux Containers,http://arxiv.org/abs/2104.04528v1,2021-04-09T13:16:06Z,2021-04-09T13:16:06Z,"  Real-time systems have recently been shown to be vulnerable to timing
inference attacks, mainly due to their predictable behavioral patterns.
Existing solutions such as schedule randomization lack the ability to protect
against such attacks, often limited by the system's real-time nature. This
paper presents SchedGuard: a temporal protection framework for Linux-based hard
real-time systems that protects against posterior scheduler side-channel
attacks by preventing untrusted tasks from executing during specific time
segments. SchedGuard is integrated into the Linux kernel using cgroups, making
it amenable to use with container frameworks. We demonstrate the effectiveness
of our system using a realistic radio-controlled rover platform and
synthetically generated workloads. Not only is SchedGuard able to protect
against the attacks mentioned above, but it also ensures that the real-time
tasks/containers meet their temporal requirements.
","['\nJiyang Chen\n', '\nTomasz Kloda\n', '\nAyoosh Bansal\n', '\nRohan Tabish\n', '\nChien-Ying Chen\n', '\nBo Liu\n', '\nSibin Mohan\n', '\nMarco Caccamo\n', '\nLui Sha\n']",,,http://arxiv.org/abs/2104.04528v1,cs.CR,"['cs.CR', 'cs.OS']",,,[]
"TermAdventure: Interactively Teaching UNIX Command Line, Text Adventure
  Style",http://arxiv.org/abs/2104.05456v1,2021-04-12T13:23:58Z,2021-04-12T13:23:58Z,"  Introductory UNIX courses are typically organized as lectures, accompanied by
a set of exercises, whose solutions are submitted to and reviewed by the
lecturers. While this arrangement has become standard practice, it often
requires the use of an external tool or interface for submission and does not
automatically check its correctness. That in turn leads to increased workload
and makes it difficult to deal with potential plagiarism.
  In this work we present TermAdventure (TA), a suite of tools for creating
interactive UNIX exercises. These resemble text adventure games, which immerse
the user in a text environment and let them interact with it using textual
commands. In our case the ''adventure'' takes place inside a UNIX system and
the user interaction happens via the standard UNIX command line. The adventure
is a set of exercises, which are presented and automatically evaluated by the
system, all from within the command line environment. The suite is released
under an open source license, has minimal dependencies and can be used either
on a UNIX-style server or a desktop computer running any major OS platform
through Docker.
  We also reflect on our experience of using the presented suite as the primary
teaching tool for an introductory UNIX course for Data Scientists and discuss
the implications of its deployment in similar courses. The suite is released
under the terms of an open-source license at
\url{https://github.com/NaiveNeuron/TermAdventure}.
","['\nMarek Šuppa\n', '\nOndrej Jariabka\n', '\nAdrián Matejov\n', '\nMarek Nagy\n']",Accepted at ITiCSE 2021,,http://dx.doi.org/10.1145/3430665.3456387,cs.CY,"['cs.CY', 'cs.HC', 'cs.OS']",10.1145/3430665.3456387,,[]
"A First Look at RISC-V Virtualization from an Embedded Systems
  Perspective",http://arxiv.org/abs/2103.14951v2,2021-03-27T17:44:29Z,2021-08-16T16:24:21Z,"  This article describes the first public implementation and evaluation of the
latest version of the RISC-V hypervisor extension (H-extension v0.6.1)
specification in a Rocket chip core. To perform a meaningful evaluation for
modern multi-core embedded and mixedcriticality systems, we have ported Bao, an
open-source static partitioning hypervisor, to RISC-V. We have also extended
the RISC-V platformlevel interrupt controller (PLIC) to enable direct guest
interrupt injection with low and deterministic latency and we have enhanced the
timer infrastructure to avoid trap and emulation overheads. Experiments were
carried out in FireSim, a cycle-accurate, FPGA-accelerated simulator, and the
system was also successfully deployed and tested in a Zynq UltraScale+ MPSoC
ZCU104. Our hardware implementation was opensourced and is currently in use by
the RISC-V community towards the ratification of the H-extension specification.
","['\nBruno Sá\n', '\nJosé Martins\n', '\nSandro Pinto\n']",,,http://arxiv.org/abs/2103.14951v2,cs.AR,"['cs.AR', 'cs.OS']",,,[]
Page Table Management for Heterogeneous Memory Systems,http://arxiv.org/abs/2103.10779v1,2021-03-16T08:46:59Z,2021-03-16T08:46:59Z,"  Modern enterprise servers are increasingly embracing tiered memory systems
with a combination of low latency DRAMs and large capacity but high latency
non-volatile main memories (NVMMs) such as Intel's Optane DC PMM. Prior works
have focused on efficient placement and migration of data on a tiered memory
system, but have not studied the optimal placement of page tables.
  Explicit and efficient placement of page tables is crucial for large memory
footprint applications with high TLB miss rates because they incur dramatically
higher page walk latency when page table pages are placed in NVMM. We show that
(i) page table pages can end up on NVMM even when enough DRAM memory is
available and (ii) page table pages that spill over to NVMM due to DRAM memory
pressure are not migrated back later when memory is available in DRAM.
  We study the performance impact of page table placement in a tiered memory
system and propose an efficient and transparent page table management technique
that (i) applies different placement policies for data and page table pages,
(ii) introduces a differentiating policy for page table pages by placing a
small but critical part of the page table in DRAM, and (iii) dynamically and
judiciously manages the rest of the page table by transparently migrating the
page table pages between DRAM and NVMM. Our implementation on a real system
equipped with Intel's Optane NVMM running Linux reduces the page table walk
cycles by 12% and total cycles by 20% on an average. This improves the runtime
by 20% on an average for a set of synthetic and real-world large memory
footprint applications when compared with various default Linux kernel
techniques.
","['\nSandeep Kumar\n', '\nAravinda Prasad\n', '\nSmruti R. Sarangi\n', '\nSreenivas Subramoney\n']",,,http://arxiv.org/abs/2103.10779v1,cs.DC,"['cs.DC', 'cs.OS', 'cs.PF']",,,[]
BPF for storage: an exokernel-inspired approach,http://arxiv.org/abs/2102.12922v1,2021-02-25T15:22:38Z,2021-02-25T15:22:38Z,"  The overhead of the kernel storage path accounts for half of the access
latency for new NVMe storage devices. We explore using BPF to reduce this
overhead, by injecting user-defined functions deep in the kernel's I/O
processing stack. When issuing a series of dependent I/O requests, this
approach can increase IOPS by over 2.5$\times$ and cut latency by half, by
bypassing kernel layers and avoiding user-kernel boundary crossings. However,
we must avoid losing important properties when bypassing the file system and
block layer such as the safety guarantees of the file system and translation
between physical blocks addresses and file offsets. We sketch potential
solutions to these problems, inspired by exokernel file systems from the late
90s, whose time, we believe, has finally come!
","['\nYu Jian Wu\n', '\nHongyi Wang\n', '\nYuhong Zhong\n', '\nAsaf Cidon\n', '\nRyan Stutsman\n', '\nAmy Tai\n', '\nJunfeng Yang\n']",,,http://arxiv.org/abs/2102.12922v1,cs.OS,"['cs.OS', 'cs.DB']",,,[]
A flow-based IDS using Machine Learning in eBPF,http://arxiv.org/abs/2102.09980v3,2021-02-19T15:20:51Z,2022-03-04T16:51:28Z,"  eBPF is a new technology which allows dynamically loading pieces of code into
the Linux kernel. It can greatly speed up networking since it enables the
kernel to process certain packets without the involvement of a userspace
program. So far eBPF has been used for simple packet filtering applications
such as firewalls or Denial of Service protection. We show that it is possible
to develop a flow based network intrusion detection system based on machine
learning entirely in eBPF. Our solution uses a decision tree and decides for
each packet whether it is malicious or not, considering the entire previous
context of the network flow. We achieve a performance increase of over 20%
compared to the same solution implemented as a userspace program.
","['\nMaximilian Bachl\n', '\nJoachim Fabini\n', '\nTanja Zseby\n']",,,http://arxiv.org/abs/2102.09980v3,cs.CR,"['cs.CR', 'cs.LG', 'cs.NI', 'cs.OS']",,,[]
"SoftTRR: Protect Page Tables Against RowHammer Attacks using
  Software-only Target Row Refresh",http://arxiv.org/abs/2102.10269v2,2021-02-20T06:20:33Z,2021-12-12T11:14:22Z,"  Rowhammer attacks that corrupt level-1 page tables to gain kernel privilege
are the most detrimental to system security and hard to mitigate. However,
recently proposed software-only mitigations are not effective against such
kernel privilege escalation attacks. In this paper, we propose an effective and
practical software-only defense, called SoftTRR, to protect page tables from
all existing rowhammer attacks on x86. The key idea of SoftTRR is to refresh
the rows occupied by page tables when a suspicious rowhammer activity is
detected. SoftTRR is motivated by DRAM-chip-based target row refresh (ChipTRR)
but eliminates its main security limitation (i.e., ChipTRR tracks a limited
number of rows and thus can be bypassed by many-sided hammer). Specifically,
SoftTRR protects an unlimited number of page tables by tracking memory accesses
to the rows that are in close proximity to page-table rows and refreshing the
page-table rows once the tracked access count exceeds a pre-defined threshold.
We implement a prototype of SoftTRR as a loadable kernel module, and evaluate
its security effectiveness, performance overhead, and memory consumption. The
experimental results show that SoftTRR protects page tables from real-world
rowhammer attacks and incurs small performance overhead as well as memory cost.
","['\nZhi Zhang\n', '\nYueqiang Cheng\n', '\nMinghua Wang\n', '\nWei He\n', '\nWenhao Wang\n', '\nNepal Surya\n', '\nYansong Gao\n', '\nKang Li\n', '\nZhe Wang\n', '\nChenggang Wu\n']",,,http://arxiv.org/abs/2102.10269v2,cs.CR,"['cs.CR', 'cs.OS', 'cs.SE']",,,[]
Reading from External Memory,http://arxiv.org/abs/2102.11198v1,2021-02-22T17:24:08Z,2021-02-22T17:24:08Z,"  Modern external memory is represented by several device classes. At present,
HDD, SATA SSD and NVMe SSD are widely used. Recently ultra-low latency SSD such
as Intel Optane became available on the market. Each of these types exhibits
it's own pattern for throughput, latency and parallelism. To achieve the
highest performance one has to pick an appropriate I/O interface provided by
the operating system. In this work we present a detailed overview and
evaluation of modern storage reading performance with regard to available Linux
synchronous and asynchronous interfaces. While throughout this work we aim for
the highest throughput we also measure latency and CPU usage. We provide this
report in hope the detailed results could be interesting to both researchers
and practitioners.
",['\nRuslan Savchenko\n'],,,http://arxiv.org/abs/2102.11198v1,cs.DC,"['cs.DC', 'cs.OS', 'cs.PF']",,,[]
BPFContain: Fixing the Soft Underbelly of Container Security,http://arxiv.org/abs/2102.06972v1,2021-02-13T18:12:34Z,2021-02-13T18:12:34Z,"  Linux containers currently provide limited isolation guarantees. While
containers separate namespaces and partition resources, the patchwork of
mechanisms used to ensure separation cannot guarantee consistent security
semantics. Even worse, attempts to ensure complete coverage results in a
mishmash of policies that are difficult to understand or audit. Here we present
BPFContain, a new container confinement mechanism designed to integrate with
existing container management systems. BPFContain combines a simple yet
flexible policy language with an eBPF-based implementation that allows for
deployment on virtually any Linux system running a recent kernel. In this
paper, we present BPFContain's policy language, describe its current
implementation as integrated into docker, and present benchmarks comparing it
with current container confinement technologies.
","['\nWilliam Findlay\n', '\nDavid Barrera\n', '\nAnil Somayaji\n']",,,http://arxiv.org/abs/2102.06972v1,cs.CR,"['cs.CR', 'cs.OS']",,,[]
Restoring Uniqueness in MicroVM Snapshots,http://arxiv.org/abs/2102.12892v1,2021-02-04T21:56:28Z,2021-02-04T21:56:28Z,"  Code initialization -- the step of loading code, executing static code,
filling caches, and forming re-used connections -- tends to dominate cold-start
time in serverless compute systems such as AWS Lambda. Post-initialization
memory snapshots, cloned and restored on start, have emerged as a viable
solution to this problem, with incremental snapshot and fast restore support in
VMMs like Firecracker.
  Saving memory introduces the challenge of managing high-value memory
contents, such as cryptographic secrets. Cloning introduces the challenge of
restoring the uniqueness of the VMs, to allow them to do unique things like
generate UUIDs, secrets, and nonces. This paper examines solutions to these
problems in the every microsecond counts context of serverless cold-start, and
discusses the state-of-the-art of available solutions. We present two new
interfaces aimed at solving this problem -- MADV\_WIPEONSUSPEND and SysGenId --
and compare them to alternative solutions.
","['\nMarc Brooker\n', '\nAdrian Costin Catangiu\n', '\nMike Danilov\n', '\nAlexander Graf\n', '\nColm MacCarthaigh\n', '\nAndrei Sandu\n']",,,http://arxiv.org/abs/2102.12892v1,cs.CR,"['cs.CR', 'cs.DC', 'cs.OS']",,,[]
User-Aware Power Management for Mobile Devices,http://arxiv.org/abs/2101.08885v1,2021-01-21T23:17:42Z,2021-01-21T23:17:42Z,"  The power management techniques to extend battery lifespan is becoming
increasingly important due to longer user applications' running time in mobile
devices. Even when users do not use any applications, battery lifespan
decreases continually. It occurs because of service daemons of mobile platform
and network-based data synchronization operations. In this paper, we propose a
new power management system that recognizes the idle time of the device to
reduce the battery consumption of mobile devices.
","['\nGeunsik Lim\n', '\nChangwoo Min\n', '\nDong Hyun Kang\n', '\nYoung Ik Eom\n']",,,http://dx.doi.org/10.1109/GCCE.2013.6664780,cs.AR,"['cs.AR', 'cs.OS']",10.1109/GCCE.2013.6664780,,[]
Resilient Virtualized Systems Using ReHype,http://arxiv.org/abs/2101.09282v1,2021-01-23T22:43:28Z,2021-01-23T22:43:28Z,"  System-level virtualization introduces critical vulnerabilities to failures
of the software components that implement virtualization -- the virtualization
infrastructure (VI). To mitigate the impact of such failures, we introduce a
resilient VI (RVI) that can recover individual VI components from failure,
caused by hardware or software faults, transparently to the hosted virtual
machines (VMs). Much of the focus is on the ReHype mechanism for recovery from
hypervisor failures, that can lead to state corruption and to inconsistencies
among the states of system components. ReHype's implementation for the Xen
hypervisor was done incrementally, using fault injection results to identify
sources of critical corruption and inconsistencies. This implementation
involved 900 LOC, with memory space overhead of 2.1MB. Fault injection
campaigns, with a variety of fault types, show that ReHype can successfully
recover, in less than 750ms, from over 88% of detected hypervisor failures. In
addition to ReHype, recovery mechanisms for the other VI components are
described. The overall effectiveness of our RVI is evaluated hosting a Web
service application, on a cluster of VMs. With faults in any VI component, for
over 87% of detected failures, our recovery mechanisms allow services provided
by the application to be continuously maintained despite the resulting failures
of VI components.
","['\nMichael Le\n', '\nYuval Tamir\n']",,,http://arxiv.org/abs/2101.09282v1,cs.SE,"['cs.SE', 'cs.OS']",,,[]
"User-Level Memory Scheduler for Optimizing Application Performance in
  NUMA-Based Multicore Systems",http://arxiv.org/abs/2101.09284v1,2021-01-21T23:28:55Z,2021-01-21T23:28:55Z,"  Multicore CPU architectures have been established as a structure for
general-purpose systems for high-performance processing of applications. Recent
multicore CPU has evolved as a system architecture based on non-uniform memory
architecture. For the technique of using the kernel space that shifts the tasks
to the ideal memory node, the characteristics of the applications of the
user-space cannot be considered. Therefore, kernel level approaches cannot
execute memory scheduling to recognize the importance of user applications.
Moreover, users need to run applications after sufficiently understanding the
multicore CPU based on non-uniform memory architecture to ensure the high
performance of the user's applications. This paper presents a user-space memory
scheduler that allocates the ideal memory node for tasks by monitoring the
characteristics of non-uniform memory architecture. From our experiment, the
proposed system improved the performance of the application by up to 25%
compared to the existing system.
","['\nGeunsik Lim\n', '\nSang-Bum Suh\n']",,,http://dx.doi.org/10.1109/ICSESS.2014.6933553,cs.DC,"['cs.DC', 'cs.OS']",10.1109/ICSESS.2014.6933553,,[]
BB: Booting Booster for Consumer Electronics with Modern OS,http://arxiv.org/abs/2101.09360v1,2021-01-21T08:32:13Z,2021-01-21T08:32:13Z,"  Unconventional computing platforms have spread widely and rapidly following
smart phones and tablets: consumer electronics such as smart TVs and digital
cameras. For such devices, fast booting is a critical requirement; waiting tens
of seconds for a TV or a camera to boot up is not acceptable, unlike a PC or
smart phone. Moreover, the software platforms of these devices have become as
rich as conventional computing devices to provide comparable services. As a
result, the booting procedure to start every required OS service, hardware
component, and application, the quantity of which is ever increasing, may take
unbearable time for most consumers. To accelerate booting, this paper
introduces \textit{Booting Booster} (BB), which is used in all 2015 Samsung
Smart TV models, and which runs the Linux-based Tizen OS. BB addresses the init
scheme of Linux, which launches initial user-space OS services and applications
and manages the life cycles of all user processes, by identifying and isolating
booting-critical tasks, deferring non-critical tasks, and enabling execution of
more tasks in parallel. BB has been successfully deployed in Samsung Smart TV
2015 models achieving a cold boot in 3.5 s (compared to 8.1 s with full
commercial-grade optimizations without BB) without the need for suspend-to-RAM
or hibernation. After this successful deployment, we have released the source
code via http://opensource.samsung.com, and BB will be included in the
open-source OS, Tizen (http://tizen.org).
","['\nGeunsik Lim\n', '\nMyungJoo Ham\n']",,,http://dx.doi.org/10.1145/2901318.2901320,cs.DC,"['cs.DC', 'cs.OS']",10.1145/2901318.2901320,,[]
"HyCoR: Fault-Tolerant Replicated Containers Based on Checkpoint and
  Replay",http://arxiv.org/abs/2101.09584v1,2021-01-23T21:08:25Z,2021-01-23T21:08:25Z,"  HyCoR is a fully-operational fault tolerance mechanism for multiprocessor
workloads, based on container replication, using a hybrid of checkpointing and
replay. HyCoR derives from two insights regarding replication mechanisms: 1)
deterministic replay can overcome a key disadvantage of checkpointing alone --
unacceptably long delays of outputs to clients, and 2) checkpointing can
overcome a key disadvantage of active replication with deterministic replay
alone -- vulnerability to even rare replay failures due to an untracked
nondeterministic events. With HyCoR, the primary sends periodic checkpoints to
the backup and logs the outcomes of sources of nondeterminism. Outputs to
clients are delayed only by the short time it takes to send the corresponding
log to the backup. Upon primary failure, the backup replays only the short
interval since the last checkpoint, thus minimizing the window of
vulnerability. HyCoR includes a ""best effort"" mechanism that results in a high
recovery rate even in the presence of data races, as long as their rate is low.
The evaluation includes measurement of the recovery rate and recovery latency
based on fault injection. On average, HyCoR delays responses to clients by less
than 1ms and recovers in less than 1s. For a set of eight real-world
benchmarks, if data races are eliminated, the performance overhead of HyCoR is
under 59%.
","['\nDiyu Zhou\n', '\nYuval Tamir\n']",,,http://arxiv.org/abs/2101.09584v1,cs.DC,"['cs.DC', 'cs.OS']",,,[]
"Enhancing Application Performance by Memory Partitioning in Android
  Platforms",http://arxiv.org/abs/2101.10707v1,2021-01-26T11:03:02Z,2021-01-26T11:03:02Z,"  This paper suggests a new memory partitioning scheme that can enhance process
lifecycle, while avoiding Low Memory Killer and Out-of-Memory Killer operations
on mobile devices. Our proposed scheme offers the complete concept of virtual
memory nodes in operating systems of Android devices.
","['\nGeunsik Lim\n', '\nChangwoo Min\n', '\nYoung Ik Eom\n']",,,http://dx.doi.org/10.1109/ICCE.2013.6487055,cs.OS,"['cs.OS', 'cs.PF']",10.1109/ICCE.2013.6487055,,[]
"OpenUVR: an Open-Source System Framework for Untethered Virtual Reality
  Applications",http://arxiv.org/abs/2101.07327v1,2021-01-18T21:02:16Z,2021-01-18T21:02:16Z,"  Advancements in heterogeneous computing technologies enable the significant
potential of virtual reality (VR) applications. To offer the best user
experience (UX), a system should adopt an untethered, wireless-network-based
architecture to transfer VR content between the user and the content generator.
However, modern wireless network technologies make implementing such an
architecture challenging, as VR applications require superior video quality --
with high resolution, high frame rates, and very low latency.
  This paper presents OpenUVR, an open-source framework that uses commodity
hardware components to satisfy the demands of interactive, real-time VR
applications. OpenUVR significantly improves UX through a redesign of the
system stack and addresses the most time-sensitive issues associated with
redundant memory copying in modern computing systems. OpenUVR presents a
cross-layered VR datapath to avoid redundant data operations and computation
among system components, OpenUVR customizes the network stack to eliminate
unnecessary memory operations incurred by mismatching data formats in each
layer, and OpenUVR uses feedback from mobile devices to remove memory buffers.
  Together, these modifications allow OpenUVR to reduce VR application delays
to 14.32 ms, meeting the 20 ms minimum latency in avoiding motion sickness. As
an open-source system that is fully compatible with commodity hardware, OpenUVR
offers the research community an opportunity to develop, investigate, and
optimize applications for untethered, high-performance VR architectures.
","['\nAlec Rohloff\n', '\nZackary Allen\n', '\nKung-Min Lin\n', '\nJoshua Okrend\n', '\nChengyi Nie\n', '\nYu-Chia Liu\n', '\nHung-Wei Tseng\n']",,,http://arxiv.org/abs/2101.07327v1,cs.NI,"['cs.NI', 'cs.HC', 'cs.OS']",,,[]
Thread Evolution Kit for Optimizing Thread Operations on CE/IoT Devices,http://arxiv.org/abs/2101.08062v1,2021-01-20T10:54:59Z,2021-01-20T10:54:59Z,"  Most modern operating systems have adopted the one-to-one thread model to
support fast execution of threads in both multi-core and single-core systems.
This thread model, which maps the kernel-space and user-space threads in a
one-to-one manner, supports quick thread creation and termination in
high-performance server environments. However, the performance of time-critical
threads is degraded when multiple threads are being run in low-end CE devices
with limited system resources. When a CE device runs many threads to support
diverse application functionalities, low-level hardware specifications often
lead to significant resource contention among the threads trying to obtain
system resources. As a result, the operating system encounters challenges, such
as excessive thread context switching overhead, execution delay of
time-critical threads, and a lack of virtual memory for thread stacks. This
paper proposes a state-of-the-art Thread Evolution Kit (TEK) that consists of
three primary components: a CPU Mediator, Stack Tuner, and Enhanced Thread
Identifier. From the experiment, we can see that the proposed scheme
significantly improves user responsiveness (7x faster) under high CPU
contention compared to the traditional thread model. Also, TEK solves the
segmentation fault problem that frequently occurs when a CE application
increases the number of threads during its execution.
","['\nGeunsik Lim\n', '\nDonghyun Kang\n', '\nYoung Ik Eom\n']",,,http://dx.doi.org/10.1109/TCE.2020.3033328,cs.OS,"['cs.OS', 'cs.DC', 'cs.PF']",10.1109/TCE.2020.3033328,,[]
"Virtual Memory Partitioning for Enhancing Application Performance in
  Mobile Platforms",http://arxiv.org/abs/2101.08877v1,2021-01-21T22:54:21Z,2021-01-21T22:54:21Z,"  Recently, the amount of running software on smart mobile devices is gradually
increasing due to the introduction of application stores. The application store
is a type of digital distribution platform for application software, which is
provided as a component of an operating system on a smartphone or tablet.
Mobile devices have limited memory capacity and, unlike server and desktop
systems, due to their mobility they do not have a memory slot that can expand
the memory capacity. Low memory killer (LMK) and out-of-memory killer (OOMK)
are widely used memory management solutions in mobile systems. They forcibly
terminate applications when the available physical memory becomes insufficient.
In addition, before the forced termination, the memory shortage incurs
thrashing and fragmentation, thus slowing down application performance.
Although the existing page reclamation mechanism is designed to secure
available memory, it could seriously degrade user responsiveness due to the
thrashing. Memory management is therefore still important especially in mobile
devices with small memory capacity. This paper presents a new memory
partitioning technique that resolves the deterioration of the existing
application life cycle induced by LMK and OOMK. It provides a completely
isolated virtual memory node at the operating system level. Evaluation results
demonstrate that the proposed method improves application execution time under
memory shortage, compared with methods in previous studies.
","['\nGeunsik Lim\n', '\nChangwoo Min\n', '\nYoung Ik Eom\n']",,,http://dx.doi.org/10.1109/TCE.2013.6689690,cs.AR,"['cs.AR', 'cs.OS', 'cs.PF']",10.1109/TCE.2013.6689690,,[]
"Tuning the Frequency of Periodic Data Movements over Hybrid Memory
  Systems",http://arxiv.org/abs/2101.07200v1,2021-01-15T01:30:48Z,2021-01-15T01:30:48Z,"  Emerging hybrid memory systems that comprise technologies such as Intel's
Optane DC Persistent Memory, exhibit disparities in the access speeds and
capacity ratios of their heterogeneous memory components. This breaks many
assumptions and heuristics designed for traditional DRAM-only platforms. High
application performance is feasible via dynamic data movement across memory
units, which maximizes the capacity use of DRAM while ensuring efficient use of
the aggregate system resources. Newly proposed solutions use performance models
and machine intelligence to optimize which and how much data to move
dynamically; however, the decision of when to move this data is based on
empirical selection of time intervals, or left to the applications. Our
experimental evaluation shows that failure to properly configure the data
movement frequency can lead to 10%-100% slowdown for a given data movement
policy; yet, there is no established methodology on how to properly configure
this value for a given workload, platform and policy. We propose Cori, a
system-level tuning solution that identifies and extracts the necessary
application-level data reuse information, and guides the selection of data
movement frequency to deliver gains in application performance and system
resource efficiency. Experimental evaluation shows that Cori configures data
movement frequencies that provide application performance within 3% of the
optimal one, and that it can achieve this up to 5x more quickly than random or
brute-force approaches. System-level validation of Cori on a platform with DRAM
and Intel's Optane DC PMEM confirms its practicality and tuning efficiency.
","['\nThaleia Dimitra Doudali\n', '\nDaniel Zahka\n', '\nAda Gavrilovska\n']",,,http://arxiv.org/abs/2101.07200v1,cs.DC,"['cs.DC', 'cs.OS']",,,[]
Enabling Large Neural Networks on Tiny Microcontrollers with Swapping,http://arxiv.org/abs/2101.08744v3,2021-01-14T21:38:57Z,2021-09-01T14:53:54Z,"  Running neural networks (NNs) on microcontroller units (MCUs) is becoming
increasingly important, but is very difficult due to the tiny SRAM size of MCU.
Prior work proposes many algorithm-level techniques to reduce NN memory
footprints, but all at the cost of sacrificing accuracy and generality, which
disqualifies MCUs for many important use cases. We investigate a system
solution for MCUs to execute NNs out of core: dynamically swapping NN data
chunks between an MCU's tiny SRAM and its large, low-cost external flash.
Out-of-core NNs on MCUs raise multiple concerns: execution slowdown, storage
wear out, energy consumption, and data security. We present a study showing
that none is a showstopper; the key benefit -- MCUs being able to run large NNs
with full accuracy and generality -- triumphs the overheads. Our findings
suggest that MCUs can play a much greater role in edge intelligence.
","['\nHongyu Miao\n', '\nFelix Xiaozhu Lin\n']",,,http://arxiv.org/abs/2101.08744v3,cs.AR,"['cs.AR', 'cs.OS']",,,[]
New Directions in Cloud Programming,http://arxiv.org/abs/2101.01159v1,2021-01-04T18:42:54Z,2021-01-04T18:42:54Z,"  Nearly twenty years after the launch of AWS, it remains difficult for most
developers to harness the enormous potential of the cloud. In this paper we lay
out an agenda for a new generation of cloud programming research aimed at
bringing research ideas to programmers in an evolutionary fashion. Key to our
approach is a separation of distributed programs into a PACT of four facets:
Program semantics, Availablity, Consistency and Targets of optimization. We
propose to migrate developers gradually to PACT programming by lifting familiar
code into our more declarative level of abstraction. We then propose a
multi-stage compiler that emits human-readable code at each stage that can be
hand-tuned by developers seeking more control. Our agenda raises numerous
research challenges across multiple areas including language design, query
optimization, transactions, distributed consistency, compilers and program
synthesis.
","['\nAlvin Cheung\n', '\nNatacha Crooks\n', '\nJoseph M. Hellerstein\n', '\nMae Milano\n']",,CIDR 2021,http://arxiv.org/abs/2101.01159v1,cs.DC,"['cs.DC', 'cs.DB', 'cs.OS', 'cs.PL']",,,[]
SimBricks: End-to-End Network System Evaluation with Modular Simulation,http://arxiv.org/abs/2012.14219v3,2020-12-28T13:03:04Z,2022-07-06T10:10:41Z,"  Full system ""end-to-end"" measurements in physical testbeds are the gold
standard for network systems evaluation but are often not feasible. When
physical testbeds are not available we frequently turn to simulation for
evaluation. Unfortunately, existing simulators are insufficient for end-to-end
evaluation, as they either cannot simulate all components, or simulate them
with inadequate detail. We address this through modular simulation, flexibly
combining and connecting multiple existing simulators for different components,
including processor and memory, devices, and network, into virtual end-to-end
testbeds tuned for each use-case. Our architecture, SimBricks, combines
well-defined component interfaces for extensibility and modularity, efficient
communication channels for local and distributed simulation, and a co-designed
efficient synchronization mechanism for accurate timing across simulators. We
demonstrate SimBricks scales to 1000 simulated hosts, each running a full
software stack including Linux, and that it can simulate testbeds with existing
NIC and switch RTL implementations. We also reproduce key findings from prior
work in congestion control, NIC architecture, and in-network computing in
SimBricks.
","['\nHejing Li\n', '\nJialin Li\n', '\nAntoine Kaufmann\n']","17 pages, 13 figures, appeared in In Proceedings of ACM SIGCOMM 2022
  Conference (SIGCOMM '22), August 22-26, 2022, Amsterdam, Netherlands",,http://dx.doi.org/10.1145/3544216.3544253,cs.DC,"['cs.DC', 'cs.NI', 'cs.OS']",10.1145/3544216.3544253,,[]
Sensifi: A Wireless Sensing System for Ultra-High-Rate Applications,http://arxiv.org/abs/2012.14635v2,2020-12-29T07:17:38Z,2021-06-20T00:16:47Z,"  Wireless Sensor Networks (WSNs) are being used in various applications such
as structural health monitoring and industrial control. Since energy efficiency
is one of the major design factors, the existing WSNs primarily rely on
low-power, low-rate wireless technologies such as 802.15.4 and Bluetooth. In
this paper, we strive to tackle the challenges of developing ultra-high-rate
WSNs based on 802.11 (WiFi) standard by proposing Sensifi. As an illustrative
application of this system, we consider vibration test monitoring of spacecraft
and identify system design requirements and challenges. Our main contributions
are as follows. First, we propose packet encoding methods to reduce the
overhead of assigning accurate timestamps to samples. Second, we propose energy
efficiency methods to enhance the system's lifetime. Third, we reduce the
overhead of processing outgoing packets through network stack to enhance
sampling rate and mitigate sampling rate instability. Fourth, we study and
reduce the delay of processing incoming packets through network stack to
enhance the accuracy of time synchronization among nodes. Fifth, we propose a
low-power node design for ultra-high-rate applications. Sixth, we use our node
design to empirically evaluate the system.
","['\nChia-Chi Li\n', '\nVikram K. Ramanna\n', '\nDaniel Webber\n', '\nCole Hunter\n', '\nTyler Hack\n', '\nBehnam Dezfouli\n']",,,http://arxiv.org/abs/2012.14635v2,cs.NI,"['cs.NI', 'cs.OS', 'cs.PF']",,,[]
"Fairness-Oriented User Scheduling for Bursty Downlink Transmission Using
  Multi-Agent Reinforcement Learning",http://arxiv.org/abs/2012.15081v14,2020-12-30T08:41:51Z,2022-06-19T04:45:35Z,"  In this work, we develop practical user scheduling algorithms for downlink
bursty traffic with emphasis on user fairness. In contrast to the conventional
scheduling algorithms that either equally divides the transmission time slots
among users or maximizing some ratios without physcial meanings, we propose to
use the 5%-tile user data rate (5TUDR) as the metric to evaluate user fairness.
Since it is difficult to directly optimize 5TUDR, we first cast the problem
into the stochastic game framework and subsequently propose a Multi-Agent
Reinforcement Learning (MARL)-based algorithm to perform distributed
optimization on the resource block group (RBG) allocation. Furthermore, each
MARL agent is designed to take information measured by network counters from
multiple network layers (e.g. Channel Quality Indicator, Buffer size) as the
input states while the RBG allocation as action with a proposed reward function
designed to maximize 5TUDR. Extensive simulation is performed to show that the
proposed MARL-based scheduler can achieve fair scheduling while maintaining
good average network throughput as compared to conventional schedulers.
","['\nMingqi Yuan\n', '\nQi Cao\n', '\nMan-on Pun\n', '\nYi Chen\n']","16 pages, 13 figures",APSIPA Transactions on Signal and Information Processing (2022),http://arxiv.org/abs/2012.15081v14,cs.OS,"['cs.OS', 'cs.AI', 'cs.LG', 'cs.NI']",,,[]
Flat-Combining-Based Persistent Data Structures for Non-Volatile Memory,http://arxiv.org/abs/2012.12868v4,2020-12-23T18:34:45Z,2021-12-08T19:57:34Z,"  Flat combining (FC) is a synchronization paradigm in which a single thread,
holding a global lock, collects requests by multiple threads for accessing a
concurrent data structure and applies their combined requests to it. Although
FC is sequential, it significantly reduces synchronization overheads and cache
invalidations and thus often provides better performance than that of lock-free
implementations. The recent emergence of non-volatile memory (NVM) technologies
increases the interest in the development of persistent concurrent objects.
These are objects that are able to recover from system failures and ensure
consistency by retaining their state in NVM and fixing it, if required, upon
recovery. Of particular interest are detectable objects that, in addition to
ensuring consistency, allow recovery code to infer if a failed operation took
effect before the crash and, if it did, obtain its response. In this work, we
present the first FC-based persistent object implementations. Specifically, we
introduce a detectable FC-based implementation of a concurrent LIFO stack, a
concurrent FIFO queue, and a double-ended queue. Our empirical evaluation
establishes that due to flat combining, the novel implementations require a
much smaller number of costly persistence instructions than competing
algorithms and are therefore able to significantly outperform them.
","['\nMatan Rusanovsky\n', '\nHagit Attiya\n', '\nOhad Ben-Baruch\n', '\nTom Gerby\n', '\nDanny Hendler\n', '\nPedro Ramalhete\n']",,,http://arxiv.org/abs/2012.12868v4,cs.DC,"['cs.DC', 'cs.OS']",,,[]
"On the Applicability of PEBS based Online Memory Access Tracking for
  Heterogeneous Memory Management at Scale",http://arxiv.org/abs/2011.13432v1,2020-11-26T18:39:55Z,2020-11-26T18:39:55Z,"  Operating systems have historically had to manage only a single type of
memory device. The imminent availability of heterogeneous memory devices based
on emerging memory technologies confronts the classic single memory model and
opens a new spectrum of possibilities for memory management. Transparent data
movement between different memory devices based on access patterns of
applications is a desired feature to make optimal use of such devices and to
hide the complexity of memory management to the end-user. However, capturing
memory access patterns of an application at runtime comes at a cost, which is
particularly challenging for large scale parallel applications that may be
sensitive to system noise.
  In this work, we focus on the access pattern profiling phase prior to the
actual memory relocation. We study the feasibility of using Intel's Processor
Event-Based Sampling (PEBS) feature to record memory accesses by sampling at
runtime and study the overhead at scale. We have implemented a custom PEBS
driver in the IHK/McKernel lightweight multi-kernel operating system, one of
whose advantages is minimal system interference due to the lightweight kernel's
simple design compared to other OS kernels such as Linux. We present the PEBS
overhead of a set of scientific applications and show the access patterns
identified in noise-sensitive HPC applications. Our results show that clear
access patterns can be captured with a 10% overhead in the worst-case and 1% in
the best case when running on up to 128k CPU cores (2,048 Intel Xeon Phi
Knights Landing nodes). We conclude that online memory access profiling using
PEBS at large scale is promising for memory management in heterogeneous memory
environments.
","['\nAleix Roca Nonell\n', '\nBalazs Gerofi\n', '\nLeonardo Bautista-Gomez\n', '\nDominique Martinet\n', '\nVicenç Beltran Querol\n', '\nYutaka Ishikawa\n']","8 pages, 16 figures, conference","Proceedings of the Workshop on Memory Centric High Performance
  Computing (2018) 50-57",http://dx.doi.org/10.1145/3286475.3286477,cs.OS,"['cs.OS', 'D.4.8']",10.1145/3286475.3286477,,[]
Routing Approach for P2P Systems Over MANET Network,http://arxiv.org/abs/2101.03109v1,2020-11-27T12:27:31Z,2020-11-27T12:27:31Z,"  Thanks to the great progress in mobile and wireless technologies,
Internet-distributed applications like P2P file sharing are nowadays deployed
over MANET (i.e., P2P mobile systems). These applications allow users to search
and share diverse multimedia resources over MANET. Due the nature of MANET, P2P
mobile systems brought up many new thriving challenges regarding the query
routing issue. To tackle this problem, we introduce a novel context-aware query
routing protocol for unstructured P2P mobile file sharing systems. Our protocol
(i) locates relevant peers sharing pertinent resources for user's query and
(ii) ensures that those peers would be reached by considering different MANET
constraints (e.g., query content, peer mobility, battery energy, peer load). In
order to consider all these constraints for choosing the relevant peers, we are
based on the technique for order preferences by similarity to ideal solution
(TOPSIS). We implemented the proposed protocol and compared its routing
efficiency and retrieval effectiveness with another protocol taken from the
literature. Experimental results show that our scheme carries out better than
the baseline protocol with respect to accuracy
","['\nSofian Hamad\n', '\nTaoufik Yeferny\n']",,"IJCSNS International Journal of Computer Science and Network
  Security, VOL.20 No.3, March 2020",http://arxiv.org/abs/2101.03109v1,cs.NI,"['cs.NI', 'cs.OS']",,,[]
"Minimal Virtual Machines on IoT Microcontrollers: The Case of Berkeley
  Packet Filters with rBPF",http://arxiv.org/abs/2011.12047v2,2020-11-24T11:46:00Z,2020-12-09T08:15:30Z,"  Virtual machines (VM) are widely used to host and isolate software modules.
However, extremely small memory and low-energy budgets have so far prevented
wide use of VMs on typical microcontroller-based IoT devices. In this paper, we
explore the potential of two minimal VM approaches on such low-power hardware.
We design rBPF, a register-based VM based on extended Berkeley Packet Filters
(eBPF). We compare it with a stack-based VM based on WebAssembly (Wasm) adapted
for embedded systems. We implement prototypes of each VM, hosted in the IoT
operating system RIOT. We perform measurements on commercial off-the-shelf IoT
hardware. Unsurprisingly, we observe that both Wasm and rBPF virtual machines
yield execution time and memory overhead, compared to not using a VM. We show
however that this execution time overhead is tolerable for low-throughput,
low-energy IoT devices. We further show that, while using a VM based on Wasm
entails doubling the memory budget for a simple networked IoT application using
a 6LoWPAN/CoAP stack, using a VM based on rBPF requires only negligible memory
overhead (less than 10% more memory). rBPF is thus a promising approach to host
small software modules, isolated from OS software, and updatable on-demand,
over low-power networks.
","['\nKoen Zandberg\n', '\nEmmanuel Baccelli\n']",,In proceedings of IFIP/IEEE PEMWN 2020,http://arxiv.org/abs/2011.12047v2,cs.NI,"['cs.NI', 'cs.OS']",,,[]
"No Crash, No Exploit: Automated Verification of Embedded Kernels",http://arxiv.org/abs/2011.15065v2,2020-11-30T18:03:28Z,2021-05-21T21:47:51Z,"  The kernel is the most safety- and security-critical component of many
computer systems, as the most severe bugs lead to complete system crash or
exploit. It is thus desirable to guarantee that a kernel is free from these
bugs using formal methods, but the high cost and expertise required to do so
are deterrent to wide applicability. We propose a method that can verify both
absence of runtime errors (i.e. crashes) and absence of privilege escalation
(i.e. exploits) in embedded kernels from their binary executables. The method
can verify the kernel runtime independently from the application, at the
expense of only a few lines of simple annotations. When given a specific
application, the method can verify simple kernels without any human
intervention. We demonstrate our method on two different use cases: we use our
tool to help the development of a new embedded real-time kernel, and we verify
an existing industrial real-time kernel executable with no modification.
Results show that the method is fast, simple to use, and can prevent real
errors and security vulnerabilities.
","['\nOlivier Nicole\n', '\nMatthieu Lemerre\n', '\nSébastien Bardin\n', '\nXavier Rival\n']","Published in IEEE Real-Time and Embedded Technology and Applications
  Symposium (RTAS'21)",,http://dx.doi.org/10.1109/RTAS52030.2021.00011,cs.CR,"['cs.CR', 'cs.OS']",10.1109/RTAS52030.2021.00011,,[]
Leveraging Architectural Support of Three Page Sizes with Trident,http://arxiv.org/abs/2011.12092v1,2020-11-24T13:54:55Z,2020-11-24T13:54:55Z,"  Large pages are commonly deployed to reduce address translation overheads for
big-memory workloads. Modern x86-64 processors from Intel and AMD support two
large page sizes -- 1GB and 2MB. However, previous works on large pages have
primarily focused on 2MB pages, partly due to lack of substantial evidence on
the profitability of 1GB pages to real-world applications. We argue that in
fact, inadequate system software support is responsible for a decade of
underutilized hardware support for 1GB pages.
  Through extensive experimentation on a real system, we demonstrate that 1GB
pages can improve performance over 2MB pages, and when used in tandem with 2MB
pages for an important set of applications; the support for the latter is
crucial but missing in current systems. Our design and implementation of
\trident{} in Linux fully exploit hardware supported large pages by dynamically
and transparently allocating 1GB, 2MB, and 4KB pages as deemed suitable.
\trident{} speeds up eight memory-intensive applications by {$18\%$}, on
average, over Linux's use of 2MB pages. We also propose \tridentpv{}, an
extension to \trident{} that effectively virtualizes 1GB pages via copy-less
promotion and compaction in the guest OS. Overall, this paper shows that even
GB-sized pages have considerable practical significance with adequate software
enablement, in turn motivating architects to continue investing/innovating in
large pages.
","['\nVenkat Sri Sai Ram\n', '\nAshish Panwar\n', '\nArkaprava Basu\n']","13 pages, 16 figures, 5 tables",,http://arxiv.org/abs/2011.12092v1,cs.OS,"['cs.OS', 'cs.AR', 'cs.PF', 'D.4']",,,[]
"Phoebe: Reuse-Aware Online Caching with Reinforcement Learning for
  Emerging Storage Models",http://arxiv.org/abs/2011.07160v1,2020-11-13T22:55:15Z,2020-11-13T22:55:15Z,"  With data durability, high access speed, low power efficiency and byte
addressability, NVMe and SSD, which are acknowledged representatives of
emerging storage technologies, have been applied broadly in many areas.
However, one key issue with high-performance adoption of these technologies is
how to properly define intelligent cache layers such that the performance gap
between emerging technologies and main memory can be well bridged. To this end,
we propose Phoebe, a reuse-aware reinforcement learning framework for the
optimal online caching that is applicable for a wide range of emerging storage
models. By continuous interacting with the cache environment and the data
stream, Phoebe is capable to extract critical temporal data dependency and
relative positional information from a single trace, becoming ever smarter over
time. To reduce training overhead during online learning, we utilize periodical
training to amortize costs. Phoebe is evaluated on a set of Microsoft cloud
storage workloads. Experiment results show that Phoebe is able to close the gap
of cache miss rate from LRU and a state-of-the-art online learning based cache
policy to the Belady's optimal policy by 70.3% and 52.6%, respectively.
","['\nNan Wu\n', '\nPengcheng Li\n']",,,http://arxiv.org/abs/2011.07160v1,cs.PF,"['cs.PF', 'cs.AI', 'cs.LG', 'cs.OS']",,,[]
"SIMF: Single-Instruction Multiple-Flush Mechanism for Processor Temporal
  Isolation",http://arxiv.org/abs/2011.10249v2,2020-11-20T07:48:27Z,2022-04-13T07:10:20Z,"  Microarchitectural timing attacks are a type of information leakage attack,
which exploit the time-shared microarchitectural components, such as caches,
translation look-aside buffers (TLBs), branch prediction unit (BPU), and
speculative execution, in modern processors to leak critical information from a
victim process or thread. To mitigate such attacks, the mechanism for flushing
the on-core state is extensively used by operating-system-level solutions,
since on-core state is too expensive to partition. In these systems, the
flushing operations are implemented in software (using cache maintenance
instructions), which severely limit the efficiency of timing attack protection.
  To bridge this gap, we propose specialized hardware support, a
single-instruction multiple-flush (SIMF) mechanism to flush the core-level
state, which consists of L1 caches, BPU, TLBs, and register file. We
demonstrate SIMF by implementing it as an ISA extension, i.e., flushx
instruction, in scalar in-order RISC-V processor. The resultant processor is
prototyped on Xilinx ZCU102 FPGA and validated with state-of-art seL4
microkernel, Linux kernel in multi-core scenarios, and a cache timing attack.
Our evaluation shows that SIMF significantly alleviates the overhead of
flushing by more than a factor of two in execution time and reduces dynamic
instruction count by orders-of-magnitude.
","['\nTuo Li\n', '\nBradley Hopkins\n', '\nSri Parameswaran\n']",,,http://arxiv.org/abs/2011.10249v2,cs.CR,"['cs.CR', 'cs.AR', 'cs.OS']",,,[]
Efficient Data Management with a Flexible Address Space,http://arxiv.org/abs/2011.01024v3,2020-11-02T15:48:20Z,2021-08-04T18:08:18Z,"  Data management applications store their data using structured files in which
data are usually sorted to serve indexing and queries. However, in-place
insertions and removals of data are not naturally supported in a file's address
space. To avoid repeatedly rewriting existing data in a sorted file to admit
changes in place, applications usually employ extra layers of indirections,
such as mapping tables and logs, to admit changes out of place. However, this
approach leads to increased access cost and excessive complexity.
  This paper presents a novel storage engine that provides a flexible address
space, where in-place updates of arbitrary-sized data, such as insertions and
removals, can be performed efficiently. With this mechanism, applications can
manage sorted data in a linear address space with minimal complexity. Extensive
evaluations show that a key-value store built on top of it can achieve high
performance and efficiency with a simple implementation.
","['\nChen Chen\n', '\nWenshao Zhong\n', '\nXingbo Wu\n']",14 pages incl. references; 13 figures; 5 tables,,http://arxiv.org/abs/2011.01024v3,cs.OS,"['cs.OS', 'cs.DB', 'cs.DS', 'D.4.3; E.5']",,,[]
Hints and Principles for Computer System Design,http://arxiv.org/abs/2011.02455v3,2020-11-03T17:40:36Z,2021-05-20T15:45:04Z,"  This new long version of my 1983 paper suggests the goals you might have for
your system -- Simple, Timely, Efficient, Adaptable, Dependable, Yummy (STEADY)
-- and techniques for achieving them -- Approximate, Incremental, Divide &
Conquer (AID). It also gives some principles for system design that are more
than just hints, and many examples of how to apply the ideas.
",['\nButler Lampson\n'],"There is also a short version of this paper, about half the length of
  this one",,http://arxiv.org/abs/2011.02455v3,cs.DC,"['cs.DC', 'cs.OS', 'cs.SE']",,,[]
Disaggregated Accelerator Management System for Cloud Data Centers,http://arxiv.org/abs/2010.13594v1,2020-10-26T14:07:06Z,2020-10-26T14:07:06Z,"  A conventional data center that consists of monolithic-servers is confronted
with limitations including lack of operational flexibility, low resource
utilization, low maintainability, etc. Resource disaggregation is a promising
solution to address the above issues. We propose a concept of disaggregated
cloud data center architecture called Flow-in-Cloud (FiC) that enables an
existing cluster computer system to expand an accelerator pool through a
high-speed network. FlowOS-RM manages the entire pool resources, and deploys a
user job on a dynamically constructed slice according to a user request. This
slice consists of compute nodes and accelerators where each accelerator is
attached to the corresponding compute node. This paper demonstrates the
feasibility of FiC in a proof of concept experiment running a distributed deep
learning application on the prototype system. The result successfully warrants
the applicability of the proposed system.
","['\nRyousei Takano\n', '\nKuniyasu Suzaki\n']","To appear in IEICE Transactions on Information and Systems, 2020",,http://arxiv.org/abs/2010.13594v1,cs.OS,['cs.OS'],,,[]
"Towards Efficiently Establishing Mutual Distrust Between Host
  Application and Enclave for SGX",http://arxiv.org/abs/2010.12400v1,2020-10-23T13:43:45Z,2020-10-23T13:43:45Z,"  Since its debut, SGX has been used in many applications, e.g., secure data
processing. However, previous systems usually assume a trusted enclave and
ignore the security issues caused by an untrusted enclave. For instance, a
vulnerable (or even malicious) third-party enclave can be exploited to attack
the host application and the rest of the system. In this paper, we propose an
efficient mechanism to confine an untrusted enclave's behaviors. The threats of
an untrusted enclave come from the enclave-host asymmetries. They can be abused
to access arbitrary memory regions of its host application, jump to any code
location after leaving the enclave and forge the stack register to manipulate
the saved context. Our solution breaks such asymmetries and establishes mutual
distrust between the host application and the enclave. It leverages Intel MPK
for efficient memory isolation and the x86 single-step debugging mechanism to
capture the event when an enclave is existing. It then performs the integrity
check for the jump target and the stack pointer. We have solved two practical
challenges and implemented a prototype system. The evaluation with multiple
micro-benchmarks and representative real-world applications demonstrated the
efficiency of our system, with less than 4% performance overhead.
","['\nYuan Chen\n', '\nJiaqi Li\n', '\nGuorui Xu\n', '\nYajin Zhou\n', '\nZhi Wang\n', '\nCong Wang\n', '\nKui Ren\n']",,,http://arxiv.org/abs/2010.12400v1,cs.CR,"['cs.CR', 'cs.OS']",,,[]
"Experimental Analysis of Communication Relaying Delay in Low-Energy
  Ad-hoc Networks",http://arxiv.org/abs/2010.15572v2,2020-10-29T13:44:58Z,2020-12-10T09:37:11Z,"  In recent years, more and more applications use ad-hoc networks for local M2M
communications, but in some cases such as when using WSNs, the software
processing delay induced by packets relaying may not be negligible. In this
paper, we planned and carried out a delay measurement experiment using
Raspberry Pi Zero W. The results demonstrated that, in low-energy ad-hoc
networks, processing delay of the application is always too large to ignore; it
is at least ten times greater than the kernel routing and corresponds to 30% of
the transmission delay. Furthermore, if the task is CPU-intensive, such as
packet encryption, the processing delay can be greater than the transmission
delay and its behavior is represented by a simple linear model. Our findings
indicate that the key factor for achieving QoS in ad-hoc networks is an
appropriate node-to-node load balancing that takes into account the CPU
performance and the amount of traffic passing through each node.
","['\nTaichi Miya\n', '\nKohta Ohshima\n', '\nYoshiaki Kitaguchi\n', '\nKatsunori Yamaoka\n']","6 pages, 19 figures, IEEE Consumer Communications & Networking
  Conference 2021",,http://arxiv.org/abs/2010.15572v2,cs.NI,"['cs.NI', 'cs.OS', 'cs.PF']",,,[]
Stage Lookup: Accelerating Path Lookup using Directory Shortcuts,http://arxiv.org/abs/2010.08741v1,2020-10-17T08:34:08Z,2020-10-17T08:34:08Z,"  The lookup procedure in Linux costs a significant portion of file accessing
time as the virtual file system (VFS) traverses the file path components one
after another. The lookup procedure becomes more time consuming when
applications frequently access files, especially those with small sizes. We
propose Stage Lookup, which dynamically caches popular directories to speed up
lookup procedures and further reduce file accessing latency. The core of Stage
Lookup is to cache popular dentries as shortcuts, so that path walks do not
bother to traverse directory trees from the root. Furthermore, Stage Lookup
enriches backward path walks as it treats the directory tree in a VFS as an
undirected map. We implement a Stage Lookup prototype and integrate it into
Linux Kernel v3.14. Our extensive performance evaluation studies show that
Stage Lookup offers up to 46.9% performance gain compared to ordinary path
lookup schemes. Furthermore, Stage Lookup shows smaller performance overheads
in rename and chmod operations compared to the original method of the kernel.
","['\nYanliang Zou\n', '\nTongliang Deng\n', '\nJian Zhang\n', '\nChen Chen\n', '\nShu Yin\n']","8 pages, 9 figures",,http://arxiv.org/abs/2010.08741v1,cs.OS,['cs.OS'],,,[]
"PIMOD: A Tool for Configuring Single-Board Computer Operating System
  Images",http://arxiv.org/abs/2010.07833v1,2020-10-15T15:52:25Z,2020-10-15T15:52:25Z,"  Computer systems used in the field of humanitarian technology are often based
on general-purpose single-board computers, such as Raspberry Pis. While these
systems offer great flexibility for developers and users, configuration and
deployment either introduces overhead by executing scripts on multiple devices
or requires deeper technical understanding when building operating system
images for such small computers from scratch. In this paper, we present PIMOD,
a software tool for configuring operating system images for single-board
computer systems. We propose a simple yet comprehensive configuration language.
In a configuration profile, called Pifile, a small set of commands is used to
describe the configuration of an operating system image. Virtualization
techniques are used during the execution of the profile in order to be
distribution and platform independent. Commands can be issued in the guest
operating system, providing access to the distribution specific tools, e.g., to
configure hardware parameters. The implementation of PIMOD is made public under
a free and open source license. PIMOD is evaluated in terms of user benefits,
performance compared to on-system configuration, and applicability across
different hardware platforms and operating systems.
","['\nJonas Höchst\n', '\nAlvar Penning\n', '\nPatrick Lampe\n', '\nBernd Freisleben\n']",,,http://arxiv.org/abs/2010.07833v1,cs.OS,"['cs.OS', 'cs.CY']",,,[]
Akita: A CPU scheduler for virtualized Clouds,http://arxiv.org/abs/2009.09104v1,2020-09-18T22:07:17Z,2020-09-18T22:07:17Z,"  Clouds inherit CPU scheduling policies of operating systems. These policies
enforce fairness while leveraging best-effort mechanisms to enhance
responsiveness of all schedulable entities, irrespective of their service level
objectives (SLOs). This leads to unpredictable performance that forces cloud
providers to enforce strict reservation and isolation policies to prevent
high-criticality services (e.g., Memcached) from being impacted by
low-criticality ones (e.g., logging), which results in low utilization.
  In this paper, we present Akita, a hypervisor CPU scheduler that delivers
predictable performance at high utilization. Akita allows virtual machines
(VMs) to be categorized into high- and low-criticality VMs. Akita provides
strong guarantees on the ability of cloud providers to meet SLOs of
high-criticality VMs, by temporarily slowing down low-criticality VMs if
necessary. Akita, therefore, allows the co-existence of high and
low-criticality VMs on the same physical machine, leading to higher
utilization. The effectiveness of Akita is demonstrated by a prototype
implementation in the Xen hypervisor. We present experimental results that show
the many advantages of adopting Akita as the hypervisor CPU scheduler. In
particular, we show that high-criticality Memcached VMs are able to deliver
predictable performance despite being co-located with low-criticality CPU-bound
VMs.
","['\nEsmail Asyabi\n', '\nAzer Bestavros\n', '\nRenato Mancuso\n', '\nRichard West\n', '\nErfan Sharafzadeh\n']",,,http://arxiv.org/abs/2009.09104v1,cs.OS,['cs.OS'],,,[]
DEAP Cache: Deep Eviction Admission and Prefetching for Cache,http://arxiv.org/abs/2009.09206v1,2020-09-19T10:23:15Z,2020-09-19T10:23:15Z,"  Recent approaches for learning policies to improve caching, target just one
out of the prefetching, admission and eviction processes. In contrast, we
propose an end to end pipeline to learn all three policies using machine
learning. We also take inspiration from the success of pretraining on large
corpora to learn specialized embeddings for the task. We model prefetching as a
sequence prediction task based on past misses. Following previous works
suggesting that frequency and recency are the two orthogonal fundamental
attributes for caching, we use an online reinforcement learning technique to
learn the optimal policy distribution between two orthogonal eviction
strategies based on them. While previous approaches used the past as an
indicator of the future, we instead explicitly model the future frequency and
recency in a multi-task fashion with prefetching, leveraging the abilities of
deep networks to capture futuristic trends and use them for learning eviction
and admission. We also model the distribution of the data in an online fashion
using Kernel Density Estimation in our approach, to deal with the problem of
caching non-stationary data. We present our approach as a ""proof of concept"" of
learning all three components of cache strategies using machine learning and
leave improving practical deployment for future work.
","['\nAyush Mangal\n', '\nJitesh Jain\n', '\nKeerat Kaur Guliani\n', '\nOmkar Bhalerao\n']",,,http://arxiv.org/abs/2009.09206v1,cs.OS,"['cs.OS', 'cs.AI', 'cs.LG', 'stat.ML']",,,[]
A FaaS File System for Serverless Computing,http://arxiv.org/abs/2009.09845v1,2020-09-16T08:16:27Z,2020-09-16T08:16:27Z,"  Serverless computing with cloud functions is quickly gaining adoption, but
constrains programmers with its limited support for state management. We
introduce a shared file system for cloud functions. It offers familiar POSIX
semantics while taking advantage of distinctive aspects of cloud functions to
achieve scalability and performance beyond what traditional shared file systems
can offer. We take advantage of the function-grained fault tolerance model of
cloud functions to proceed optimistically using local state, safe in the
knowledge that we can restart if cache reads or lock activity cannot be
reconciled upon commit. The boundaries of cloud functions provide implicit
commit and rollback points, giving us the flexibility to use transaction
processing techniques without changing the programming model or API. This
allows a variety of stateful sever-based applications to benefit from the
simplicity and scalability of serverless computing, often with little or no
modification.
","['\nJohann Schleier-Smith\n', '\nLeonhard Holz\n', '\nNathan Pemberton\n', '\nJoseph M. Hellerstein\n']",,,http://arxiv.org/abs/2009.09845v1,cs.DC,"['cs.DC', 'cs.OS']",,,[]
"MigrOS: Transparent Operating Systems Live Migration Support for
  Containerised RDMA-applications",http://arxiv.org/abs/2009.06988v2,2020-09-15T11:15:25Z,2020-10-23T12:21:28Z,"  Major data centre providers are introducing RDMA-based networks for their
tenants, as well as for operating the underlying infrastructure. In comparison
to traditional socket-based network stacks, RDMA-based networks offer higher
throughput, lower latency and reduced CPU overhead. However, transparent
checkpoint and migration operations become much more difficult. The key reason
is that the OS is removed from the critical path of communication. As a result,
some of the communication state itself resides in the NIC hardware and is no
more under the direct control of the OS. This control includes especially the
support for virtualisation of communication which is needed for live migration
of communication partners. In this paper, we propose the basic principles
required to implement a migration-capable RDMA-based network. We recommend some
changes at the software level and small changes at the hardware level. As a
proof of concept, we integrate the proposed changes into SoftRoCE, an
open-source kernel-level implementation of the RoCE protocol. We claim that
these changes introduce no runtime overhead when migration does not happen.
Finally, we develop a proof-of-concept implementation for migrating
containerised applications that use RDMA-based networks.
","['\nMaksym Planeta\n', '\nJan Bierbaum\n', '\nLeo Sahaya Daphne Antony\n', '\nTorsten Hoefler\n', '\nHermann Härtig\n']","16 pages, 13 figures, 4 tables, 1 listing",,http://arxiv.org/abs/2009.06988v2,cs.OS,"['cs.OS', 'cs.DC', 'cs.NI']",,,[]
"toki: A Build- and Test-Platform for Prototyping and Evaluating
  Operating System Concepts in Real-Time Environments",http://arxiv.org/abs/2009.00466v1,2020-09-01T14:24:20Z,2020-09-01T14:24:20Z,"  Typically, even low-level operating system concepts, such as resource sharing
strategies and predictability measures, are evaluated with Linux on PC
hardware. This leaves a large gap to real industrial applications. Hence, the
direct transfer of the results might be difficult. As a solution, we present
toki, a prototyping and evaluation platform based on FreeRTOS and several
open-source libraries. toki comes with a unified build- and test-environment
based on Yocto and Qemu, which makes it well suited for rapid prototyping. With
its architecture chosen similar to production industrial systems, toki provides
the ground work to implement early prototypes of real-time systems research
results, up to technology readiness level 7, with little effort.
","['\nOliver Horst\n', '\nUwe Baumgarten\n']","Appeared in proceedings of the Open Demo Session of Real-Time Systems
  (RTSS@Work) held in conjunction with the 40th IEEE Real-Time Systems
  Symposium (RTSS)",,http://arxiv.org/abs/2009.00466v1,cs.OS,['cs.OS'],,,[]
"Quantifying the Latency and Possible Throughput of External Interrupts
  on Cyber-Physical Systems",http://arxiv.org/abs/2009.00506v1,2020-09-01T15:08:31Z,2020-09-01T15:08:31Z,"  An important characteristic of cyber-physical systems is their capability to
respond, in-time, to events from their physical environment. However, to the
best of our knowledge there exists no benchmark for assessing and comparing the
interrupt handling performance of different software stacks. Hence, we present
a flexible evaluation method for measuring the interrupt latency and throughput
on ARMv8-A based platforms. We define and validate seven test-cases that stress
individual parts of the overall process and combine them to three benchmark
functions that provoke the minimal and maximal interrupt latency, and maximal
interrupt throughput.
","['\nOliver Horst\n', '\nJohannes Wiesböck\n', '\nRaphael Wild\n', '\nUwe Baumgarten\n']","Appeared in proceedings of the 3rd Workshop on Benchmarking
  Cyber-Physical Systems and Internet of Things (CPS-IoTBench) held in
  conjunction with the 26th Annual International Conference on Mobile Computing
  and Networking (MobiCom)",,http://arxiv.org/abs/2009.00506v1,cs.OS,['cs.OS'],,,[]
BumbleBee: Application-aware adaptation for container orchestration,http://arxiv.org/abs/2008.11868v3,2020-08-27T00:20:00Z,2021-05-09T19:21:04Z,"  Modern applications have embraced separation of concerns as a first-order
organizing principle through the use of containers, container orchestration,
and service meshes. However, adaptation to unexpected network variation has not
followed suit. We present BumbleBee, a lightweight extension to the container
ecosystem that supports application-aware adaptation. BumbleBee provides a
simple abstraction for making decisions about network data using application
semantics. Because this abstraction is placed within the communications
framework of a modern service mesh, it is closer to the point at which changes
are detected, providing more responsive and effective adaptation than possible
at endpoints.
","['\nHyunJong Lee\n', '\nShadi Noghabi\n', '\nBrian Noble\n', '\nMatthew Furlong\n', '\nLandon P. Cox\n']","This version fixes problems (e.g., with the video-streaming
  experiments) from the previous versions",,http://arxiv.org/abs/2008.11868v3,cs.DC,"['cs.DC', 'cs.OS']",,,[]
Enclave-Aware Compartmentalization and Secure Sharing with Sirius,http://arxiv.org/abs/2009.01869v3,2020-09-03T18:30:02Z,2020-11-23T14:19:30Z,"  Hardware-assisted trusted execution environments (TEEs) are critical building
blocks of many modern applications. However, they have a one-way isolation
model that introduces a semantic gap between a TEE and its outside world. This
lack of information causes an ever-increasing set of attacks on TEE-enabled
applications that exploit various insecure interactions with the host OSs,
applications, or other enclaves. We introduce Sirius, the first
compartmentalization framework that achieves strong isolation and secure
sharing in TEE-assisted applications by controlling the dataflows within
primary kernel objects (e.g. threads, processes, address spaces, files,
sockets, pipes) in both the secure and normal worlds. Sirius replaces ad-hoc
interactions in current TEE systems with a principled approach that adds strong
inter- and intra-address space isolation and effectively eliminates a wide
range of attacks. We evaluate Sirius on ARM platforms and find that it is
lightweight ($\approx 15K$ LoC) and only adds $\approx 10.8\%$ overhead to
enable TEE support on applications such as httpd, and improves the performance
of existing TEE-enabled applications such as the Darknet ML framework and ARM's
LibDDSSec by $0.05\%-5.6\%$.
","['\nZahra Tarkhani\n', '\nAnil Madhavapeddy\n']",,,http://arxiv.org/abs/2009.01869v3,cs.CR,"['cs.CR', 'cs.OS']",,,[]
"Analysis of Interference between RDMA and Local Access on Hybrid Memory
  System",http://arxiv.org/abs/2008.12501v1,2020-08-28T06:52:41Z,2020-08-28T06:52:41Z,"  We can use a hybrid memory system consisting of DRAM and Intel Optane DC
Persistent Memory (We call it DCPM in this paper) as DCPM is now commercially
available since April 2019. Even if the latency for DCPM is several times
higher than that for DRAM, the capacity for DCPM is several times higher than
that for DRAM and the cost of DCPM is also several times lower than that for
DRAM. In addition, DCPM is non-volatile. A Server with this hybrid memory
system could improve the performance for in-memory database systems and virtual
machine (VM) systems because these systems often consume a large amount of
memory. Moreover, a high-speed shared storage system can be implemented by
accessing DCPM via remote direct memory access (RDMA). I assume that some of
the DCPM is often assigned as a shared area among other remote servers because
applications executed on a server with a hybrid memory system often cannot use
the entire capacity of DCPM. This paper evaluates the interference between
local memory access and RDMA from a remote server. As a result, I indicate that
the interference on this hybrid memory system is significantly different from
that on a conventional DRAM-only memory system. I also believe that some kind
of throttling implementation is needed when this interference occures.
",['\nKazuichi Oe\n'],,,http://arxiv.org/abs/2008.12501v1,cs.PF,"['cs.PF', 'cs.AR', 'cs.OS']",,,[]
Interprocess Communication in FreeBSD 11: Performance Analysis,http://arxiv.org/abs/2008.02145v1,2020-08-05T14:07:24Z,2020-08-05T14:07:24Z,"  Interprocess communication, IPC, is one of the most fundamental functions of
a modern operating system, playing an essential role in the fabric of
contemporary applications. This report conducts an investigation in FreeBSD of
the real world performance considerations behind two of the most common IPC
mechanisms; pipes and sockets. A simple benchmark provides a fair sense of
effective bandwidth for each, and analysis using DTrace, hardware performance
counters and the operating system's source code is presented. We note that
pipes outperform sockets by 63% on average across all configurations, and
further that the size of userspace transmission buffers has a profound effect
on performance - larger buffers are beneficial up to a point (~32-64 KiB) after
which performance collapses as a result of devastating cache exhaustion. A deep
scrutiny of the probe effects at play is also presented, justifying the
validity of conclusions drawn from these experiments.
",['\nA. H. Bell-Thomas\n'],"10 pages, 7 figures",,http://arxiv.org/abs/2008.02145v1,cs.OS,['cs.OS'],,,[]
"eXpOS: A Simple Pedagogical Operating System for Undergraduate
  Instruction",http://arxiv.org/abs/2008.03563v1,2020-08-08T17:41:15Z,2020-08-08T17:41:15Z,"  An operating system project suitable for undergraduate computing/electrical
sciences students is presented. The project can be used as a course project in
a one semester course, or as a self-study project for motivated students. The
course is organized such that a student with a basic background in programming
and computer organization can follow the implementation road map available
online, and build the OS from scratch on her personal machine/laptop, with
minimal instructional supervision. The student is provided with a simulated
abstract machine, an application interface specification, specification and
design of the OS, and a step by step project implementation road map. The
functionalities of the OS include multitasking, virtual memory, semaphores,
shared memory, an elementary file system, interrupt driven disk and console
I/O, and a limited multi-user support. The final stage of the project involves
porting the OS to a two-core machine. An independent one semester compiler
design project, where the student builds a compiler for a tiny object oriented
programming language that generates target code that can be loaded and executed
by the OS is also briefly discussed.
",['\nK. Murali Krishnan\n'],,,http://arxiv.org/abs/2008.03563v1,cs.OS,['cs.OS'],,,[]
"Making Distributed Mobile Applications SAFE: Enforcing User Privacy
  Policies on Untrusted Applications with Secure Application Flow Enforcement",http://arxiv.org/abs/2008.06536v1,2020-08-14T18:35:38Z,2020-08-14T18:35:38Z,"  Today's mobile devices sense, collect, and store huge amounts of personal
information, which users share with family and friends through a wide range of
applications. Once users give applications access to their data, they must
implicitly trust that the apps correctly maintain data privacy. As we know from
both experience and all-too-frequent press articles, that trust is often
misplaced. While users do not trust applications, they do trust their mobile
devices and operating systems. Unfortunately, sharing applications are not
limited to mobile clients but must also run on cloud services to share data
between users. In this paper, we leverage the trust that users have in their
mobile OSes to vet cloud services. To do so, we define a new Secure Application
Flow Enforcement (SAFE) framework, which requires cloud services to attest to a
system stack that will enforce policies provided by the mobile OS for user
data. We implement a mobile OS that enforces SAFE policies on unmodified mobile
apps and two systems for enforcing policies on untrusted cloud services. Using
these prototypes, we demonstrate that it is possible to enforce existing user
privacy policies on unmodified applications.
","['\nAdriana Szekeres\n', '\nIrene Zhang\n', '\nKatelin Bailey\n', '\nIsaac Ackerman\n', '\nHaichen Shen\n', '\nFranziska Roesner\n', '\nDan R. K. Ports\n', '\nArvind Krishnamurthy\n', '\nHenry M. Levy\n']",,,http://arxiv.org/abs/2008.06536v1,cs.CR,"['cs.CR', 'cs.OS']",,,[]
"Consideration for effectively handling parallel workloads on public
  cloud system",http://arxiv.org/abs/2008.06152v1,2020-08-14T01:18:04Z,2020-08-14T01:18:04Z,"  We retrieved and analyzed parallel storage workloads of the FUJITSU K5 cloud
service to clarify how to build cost-effective hybrid storage systems. A hybrid
storage system consists of fast but low-capacity tier (first tier) and slow but
high-capacity tier (second tier). And, it typically consists of either SSDs and
HDDs or NVMs and SSDs. As a result, we found that 1) regions for first tier
should be assigned only if a workload includes large number of IO accesses for
a whole day, 2) the regions that include a large number of IO accesses should
be dynamically chosen and moved from second tier to first tier for a short
interval, and 3) if a cache hit ratio is regularly low, use of the cache for
the workload should be cancelled, and the whole workload region should be
assigned to the region for first tier. These workloads already have been
released from the SNIA web site.
",['\nKazuichi Oe\n'],,,http://arxiv.org/abs/2008.06152v1,cs.DC,"['cs.DC', 'cs.DS', 'cs.OS', 'cs.PF']",,,[]
"Scheduling of Real-Time Tasks with Multiple Critical Sections in
  Multiprocessor Systems",http://arxiv.org/abs/2007.08302v1,2020-07-16T12:43:15Z,2020-07-16T12:43:15Z,"  The performance of multiprocessor synchronization and locking protocols is a
key factor to utilize the computation power of multiprocessor systems under
real-time constraints. While multiple protocols have been developed in the past
decades, their performance highly depends on the task partition and
prioritization. The recently proposed Dependency Graph Approach showed its
advantages and attracted a lot of interest. It is, however, restricted to task
sets where each task has at most one critical section. In this paper, we remove
this restriction and demonstrate how to utilize algorithms for the classical
job shop scheduling problem to construct a dependency graph for tasks with
multiple critical sections. To show the applicability, we discuss the
implementation in Litmus^{RT} and report the overheads. Moreover, we provide
extensive numerical evaluations under different configurations, which in many
situations show significant improvement compared to the state-of-the-art.
","['\nJian-Jia Chen\n', '\nJunjie Shi\n', '\nGeorg von der Brüggen\n', '\nNiklas Ueter\n']",,,http://arxiv.org/abs/2007.08302v1,cs.OS,['cs.OS'],,,[]
HeRTA: Heaviside Real-Time Analysis,http://arxiv.org/abs/2007.12112v1,2020-07-23T16:33:23Z,2020-07-23T16:33:23Z,"  We investigate the mathematical properties of event bound functions as they
are used in the worst-case response time analysis and utilization tests. We
figure out the differences and similarities between the two approaches. Based
on this analysis, we derive a more general form do describe events and event
bounds. This new unified approach gives clear new insights in the investigation
of real-time systems, simplifies the models and will support algebraic proofs
in future work. In the end, we present a unified analysis which allows the
algebraic definition of any scheduler. Introducing such functions to the
real-time scheduling theory will lead two a more systematic way to integrate
new concepts and applications to the theory. Last but not least, we show how
the response time analysis in dynamic scheduling can be improved.
","['\nFrank Slomka\n', '\nMohammadreza Sadeghi\n']",,,http://arxiv.org/abs/2007.12112v1,cs.OS,"['cs.OS', 'C.4']",,,[]
Analyzing and Mitigating Data Stalls in DNN Training,http://arxiv.org/abs/2007.06775v3,2020-07-14T02:16:56Z,2021-01-19T18:35:27Z,"  Training Deep Neural Networks (DNNs) is resource-intensive and
time-consuming. While prior research has explored many different ways of
reducing DNN training time, the impact of input data pipeline, i.e., fetching
raw data items from storage and performing data pre-processing in memory, has
been relatively unexplored. This paper makes the following contributions: (1)
We present the first comprehensive analysis of how the input data pipeline
affects the training time of widely-used computer vision and audio Deep Neural
Networks (DNNs), that typically involve complex data preprocessing. We analyze
nine different models across three tasks and four datasets while varying
factors such as the amount of memory, number of CPU threads, storage device,
GPU generation etc on servers that are a part of a large production cluster at
Microsoft. We find that in many cases, DNN training time is dominated by data
stall time: time spent waiting for data to be fetched and preprocessed. (2) We
build a tool, DS-Analyzer to precisely measure data stalls using a differential
technique, and perform predictive what-if analysis on data stalls. (3) Finally,
based on the insights from our analysis, we design and implement three simple
but effective techniques in a data-loading library, CoorDL, to mitigate data
stalls. Our experiments on a range of DNN tasks, models, datasets, and hardware
configs show that when PyTorch uses CoorDL instead of the state-of-the-art DALI
data loading library, DNN training time is reduced significantly (by as much as
5x on a single server).
","['\nJayashree Mohan\n', '\nAmar Phanishayee\n', '\nAshish Raniwala\n', '\nVijay Chidambaram\n']",,,http://arxiv.org/abs/2007.06775v3,cs.DC,"['cs.DC', 'cs.LG', 'cs.OS']",,,[]
DBOS: A Proposal for a Data-Centric Operating System,http://arxiv.org/abs/2007.11112v1,2020-07-21T22:01:00Z,2020-07-21T22:01:00Z,"  Current operating systems are complex systems that were designed before
today's computing environments. This makes it difficult for them to meet the
scalability, heterogeneity, availability, and security challenges in current
cloud and parallel computing environments. To address these problems, we
propose a radically new OS design based on data-centric architecture: all
operating system state should be represented uniformly as database tables, and
operations on this state should be made via queries from otherwise stateless
tasks. This design makes it easy to scale and evolve the OS without
whole-system refactoring, inspect and debug system state, upgrade components
without downtime, manage decisions using machine learning, and implement
sophisticated security features. We discuss how a database OS (DBOS) can
improve the programmability and performance of many of today's most important
applications and propose a plan for the development of a DBOS proof of concept.
","['\nMichael Cafarella\n', '\nDavid DeWitt\n', '\nVijay Gadepally\n', '\nJeremy Kepner\n', '\nChristos Kozyrakis\n', '\nTim Kraska\n', '\nMichael Stonebraker\n', '\nMatei Zaharia\n']",,,http://arxiv.org/abs/2007.11112v1,cs.OS,"['cs.OS', 'cs.AR', 'cs.DB', 'cs.DC', 'cs.NI']",,,[]
"LINTS^RT: A Learning-driven Testbed for Intelligent Scheduling in
  Embedded Systems",http://arxiv.org/abs/2007.05136v1,2020-07-10T02:03:52Z,2020-07-10T02:03:52Z,"  Due to the increasing complexity seen in both workloads and hardware
resources in state-of-the-art embedded systems, developing efficient real-time
schedulers and the corresponding schedulability tests becomes rather
challenging. Although close to optimal schedulability performance can be
achieved for supporting simple system models in practice, adding any small
complexity element into the problem context such as non-preemption or resource
heterogeneity would cause significant pessimism, which may not be eliminated by
any existing scheduling technique. In this paper, we present LINTS^RT, a
learning-based testbed for intelligent real-time scheduling, which has the
potential to handle various complexities seen in practice. The design of
LINTS^RT is fundamentally motivated by AlphaGo Zero for playing the board game
Go, and specifically addresses several critical challenges due to the real-time
scheduling context. We first present a clean design of LINTS^RT for supporting
the basic case: scheduling sporadic workloads on a homogeneous multiprocessor,
and then demonstrate how to easily extend the framework to handle further
complexities such as non-preemption and resource heterogeneity. Both
application and OS-level implementation and evaluation demonstrate that
LINTS^RT is able to achieve significantly higher runtime schedulability under
different settings compared to perhaps the most commonly applied schedulers,
global EDF, and RM. To our knowledge, this work is the first attempt to design
and implement an extensible learning-based testbed for autonomously making
real-time scheduling decisions.
","['\nZelun Kong\n', '\nYaswanth Yadlapalli\n', '\nSoroush Bateni\n', '\nJunfeng Guo\n', '\nCong Liu\n']","11 pages, 9 figures",,http://arxiv.org/abs/2007.05136v1,cs.OS,['cs.OS'],,,[]
"IOCA: High-Speed I/O-Aware LLC Management for Network-Centric
  Multi-Tenant Platform",http://arxiv.org/abs/2007.04552v2,2020-07-09T04:45:54Z,2021-03-04T16:14:42Z,"  In modern server CPUs, last-level cache (LLC) is a critical hardware resource
that exerts significant influence on the performance of the workloads, and how
to manage LLC is a key to the performance isolation and QoS in the cloud with
multi-tenancy. In this paper, we argue that besides CPU cores, high-speed
network I/O is also important for LLC management. This is because of an Intel
architectural innovation -- Data Direct I/O (DDIO) -- that directly injects the
inbound I/O traffic to (part of) the LLC instead of the main memory. We
summarize two problems caused by DDIO and show that (1) the default DDIO
configuration may not always achieve optimal performance, (2) DDIO can decrease
the performance of non-I/O workloads which share LLC with it by as high as 32%.
  We then present IOCA, the first LLC management mechanism for network-centric
platforms that treats the I/O as the first-class citizen. IOCA monitors and
analyzes the performance of the cores, LLC, and DDIO using CPU's hardware
performance counters, and adaptively adjusts the number of LLC ways for DDIO or
the tenants that demand more LLC capacity. In addition, IOCA dynamically
chooses the tenants that share its LLC resource with DDIO, to minimize the
performance interference by both the tenants and the I/O. Our experiments with
multiple microbenchmarks and real-world applications in two major end-host
network models demonstrate that IOCA can effectively reduce the performance
degradation caused by DDIO, with minimal overhead.
","['\nYifan Yuan\n', '\nMohammad Alian\n', '\nYipeng Wang\n', '\nIlia Kurakin\n', '\nRen Wang\n', '\nCharlie Tai\n', '\nNam Sung Kim\n']","Accepted by the 48th IEEE/ACM International Symposium on Computer
  Architecture (ISCA'21). The title is ""Don't Forget the I/O When Allocating
  Your LLC""",,http://arxiv.org/abs/2007.04552v2,cs.AR,"['cs.AR', 'cs.OS']",,,[]
DPCP-p: A Distributed Locking Protocol for Parallel Real-Time Tasks,http://arxiv.org/abs/2007.00706v1,2020-07-01T19:13:26Z,2020-07-01T19:13:26Z,"  Real-time scheduling and locking protocols are fundamental facilities to
construct time-critical systems. For parallel real-time tasks, predictable
locking protocols are required when concurrent sub-jobs mutually exclusive
access to shared resources. This paper for the first time studies the
distributed synchronization framework of parallel real-time tasks, where both
tasks and global resources are partitioned to designated processors, and
requests to each global resource are conducted on the processor on which the
resource is partitioned. We extend the Distributed Priority Ceiling Protocol
(DPCP) for parallel tasks under federated scheduling, with which we proved that
a request can be blocked by at most one lower-priority request. We develop task
and resource partitioning heuristics and propose analysis techniques to safely
bound the task response times. Numerical evaluation (with heavy tasks on 8-,
16-, and 32-core processors) indicates that the proposed methods improve the
schedulability significantly compared to the state-of-the-art locking protocols
under federated scheduling.
","['\nMaolin Yang\n', '\nZewei Chen\n', '\nXu Jiang\n', '\nNan Guan\n', '\nHang Lei\n']",,,http://arxiv.org/abs/2007.00706v1,cs.OS,['cs.OS'],,,[]
Scalable Range Locks for Scalable Address Spaces and Beyond,http://arxiv.org/abs/2006.12144v1,2020-06-22T11:12:44Z,2020-06-22T11:12:44Z,"  Range locks are a synchronization construct designed to provide concurrent
access to multiple threads (or processes) to disjoint parts of a shared
resource. Originally conceived in the file system context, range locks are
gaining increasing interest in the Linux kernel community seeking to alleviate
bottlenecks in the virtual memory management subsystem. The existing
implementation of range locks in the kernel, however, uses an internal spin
lock to protect the underlying tree structure that keeps track of acquired and
requested ranges. This spin lock becomes a point of contention on its own when
the range lock is frequently acquired. Furthermore, where and exactly how
specific (refined) ranges can be locked remains an open question.
  In this paper, we make two independent, but related contributions. First, we
propose an alternative approach for building range locks based on linked lists.
The lists are easy to maintain in a lock-less fashion, and in fact, our range
locks do not use any internal locks in the common case. Second, we show how the
range of the lock can be refined in the mprotect operation through a
speculative mechanism. This refinement, in turn, allows concurrent execution of
mprotect operations on non-overlapping memory regions. We implement our new
algorithms and demonstrate their effectiveness in user-space and kernel-space,
achieving up to 9$\times$ speedup compared to the stock version of the Linux
kernel. Beyond the virtual memory management subsystem, we discuss other
applications of range locks in parallel software. As a concrete example, we
show how range locks can be used to facilitate the design of scalable
concurrent data structures, such as skip lists.
","['\nAlex Kogan\n', '\nDave Dice\n', '\nShady Issa\n']","17 pages, 9 figures, Eurosys 2020",,http://dx.doi.org/10.1145/3342195.3387533,cs.OS,"['cs.OS', 'cs.DC', 'D.1.3; D.4.1; D.4.2']",10.1145/3342195.3387533,,[]
"Optimizing Placement of Heap Memory Objects in Energy-Constrained Hybrid
  Memory Systems",http://arxiv.org/abs/2006.12133v2,2020-06-22T10:37:40Z,2020-06-23T01:28:19Z,"  Main memory (DRAM) significantly impacts the power and energy utilization of
the overall server system. Non-Volatile Memory (NVM) devices, such as Phase
Change Memory and Spin-Transfer Torque RAM, are suitable candidates for main
memory to reduce energy consumption. But unlike DRAM, NVMs access latencies are
higher than DRAM and NVM writes are more energy sensitive than DRAM write
operations. Thus, Hybrid Main Memory Systems (HMMS) employing DRAM and NVM have
been proposed to reduce the overall energy depletion of main memory while
optimizing the performance of NVM. This paper proposes eMap, an optimal heap
memory object placement planner in HMMS. eMap considers the object-level access
patterns and energy consumption at the application level and provides an ideal
placement strategy for each object to augment performance and energy
utilization. eMap is equipped with two modules, eMPlan and eMDyn. Specifically,
eMPlan is a static placement planner which provides one time placement policies
for memory object to meet the energy budget while eMDyn is a runtime placement
planner to consider the change in energy limiting constraint during the runtime
and shuffles the memory objects by taking into account the access patterns as
well as the migration cost in terms of energy and performance. The evaluation
shows that our proposed solution satisfies both the energy limiting constraint
and the performance. We compare our methodology with the state-of-the-art
memory object classification and allocation (MOCA) framework. Our extensive
evaluation shows that our proposed solution, eMPlan meets the energy constraint
with 4.17 times less costly and reducing the energy consumption up to 14% with
the same performance. eMDyn also satisfies the performance and energy
requirement while considering the migration cost in terms of time and energy.
","['\nTaeuk Kim\n', '\nSafdar Jamil\n', '\nJoongeon Park\n', '\nYoungjae Kim\n']",,,http://arxiv.org/abs/2006.12133v2,cs.AR,"['cs.AR', 'cs.ET', 'cs.OS']",,,[]
FastDrain: Removing Page Victimization Overheads in NVMe Storage Stack,http://arxiv.org/abs/2006.08966v2,2020-06-16T07:45:22Z,2020-06-22T13:11:11Z,"  Host-side page victimizations can easily overflow the SSD internal buffer,
which interferes I/O services of diverse user applications thereby degrading
user-level experiences. To address this, we propose FastDrain, a co-design of
OS kernel and flash firmware to avoid the buffer overflow, caused by page
victimizations. Specifically, FastDrain can detect a triggering point where a
near-future page victimization introduces an overflow of the SSD internal
buffer. Our new flash firmware then speculatively scrubs the buffer space to
accommodate the requests caused by the page victimization. In parallel, our new
OS kernel design controls the traffic of page victimizations by considering the
target device buffer status, which can further reduce the risk of buffer
overflow. To secure more buffer spaces, we also design a latency-aware FTL,
which dumps the dirty data only to the fast flash pages. Our evaluation results
reveal that FastDrain reduces the 99th response time of user applications by
84%, compared to a conventional system.
","['\nJie Zhang\n', '\nMiryeong Kwon\n', '\nSanghyun Han\n', '\nNam Sung Kim\n', '\nMahmut Kandemir\n', '\nMyoungsoo Jung\n']",,,http://arxiv.org/abs/2006.08966v2,cs.OS,['cs.OS'],,,[]
An Adaptive Approach to Recoverable Mutual Exlcusion,http://arxiv.org/abs/2006.07086v2,2020-06-12T11:18:04Z,2020-08-03T06:04:27Z,"  Mutual exclusion (ME) is one of the most commonly used techniques to handle
conflicts in concurrent systems. Traditionally, mutual exclusion algorithms
have been designed under the assumption that a process does not fail while
acquiring/releasing a lock or while executing its critical section. However,
failures do occur in real life, potentially leaving the lock in an inconsistent
state. This gives rise to the problem of \emph{recoverable mutual exclusion
(RME)} that involves designing a mutual exclusion algorithm that can tolerate
failures, while maintaining safety and liveness properties.
  One of the important measures of performance of any ME algorithm, including
an RME algorithm, is the number of \emph{remote memory references (RMRs)} made
by a process (for acquiring and releasing a lock as well as recovering the lock
structure after a failure). The best known RME algorithm solves the problem for
$n$ processes in sub-logarithmic number of RMRs, given by
$\mathcal{O}(\frac{\log n}{\log \log n})$, irrespective of the number of
failures in the system.
  In this work, we present a new algorithm for solving the RME problem whose
RMR complexity gradually \emph{adapts} to the number of failures that have
occurred in the system ""recently"". In the absence of failures, our algorithm
generates only $\mathcal{O}(1)$ RMRs. Furthermore, its RMR complexity is given
by $\mathcal{O}(\min\{ \sqrt{F}, \frac{\log n}{\log \log n} \})$ where $F$ is
the total number of failures in the ""recent"" past. In addition to read and
write instructions, our algorithm uses compare-and-swap (\CAS{}) and
fetch-and-store (\FAS{}) hardware instructions, both of which are commonly
available in most modern processors.
","['\nSahil Dhoked\n', '\nNeeraj Mittal\n']",,,http://arxiv.org/abs/2006.07086v2,cs.DC,"['cs.DC', 'cs.DS', 'cs.OS']",,,[]
Nefele: Process Orchestration for the Cloud,http://arxiv.org/abs/2006.07163v2,2020-06-12T13:21:59Z,2020-06-16T08:32:14Z,"  Virtualization, either at OS- or hardware level, plays an important role in
cloud computing. It enables easier automation and faster deployment in
distributed environments. While virtualized infrastructures provide a level of
management flexibility, they lack practical abstraction of the distributed
resources. A developer in such an environment still needs to deal with all the
complications of building a distributed software system. Different
orchestration systems are built to provide that abstraction; however, they do
not solve the inherent challenges of distributed systems, such as
synchronization issues or resilience to failures. This paper introduces Nefele,
a decentralized process orchestration system that automatically deploys and
manages individual processes, rather than containers/VMs, within a cluster.
Nefele is inspired by the Single System Image (SSI) vision of mitigating the
intricacies of remote execution, yet it maintains the flexibility and
performance of virtualized infrastructures. Nefele offers a set of APIs for
building cloud-native applications that lets the developer easily build,
deploy, and scale applications in a cloud environment. We have implemented and
deployed Nefele on a cluster in our datacenter and evaluated its performance.
Our evaluations show that Nefele can effectively deploy, scale, and monitor
processes across a distributed environment, while it incorporates essential
primitives to build a distributed software system.
","['\nMina Sedaghat\n', '\nPontus Sköldström\n', '\nDaniel Turull\n', '\nVinay Yadhav\n', '\nJoacim Halén\n', '\nMadhubala Ganesan\n', '\nAmardeep Mehta\n', '\nWolfgang John\n']",,,http://arxiv.org/abs/2006.07163v2,cs.DC,"['cs.DC', 'cs.MA', 'cs.OS', 'D.4.7; D.4.8; D.4.1; D.4.3; D.4.4; F.1.2']",,,[]
Flex: Closing the Gaps between Usage and Allocation,http://arxiv.org/abs/2006.01354v1,2020-06-02T02:41:39Z,2020-06-02T02:41:39Z,"  Data centers are giant factories of Internet data and services. Worldwide
data centers consume energy and emit emissions more than airline industry.
Unfortunately, most of data centers are significantly underutilized. One of the
major reasons is the big gaps between the real usage and the provisioned
resources because users tend to over-estimate their demand and data center
operators often rely on users' requests for resource allocation. In this paper,
we first conduct an in-depth analysis of a Google cluster trace to unveil the
root causes for low utilization and highlight the great potential to improve
it. We then developed an online resource manager Flex to maximize the cluster
utilization while satisfying the Quality of Service (QoS). Large-scale
evaluations based on real-world traces show that Flex admits up to 1.74x more
requests and 1.6x higher utilization compared to tradition schedulers while
maintaining the QoS.
","['\nTan N. Le\n', '\nZhenhua Liu\n']",,,http://arxiv.org/abs/2006.01354v1,cs.DC,"['cs.DC', 'cs.NI', 'cs.OS']",,,[]
"The Art of CPU-Pinning: Evaluating and Improving the Performance of
  Virtualization and Containerization Platforms",http://arxiv.org/abs/2006.02055v1,2020-06-03T05:47:14Z,2020-06-03T05:47:14Z,"  Cloud providers offer a variety of execution platforms in form of bare-metal,
VM, and containers. However, due to the pros and cons of each execution
platform, choosing the appropriate platform for a specific cloud-based
application has become a challenge for solution architects. The possibility to
combine these platforms (e.g. deploying containers within VMs) offers new
capacities that makes the challenge even further complicated. However, there is
a little study in the literature on the pros and cons of deploying different
application types on various execution platforms. In particular, evaluation of
diverse hardware configurations and different CPU provisioning methods, such as
CPU pinning, have not been sufficiently studied in the literature. In this
work, the performance overhead of container, VM, and bare-metal execution
platforms are measured and analyzed for four categories of real-world
applications, namely video processing, parallel processing (MPI), web
processing, and No-SQL, respectively representing CPU intensive, parallel
processing, and two IO intensive processes. Our analyses reveal a set of
interesting and sometimes counterintuitive findings that can be used as best
practices by the solution architects to efficiently deploy cloud-based
applications. Here are some notable mentions: (A) Under specific circumstances,
containers can impose a higher overhead than VMs; (B) Containers on top of VMs
can mitigate the overhead of VMs for certain applications; (C) Containers with
a large number of cores impose a lower overhead than those with a few cores.
","['\nDavood Ghatreh Samani\n', '\nChavit Denninnart\n', '\nJosef Bacik\n', '\nMohsen Amini Salehi\n']",,"The 49th International Conference on Parallel Processing (ICPP
  2020)",http://arxiv.org/abs/2006.02055v1,cs.DC,"['cs.DC', 'cs.OS', 'cs.PF']",,,[]
Study of Firecracker MicroVM,http://arxiv.org/abs/2005.12821v1,2020-05-26T15:56:06Z,2020-05-26T15:56:06Z,"  Firecracker is a virtualization technology that makes use of Kernel Virtual
Machine (KVM). Firecracker belongs to a new virtualization class named the
micro-virtual machines (MicroVMs). Using Firecracker, we can launch lightweight
MicroVMs in non-virtualized environments in a fraction of a second, at the same
time offering the security and workload isolation provided by traditional VMs
and also the resource efficiency that comes along with containers \cite{b1}.
Firecracker aims to provide a slimmed-down MicroVM, comprised of approximately
50K lines of code in Rust and with a reduced attack surface for guest VMs. This
report will examine the internals of Firecracker and understand why Firecracker
is the next big thing going forward in virtualization and cloud computing.
",['\nMadhur Jain\n'],"4 pages, 3 figures",,http://arxiv.org/abs/2005.12821v1,cs.OS,"['cs.OS', 'cs.DC']",,,[]
"A Way Around UMIP and Descriptor-Table Exiting via TSX-based
  Side-Channel",http://arxiv.org/abs/2005.10333v2,2020-05-20T19:54:38Z,2021-04-22T08:57:18Z,"  Nowadays, in operating systems, numerous protection mechanisms prevent or
limit the user-mode applicationsto access the kernels internal information.
This is regularlycarried out by software-based defenses such as Address Space
Layout Randomization (ASLR) and Kernel ASLR(KASLR). They play pronounced roles
when the security of sandboxed applications such as Web-browser are
considered.Armed with arbitrary write access in the kernel memory, if these
protections are bypassed, an adversary could find a suitable where to write in
order to get an elevation of privilege or code execution in ring 0. In this
paper, we introduce a reliable method based on Transactional Synchronization
Extensions (TSX) side-channel leakage to reveal the address of the Global
Descriptor Table (GDT) and Interrupt Descriptor Table (IDT). We indicate that
by detecting these addresses, one could execute instructions to sidestep the
Intels User-Mode InstructionPrevention (UMIP) and the Hypervisor-based
mitigation and, consequently, neutralized them. The introduced method is
successfully performed after the most recent patches for Meltdown and Spectre.
Moreover, the implementation of the proposed approach on different platforms,
including the latest releases of Microsoft Windows, Linux, and, Mac OSX with
the latest 9th generation of Intel processors, shows that the proposed
mechanism is independent from the Operating System implementation. We
demonstrate that a combinationof this method with call-gate mechanism
(available in modernprocessors) in a chain of events will eventually lead toa
system compromise despite the limitations of a super-secure sandboxed
environment in the presence of Windows proprietary Virtualization Based
Security (VBS). Finally, we suggest the software-based mitigation to avoid
these issues with an acceptable overhead cost.
","['\nMohammad Sina Karvandi\n', '\nSaleh Khalaj Monfared\n', '\nMohammad Sina Kiarostami\n', '\nDara Rahmati\n', '\nSaeid Gorgin\n']",,,http://arxiv.org/abs/2005.10333v2,cs.CR,"['cs.CR', 'cs.AR', 'cs.OS']",,,[]
"Autonomous Task Dropping Mechanism to Achieve Robustness in
  Heterogeneous Computing Systems",http://arxiv.org/abs/2005.11050v1,2020-05-22T08:14:04Z,2020-05-22T08:14:04Z,"  Robustness of a distributed computing system is defined as the ability to
maintain its performance in the presence of uncertain parameters. Uncertainty
is a key problem in heterogeneous (and even homogeneous) distributed computing
systems that perturbs system robustness. Notably, the performance of these
systems is perturbed by uncertainty in both task execution time and arrival.
Accordingly, our goal is to make the system robust against these uncertainties.
Considering task execution time as a random variable, we use probabilistic
analysis to develop an autonomous proactive task dropping mechanism to attain
our robustness goal. Specifically, we provide a mathematical model that
identifies the optimality of a task dropping decision, so that the system
robustness is maximized. Then, we leverage the mathematical model to develop a
task dropping heuristic that achieves the system robustness within a feasible
time complexity. Although the proposed model is generic and can be applied to
any distributed system, we concentrate on heterogeneous computing (HC) systems
that have a higher degree of exposure to uncertainty than homogeneous systems.
Experimental results demonstrate that the autonomous proactive dropping
mechanism can improve the system robustness by up to 20%.
","['\nAli Mokhtari\n', '\nChavit Denninnart\n', '\nMohsen Amini Salehi\n']",,"in 29th Heterogeneity in Computing Workshop (HCW 2019), in the
  Proceedings of the IPDPS 2019 Workshops & PhD Forum (IPDPSW)",http://arxiv.org/abs/2005.11050v1,cs.DC,"['cs.DC', 'cs.OS', 'cs.PF']",,,[]
"Exploiting Inter- and Intra-Memory Asymmetries for Data Mapping in
  Hybrid Tiered-Memories",http://arxiv.org/abs/2005.04750v1,2020-05-10T18:53:02Z,2020-05-10T18:53:02Z,"  Modern computing systems are embracing hybrid memory comprising of DRAM and
non-volatile memory (NVM) to combine the best properties of both memory
technologies, achieving low latency, high reliability, and high density. A
prominent characteristic of DRAM-NVM hybrid memory is that it has NVM access
latency much higher than DRAM access latency. We call this inter-memory
asymmetry. We observe that parasitic components on a long bitline are a major
source of high latency in both DRAM and NVM, and a significant factor
contributing to high-voltage operations in NVM, which impact their reliability.
We propose an architectural change, where each long bitline in DRAM and NVM is
split into two segments by an isolation transistor. One segment can be accessed
with lower latency and operating voltage than the other. By introducing tiers,
we enable non-uniform accesses within each memory type (which we call
intra-memory asymmetry), leading to performance and reliability trade-offs in
DRAM-NVM hybrid memory. We extend existing NVM-DRAM OS in three ways. First, we
exploit both inter- and intra-memory asymmetries to allocate and migrate memory
pages between the tiers in DRAM and NVM. Second, we improve the OS's page
allocation decisions by predicting the access intensity of a newly-referenced
memory page in a program and placing it to a matching tier during its initial
allocation. This minimizes page migrations during program execution, lowering
the performance overhead. Third, we propose a solution to migrate pages between
the tiers of the same memory without transferring data over the memory channel,
minimizing channel occupancy and improving performance. Our overall approach,
which we call MNEME, to enable and exploit asymmetries in DRAM-NVM hybrid
tiered memory improves both performance and reliability for both single-core
and multi-programmed workloads.
","['\nShihao Song\n', '\nAnup Das\n', '\nNagarajan Kandasamy\n']","15 pages, 29 figures, accepted at ACM SIGPLAN International Symposium
  on Memory Management",,http://dx.doi.org/10.1145/3381898.3397215,cs.AR,"['cs.AR', 'cs.OS']",10.1145/3381898.3397215,,[]
Improving Phase Change Memory Performance with Data Content Aware Access,http://arxiv.org/abs/2005.04753v1,2020-05-10T19:07:08Z,2020-05-10T19:07:08Z,"  A prominent characteristic of write operation in Phase-Change Memory (PCM) is
that its latency and energy are sensitive to the data to be written as well as
the content that is overwritten. We observe that overwriting unknown memory
content can incur significantly higher latency and energy compared to
overwriting known all-zeros or all-ones content. This is because all-zeros or
all-ones content is overwritten by programming the PCM cells only in one
direction, i.e., using either SET or RESET operations, not both. In this paper,
we propose data content aware PCM writes (DATACON), a new mechanism that
reduces the latency and energy of PCM writes by redirecting these requests to
overwrite memory locations containing all-zeros or all-ones. DATACON operates
in three steps. First, it estimates how much a PCM write access would benefit
from overwriting known content (e.g., all-zeros, or all-ones) by
comprehensively considering the number of set bits in the data to be written,
and the energy-latency trade-offs for SET and RESET operations in PCM. Second,
it translates the write address to a physical address within memory that
contains the best type of content to overwrite, and records this translation in
a table for future accesses. We exploit data access locality in workloads to
minimize the address translation overhead. Third, it re-initializes unused
memory locations with known all-zeros or all-ones content in a manner that does
not interfere with regular read and write accesses. DATACON overwrites unknown
content only when it is absolutely necessary to do so. We evaluate DATACON with
workloads from state-of-the-art machine learning applications, SPEC CPU2017,
and NAS Parallel Benchmarks. Results demonstrate that DATACON significantly
improves system performance and memory system energy consumption compared to
the best of performance-oriented state-of-the-art techniques.
","['\nShihao Song\n', '\nAnup Das\n', '\nOnur Mutlu\n', '\nNagarajan Kandasamy\n']","18 pages, 21 figures, accepted at ACM SIGPLAN International Symposium
  on Memory Management (ISMM)",,http://dx.doi.org/10.1145/3381898.3397210,cs.AR,"['cs.AR', 'cs.OS']",10.1145/3381898.3397210,,[]
Dim Silicon and the Case for Improved DVFS Policies,http://arxiv.org/abs/2005.01498v1,2020-05-04T14:04:43Z,2020-05-04T14:04:43Z,"  Due to thermal and power supply limits, modern Intel CPUs reduce their
frequency when AVX2 and AVX-512 instructions are executed. As the CPUs wait for
670{\mu}s before increasing the frequency again, the performance of some
heterogeneous workloads is reduced. In this paper, we describe parallels
between this situation and dynamic power management as well as between the
policy implemented by these CPUs and fixed-timeout device shutdown policies. We
show that the policy implemented by Intel CPUs is not optimal and describe
potential better policies. In particular, we present a mechanism to classify
applications based on their likeliness to cause frequency reduction. Our
approach takes either the resulting classification information or information
provided by the application and generates hints for the DVFS policy. We show
that faster frequency changes based on these hints are able to improve
performance for a web server using the OpenSSL library.
","['\nMathias Gottschlag\n', '\nYussuf Khalil\n', '\nFrank Bellosa\n']","19 pages, 8 figures",,http://arxiv.org/abs/2005.01498v1,cs.OS,['cs.OS'],,,[]
On Failure Diagnosis of the Storage Stack,http://arxiv.org/abs/2005.02547v2,2020-05-06T01:07:29Z,2021-07-21T21:36:31Z,"  Diagnosing storage system failures is challenging even for professionals. One
example is the ""When Solid State Drives Are Not That Solid"" incident occurred
at Algolia data center, where Samsung SSDs were mistakenly blamed for failures
caused by a Linux kernel bug. With the system complexity keeps increasing, such
obscure failures will likely occur more often. As one step to address the
challenge, we present our on-going efforts called X-Ray. Different from
traditional methods that focus on either the software or the hardware, X-Ray
leverages virtualization to collects events across layers, and correlates them
to generate a correlation tree. Moreover, by applying simple rules, X-Ray can
highlight critical nodes automatically. Preliminary results based on 5 failure
cases shows that X-Ray can effectively narrow down the search space for
failures.
","['\nDuo Zhang\n', '\nOm Rameshwar Gatla\n', '\nRunzhou Han\n', '\nMai Zheng\n']",,,http://arxiv.org/abs/2005.02547v2,cs.OS,['cs.OS'],,,[]
Designing Robust API Monitoring Solutions,http://arxiv.org/abs/2005.00323v2,2020-05-01T11:57:22Z,2021-03-11T19:49:06Z,"  Tracing the sequence of library and system calls that a program makes is very
helpful in the characterization of its interactions with the surrounding
environment and ultimately of its semantics. Due to entanglements of real-world
software stacks, accomplishing this task can be surprisingly challenging as we
take accuracy, reliability, and transparency into the equation. To manage these
dimensions effectively, we identify six challenges that API monitoring
solutions should overcome and outline actionable design points for them,
reporting insights from our experience in building API tracers for software
security research. We detail two implementation variants, based on
hardware-assisted virtualization (realizing the first general-purpose
user-space tracer of this kind) and on dynamic binary translation, that achieve
API monitoring robustly. We share our SNIPER system as open source.
","[""\nDaniele Cono D'Elia\n"", '\nSimone Nicchi\n', '\nMatteo Mariani\n', '\nMatteo Marini\n', '\nFederico Palmaro\n']","This work has been submitted to the IEEE for possible publication.
  Copyright may be transferred without notice, after which this version may no
  longer be accessible",,http://arxiv.org/abs/2005.00323v2,cs.CR,"['cs.CR', 'cs.OS', 'cs.SE']",,,[]
Vilamb: Low Overhead Asynchronous Redundancy for Direct Access NVM,http://arxiv.org/abs/2004.09619v1,2020-04-20T20:35:12Z,2020-04-20T20:35:12Z,"  Vilamb provides efficient asynchronous systemredundancy for direct access
(DAX) non-volatile memory (NVM) storage. Production storage deployments often
use system-redundancy in form of page checksums and cross-page parity.
State-of-the-art solutions for maintaining system-redundancy for DAX NVM either
incur a high performance overhead or require specialized hardware. The Vilamb
user-space library maintains system-redundancy with low overhead by delaying
and amortizing the system-redundancy updates over multiple data writes. As a
result, Vilamb provides 3--5x the throughput of the state-of-the-art software
solution at high operation rates. For applications that need system-redundancy
with high performance, and can tolerate some delaying of data redundancy,
Vilamb provides a tunable knob between performance and quicker redundancy. Even
with the delayed coverage, Vilamb increases the mean time to data loss due to
firmware-induced corruptions by up to two orders of magnitude in comparison to
maintaining no system-redundancy.
","['\nRajat Kateja\n', '\nAndy Pavlo\n', '\nGregory R. Ganger\n']",,,http://arxiv.org/abs/2004.09619v1,cs.OS,['cs.OS'],,,[]
MemShield: GPU-assisted software memory encryption,http://arxiv.org/abs/2004.09252v1,2020-04-20T13:03:14Z,2020-04-20T13:03:14Z,"  Cryptographic algorithm implementations are vulnerable to Cold Boot attacks,
which consist in exploiting the persistence of RAM cells across reboots or
power down cycles to read the memory contents and recover precious sensitive
data. The principal defensive weapon against Cold Boot attacks is memory
encryption. In this work we propose MemShield, a memory encryption framework
for user space applications that exploits a GPU to safely store the master key
and perform the encryption/decryption operations. We developed a prototype that
is completely transparent to existing applications and does not require changes
to the OS kernel. We discuss the design, the related works, the implementation,
the security analysis, and the performances of MemShield.
","['\nPierpaolo Santucci\n', '\nEmiliano Ingrassia\n', '\nGiulio Picierro\n', '\nMarco Cesati\n']","14 pages, 2 figures. In proceedings of the 18th International
  Conference on Applied Cryptography and Network Security, ACNS 2020, October
  19-22 2020, Rome, Italy",,http://arxiv.org/abs/2004.09252v1,cs.CR,"['cs.CR', 'cs.OS', 'D.4.6; K.6.5']",,,[]
Resource Efficient Isolation Mechanisms in Mixed-Criticality Scheduling,http://arxiv.org/abs/2004.02400v1,2020-04-06T04:39:58Z,2020-04-06T04:39:58Z,"  Mixed-criticality real-time scheduling has been developed to improve resource
utilization while guaranteeing safe execution of critical applications. These
studies use optimistic resource reservation for all the applications to improve
utilization, but prioritize critical applications when the reservations become
insufficient at runtime. Many of them however share an impractical assumption
that all the critical applications will simultaneously demand additional
resources. As a consequence, they under-utilize resources by penalizing all the
low-criticality applications. In this paper we overcome this shortcoming using
a novel mechanism that comprises a parameter to model the expected number of
critical applications simultaneously demanding more resources, and an execution
strategy based on the parameter to improve resource utilization. Since most
mixed-criticality systems in practice are component-based, we design our
mechanism such that the component boundaries provide the isolation necessary to
support the execution of low-criticality applications, and at the same time
protect the critical ones. We also develop schedulability tests for the
proposed mechanism under both a flat as well as a hierarchical scheduling
framework. Finally, through simulations, we compare the performance of the
proposed approach with existing studies in terms of schedulability and the
capability to support low-criticality applications.
","['\nXiaozhe Gu\n', '\nArvind Easwaran\n', '\nKieu-My Phan\n', '\nInsik Shin\n']","\copyright 2015 IEEE. Personal use of this material is permitted.
  Permission from IEEE must be obtained for all other uses, in any current or
  future media, including reprinting/republishing this material for advertising
  or promotional purposes, creating new collective works, for resale or
  redistribution to servers or lists, or reuse of any copyrighted component of
  this work in other works","Euromicro Conference on Real-Time Systems (ECRTS), Lund, 2015, pp.
  13-24",http://dx.doi.org/10.1109/ECRTS.2015.9,cs.OS,['cs.OS'],10.1109/ECRTS.2015.9,,[]
Efficient Kernel Object Management for Tiered Memory Systems with KLOC,http://arxiv.org/abs/2004.04760v1,2020-04-09T18:04:25Z,2020-04-09T18:04:25Z,"  Software-controlled heterogeneous memory systems have the potential to
improve performance, efficiency, and cost tradeoffs in emerging systems.
Delivering on this promise requires an efficient operating system (OS)
mechanisms and policies for data management. Unfortunately, modern OSes do not
support efficient tiering of data between heterogeneous memories. While this
problem is known (and is being studied) for application-level data pages, the
question of how best to tier OS kernel objects has largely been ignored. We
show that careful kernel object management is vital to the performance of
software-controlled tiered memory systems. We find that the state-of-the-art OS
page management research leaves considerable performance on the table by
overlooking how best to tier, migrate, and manage kernel objects like inodes,
dentry caches, journal blocks, network socket buffers, etc., associated with
the filesystem and networking stack. In response, we characterize hotness,
reuse, and liveness properties of kernel objects to develop appropriate
tiering/migration mechanisms and policies. We evaluate our proposal using a
real-system emulation framework on large-scale workloads like RocksDB, Redis,
Cassandra, and Spark and achieve 1.4X to 4X higher throughput compared to the
prior art.
","['\nSudarsun Kannan\n', '\nYujie Ren\n', '\nAbhishek Bhatacharjee\n']",,,http://arxiv.org/abs/2004.04760v1,cs.OS,['cs.OS'],,,[]
"$μ$Tiles: Efficient Intra-Process Privilege Enforcement of Memory
  Regions",http://arxiv.org/abs/2004.04846v1,2020-04-09T23:07:44Z,2020-04-09T23:07:44Z,"  With the alarming rate of security advisories and privacy concerns on
connected devices, there is an urgent need for strong isolation guarantees in
resource-constrained devices that demand very lightweight solutions. However,
the status quo is that Unix-like operating systems do not offer privilege
separation inside a process. Lack of practical fine-grained
compartmentalization inside a shared address space leads to private data
leakage through applications' untrusted dependencies and compromised threads.
To this end, we propose $\mu$Tiles, a lightweight kernel abstraction and set of
security primitives based on mutual distrust for intra-process privilege
separation, memory protection, and secure multithreading. $\mu$Tiles takes
advantage of hardware support for virtual memory tagging (e.g., ARM memory
domains) to achieve significant performance gain while eliminating various
hardware limitations. Our results (based on OpenSSL, the Apache HTTP server,
and LevelDB) show that $\mu$Tiles is extremely lightweight (adds $\approx 10KB$
to kernel image) for IoT use cases. It adds negligible runtime overhead
($\approx 0.5\%-3.5\%$) and is easy to integrate with existing applications for
providing strong privilege separation.
","['\nZahra Tarkhani\n', '\nAnil Madhavapeddy\n']",,,http://arxiv.org/abs/2004.04846v1,cs.OS,['cs.OS'],,,[]
Accelerating Filesystem Checking and Repair with pFSCK,http://arxiv.org/abs/2004.05524v1,2020-04-12T01:59:20Z,2020-04-12T01:59:20Z,"  File system checking and recovery (C/R) tools play a pivotal role in
increasing the reliability of storage software, identifying and correcting file
system inconsistencies. However, with increasing disk capacity and data
content, file system C/R tools notoriously suffer from long runtimes. We posit
that current file system checkers fail to exploit CPU parallelism and high
throughput offered by modern storage devices. To overcome these challenges, we
propose pFSCK, a tool that redesigns C/R to enable fine-grained parallelism at
the granularity of inodes without impacting the correctness of C/R's
functionality. To accelerate C/R, pFSCK first employs data parallelism by
identifying functional operations in each stage of the checker and isolating
dependent operation and their shared data structures. However, fully isolating
shared structures is infeasible, consequently requiring serialization that
limits scalability. To reduce the impact of synchronization bottlenecks and
exploit CPU parallelism, pFSCK designs pipeline parallelism allowing multiple
stages of C/R to run simultaneously without impacting correctness. To realize
efficient pipeline parallelism for different file system data configurations,
pFSCK provides techniques for ordering updates to global data structures,
efficient per-thread I/O cache management, and dynamic thread placement across
different passes of a C/R. Finally, pFSCK designs a resource-aware scheduler
aimed towards reducing the impact of C/R on other applications sharing CPUs and
the file system. Evaluation of pFSCK shows more than 2.6x gains of e2fsck and
more than 1.8x over XFS's checker that provides coarse-grained parallelism.
","['\nDavid Domingo\n', '\nKyle Stratton\n', '\nSudarsun Kannan\n']","13 pages, 9 figures",,http://arxiv.org/abs/2004.05524v1,cs.OS,['cs.OS'],,,[]
Optimal Virtual Cluster-based Multiprocessor Scheduling,http://arxiv.org/abs/2004.02439v1,2020-04-06T07:24:40Z,2020-04-06T07:24:40Z,"  Scheduling of constrained deadline sporadic task systems on multiprocessor
platforms is an area which has received much attention in the recent past. It
is widely believed that finding an optimal scheduler is hard, and therefore
most studies have focused on developing algorithms with good processor
utilization bounds. These algorithms can be broadly classified into two
categories: partitioned scheduling in which tasks are statically assigned to
individual processors, and global scheduling in which each task is allowed to
execute on any processor in the platform. In this paper we consider a third,
more general, approach called cluster-based scheduling. In this approach each
task is statically assigned to a processor cluster, tasks in each cluster are
globally scheduled among themselves, and clusters in turn are scheduled on the
multiprocessor platform. We develop techniques to support such cluster-based
scheduling algorithms, and also consider properties that minimize total
processor utilization of individual clusters. In the last part of this paper,
we develop new virtual cluster-based scheduling algorithms. For implicit
deadline sporadic task systems, we develop an optimal scheduling algorithm that
is neither Pfair nor ERfair. We also show that the processor utilization bound
of US-EDF{m/(2m-1)} can be improved by using virtual clustering. Since neither
partitioned nor global strategies dominate over the other, cluster-based
scheduling is a natural direction for research towards achieving improved
processor utilization bounds.
","['\nArvind Easwaran\n', '\nInsik Shin\n', '\nInsup Lee\n']","This is a post-peer-review, pre-copyedit version of an article
  published in Springer Real-Time Systems journal. The final authenticated
  version is available online at: https://doi.org/10.1007/s11241-009-9073-x","Springer Real-Time Systems, Volume 43, Pages 25-59, July 2009",http://dx.doi.org/10.1007/s11241-009-9073-x,cs.OS,"['cs.OS', 'cs.DC']",10.1007/s11241-009-9073-x,,[]
"SoftWear: Software-Only In-Memory Wear-Leveling for Non-Volatile Main
  Memory",http://arxiv.org/abs/2004.03244v2,2020-04-07T10:33:37Z,2020-04-08T17:05:35Z,"  Several emerging technologies for byte-addressable non-volatile memory (NVM)
have been considered to replace DRAM as the main memory in computer systems
during the last years. The disadvantage of a lower write endurance, compared to
DRAM, of NVM technologies like Phase-Change Memory (PCM) or Ferroelectric RAM
(FeRAM) has been addressed in the literature. As a solution, in-memory
wear-leveling techniques have been proposed, which aim to balance the
wear-level over all memory cells to achieve an increased memory lifetime.
Generally, to apply such advanced aging-aware wear-leveling techniques proposed
in the literature, additional special hardware is introduced into the memory
system to provide the necessary information about the cell age and thus enable
aging-aware wear-leveling decisions.
  This paper proposes software-only aging-aware wear-leveling based on common
CPU features and does not rely on any additional hardware support from the
memory subsystem. Specifically, we exploit the memory management unit (MMU),
performance counters, and interrupts to approximate the memory write counts as
an aging indicator. Although the software-only approach may lead to slightly
worse wear-leveling, it is applicable on commonly available hardware. We
achieve page-level coarse-grained wear-leveling by approximating the current
cell age through statistical sampling and performing physical memory remapping
through the MMU. This method results in non-uniform memory usage patterns
within a memory page. Hence, we further propose a fine-grained wear-leveling in
the stack region of C / C++ compiled software.
  By applying both wear-leveling techniques, we achieve up to $78.43\%$ of the
ideal memory lifetime, which is a lifetime improvement of more than a factor of
$900$ compared to the lifetime without any wear-leveling.
","['\nChristian Hakert\n', '\nKuan-Hsun Chen\n', '\nPaul R. Genssler\n', '\nGeorg von der Brüggen\n', '\nLars Bauer\n', '\nHussam Amrouch\n', '\nJian-Jia Chen\n', '\nJörg Henkel\n']",,,http://arxiv.org/abs/2004.03244v2,cs.OS,"['cs.OS', 'cs.AR']",,,[]
Hardware Memory Management for Future Mobile Hybrid Memory Systems,http://arxiv.org/abs/2004.05518v1,2020-04-12T01:25:04Z,2020-04-12T01:25:04Z,"  The current mobile applications have rapidly growing memory footprints,
posing a great challenge for memory system design. Insufficient DRAM main
memory will incur frequent data swaps between memory and storage, a process
that hurts performance, consumes energy and deteriorates the write endurance of
typical flash storage devices. Alternately, a larger DRAM has higher leakage
power and drains the battery faster. Further, DRAM scaling trends make further
growth of DRAMin the mobile space prohibitive due to cost. Emerging
non-volatile memory (NVM) has the potential to alleviate these issues due to
its higher capacity per cost than DRAM and mini-mal static power. Recently, a
wide spectrum of NVM technologies, including phase-change memories (PCM),
memristor, and 3D XPoint have emerged. Despite the mentioned advantages, NVM
has longer access latency compared to DRAMand NVM writes can incur higher
latencies and wear costs. Therefore integration of these new memory
technologies in the memory hierarchy requires a fundamental rearchitect-ing of
traditional system designs. In this work, we propose a hardware-accelerated
memory manager (HMMU) that addresses both types of memory in a flat space
address space. We design a set of data placement and data migration policies
within this memory manager, such that we may exploit the advantages of each
memory technology. By augmenting the system with this HMMU, we reduce the
overall memory latency while also reducing writes to the NVM. Experimental
results show that our design achieves a 39% reduction in energy consumption
with only a 12% performance degradation versus an all-DRAM baseline that is
likely untenable in the future.
","['\nFei Wen\n', '\nMian Qin\n', '\nPaul Gratz\n', '\nNarasimha Reddy\n']",,,http://arxiv.org/abs/2004.05518v1,cs.AR,"['cs.AR', 'cs.OS']",,,[]
A Linux Kernel Scheduler Extension for Multi-core Systems,http://arxiv.org/abs/2004.06354v1,2020-04-14T08:35:33Z,2020-04-14T08:35:33Z,"  The Linux kernel is mostly designed for multi-programed environments, but
high-performance applications have other requirements. Such applications are
run standalone, and usually rely on runtime systems to distribute the
application's workload on worker threads, one per core. However, due to current
OSes limitations, it is not feasible to track whether workers are actually
running or blocked due to, for instance, a requested resource. For I/O
intensive applications, this leads to a significant performance degradation
given that the core of a blocked thread becomes idle until it is able to run
again. In this paper, we present the proof-of-concept of a Linux kernel
extension denoted User-Monitored Threads (UMT) which tackles this problem. Our
extension allows a user-space process to be notified of when the selected
threads become blocked or unblocked, making it possible for a runtime to
schedule additional work on the idle core. We implemented the extension on the
Linux Kernel 5.1 and adapted the Nanos6 runtime of the OmpSs-2 programming
model to take advantage of it. The whole prototype was tested on two
applications which, on the tested hardware and the appropriate conditions,
reported speedups of almost 2x.
","['\nAleix Roca\n', '\nSamuel Rodríguez\n', '\nAlbert Segura\n', '\nKevin Marquet\n', '\nVicenç Beltran\n']","10 pages, 5 figures, conference",,http://dx.doi.org/10.1109/HiPC.2019.00050,cs.OS,"['cs.OS', 'cs.DC', 'D.4.1; D.3.3']",10.1109/HiPC.2019.00050,,[]
A File System For Write-Once Media,http://arxiv.org/abs/2004.00402v1,2020-03-31T01:44:14Z,2020-03-31T01:44:14Z,"  A file system standard for use with write-once media such as digital compact
disks is proposed. The file system is designed to work with any operating
system and a variety of physical media. Although the implementation is simple,
it provides a a full-featured and high-performance alternative to conventional
file systems on traditional, multiple-write media such as magnetic disks.
","['\nSimson L. Garfinkel\n', '\nJ. Spencer Love\n']","MIT Media Laboratory Tech Report, 1985",,http://arxiv.org/abs/2004.00402v1,cs.OS,['cs.OS'],,,[]
"Co-Optimizing Performance and Memory FootprintVia Integrated CPU/GPU
  Memory Management, anImplementation on Autonomous Driving Platform",http://arxiv.org/abs/2003.07945v2,2020-03-17T21:09:24Z,2020-03-19T02:43:49Z,"  Cutting-edge embedded system applications, such as self-driving cars and
unmanned drone software, are reliant on integrated CPU/GPU platforms for their
DNNs-driven workload, such as perception and other highly parallel components.
In this work, we set out to explore the hidden performance implication of GPU
memory management methods of integrated CPU/GPU architecture. Through a series
of experiments on micro-benchmarks and real-world workloads, we find that the
performance under different memory management methods may vary according to
application characteristics. Based on this observation, we develop a
performance model that can predict system overhead for each memory management
method based on application characteristics. Guided by the performance model,
we further propose a runtime scheduler. By conducting per-task memory
management policy switching and kernel overlapping, the scheduler can
significantly relieve the system memory pressure and reduce the multitasking
co-run response time. We have implemented and extensively evaluated our system
prototype on the NVIDIA Jetson TX2, Drive PX2, and Xavier AGX platforms, using
both Rodinia benchmark suite and two real-world case studies of drone software
and autonomous driving software.
","['\nSoroush Bateni\n', '\nZhendong Wang\n', '\nYuankun Zhu\n', '\nYang Hu\n', '\nCong Liu\n']",,,http://arxiv.org/abs/2003.07945v2,cs.DC,"['cs.DC', 'cs.OS']",,,[]
Fissile Locks,http://arxiv.org/abs/2003.05025v2,2020-03-10T22:51:16Z,2020-05-01T21:24:00Z,"  Classic test-and-test (TS) mutual exclusion locks are simple, and enjoy high
performance and low latency of ownership transfer under light or no contention.
However, they do not scale gracefully under high contention and do not provide
any admission order guarantees. Such concerns led to the development of
scalable queue-based locks, such as a recent Compact NUMA-aware (CNA) lock, a
variant of another popular queue-based MCS lock. CNA scales well under load and
provides certain admission guarantees, but has more complicated lock handover
operations than TS and incurs higher latencies at low contention. We propose
Fissile locks, which capture the most desirable properties of both TS and CNA.
A Fissile lock consists of two underlying locks: a TS lock, which serves as a
fast path, and a CNA lock, which serves as a slow path. The key feature of
Fissile locks is the ability of threads on the fast path to bypass threads
enqueued on the slow path, and acquire the lock with less overhead than CNA.
Bypass is bounded (by a tunable parameter) to avoid starvation and ensure
long-term fairness. The result is a highly scalable NUMA-aware lock with
progress guarantees that performs like TS at low contention and like CNA at
high contention.
","['\nDave Dice\n', '\nAlex Kogan\n']",,,http://arxiv.org/abs/2003.05025v2,cs.OS,['cs.OS'],,,[]
"Combining Task-level and System-level Scheduling Modes for Mixed
  Criticality Systems",http://arxiv.org/abs/2003.05442v1,2020-03-11T03:32:55Z,2020-03-11T03:32:55Z,"  Different scheduling algorithms for mixed criticality systems have been
recently proposed. The common denominator of these algorithms is to discard low
critical tasks whenever high critical tasks are in lack of computation
resources. This is achieved upon a switch of the scheduling mode from Normal to
Critical. We distinguish two main categories of the algorithms: system-level
mode switch and task-level mode switch. System-level mode algorithms allow low
criticality (LC) tasks to execute only in normal mode. Task-level mode switch
algorithms enable to switch the mode of an individual high criticality task
(HC), from low (LO) to high (HI), to obtain priority over all LC tasks. This
paper investigates an online scheduling algorithm for mixed-criticality systems
that supports dynamic mode switches for both task level and system level. When
a HC task job overruns its LC budget, then only that particular job is switched
to HI mode. If the job cannot be accommodated, then the system switches to
Critical mode. To accommodate for resource availability of the HC jobs, the LC
tasks are degraded by stretching their periods until the Critical mode
exhibiting job complete its execution. The stretching will be carried out until
the resource availability is met. We have mechanized and implemented the
proposed algorithm using Uppaal. To study the efficiency of our scheduling
algorithm, we examine a case study and compare our results to the state of the
art algorithms.
","['\nJalil Boudjadar\n', '\nSaravanan Ramanathan\n', '\nArvind Easwaran\n', '\nUlrik Nyman\n']","\copyright 2019 IEEE. Personal use of this material is permitted.
  Permission from IEEE must be obtained for all other uses, in any current or
  future media, including reprinting/republishing this material for advertising
  or promotional purposes, creating new collective works, for resale or
  redistribution to servers or lists, or reuse of any copyrighted component of
  this work in other works","IEEE/ACM International Symposium on Distributed Simulation and
  Real Time Applications (DS-RT), Cosenza, Italy, 2019, pages 1-10",http://dx.doi.org/10.1109/DS-RT47707.2019.8958666,cs.OS,['cs.OS'],10.1109/DS-RT47707.2019.8958666,,[]
"Demand-based Scheduling of Mixed-Criticality Sporadic Tasks on One
  Processor",http://arxiv.org/abs/2003.05444v1,2020-03-11T05:00:53Z,2020-03-11T05:00:53Z,"  Strategies that artificially tighten high-criticality task deadlines in
low-criticality behaviors have been successfully employed for scheduling
mixed-criticality systems. Although efficient scheduling algorithms have been
developed for implicit deadline task systems, the same is not true for more
general sporadic tasks. In this paper we develop a new demand-based
schedulability test for such general mixed-criticality task systems, in which
we collectively bound the low- and high-criticality demand of tasks. We show
that the new test strictly dominates the only other known demand-based test for
such systems. We also propose a new deadline tightening strategy based on this
test, and show through simulations that the strategy significantly outperforms
all known scheduling algorithms for a variety of sporadic task systems.
",['\nArvind Easwaran\n'],"\copyright 2013 IEEE. Personal use of this material is permitted.
  Permission from IEEE must be obtained for all other uses, in any current or
  future media, including reprinting/republishing this material for advertising
  or promotional purposes, creating new collective works, for resale or
  redistribution to servers or lists, or reuse of any copyrighted component of
  this work in other works","IEEE Real-Time Systems Symposium (RTSS), Vancouver, Canada, 2013,
  pages 78-87",http://dx.doi.org/10.1109/RTSS.2013.16,cs.OS,['cs.OS'],10.1109/RTSS.2013.16,,[]
"Utilization Difference Based Partitioned Scheduling of Mixed-Criticality
  Systems",http://arxiv.org/abs/2003.05445v1,2020-03-11T05:11:33Z,2020-03-11T05:11:33Z,"  Mixed-Criticality (MC) systems consolidate multiple functionalities with
different criticalities onto a single hardware platform. Such systems improve
the overall resource utilization while guaranteeing resources to critical
tasks. In this paper, we focus on the problem of partitioned multiprocessor MC
scheduling, in particular the problem of designing efficient partitioning
strategies. We develop two new partitioning strategies based on the principle
of evenly distributing the difference between total high-critical utilization
and total low-critical utilization for the critical tasks among all processors.
By balancing this difference, we are able to reduce the pessimism in
uniprocessor MC schedulability tests that are applied on each processor, thus
improving overall schedulability. To evaluate the schedulability performance of
the proposed strategies, we compare them against existing partitioned
algorithms using extensive experiments. We show that the proposed strategies
are effective with both dynamic-priority Earliest Deadline First with Virtual
Deadlines (EDF-VD) and fixed-priority Adaptive Mixed-Criticality (AMC)
algorithms. Specifically, our results show that the proposed strategies improve
schedulability by as much as 28.1% and 36.2% for implicit and
constrained-deadline task systems respectively.
","['\nSaravanan Ramanathan\n', '\nArvind Easwaran\n']","\copyright 2017 IEEE. Personal use of this material is permitted.
  Permission from IEEE must be obtained for all other uses, in any current or
  future media, including reprinting/republishing this material for advertising
  or promotional purposes, creating new collective works, for resale or
  redistribution to servers or lists, or reuse of any copyrighted component of
  this work in other works","Design, Automation & Test in Europe Conference & Exhibition
  (DATE), 2017, Lausanne, 2017, pages 238-243",http://dx.doi.org/10.23919/DATE.2017.7926989,cs.OS,['cs.OS'],10.23919/DATE.2017.7926989,,[]
"Dynamic Budget Management with Service Guarantees for Mixed-Criticality
  Systems",http://arxiv.org/abs/2003.08364v1,2020-03-11T04:03:40Z,2020-03-11T04:03:40Z,"  Many existing studies on mixed-criticality (MC) scheduling assume that
low-criticality budgets for high-criticality applications are known apriori.
These budgets are primarily used as guidance to determine when the scheduler
should switch the system mode from low to high. Based on this key observation,
in this paper we propose a dynamic MC scheduling model under which
low-criticality budgets for individual high-criticality applications are
determined at runtime as opposed to being fixed offline. To ensure sufficient
budget for high-criticality applications at all times, we use offline
schedulability analysis to determine a system-wide total low-criticality budget
allocation for all the high-criticality applications combined. This total
budget is used as guidance in our model to determine the need for a
mode-switch. The runtime strategy then distributes this total budget among the
various applications depending on their execution requirement and with the
objective of postponing mode-switch as much as possible. We show that this
runtime strategy is able to postpone mode-switches for a longer time than any
strategy that uses a fixed low-criticality budget allocation for each
application. Finally, since we are able to control the total budget allocation
for high-criticality applications before mode-switch, we also propose
techniques to determine these budgets considering system-wide objectives such
as schedulability and service guarantee for low-criticality applications.
","['\nXiaozhe Gu\n', '\nArvind Easwaran\n']","\copyright 2016 IEEE. Personal use of this material is permitted.
  Permission from IEEE must be obtained for all other uses, in any current or
  future media, including reprinting/republishing this material for advertising
  or promotional purposes, creating new collective works, for resale or
  redistribution to servers or lists, or reuse of any copyrighted component of
  this work in other works","IEEE Real-Time Systems Symposium (RTSS), Porto, Portugal, 2016,
  pages 47-56",http://dx.doi.org/10.1109/RTSS.2016.014,cs.DC,"['cs.DC', 'cs.OS']",10.1109/RTSS.2016.014,,[]
"Efficient Schedulability Test for Dynamic-Priority Scheduling of
  Mixed-Criticality Real-Time Systems",http://arxiv.org/abs/2003.05160v1,2020-03-11T08:38:34Z,2020-03-11T08:38:34Z,"  Systems in many safety-critical application domains are subject to
certification requirements. In such a system, there are typically different
applications providing functionalities that have varying degrees of
criticality. Consequently, the certification requirements for functionalities
at these different criticality levels are also varying, with very high levels
of assurance required for a highly critical functionality, whereas relatively
low levels of assurance required for a less critical functionality. Considering
the timing assurance given to various applications in the form of guaranteed
budgets within deadlines, a theory of real-time scheduling for such
multi-criticality systems has been under development in the recent past. In
particular, an algorithm called Earliest Deadline First with Virtual Deadlines
(EDF-VD) has shown a lot of promise for systems with two criticality levels,
especially in terms of practical performance demonstrated through experiment
results. In this paper we design a new schedulability test for EDF-VD that
extend these performance benefits to multi-criticality systems. We propose a
new test based on demand bound functions and also present a novel virtual
deadline assignment strategy. Through extensive experiments we show that the
proposed technique significantly outperforms existing strategies for a variety
of generic real-time systems.
","['\nXiaozhe Gu\n', '\nArvind Easwaran\n']",Publication rights licensed to ACM,"ACM Transactions on Embedded Computing Systems, Volume 17, Issue
  1, Pages 24:1-24:24, November 2017",http://dx.doi.org/10.1145/3105922,cs.OS,"['cs.OS', 'cs.SY', 'eess.SY']",10.1145/3105922,,[]
"Multi-Rate Fluid Scheduling of Mixed-Criticality Systems on
  Multiprocessors",http://arxiv.org/abs/2003.05168v1,2020-03-11T08:58:29Z,2020-03-11T08:58:29Z,"  In this paper we consider the problem of mixed-criticality (MC) scheduling of
implicit-deadline sporadic task systems on a homogenous multiprocessor
platform. Focusing on dual-criticality systems, algorithms based on the fluid
scheduling model have been proposed in the past. These algorithms use a
dual-rate execution model for each high-criticality task depending on the
system mode. Once the system switches to the high-criticality mode, the
execution rates of such tasks are increased to meet their increased demand.
Although these algorithms are speed-up optimal, they are unable to schedule
several feasible dual-criticality task systems. This is because a single fixed
execution rate for each high-criticality task after the mode switch is not
efficient to handle the high variability in demand during the transition period
immediately following the mode switch. This demand variability exists as long
as the carry-over jobs of high-criticality tasks, that is jobs released before
the mode switch, have not completed. Addressing this shortcoming, we propose a
multi-rate fluid execution model for dual-criticality task systems in this
paper. Under this model, high-criticality tasks are allocated varying execution
rates in the transition period after the mode switch to efficiently handle the
demand variability. We derive a sufficient schedulability test for the proposed
model and show its dominance over the dual-rate fluid execution model. Further,
we also present a speed-up optimal rate assignment strategy for the multi-rate
model, and experimentally show that the proposed model outperforms all the
existing MC scheduling algorithms with known speed-up bounds.
","['\nSaravanan Ramanathan\n', '\nArvind Easwaran\n', '\nHyeonjoong Cho\n']","This is a post-peer-review, pre-copyedit version of an article
  published in Real-Time Systems. The final authenticated version is available
  online at the below DOI","Springer Real-Time Systems, Issue 54, pages 247-277, April 2018",http://dx.doi.org/10.1007/s11241-017-9296-1,cs.OS,"['cs.OS', 'cs.SY', 'eess.SY']",10.1007/s11241-017-9296-1,,[]
"Safe and Efficient Remote Application Code Execution on Disaggregated
  NVM Storage with eBPF",http://arxiv.org/abs/2002.11528v1,2020-02-25T13:43:53Z,2020-02-25T13:43:53Z,"  With rapid improvements in NVM storage devices, the performance bottleneck is
gradually shifting to the network, thus giving rise to the notion of ""data
movement wall"". To reduce the amount of data movement over the network,
researchers have proposed near-data computing by shipping operations and
compute-extensions closer to storage devices. However, running arbitrary,
user-provided extensions in a shared, disaggregated storage environment
presents multiple challenges regarding safety, isolation, and performance.
Instead of approaching this problem from scratch, in this work we make a case
for leveraging the Linux kernel eBPF framework to program disaggregated NVM
storage devices. eBPF offers a safe, verifiable, and high-performance way of
executing untrusted, user-defined code in a shared runtime. In this paper, we
describe our experiences building a first prototype that supports remote
operations on storage using eBPF, discuss the limitations of our approach, and
directions for addressing them.
","['\nKornilios Kourtis\n', '\nAnimesh Trivedi\n', '\nNikolas Ioannou\n']",,,http://arxiv.org/abs/2002.11528v1,cs.DC,"['cs.DC', 'cs.OS']",,,[]
"Bringing Inter-Thread Cache Benefits to Federated Scheduling -- Extended
  Results & Technical Report",http://arxiv.org/abs/2002.12516v1,2020-02-28T02:37:26Z,2020-02-28T02:37:26Z,"  Multiprocessor scheduling of hard real-time tasks modeled by directed acyclic
graphs (DAGs) exploits the inherent parallelism presented by the model. For DAG
tasks, a node represents a request to execute an object on one of the available
processors. In one DAG task, there may be multiple execution requests for one
object, each represented by a distinct node. These distinct execution requests
offer an opportunity to reduce their combined cache overhead through
coordinated scheduling of objects as threads within a parallel task. The goal
of this work is to realize this opportunity by incorporating the cache-aware
BUNDLE-scheduling algorithm into federated scheduling of sporadic DAG task
sets.
  This is the first work to incorporate instruction cache sharing into
federated scheduling. The result is a modification of the DAG model named the
DAG with objects and threads (DAG-OT). Under the DAG-OT model, descriptions of
nodes explicitly include their underlying executable object and number of
threads. When possible, nodes assigned the same executable object are collapsed
into a single node; joining their threads when BUNDLE-scheduled. Compared to
the DAG model, the DAG-OT model with cache-aware scheduling reduces the number
of cores allocated to individual tasks by approximately 20 percent in the
synthetic evaluation and up to 50 percent on a novel parallel computing
platform implementation. By reducing the number of allocated cores, the DAG-OT
model is able to schedule a subset of previously infeasible task sets.
","['\nCorey Tessler\n', '\nVenkata P. Modekurthy\n', '\nNathan Fisher\n', '\nAbusayeed Saifullah\n']",,,http://arxiv.org/abs/2002.12516v1,cs.DC,"['cs.DC', 'cs.OS']",,,[]
Characterizing Synchronous Writes in Stable Memory Devices,http://arxiv.org/abs/2002.07515v1,2020-02-18T12:26:24Z,2020-02-18T12:26:24Z,"  Distributed algorithms that operate in the fail-recovery model rely on the
state stored in stable memory to guarantee the irreversibility of operations
even in the presence of failures. The performance of these algorithms lean
heavily on the performance of stable memory. Current storage technologies have
a defined performance profile: data is accessed in blocks of hundreds or
thousands of bytes, random access to these blocks is expensive and sequential
access is somewhat better. File system implementations hide some of the
performance limitations of the underlying storage devices using buffers and
caches. However, fail-recovery distributed algorithms bypass some of these
techniques and perform synchronous writes to be able to tolerate a failure
during the write itself. Assuming the distributed system designer is able to
buffer the algorithm's writes, we ask how buffer size and latency complement
each other. In this paper we start to answer this question by characterizing
the performance (throughput and latency) of typical stable memory devices using
a representative set of current file systems.
","['\nWilliam B. Mingardi\n', '\nGustavo M. D. Vieira\n']",14 pages,"WPerformance '19: Proceedings of the XVIII Computer and
  Communication Systems Workshop, SBC, 2019",http://dx.doi.org/10.5753/wperformance.2019.6458,cs.OS,['cs.OS'],10.5753/wperformance.2019.6458,,[]
LibrettOS: A Dynamically Adaptable Multiserver-Library OS,http://arxiv.org/abs/2002.08928v1,2020-02-20T18:25:42Z,2020-02-20T18:25:42Z,"  We present LibrettOS, an OS design that fuses two paradigms to simultaneously
address issues of isolation, performance, compatibility, failure
recoverability, and run-time upgrades. LibrettOS acts as a microkernel OS that
runs servers in an isolated manner. LibrettOS can also act as a library OS
when, for better performance, selected applications are granted exclusive
access to virtual hardware resources such as storage and networking.
Furthermore, applications can switch between the two OS modes with no
interruption at run-time. LibrettOS has a uniquely distinguishing advantage in
that, the two paradigms seamlessly coexist in the same OS, enabling users to
simultaneously exploit their respective strengths (i.e., greater isolation,
high performance). Systems code, such as device drivers, network stacks, and
file systems remain identical in the two modes, enabling dynamic mode switching
and reducing development and maintenance costs.
  To illustrate these design principles, we implemented a prototype of
LibrettOS using rump kernels, allowing us to reuse existent, hardened NetBSD
device drivers and a large ecosystem of POSIX/BSD-compatible applications. We
use hardware (VM) virtualization to strongly isolate different rump kernel
instances from each other. Because the original rumprun unikernel targeted a
much simpler model for uniprocessor systems, we redesigned it to support
multicore systems. Unlike kernel-bypass libraries such as DPDK, applications
need not be modified to benefit from direct hardware access. LibrettOS also
supports indirect access through a network server that we have developed.
Applications remain uninterrupted even when network components fail or need to
be upgraded. Finally, to efficiently use hardware resources, applications can
dynamically switch between the indirect and direct modes based on their I/O
load at run-time.
  [full abstract is in the paper]
","['\nRuslan Nikolaev\n', '\nMincheol Sung\n', '\nBinoy Ravindran\n']","16th ACM SIGPLAN/SIGOPS International Conference on Virtual Execution
  Environments (VEE '20), March 17, 2020, Lausanne, Switzerland",,http://dx.doi.org/10.1145/3381052.3381316,cs.OS,['cs.OS'],10.1145/3381052.3381316,,[]
Privaros: A Framework for Privacy-Compliant Delivery Drones,http://arxiv.org/abs/2002.06512v3,2020-02-16T05:51:41Z,2020-08-13T18:00:46Z,"  We present Privaros, a framework to enforce privacy policies on drones.
Privaros is designed for commercial delivery drones, such as the ones that will
likely be used by Amazon Prime Air. Such drones visit a number of host
airspaces, each of which may have different privacy requirements. Privaros
provides an information flow control framework to enforce the policies of these
hosts on the guest delivery drones. The mechanisms in Privaros are built on top
of ROS, a middleware popular in many drone platforms. This paper presents the
design and implementation of these mechanisms, describes how policies are
specified, and shows that Privaros's policy specification can be integrated
with India's Digital Sky portal. Our evaluation shows that a drone running
Privaros can robustly enforce various privacy policies specified by hosts, and
that its core mechanisms only marginally increase communication latency and
power consumption.
","['\nRakesh Rajan Beck\n', '\nAbhishek Vijeev\n', '\nVinod Ganapathy\n']",,,http://dx.doi.org/10.1145/3372297.3417858,cs.CR,"['cs.CR', 'cs.OS']",10.1145/3372297.3417858,,[]
A Recurrent Neural Network Based Patch Recommender for Linux Kernel Bugs,http://arxiv.org/abs/2002.08454v1,2020-02-19T21:35:51Z,2020-02-19T21:35:51Z,"  Software bugs in a production environment have an undesirable impact on
quality of service, unplanned system downtime, and disruption in good customer
experience, resulting in loss of revenue and reputation. Existing approaches to
automated software bug repair focuses on known bug templates detected using
static code analysis tools and test suites, and in automatic generation of
patch code for these bugs. We describe the typical bug fixing process employed
in the Linux kernel, and motivate the need for a new automated tool flow to fix
bugs. We present an initial design of such an automated tool that uses
Recurrent Neural Network (RNN) based Natural Language Processing to generate
patch recommendations from user generated bug reports. At the 50th percentile
of the test bugs, the correct patch occurs within the top 11.5 patch
recommendations output by the model. Further, we present a Linux kernel
developer's assessment of the quality of patches recommended for new unresolved
kernel bugs.
","['\nAnusha Bableshwar\n', '\nArun Ravindran\n', '\nManoj Iyer\n']",,,http://arxiv.org/abs/2002.08454v1,cs.SE,"['cs.SE', 'cs.CR', 'cs.OS']",,,[]
"SPARTA: A Divide and Conquer Approach to Address Translation for
  Accelerators",http://arxiv.org/abs/2001.07045v1,2020-01-20T10:23:12Z,2020-01-20T10:23:12Z,"  Virtual memory (VM) is critical to the usability and programmability of
hardware accelerators. Unfortunately, implementing accelerator VM efficiently
is challenging because the area and power constraints make it difficult to
employ the large multi-level TLBs used in general-purpose CPUs. Recent research
proposals advocate a number of restrictions on virtual-to-physical address
mappings in order to reduce the TLB size or increase its reach. However, such
restrictions are unattractive because they forgo many of the original benefits
of traditional VM, such as demand paging and copy-on-write.
  We propose SPARTA, a divide and conquer approach to address translation.
SPARTA splits the address translation into accelerator-side and memory-side
parts. The accelerator-side translation hardware consists of a tiny TLB
covering only the accelerator's cache hierarchy (if any), while the translation
for main memory accesses is performed by shared memory-side TLBs. Performing
the translation for memory accesses on the memory side allows SPARTA to overlap
data fetch with translation, and avoids the replication of TLB entries for data
shared among accelerators. To further improve the performance and efficiency of
the memory-side translation, SPARTA logically partitions the memory space,
delegating translation to small and efficient per-partition translation
hardware. Our evaluation on index-traversal accelerators shows that SPARTA
virtually eliminates translation overhead, reducing it by over 30x on average
(up to 47x) and improving performance by 57%. At the same time, SPARTA requires
minimal accelerator-side translation hardware, reduces the total number of TLB
entries in the system, gracefully scales with memory size, and preserves all
key VM functionalities.
","['\nJavier Picorel\n', '\nSeyed Alireza Sanaee Kohroudi\n', '\nZi Yan\n', '\nAbhishek Bhattacharjee\n', '\nBabak Falsafi\n', '\nDjordje Jevdjic\n']",,,http://arxiv.org/abs/2001.07045v1,cs.AR,"['cs.AR', 'cs.OS']",,,[]
"Intel Page Modification Logging, a hardware virtualization feature:
  study and improvement for virtual machine working set estimation",http://arxiv.org/abs/2001.09991v1,2020-01-26T16:24:08Z,2020-01-26T16:24:08Z,"  Intel Page Modification Logging (PML) is a novel hardware feature for
tracking virtual machine (VM) accessed memory pages. This task is essential in
today's data centers since it allows, among others, checkpointing, live
migration and working set size (WSS) estimation. Relying on the Xen hypervisor,
this paper studies PML from three angles: power consumption, efficiency, and
performance impact on user applications. Our findings are as follows. First,
PML does not incur any power consumption overhead. Second, PML reduces by up to
10.18% both VM live migration and checkpointing time. Third, PML slightly
reduces by up to 0.95% the performance degradation on applications incurred by
live migration and checkpointing. Fourth, PML however does not allow accurate
WSS estimation because read accesses are not tracked and hot pages cannot be
identified. A naive extension of PML for addressing these limitations could
lead to severe performance degradation (up to 34.8%) for the VM whose WSS is
computed.
  This paper presents Page Reference Logging (PRL), a smart extension of PML
for allowing both read and write accesses to be tracked. It does this without
impacting user VMs. The paper also presents a WSS estimation system which
leverages PRL and shows how this algorithm can be integrated into a data center
which implements memory overcommitment. We implement PRL and the WSS estimation
system under Gem5, a very popular hardware simulator. The evaluation results
validate the accuracy of PRL in the estimation of WSS. They also show that PRL
incurs no performance degradation for user VMs.
","['\nStella Bitchebe\n', '\nDjob Mvondo\n', '\nAlain Tchana\n', '\nLaurent Réveillère\n', '\nNoël De Palma\n']",,,http://arxiv.org/abs/2001.09991v1,cs.DC,"['cs.DC', 'cs.OS']",,,[]
"Occlum: Secure and Efficient Multitasking Inside a Single Enclave of
  Intel SGX",http://arxiv.org/abs/2001.07450v1,2020-01-21T11:42:17Z,2020-01-21T11:42:17Z,"  Intel Software Guard Extensions (SGX) enables user-level code to create
private memory regions called enclaves, whose code and data are protected by
the CPU from software and hardware attacks outside the enclaves. Recent work
introduces library operating systems (LibOSes) to SGX so that legacy
applications can run inside enclaves with few or even no modifications. As
virtually any non-trivial application demands multiple processes, it is
essential for LibOSes to support multitasking. However, none of the existing
SGX LibOSes support multitasking both securely and efficiently.
  This paper presents Occlum, a system that enables secure and efficient
multitasking on SGX. We implement the LibOS processes as SFI-Isolated Processes
(SIPs). SFI is a software instrumentation technique for sandboxing untrusted
modules (called domains). We design a novel SFI scheme named MPX-based,
Multi-Domain SFI (MMDSFI) and leverage MMDSFI to enforce the isolation of SIPs.
We also design an independent verifier to ensure the security guarantees of
MMDSFI. With SIPs safely sharing the single address space of an enclave, the
LibOS can implement multitasking efficiently. The Occlum LibOS outperforms the
state-of-the-art SGX LibOS on multitasking-heavy workloads by up to 6,600X on
micro-benchmarks and up to 500X on application benchmarks.
","['\nYouren Shen\n', '\nHongliang Tian\n', '\nYu Chen\n', '\nKang Chen\n', '\nRunji Wang\n', '\nYi Xu\n', '\nYubin Xia\n']",,,http://dx.doi.org/10.1145/3373376.3378469,cs.OS,"['cs.OS', 'cs.AR', 'cs.CR']",10.1145/3373376.3378469,,[]
"On Schedulability Analysis of EDF Scheduling by Considering Suspension
  as Blocking",http://arxiv.org/abs/2001.05747v2,2020-01-16T11:40:57Z,2020-02-11T08:51:14Z,"  During the execution of a job, it may suspend itself, i.e., its computation
ceases to process until certain activities are complete to be resumed. This
paper provides a counterexample of the schedulability analysis by Devi in
Euromicro Conference on Real-Time Systems (ECRTS) in 2003, which is the only
existing suspension-aware analysis specialized for uniprocessor systems when
preemptive earliest-deadline-first (EDF) is applied for scheduling dynamic
selfsuspending tasks.
","['\nMario Günzel\n', '\nJian-Jia Chen\n']",,,http://arxiv.org/abs/2001.05747v2,cs.OS,['cs.OS'],,,[]
"Online Scheduling with Makespan Minimization: State of the Art Results,
  Research Challenges and Open Problems",http://arxiv.org/abs/2001.04698v1,2020-01-14T10:19:15Z,2020-01-14T10:19:15Z,"  Online scheduling has been a well studied and challenging research problem
over the last five decades since the pioneering work of Graham with immense
practical significance in various applications such as interactive parallel
processing, routing in communication networks, distributed data management,
client-server communications, traffic management in transportation, industrial
manufacturing and production. In this problem, a sequence of jobs is received
one by one in order by the scheduler for scheduling over a number of machines.
On arrival of a job, the scheduler assigns the job irrevocably to a machine
before the availability of the next job with an objective to minimize the
completion time of the scheduled jobs. This paper highlights the state of the
art contributions for online scheduling of a sequence of independent jobs on
identical and uniform related machines with a special focus on preemptive and
non-preemptive processing formats by considering makespan minimization as the
optimality criterion. We present the fundamental aspects of online scheduling
from a beginner's perspective along with a background of general scheduling
framework. Important competitive analysis results obtained by well-known
deterministic and randomized online scheduling algorithms in the literature are
presented along with research challenges and open problems. Two of the emerging
recent trends such as resource augmentation and semi-online scheduling are
discussed as a motivation for future research work.
","['\nDebasis Dwibedy\n', '\nRakesh Mohanty\n']","37 pages, 13 Tables, Submitted to Computer Science Review",,http://arxiv.org/abs/2001.04698v1,cs.OS,"['cs.OS', 'cs.DS']",,,[]
"A New Fairness Model based on User's Objective for Multi-user
  Multi-processor Online Scheduling",http://arxiv.org/abs/2001.06159v1,2020-01-17T05:11:55Z,2020-01-17T05:11:55Z,"  Resources of a multi-user system in multi-processor online scheduling are
shared by competing users in which fairness is a major performance criterion
for resource allocation. Fairness ensures equality in resource sharing among
the users. According to our knowledge, fairness based on the user's objective
has neither been comprehensively studied nor a formal fairness model has been
well defined in the literature. This motivates us to explore and define a new
model to ensure algorithmic fairness with quantitative performance measures
based on optimization of the user's objective. In this paper, we propose a new
model for fairness in Multi-user Multi-processor Online Scheduling
Problem(MUMPOSP). We introduce and formally define quantitative fairness
measures based on user's objective by optimizing makespan for individual user
in our proposed fairness model. We also define the unfairness of deprived users
and absolute fairness of an algorithm. We obtain lower bound results for the
absolute fairness for m identical machines with equal length jobs. We show that
our proposed fairness model can serve as a framework for measuring algorithmic
fairness by considering various optimality criteria such as flow time and sum
of completion times.
","['\nDebasis Dwibedy\n', '\nRakesh Mohanty\n']","11 pages, 3 figures",,http://arxiv.org/abs/2001.06159v1,cs.DS,"['cs.DS', 'cs.OS']",,,[]
Runtime Verification of Linux Kernel Security Module,http://arxiv.org/abs/2001.01442v1,2020-01-06T09:04:59Z,2020-01-06T09:04:59Z,"  The Linux kernel is one of the most important Free/Libre Open Source Software
(FLOSS) projects. It is installed on billions of devices all over the world,
which process various sensitive, confidential or simply private data. It is
crucial to establish and prove its security properties. This work-in-progress
paper presents a method to verify the Linux kernel for conformance with an
abstract security policy model written in the Event-B specification language.
The method is based on system call tracing and aims at checking that the
results of system call execution do not lead to accesses that violate security
policy requirements. As a basis for it, we use an additional Event-B
specification of the Linux system call interface that is formally proved to
satisfy all the requirements of the security policy model. In order to perform
the conformance checks we use it to reproduce intercepted system calls and
verify accesses.
","['\nDenis Efremov\n', '\nIlya Shchepetkov\n']","15 pages, 4 figures, 3 listings, OpenCERT 2019",,http://arxiv.org/abs/2001.01442v1,cs.SE,"['cs.SE', 'cs.OS']",,,[]
"ARM Pointer Authentication based Forward-Edge and Backward-Edge Control
  Flow Integrity for Kernels",http://arxiv.org/abs/1912.10666v2,2019-12-23T07:58:04Z,2020-10-12T06:33:43Z,"  Code reuse attacks are still big threats to software and system security.
Control flow integrity is a promising technique to defend against such attacks.
However, its effectiveness has been weakened due to the inaccurate control flow
graph and practical strategy to trade security for performance. In recent
years, CPU vendors have integrated hardware features as countermeasures. For
instance, ARM Pointer Authentication (PA in short) was introduced in ARMV8-A
architecture. It can efficiently generate an authentication code for an
address, which is encoded in the unused bits of the address. When the address
is de-referenced, the authentication code is checked to ensure its integrity.
Though there exist systems that adopt PA to harden user programs, how to
effectively use PA to protect OS kernels is still an open research question.
  In this paper, we shed lights on how to leverage PA to protect control flows,
including function pointers and return addresses, of Linux kernel.
Specifically, to protect function pointers, we embed authentication code into
them, track their propagation and verify their values when loading from memory
or branching to targets. To further defend against the pointer substitution
attack, we use the function pointer address as its context, and take a clean
design to propagate the address by piggybacking it into the pointer value. We
have implemented a prototype system with LLVM to identify function pointers,
add authentication code and verify function pointers by emitting new machine
instructions. We applied this system to Linux kernel, and solved numerous
practical issues, e.g., function pointer comparison and arithmetic operations.
The security analysis shows that our system can protect all function pointers
and return addresses in Linux kernel.
","['\nYutian Yang\n', '\nSongbo Zhu\n', '\nWenbo Shen\n', '\nYajin Zhou\n', '\nJiadong Sun\n', '\nKui Ren\n']",,,http://arxiv.org/abs/1912.10666v2,cs.CR,"['cs.CR', 'cs.OS']",,,[]
Virtual Gang based Scheduling of Real-Time Tasks on Multicore Platforms,http://arxiv.org/abs/1912.10959v2,2019-12-23T16:36:23Z,2020-02-27T19:48:02Z,"  We propose a virtual-gang based parallel real-time task scheduling approach
for multicore platforms. Our approach is based on the notion of a virtual-gang,
which is a group of parallel real-time tasks that are statically linked and
scheduled together by a gang scheduler. We present a light-weight intra-gang
synchronization framework, called RTG-Sync, and virtual gang formation
algorithms that provide strong temporal isolation and high real-time
schedulability in scheduling real-time tasks on multicore. We evaluate our
approach both analytically, with generated tasksets against state-of-the-art
approaches, and empirically with a case-study involving real-world workloads on
a real embedded multicore platform. The results show that our approach provides
simple but powerful compositional analysis framework, achieves better analytic
schedulability, especially when the effect of interference is considered, and
is a practical solution for COTS multicore platforms.
","['\nWaqar Ali\n', '\nRodolfo Pellizzoni\n', '\nHeechul Yun\n']","23 pages, 9 figures",,http://arxiv.org/abs/1912.10959v2,cs.OS,"['cs.OS', 'cs.DC']",,,[]
Dispel: Byzantine SMR with Distributed Pipelining,http://arxiv.org/abs/1912.10367v2,2019-12-22T00:58:54Z,2020-06-12T09:57:12Z,"  Byzantine State Machine Replication (SMR) is a long studied topic that
received increasing attention recently with the advent of blockchains as
companies are trying to scale them to hundreds of nodes. Byzantine SMRs try to
increase throughput by either reducing the latency of consensus instances that
they run sequentially or by reducing the number of replicas that send messages
to others in order to reduce the network usage. Unfortunately, the former
approach makes use of resources in burst whereas the latter requires
CPU-intensive authentication mechanisms.
  In this paper, we propose a new Byzantine SMR called Dispel (Distributed
Pipeline) that allows any node to distributively start new consensus instances
whenever they detect sufficient resources locally. We evaluate the performance
of Dispel within a single datacenter and across up to 380 machines over 3
continents by comparing it against four other SMRs. On 128 nodes, Dispel speeds
up HotStuff, the Byzantine fault tolerant SMR being integrated within
Facebook's blockchain, by more than 12 times. In addition, we also test Dispel
under isolated and correlated failures and show that the Dispel distributed
design is more robust than HotStuff. Finally, we evaluate Dispel in a
cryptocurrency application with Bitcoin transactions and show that this SMR is
not the bottleneck.
","['\nGauthier Voron\n', '\nVincent Gramoli\n']",,,http://arxiv.org/abs/1912.10367v2,cs.CR,"['cs.CR', 'cs.DC', 'cs.OS']",,,[]
"Faster than Flash: An In-Depth Study of System Challenges for Emerging
  Ultra-Low Latency SSDs",http://arxiv.org/abs/1912.06998v1,2019-12-15T08:09:31Z,2019-12-15T08:09:31Z,"  Emerging storage systems with new flash exhibit ultra-low latency (ULL) that
can address performance disparities between DRAM and conventional solid state
drives (SSDs) in the memory hierarchy. Considering the advanced low-latency
characteristics, different types of I/O completion methods (polling/hybrid) and
storage stack architecture (SPDK) are proposed. While these new techniques are
expected to take costly software interventions off the critical path in
ULL-applied systems, unfortunately no study exists to quantitatively analyze
system-level characteristics and challenges of combining such newly-introduced
techniques with real ULL SSDs. In this work, we comprehensively perform
empirical evaluations with 800GB ULL SSD prototypes and characterize ULL
behaviors by considering a wide range of I/O path parameters, such as different
queues and access patterns. We then analyze the efficiencies and challenges of
the polled-mode and hybrid polling I/O completion methods (added into Linux
kernels 4.4 and 4.10, respectively) and compare them with the efficiencies of a
conventional interrupt-based I/O path. In addition, we revisit the common
expectations of SPDK by examining all the system resources and parameters.
Finally, we demonstrate the challenges of ULL SSDs in a real SPDK-enabled
server-client system. Based on the performance behaviors that this study
uncovers, we also discuss several system implications, which are required to
take a full advantage of ULL SSD in the future.
","['\nSungjoon Koh\n', '\nJunhyeok Jang\n', '\nChangrim Lee\n', '\nMiryeong Kwon\n', '\nJie Zhang\n', '\nMyoungsoo Jung\n']","12 pages, 23 figures, 2019 IEEE International Symposium on Workload
  Characterization",,http://arxiv.org/abs/1912.06998v1,cs.OS,['cs.OS'],,,[]
Dependability Assessment of the Android OS through Fault Injection,http://arxiv.org/abs/1912.03490v1,2019-12-07T11:44:37Z,2019-12-07T11:44:37Z,"  The reliability of mobile devices is a challenge for vendors, since the
mobile software stack has significantly grown in complexity. In this paper, we
study how to assess the impact of faults on the quality of user experience in
the Android mobile OS through fault injection. We first address the problem of
identifying a realistic fault model for the Android OS, by providing to
developers a set of lightweight and systematic guidelines for fault modeling.
Then, we present an extensible fault injection tool (AndroFIT) to apply such
fault model on actual, commercial Android devices. Finally, we present a large
fault injection experimentation on three Android products from major vendors,
and point out several reliability issues and opportunities for improving the
Android OS.
","['\nDomenico Cotroneo\n', '\nAntonio Ken Iannillo\n', '\nRoberto Natella\n', '\nStefano Rosiello\n']",,"IEEE Transactions on Reliability, 2019",http://dx.doi.org/10.1109/TR.2019.2954384,cs.SE,"['cs.SE', 'cs.OS']",10.1109/TR.2019.2954384,,[]
Nova -- A rainbow cloud over the Alps,http://arxiv.org/abs/1912.03923v1,2019-12-09T09:39:31Z,2019-12-09T09:39:31Z,"  A pooled and shared on-demand Infrastructure as a Service (IaaS), based on
the Openstack software suite, was rolled out on the Grenoble university campus
in 2018 and updated in 2019.We present the methods used to deploy and manage
the infrastructure: racadm and preseed for basic system installation, then
Kolla for Openstack deployment. This latter solution, based on containers for
each service, enables a centralised and logged configuration (GitLab) of
controllers and calculation nodes. The solution is the benchmark solution for a
reproducible deployment of Openstack. We have been able to expand our cloud
easily with new nodes. The change in version of the basic OS was also
successfully tested despite a few small hitches... As security is a key element
in the proper operation of this type of shared service, each project has been
made watertight and its data perfectly isolated from other projects, thanks to
the encryption of all network flows in VXLANs.This OpenStack infotainment
platform is operational. What is it all for? For example, our first users use
the Jupyter Notebook through the provision of Jupyterhub servers (web portal);
the Distributed Health Assessment IT System (SIDES project); the continuous
integration in connection with the GitLab platform; the test for the Kubernetes
container scheduler or the calculation and visualisation software, etc. Highly
varied uses that other platforms had difficulty offering.Nova, a new platform,
was born.
","['\nNicolas Gibelin\nGRICAD\n', '\nRémi Cailletaud\nOSUG\n', '\nGabriel Moreau\nLEGI\n', '\nJean-François Scariot\nGRICAD\n', '\nGabrielle Feltin\nGRICAD\n', '\nAnthony Defize\nGRICAD\n']","Vid{\'e}o
  https://replay.jres.org/videos/watch/c0ce8c10-fc41-4cf7-9069-9d2c225f9e0a ,
  in French, Congr\`es JRES : Les Journ\'ees R\'eseaux de l'Enseignement et de
  la Recherche, RENATER, Dec 2019, Dijon, France",,http://arxiv.org/abs/1912.03923v1,cs.DC,"['cs.DC', 'cs.OS']",,,"['GRICAD', 'OSUG', 'LEGI', 'GRICAD', 'GRICAD', 'GRICAD']"
"Survivor: A Fine-Grained Intrusion Response and Recovery Approach for
  Commodity Operating Systems",http://arxiv.org/abs/1912.06863v1,2019-12-14T15:17:00Z,2019-12-14T15:17:00Z,"  Despite the deployment of preventive security mechanisms to protect the
assets and computing platforms of users, intrusions eventually occur. We
propose a novel intrusion survivability approach to withstand ongoing
intrusions. Our approach relies on an orchestration of fine-grained recovery
and per-service responses (e.g., privileges removal). Such an approach may put
the system into a degraded mode. This degraded mode prevents attackers to
reinfect the system or to achieve their goals if they managed to reinfect it.
It maintains the availability of core functions while waiting for patches to be
deployed. We devised a cost-sensitive response selection process to ensure that
while the service is in a degraded mode, its core functions are still
operating. We built a Linux-based prototype and evaluated the effectiveness of
our approach against different types of intrusions. The results show that our
solution removes the effects of the intrusions, that it can select appropriate
responses, and that it allows services to survive when reinfected. In terms of
performance overhead, in most cases, we observed a small overhead, except in
the rare case of services that write many small files asynchronously in a
burst, where we observed a higher but acceptable overhead.
","['\nRonny Chevalier\n', '\nDavid Plaquin\n', '\nChris Dalton\n', '\nGuillaume Hiet\n']","The final version of this paper has been published in the Proceedings
  of the 35th Annual Computer Security Applications Conference (ACSAC), 2019.
  14 pages, 5 figures, 6 tables","Proceedings of the 35th Annual Computer Security Applications
  Conference. ACM, 2019. p. 762-775",http://dx.doi.org/10.1145/3359789.3359792,cs.CR,"['cs.CR', 'cs.OS']",10.1145/3359789.3359792,,[]
"AppStreamer: Reducing Storage Requirements of Mobile Games through
  Predictive Streaming",http://arxiv.org/abs/2001.08169v1,2019-12-16T08:42:59Z,2019-12-16T08:42:59Z,"  Storage has become a constrained resource on smartphones. Gaming is a popular
activity on mobile devices and the explosive growth in the number of games
coupled with their growing size contributes to the storage crunch. Even where
storage is plentiful, it takes a long time to download and install a heavy app
before it can be launched. This paper presents AppStreamer, a novel technique
for reducing the storage requirements or startup delay of mobile games, and
heavy mobile apps in general. AppStreamer is based on the intuition that most
apps do not need the entirety of its files (images, audio and video clips,
etc.) at any one time. AppStreamer can, therefore, keep only a small part of
the files on the device, akin to a ""cache"", and download the remainder from a
cloud storage server or a nearby edge server when it predicts that the app will
need them in the near future. AppStreamer continuously predicts file blocks for
the near future as the user uses the app, and fetches them from the storage
server before the user sees a stall due to missing resources. We implement
AppStreamer at the Android file system layer. This ensures that the apps
require no source code or modification, and the approach generalizes across
apps. We evaluate AppStreamer using two popular games: Dead Effect 2, a 3D
first-person shooter, and Fire Emblem Heroes, a 2D turn-based strategy
role-playing game. Through a user study, 75% and 87% of the users respectively
find that AppStreamer provides the same quality of user experience as the
baseline where all files are stored on the device. AppStreamer cuts down the
storage requirement by 87% for Dead Effect 2 and 86% for Fire Emblem Heroes.
","['\nNawanol Theera-Ampornpunt\n', '\nShikhar Suryavansh\n', '\nSameer Manchanda\n', '\nRajesh Panta\n', '\nKaustubh Joshi\n', '\nMostafa Ammar\n', '\nMung Chiang\n', '\nSaurabh Bagchi\n']",12 pages; EWSN 2020,,http://arxiv.org/abs/2001.08169v1,cs.OS,"['cs.OS', 'cs.LG', 'stat.ML']",,,[]
"Exact Polynomial Time Algorithm for the Response Time Analysis of
  Harmonic Tasks with Constrained Release Jitter",http://arxiv.org/abs/1912.01161v1,2019-11-30T09:12:12Z,2019-11-30T09:12:12Z,"  In some important application areas of hard real-time systems, preemptive
sporadic tasks with harmonic periods and constraint deadlines running upon a
uni-processor platform play an important role. We propose a new algorithm for
determining the exact worst-case response time for a task that has a lower
computational complexity (linear in the number of tasks) than the known
algorithm developed for the same system class. We also allow the task
executions to start delayed due to release jitter if they are within certain
value ranges. For checking if these constraints are met we define a constraint
programming problem that has a special structure and can be solved with
heuristic components in a time that is linear in the task number. If the check
determines the admissibility of the jitter values, the linear time algorithm
can be used to determine the worst-case response time also for jitter-aware
systems.
","['\nThi Huyen Chau Nguyen\n', '\nWerner Grass\n', '\nKlaus Jansen\n']",,,http://arxiv.org/abs/1912.01161v1,cs.OS,['cs.OS'],,,[]
"Period Adaptation for Continuous Security Monitoring in Multicore
  Real-Time Systems",http://arxiv.org/abs/1911.11937v2,2019-11-27T03:52:34Z,2020-03-14T14:58:13Z,"  We propose a design-time framework (named HYDRA-C) for integrating security
tasks into partitioned real-time systems (RTS) running on multicore platforms.
Our goal is to opportunistically execute security monitoring mechanisms in a
'continuous' manner -- i.e., as often as possible, across cores, to ensure that
security tasks run with as few interruptions as possible. Our framework will
allow designers to integrate security mechanisms without perturbing existing
real-time (RT) task properties or execution order. We demonstrate the framework
using a proof-of-concept implementation with intrusion detection mechanisms as
security tasks. We develop and use both, (a) a custom intrusion detection
system (IDS), as well as (b) Tripwire -- an open source data integrity checking
tool. These are implemented on a realistic rover platform designed using an ARM
multicore chip. We compare the performance of HYDRA-C with a state-of-the-art
RT security integration approach for multicore-based RTS and find that our
method can, on average, detect intrusions 19.05% faster without impacting the
performance of RT tasks.
","['\nMonowar Hasan\n', '\nSibin Mohan\n', '\nRodolfo Pellizzoni\n', '\nRakesh B. Bobba\n']",Accepted for publication DATE 2020,,http://arxiv.org/abs/1911.11937v2,cs.OS,"['cs.OS', 'cs.CR']",,,[]
Multi-version Indexing in Flash-based Key-Value Stores,http://arxiv.org/abs/1912.00580v1,2019-12-02T05:05:39Z,2019-12-02T05:05:39Z,"  Maintaining multiple versions of data is popular in key-value stores since it
increases concurrency and improves performance. However, designing a
multi-version key-value store entails several challenges, such as additional
capacity for storing extra versions and an indexing mechanism for mapping
versions of a key to their values. We present SkimpyFTL, a FTL-integrated
multi-version key-value store that exploits the remap-on-write property of
flash-based SSDs for multi-versioning and provides a tradeoff between memory
capacity and lookup latency for indexing.
","['\nPulkit A. Misra\n', '\nJeffrey S. Chase\n', '\nJohannes Gehrke\n', '\nAlvin R. Lebeck\n']","7 pages, 6 figures",,http://arxiv.org/abs/1912.00580v1,cs.DB,"['cs.DB', 'cs.OS']",,,[]
"Intelligent Resource Scheduling for Co-located Latency-critical
  Services: A Multi-Model Collaborative Learning Approach",http://arxiv.org/abs/1911.13208v3,2019-11-26T21:05:00Z,2022-09-06T11:10:37Z,"  Latency-critical services have been widely deployed in cloud environments.
For cost-efficiency, multiple services are usually co-located on a server.
Thus, run-time resource scheduling becomes the pivot for QoS control in these
complicated co-location cases. However, the scheduling exploration space
enlarges rapidly with the increasing server resources, making the schedulers
hardly provide ideal solutions quickly. More importantly, we observe that there
are ""resource cliffs"" in the scheduling exploration space. They affect the
exploration efficiency and always lead to severe QoS fluctuations. Resource
cliffs cannot be easily avoided in previous schedulers. To address these
problems, we propose a novel ML-based intelligent scheduler - OSML. It learns
the correlation between architectural hints (e.g., IPC, cache misses, memory
footprint, etc.), scheduling solutions and the QoS demands based on a data set
we collected from 11 widely deployed services running on off-the-shelf servers.
OSML employs multiple ML models to work collaboratively to predict QoS
variations, shepherd the scheduling, and recover from QoS violations in
complicated co-location cases. OSML can intelligently avoid resource cliffs
during scheduling and reach an optimal solution much faster than previous
approaches for co-located LC services. Experimental results show that OSML
supports higher loads and meets QoS targets with lower scheduling overheads and
shorter convergence time than previous studies.
",['\nLei Liu\n'],,,http://arxiv.org/abs/1911.13208v3,cs.DC,"['cs.DC', 'cs.LG', 'cs.OS']",,,[]
Cichlid: Explicit physical memory management for large machines,http://arxiv.org/abs/1911.08367v1,2019-11-19T15:56:46Z,2019-11-19T15:56:46Z,"  In this paper, we rethink how an OS supports virtual memory. Classical VM is
an opaque abstraction of RAM, backed by demand paging. However, most systems
today (from phones to data-centers) do not page, and indeed may require the
performance benefits of non-paged physical memory, precise NUMA allocation,
etc. Moreover, MMU hardware is now useful for other purposes, such as detecting
page access or providing large page translation. Accordingly, the venerable VM
abstraction in OSes like Windows and Linux has acquired a plethora of extra
APIs to poke at the policy behind the illusion of a virtual address space.
  Instead, we present Cichlid, a memory system which inverts this model.
Applications explicitly manage their physical RAM of different types, and
directly (though safely) program the translation hardware. Cichlid is
implemented in Barrelfish, requires no virtualization support, and outperforms
VMM-based approaches for all but the smallest working sets. We show that
Cichlid enables use-cases for virtual memory not possible in Linux today, and
other use-cases are simple to program and significantly faster.
","['\nSimon Gerber\n', '\nGerd Zellweger\n', '\nReto Achermann\n', '\nMoritz Hoffmann\n', '\nKornilios Kourtis\n', '\nTimothy Roscoe\n', '\nDejan Milojicic\n']",,,http://arxiv.org/abs/1911.08367v1,cs.OS,['cs.OS'],,,[]
"CleanQ: a lightweight, uniform, formally specified interface for
  intra-machine data transfer",http://arxiv.org/abs/1911.08773v1,2019-11-20T08:57:09Z,2019-11-20T08:57:09Z,"  We present CleanQ, a high-performance operating-system interface for
descriptor-based data transfer with rigorous formal semantics, based on a
simple, formally-verified notion of ownership transfer, with a fast reference
implementation. CleanQ aims to replace the current proliferation of similar,
but subtly diverse, and loosely specified, descriptor-based interfaces in OS
kernels and device drivers. CleanQ has strict semantics that not only clarify
both the implementation of the interface for different hardware devices and
software usecases, but also enable composition of modules as in more
heavyweight frameworks like Unix streams. We motivate CleanQ by showing that
loose specifications derived from implementation lead to security and
correctness bugs in production systems that a clean, formal, and
easilyunderstandable abstraction helps eliminate. We further demonstrate by
experiment that there is negligible performance cost for a clean design: we
show overheads in the tens of cycles for operations, and comparable end-to-end
performance to the highly-tuned Virtio and DPDK implementations on Linux.
","['\nRoni Haecki\n', '\nLukas Humbel\n', '\nReto Achermann\n', '\nDavid Cock\n', '\nDaniel Schwyn\n', '\nTimothy Roscoe\n']",,,http://arxiv.org/abs/1911.08773v1,cs.OS,['cs.OS'],,,[]
Effectively Prefetching Remote Memory with Leap,http://arxiv.org/abs/1911.09829v1,2019-11-22T03:29:52Z,2019-11-22T03:29:52Z,"  Memory disaggregation over RDMA can improve the performance of
memory-constrained applications by replacing disk swapping with remote memory
accesses. However, state-of-the-art memory disaggregation solutions still use
data path components designed for slow disks. As a result, applications
experience remote memory access latency significantly higher than that of the
underlying low-latency network, which itself is too high for many applications.
  In this paper, we propose Leap, a prefetching solution for remote memory
accesses due to memory disaggregation. At its core, Leap employs an online,
majority-based prefetching algorithm, which increases the page cache hit rate.
We complement it with a lightweight and efficient data path in the kernel that
isolates each application's data path to the disaggregated memory and mitigates
latency bottlenecks arising from legacy throughput-optimizing operations.
Integration of Leap in the Linux kernel improves the median and tail remote
page access latencies of memory-bound applications by up to 104.04x and 22.62x,
respectively, over the default data path. This leads to up to 10.16x
performance improvements for applications using disaggregated memory in
comparison to the state-of-the-art solutions.
","['\nHasan Al Maruf\n', '\nMosharaf Chowdhury\n']",,"2020 USENIX Annual Technical Conference, USENIX ATC 2020, July
  15-17, pages: 843--857",http://arxiv.org/abs/1911.09829v1,cs.DC,"['cs.DC', 'cs.OS']",,,[]
Disaggregation and the Application,http://arxiv.org/abs/1910.13056v1,2019-10-29T02:56:07Z,2019-10-29T02:56:07Z,"  This paper examines disaggregated data center architectures from the
perspective of the applications that would run on these data centers, and
challenges the abstractions that have been proposed to date. In particular, we
argue that operating systems for disaggregated data centers should not abstract
disaggregated hardware resources, such as memory, compute, and storage away
from applications, but should instead give them information about, and control
over, these resources. To this end, we propose additional OS abstractions and
interfaces for disaggregation and show how they can improve data transfer in
data parallel frameworks and speed up failure recovery in replicated,
fault-tolerant applications. This paper studies the technical challenges in
providing applications with this additional functionality and advances several
preliminary proposals to overcome these challenges.
","['\nSebastian Angel\n', '\nMihir Nanavati\n', '\nSiddhartha Sen\n']",,,http://arxiv.org/abs/1910.13056v1,cs.OS,"['cs.OS', 'cs.DC']",,,[]
Debian Package usage profiler for Debian based Systems,http://arxiv.org/abs/1910.14546v1,2019-10-31T15:47:35Z,2019-10-31T15:47:35Z,"  The embedded devices of today due to their CPU, RAM capabilities can run
various Linux distributions but in most cases they are different from general
purpose distributions as they are usually lighter and specific to the needs of
that particular system. In this project, we share the problems associated in
adopting a fully heavy-weight Debian based system like Ubuntu in
embedded/automotive platforms and provide solutions to optimize them to
identify unused/redundant content in the system. This helps developer to reduce
the hefty general purpose distribution to an application specific distribution.
The solution involves collecting usage data in the system in a non-invasive
manner (to avoid any drop in performance) to suggest users the redundant,
unused parts of the system that can be safely removed without impacting the
system functionality.
","['\nBharath Honnesara Sreenivasa\n', '\nAjay Rajan\n']",,,http://arxiv.org/abs/1910.14546v1,cs.PF,"['cs.PF', 'cs.OS']",,,[]
"PiBooster: A Light-Weight Approach to Performance Improvements in Page
  Table Management for Paravirtual Virtual-Machines",http://arxiv.org/abs/1910.09277v1,2019-10-21T11:59:01Z,2019-10-21T11:59:01Z,"  In paravirtualization, the page table management components of the guest
operating systems are properly patched for the security guarantees of the
hypervisor. However, none of them pay enough attentions to the performance
improvements, which results in two noticeable performance issues. First, such
security patches exacerbate the problem that the execution paths of the guest
page table (de)allocations become extremely long, which would consequently
increase the latencies of process creations and exits. Second, the patches
introduce many additional IOTLB flushes, leading to extra IOTLB misses, and the
misses would have negative impacts on I/O performance of all peripheral
devices. In this paper, we propose PiBooster, a novel lightweight approach for
improving the performance in page table management. First, PiBooster shortens
the execution paths of the page table (de)allocations by the PiBooster cache,
which maintains dedicated buffers for serving page table (de)allocations.
Second, PiBooster eliminates the additional IOTLB misses with a fine-grained
validation scheme, which performs page table and DMA validations separately,
instead of doing both together. We implement a prototype on Xen with Linux as
the guest kernel. We do small modifications on Xen (166 SLoC) and Linux kernel
(350 SLoC). We evaluate the I/O performance in both micro and macro ways. The
micro experiment results indicate that PiBooster is able to completely
eliminate the additional IOTLB flushes in the workload-stable environments, and
effectively reduces (de)allocation time of the page table by 47% on average.
The macro benchmarks show that the latencies of the process creations and exits
are expectedly reduced by 16% on average. Moreover, the SPECINT,lmbench and
netperf results indicate that PiBooster has no negative performance impacts on
CPU computation, network I/O, and disk I/O.
","['\nZhi Zhang\n', '\nYueqiang Cheng\n']",,,http://dx.doi.org/10.1109/CLOUD.2016.0074,cs.OS,"['cs.OS', 'cs.CR', 'cs.PF']",10.1109/CLOUD.2016.0074,,[]
SEUSS: Rapid serverless deployment using environment snapshots,http://arxiv.org/abs/1910.01558v1,2019-10-03T15:48:44Z,2019-10-03T15:48:44Z,"  Modern FaaS systems perform well in the case of repeat executions when
function working sets stay small. However, these platforms are less effective
when applied to more complex, large-scale and dynamic workloads. In this paper,
we introduce SEUSS (serverless execution via unikernel snapshot stacks), a new
system-level approach for rapidly deploying serverless functions. Through our
approach, we demonstrate orders of magnitude improvements in function start
times and cacheability, which improves common re-execution paths while also
unlocking previously-unsupported large-scale bursty workloads.
","['\nJames Cadden\n', '\nThomas Unger\n', '\nYara Awad\n', '\nHan Dong\n', '\nOrran Krieger\n', '\nJonathan Appavoo\n']",,,http://arxiv.org/abs/1910.01558v1,cs.OS,['cs.OS'],,,[]
"APEX: Adaptive Ext4 File System for Enhanced Data Recoverability in Edge
  Devices",http://arxiv.org/abs/1910.01642v1,2019-10-03T09:28:35Z,2019-10-03T09:28:35Z,"  Recently Edge Computing paradigm has gained significant popularity both in
industry and academia. With its increased usage in real-life scenarios,
security, privacy and integrity of data in such environments have become
critical. Malicious deletion of mission-critical data due to ransomware,
trojans and viruses has been a huge menace and recovering such lost data is an
active field of research. As most of Edge computing devices have compute and
storage limitations, difficult constraints arise in providing an optimal scheme
for data protection. These devices mostly use Linux/Unix based operating
systems. Hence, this work focuses on extending the Ext4 file system to APEX
(Adaptive Ext4): a file system based on novel on-the-fly learning model that
provides an Adaptive Recover-ability Aware file allocation platform for
efficient post-deletion data recovery and therefore maintaining data integrity.
Our recovery model and its lightweight implementation allow significant
improvement in recover-ability of lost data with lower compute, space, time,
and cost overheads compared to other methods. We demonstrate the effectiveness
of APEX through a case study of overwriting surveillance videos by CryPy
malware on Raspberry-Pi based Edge deployment and show 678% and 32% higher
recovery than Ext4 and current state-of-the-art File Systems. We also evaluate
the overhead characteristics and experimentally show that they are lower than
other related works.
","['\nShreshth Tuli\n', '\nShikhar Tuli\n', '\nUdit Jain\n', '\nRajkumar Buyya\n']",,"Proceedings of the 11th IEEE International Conference on Cloud
  Computing, Sydney, Australia, December 11-13, 2019",http://arxiv.org/abs/1910.01642v1,cs.OS,['cs.OS'],,,[]
"Enabling Failure-resilient Intermittent Systems Without Runtime
  Checkpointing",http://arxiv.org/abs/1910.04949v1,2019-10-11T02:45:20Z,2019-10-11T02:45:20Z,"  Self-powered intermittent systems typically adopt runtime checkpointing as a
means to accumulate computation progress across power cycles and recover system
status from power failures. However, existing approaches based on the
checkpointing paradigm normally require system suspension and/or logging at
runtime. This paper presents a design which overcomes the drawbacks of
checkpointing-based approaches, to enable failure-resilient intermittent
systems. Our design allows accumulative execution and instant system recovery
under frequent power failures while enforcing the serializability of concurrent
task execution to improve computation progress and ensuring data consistency
without system suspension during runtime, by leveraging the characteristics of
data accessed in hybrid memory. We integrated the design into FreeRTOS running
on a Texas Instruments device. Experimental results show that our design can
still accumulate progress when the power source is too weak for
checkpointing-based approaches to make progress, and improves the computation
progress by up to 43% under a relatively strong power source, while reducing
the recovery time by at least 90%.
","['\nWei-Ming Chen\n', '\n Tei-Wei-Kuo\n', '\nPi-Cheng Hsiu\n']",,,http://arxiv.org/abs/1910.04949v1,cs.OS,['cs.OS'],,,[]
"Assise: Performance and Availability via NVM Colocation in a Distributed
  File System",http://arxiv.org/abs/1910.05106v2,2019-10-07T02:13:19Z,2020-06-02T02:54:13Z,"  The adoption of very low latency persistent memory modules (PMMs) upends the
long-established model of disaggregated file system access. Instead, by
colocating computation and PMM storage, we can provide applications much higher
I/O performance, sub-second application failover, and strong consistency. To
demonstrate this, we built the Assise distributed file system, based on a
persistent, replicated coherence protocol for managing a set of
server-colocated PMMs as a fast, crash-recoverable cache between applications
and slower disaggregated storage, such as SSDs. Unlike disaggregated file
systems, Assise maximizes locality for all file IO by carrying out IO on
colocated PMM whenever possible and minimizes coherence overhead by maintaining
consistency at IO operation granularity, rather than at fixed block sizes.
  We compare Assise to Ceph/Bluestore, NFS, and Octopus on a cluster with Intel
Optane DC PMMs and SSDs for common cloud applications and benchmarks, such as
LevelDB, Postfix, and FileBench. We find that Assise improves write latency up
to 22x, throughput up to 56x, fail-over time up to 103x, and scales up to 6x
better than its counterparts, while providing stronger consistency semantics.
Assise promises to beat the MinuteSort world record by 1.5x.
","['\nThomas E. Anderson\n', '\nMarco Canini\n', '\nJongyul Kim\n', '\nDejan Kostić\n', '\nYoungjin Kwon\n', '\nSimon Peter\n', '\nWaleed Reda\n', '\nHenry N. Schuh\n', '\nEmmett Witchel\n']",,,http://arxiv.org/abs/1910.05106v2,cs.DC,"['cs.DC', 'cs.OS']",,,[]
"Mitosis: Transparently Self-Replicating Page-Tables for Large-Memory
  Machines",http://arxiv.org/abs/1910.05398v2,2019-10-11T20:26:14Z,2019-11-08T06:36:11Z,"  Multi-socket machines with 1-100 TBs of physical memory are becoming
prevalent. Applications running on multi-socket machines suffer non-uniform
bandwidth and latency when accessing physical memory. Decades of research have
focused on data allocation and placement policies in NUMA settings, but there
have been no studies on the question of how to place page-tables amongst
sockets. We make the case for explicit page-table allocation policies and show
that page-table placement is becoming crucial to overall performance. We
propose Mitosis to mitigate NUMA effects on page-table walks by transparently
replicating and migrating page-tables across sockets without application
changes. This reduces the frequency of accesses to remote NUMA nodes when
performing page-table walks. Mitosis uses two components: (i) a mechanism to
enable efficient page-table replication and migration; and (ii) policies for
processes to efficiently manage and control page-table replication and
migration. We implement Mitosis in Linux and evaluate its benefits on real
hardware. Mitosis improves performance for large-scale multi-socket workloads
by up to 1.34x by replicating page-tables across sockets. Moreover, it improves
performance by up to 3.24x in cases when the OS migrates a process across
sockets by enabling cross-socket page-table migration.
","['\nReto Achermann\n', '\nAshish Panwar\n', '\nAbhishek Bhattacharjee\n', '\nTimothy Roscoe\n', '\nJayneel Gandhi\n']",,,http://arxiv.org/abs/1910.05398v2,cs.OS,"['cs.OS', 'cs.AR', 'cs.PF']",,,[]
SIVSHM: Secure Inter-VM Shared Memory,http://arxiv.org/abs/1909.10377v1,2019-09-23T14:10:16Z,2019-09-23T14:10:16Z,"  With wide spread acceptance of virtualization, virtual machines (VMs) find
their presence in various applications such as Network Address Translation
(NAT) servers, firewall servers and MapReduce applications. Typically, in these
applications a data manager collects data from the external world and
distributes it to multiple workers for further processing. Currently, data
managers distribute data with workers either using inter-VM shared memory
(IVSHMEM) or network communication. IVSHMEM provides better data distribution
throughput sacrificing security as all untrusted workers have full access to
the shared memory region and network communication provides better security at
the cost of throughput. Secondly, IVSHMEM uses a central distributor to
exchange eventfd - a file descriptor to an event queue of length one, which is
used for inter-VM signaling. This central distributor becomes a bottleneck and
increases boot time of VMs. Secure Inter-VM Shared Memory (SIVSHM) provided
both security and better throughout by segmenting inter-VM shared memory, so
that each worker has access to segment that belong only to it, thereby enabling
security without sacrificing throughput. SIVSHM boots VMs in 30% less time
compared to IVSHMEM by eliminating central distributor from its architecture
and enabling direct exchange of eventfds amongst VMs.
","['\nShesha Sreenivasamurthy\n', '\nEthan Miller\n']",,,http://arxiv.org/abs/1909.10377v1,cs.OS,['cs.OS'],,,[]
An Improvement Over Threads Communications on Multi-Core Processors,http://arxiv.org/abs/1909.11644v2,2019-09-25T17:44:55Z,2019-10-01T18:15:48Z,"  Multicore is an integrated circuit chip that uses two or more computational
engines (cores) places in a single processor. This new approach is used to
split the computational work of a threaded application and spread it over
multiple execution cores, so that the computer system can benefits from a
better performance and better responsiveness of the system. A thread is a unit
of execution inside a process that is created and maintained to execute a set
of actions/ instructions. Threads can be implemented differently from an
operating system to another, but the operating system is in most cases
responsible to schedule the execution of different threads. Multi-threading
improving efficiency of processor performance with a cost-effective memory
system. In this paper, we explore one approach to improve communications for
multithreaded. Pre-send is a software Controlled data forwarding technique that
sends data to destination's cache before it is needed, eliminating cache misses
in the destination's cache as well as reducing the coherence traffic on the
bus. we show how we could improve the overall system performance by addition of
these architecture optimizations to multi-core processors.
","['\nReza Fotohi\n', '\nMehdi Effatparvar\n', '\nFateme Sarkohaki\n', '\nShahram Behzad\n', '\nJaber Hoseini balov\n']","This submission has been withdrawn by arXiv administrators due to
  inappropriate text reuse from external sources","2012, Volume 6, Issue 12, pp 379-384",http://arxiv.org/abs/1909.11644v2,cs.OS,['cs.OS'],,,[]
"SplitFS: Reducing Software Overhead in File Systems for Persistent
  Memory",http://arxiv.org/abs/1909.10123v1,2019-09-23T02:03:13Z,2019-09-23T02:03:13Z,"  We present SplitFS, a file system for persistent memory (PM) that reduces
software overhead significantly compared to state-of-the-art PM file systems.
SplitFS presents a novel split of responsibilities between a user-space library
file system and an existing kernel PM file system. The user-space library file
system handles data operations by intercepting POSIX calls, memory-mapping the
underlying file, and serving the read and overwrites using processor loads and
stores. Metadata operations are handled by the kernel PM file system (ext4
DAX). SplitFS introduces a new primitive termed relink to efficiently support
file appends and atomic data operations. SplitFS provides three consistency
modes, which different applications can choose from, without interfering with
each other. SplitFS reduces software overhead by up-to 4x compared to the NOVA
PM file system, and 17x compared to ext4-DAX. On a number of micro-benchmarks
and applications such as the LevelDB key-value store running the YCSB
benchmark, SplitFS increases application performance by up to 2x compared to
ext4 DAX and NOVA while providing similar consistency guarantees.
","['\nRohan Kadekodi\n', '\nSe Kwon Lee\n', '\nSanidhya Kashyap\n', '\nTaesoo Kim\n', '\nAasheesh Kolli\n', '\nVijay Chidambaram\n']",,,http://dx.doi.org/10.1145/3341301.3359631,cs.OS,"['cs.OS', 'cs.PF']",10.1145/3341301.3359631,,[]
Cache Where you Want! Reconciling Predictability and Coherent Caching,http://arxiv.org/abs/1909.05349v2,2019-09-11T20:47:58Z,2021-06-27T23:34:25Z,"  Real-time and cyber-physical systems need to interact with and respond to
their physical environment in a predictable time. While multicore platforms
provide incredible computational power and throughput, they also introduce new
sources of unpredictability. Large fluctuations in latency to access data
shared between multiple cores is an important contributor to the overall
execution-time variability. In addition to the temporal unpredictability
introduced by caching, parallel applications with data shared across multiple
cores also pay additional latency overheads due to data coherence. Analyzing
the impact of data coherence on the worst-case execution-time of real-time
applications is challenging because only scarce implementation details are
revealed by manufacturers. This paper presents application level control for
caching data at different levels of the cache hierarchy. The rationale is that
by caching data only in shared cache it is possible to bypass private caches.
The access latency to data present in caches becomes independent of its
coherence state. We discuss the existing architectural support as well as the
required hardware and OS modifications to support the proposed cacheability
control. We evaluate the system on an architectural simulator. We show that the
worst case execution time for a single memory write request is reduced by 52%.
Benchmark evaluations show that proposed technique has a minimal impact on
average performance.
","['\nAyoosh Bansal\n', '\nJayati Singh\n', '\nYifan Hao\n', '\nJen-Yang Wen\n', '\nRenato Mancuso\n', '\nMarco Caccamo\n']","13 pages, 10 figures, v2 update includes overview section with formal
  solution definition. This is a long version of a prior publication","2020 9th Mediterranean Conference on Embedded Computing (MECO),
  2020, pp. 1-6",http://dx.doi.org/10.1109/MECO49872.2020.9134262,cs.DC,"['cs.DC', 'cs.OS', 'C.0; C.3; C.4; D.4.7; J.7']",10.1109/MECO49872.2020.9134262,,[]
Data Centers Job Scheduling with Deep Reinforcement Learning,http://arxiv.org/abs/1909.07820v2,2019-09-16T02:09:58Z,2020-03-01T20:05:26Z,"  Efficient job scheduling on data centers under heterogeneous complexity is
crucial but challenging since it involves the allocation of multi-dimensional
resources over time and space. To adapt the complex computing environment in
data centers, we proposed an innovative Advantage Actor-Critic (A2C) deep
reinforcement learning based approach called A2cScheduler for job scheduling.
A2cScheduler consists of two agents, one of which, dubbed the actor, is
responsible for learning the scheduling policy automatically and the other one,
the critic, reduces the estimation error. Unlike previous policy gradient
approaches, A2cScheduler is designed to reduce the gradient estimation variance
and to update parameters efficiently. We show that the A2cScheduler can achieve
competitive scheduling performance using both simulated workloads and real data
collected from an academic data center.
","['\nSisheng Liang\n', '\nZhou Yang\n', '\nFang Jin\n', '\nYong Chen\n']",13 pages,,http://arxiv.org/abs/1909.07820v2,cs.OS,"['cs.OS', 'cs.LG', 'cs.SY', 'eess.SY']",,,[]
Porting of eChronos RTOS on RISC-V Architecture,http://arxiv.org/abs/1908.11648v3,2019-08-30T10:53:00Z,2019-12-26T05:58:49Z,"  eChronos is a formally verified Real Time Operating System(RTOS) designed for
embedded micro-controllers. eChronos was targeted for tightly constrained
devices without memory management units. Currently, eChronos is available on
proprietary designs like ARM, PowerPC and Intel architectures. eChronos is
adopted in safety critical systems like aircraft control system and medical
implant devices. eChronos is one of the very few system software not been
ported to RISC-V. RISC-V is an open-source Instruction Set Architecture (ISA)
that enables new era of processor development. Many standard Operating Systems,
software tool chain have migrated to the RISC-V architecture. According to the
latest trends, RISC-V is replacing many proprietary chips. As a secure RTOS, it
is attractive to port on an open-source ISA. SHAKTI and PicoRV32 are some of
the proven open-source RISC-V designs available. Now having a secure RTOS on an
open-source hardware design, designed based on an open-source ISA makes it more
interesting. In addition to this, the current architectures supported by
eChronos are all proprietary designs, and porting eChronos to the RISC-V
architecture increases the secure system development as a whole. This paper,
presents an idea of porting eChronos on a chip which is open-source and
effective, thus reducing the cost of embedded systems. Designing a open-source
system that is completely open-source reduces the overall cost, increased the
security and can be critically reviewed. This paper explores the design and
architecture aspect involved in porting eChronos to RISC-V. The authors have
successfully ported eChronos to RISC-V architecture and verified it on spike.
The port of RISC-V to eChronos is made available open-source by authors. Along
with that, the safe removal of architectural dependencies and subsequent
changes in eChronos are also analyzed.
","['\nShubhendra Pal Singhal\n', '\nM. Sridevi\n', '\nN Sathya Narayanan\n', '\nM J Shankar Raman\n']","11 pages, 3 figures, Accepted for Publication for Springer LNCS
  Germany",,http://arxiv.org/abs/1908.11648v3,cs.OS,['cs.OS'],,,[]
"CrowdOS: A Ubiquitous Operating System for Crowdsourcing and Mobile
  Crowd Sensing",http://arxiv.org/abs/1909.00805v1,2019-09-02T17:28:05Z,2019-09-02T17:28:05Z,"  With the rise of crowdsourcing and mobile crowdsensing techniques, a large
number of crowdsourcing applications or platforms (CAP) have appeared. In the
mean time, CAP-related models and frameworks based on different research
hypotheses are rapidly emerging, and they usually address specific issues from
a certain perspective. Due to different settings and conditions, different
models are not compatible with each other. However, CAP urgently needs to
combine these techniques to form a unified framework. In addition, these models
needs to be learned and updated online with the extension of crowdsourced data
and task types, thus requiring a unified architecture that integrates lifelong
learning concepts and breaks down the barriers between different modules. This
paper draws on the idea of ubiquitous operating systems and proposes a novel OS
(CrowdOS), which is an abstract software layer running between native OS and
application layer. In particular, based on an in-depth analysis of the complex
crowd environment and diverse characteristics of heterogeneous tasks, we
construct the OS kernel and three core frameworks including Task Resolution and
Assignment Framework (TRAF), Integrated Resource Management (IRM), and Task
Result quality Optimization (TRO). In addition, we validate the usability of
CrowdOS, module correctness and development efficiency. Our evaluation further
reveals TRO brings enormous improvement in efficiency and a reduction in energy
consumption.
","['\nYimeng Liu\n', '\nZhiwen Yu\n', '\nBin Guo\n', '\nQi Han\n', '\nJiangbin Su\n', '\nJiahao Liao\n']",,"IEEE Transactions on Mobile Computing, vol. 21, no. 3, pp.
  878-894, 1 March 2022",http://dx.doi.org/10.1109/TMC.2020.3015750,cs.CY,"['cs.CY', 'cs.OS', 'cs.SE']",10.1109/TMC.2020.3015750,,[]
Mapping Spiking Neural Networks to Neuromorphic Hardware,http://arxiv.org/abs/1909.01843v1,2019-09-04T14:39:47Z,2019-09-04T14:39:47Z,"  Neuromorphic hardware platforms implement biological neurons and synapses to
execute spiking neural networks (SNNs) in an energy-efficient manner. We
present SpiNeMap, a design methodology to map SNNs to crossbar-based
neuromorphic hardware, minimizing spike latency and energy consumption.
SpiNeMap operates in two steps: SpiNeCluster and SpiNePlacer. SpiNeCluster is a
heuristic-based clustering technique to partition SNNs into clusters of
synapses, where intracluster local synapses are mapped within crossbars of the
hardware and inter-cluster global synapses are mapped to the shared
interconnect. SpiNeCluster minimizes the number of spikes on global synapses,
which reduces spike congestion on the shared interconnect, improving
application performance. SpiNePlacer then finds the best placement of local and
global synapses on the hardware using a meta-heuristic-based approach to
minimize energy consumption and spike latency. We evaluate SpiNeMap using
synthetic and realistic SNNs on the DynapSE neuromorphic hardware. We show that
SpiNeMap reduces average energy consumption by 45% and average spike latency by
21%, compared to state-of-the-art techniques.
","['\nAdarsha Balaji\n', '\nAnup Das\n', '\nYuefeng Wu\n', '\nKhanh Huynh\n', ""\nFrancesco Dell'Anna\n"", '\nGiacomo Indiveri\n', '\nJeffrey L. Krichmar\n', '\nNikil Dutt\n', '\nSiebren Schaafsma\n', '\nFrancky Catthoor\n']","14 pages, 14 images, 69 references, Accepted in IEEE Transactions on
  Very Large Scale Integration (VLSI) Systems",,http://arxiv.org/abs/1909.01843v1,cs.ET,"['cs.ET', 'cs.LG', 'cs.OS']",,,[]
Boomerang: Real-Time I/O Meets Legacy Systems,http://arxiv.org/abs/1908.06807v2,2019-08-19T13:55:44Z,2020-03-23T20:52:02Z,"  This paper presents Boomerang, an I/O system that integrates a legacy
non-real-time OS with one that is customized for timing-sensitive tasks. A
relatively small RTOS benefits from the pre-existing libraries, drivers and
services of the legacy system. Additionally, timing-critical tasks are isolated
from less critical tasks by securely partitioning machine resources among the
separate OSes. Boomerang guarantees end-to-end processing delays on input data
that requires outputs to be generated within specific time bounds.
  We show how to construct composable task pipelines in Boomerang that combine
functionality spanning a custom RTOS and a legacy Linux system. By dedicating
time-critical I/O to the RTOS, we ensure that complementary services provided
by Linux are sufficiently predictable to meet end-to-end service guarantees.
While Boomerang benefits from spatial isolation, it also outperforms a
standalone Linux system using deadline-based CPU reservations for pipeline
tasks. We also show how Boomerang outperforms a virtualized system called ACRN,
designed for automotive systems.
","['\nAhmad Golchin\n', '\nSoham Sinha\n', '\nRichard West\n']","This paper is now accepted for publication in Proceedings of the 26th
  IEEE Real-Time and Embedded Technology and Applications Symposium (RTAS),
  Sydney, Australia, April 21-24, 2020",,http://arxiv.org/abs/1908.06807v2,cs.OS,"['cs.OS', 'D.4.0; D.4.7; D.4.4']",,,[]
A Least-Privilege Memory Protection Model for Modern Hardware,http://arxiv.org/abs/1908.08707v1,2019-08-23T08:00:38Z,2019-08-23T08:00:38Z,"  We present a new least-privilege-based model of addressing on which to base
memory management functionality in an OS for modern computers like phones or
server-based accelerators. Existing software assumptions do not account for
heterogeneous cores with different views of the address space, leading to the
related problems of numerous security bugs in memory management code (for
example programming IOMMUs), and an inability of mainstream OSes to securely
manage the complete set of hardware resources on, say, a phone System-on-Chip.
  Our new work is based on a recent formal model of address translation
hardware which views the machine as a configurable network of address spaces.
We refine this to capture existing address translation hardware from modern
SoCs and accelerators at a sufficiently fine granularity to model minimal
rights both to access memory and configure translation hardware. We then build
an executable specification in Haskell, which expresses the model and metadata
structures in terms of partitioned capabilities. Finally, we show a fully
functional implementation of the model in C created by extending the capability
system of the Barrelfish research OS.
  Our evaluation shows that our unoptimized implementation has comparable (and
in some cases) better performance than the Linux virtual memory system, despite
both capturing all the functionality of modern hardware addressing and enabling
least-privilege, decentralized authority to access physical memory and devices.
","['\nReto Achermann\n', '\nNora Hossle\n', '\nLukas Humbel\n', '\nDaniel Schwyn\n', '\nDavid Cock\n', '\nTimothy Roscoe\n']",,,http://arxiv.org/abs/1908.08707v1,cs.OS,['cs.OS'],,,[]
"Kernel/User-level Collaborative Persistent Memory File System with
  Efficiency and Protection",http://arxiv.org/abs/1908.10740v1,2019-08-28T14:15:21Z,2019-08-28T14:15:21Z,"  Emerging high performance non-volatile memories recall the importance of
efficient file system design. To avoid the virtual file system (VFS) and
syscall overhead as in these kernel-based file systems, recent works deploy
file systems directly in user level. Unfortunately, a userlevel file system can
easily be corrupted by a buggy program with misused pointers, and is hard to
scale on multi-core platforms which incorporates a centralized coordination
service. In this paper, we propose KucoFS, a Kernel and user-level
collaborative file system. It consists of two parts: a user-level library with
direct-access interfaces, and a kernel thread, which performs metadata updates
and enforces write protection by toggling the permission bits in the page
table. Hence, KucoFS achieves both direct-access of user-level designs and
fine-grained write protection of kernel-level ones. We further explore its
scalability to multicores: For metadata scalability, KucoFS rebalances the
pathname resolution overhead between the kernel and userspace, by adopting the
index offloading technique. For data access efficiency, it coordinates the data
allocation between kernel and userspace, and uses range-lock write and
lock-free read to improve concurrency. Experiments on Optane DC persistent
memory show that KucoFS significantly outperforms existing file systems and
shows better scalability.
","['\nYoumin Chen\n', '\nYouyou Lu\n', '\nBohong Zhu\n', '\nJiwu Shu\n']",,,http://arxiv.org/abs/1908.10740v1,cs.OS,['cs.OS'],,,[]
MicroTEE: Designing TEE OS Based on the Microkernel Architecture,http://arxiv.org/abs/1908.07159v1,2019-08-20T04:22:58Z,2019-08-20T04:22:58Z,"  ARM TrustZone technology is widely used to provide Trusted Execution
Environments (TEE) for mobile devices. However, most TEE OSes are implemented
as monolithic kernels. In such designs, device drivers, kernel services and
kernel modules all run in the kernel, which results in large size of the
kernel. It is difficult to guarantee that all components of the kernel have no
security vulnerabilities in the monolithic kernel architecture, such as the
integer overflow vulnerability in Qualcomm QSEE TrustZone and the TZDriver
vulnerability in HUAWEI Hisilicon TEE architecture. This paper presents
MicroTEE, a TEE OS based on the microkernel architecture. In MicroTEE, the
microkernel provides strong isolation for TEE OS's basic services, such as
crypto service and platform key management service. The kernel is only
responsible for providing core services such as address space management,
thread management, and inter-process communication. Other fundamental services,
such as crypto service and platform key management service are implemented as
applications at the user layer. Crypto Services and Key Management are used to
provide Trusted Applications (TAs) with sensitive information encryption, data
signing, and platform attestation functions. Our design avoids the compromise
of the whole TEE OS if only one kernel service is vulnerable. A monitor has
also been added to perform the switch between the secure world and the normal
world. Finally, we implemented a MicroTEE prototype on the Freescale i.MX6Q
Sabre Lite development board and tested its performance. Evaluation results
show that the performance of cryptographic operations in MicroTEE is better
than it in Linux when the size of data is small.
","['\nDongxu Ji\n', '\nQianying Zhang\n', '\nShijun Zhao\n', '\nZhiping Shi\n', '\nYong Guan\n']","8 pages, 8 figures",,http://dx.doi.org/10.1109/TrustCom/BigDataSE.2019.00014,cs.CR,"['cs.CR', 'cs.OS']",10.1109/TrustCom/BigDataSE.2019.00014,,[]
Tvarak: Software-managed hardware offload for DAX NVM storage redundancy,http://arxiv.org/abs/1908.09922v1,2019-08-26T21:04:37Z,2019-08-26T21:04:37Z,"  Tvarak efficiently implements system-level redundancy for direct-access (DAX)
NVM storage. Production storage systems complement device-level ECC (which
covers media errors) with system-checksums and cross-device parity. This
system-level redundancy enables detection of and recovery from data corruption
due to device firmware bugs (e.g., reading data from the wrong physical
location). Direct access to NVM penalizes software-only implementations of
system-level redundancy, forcing a choice between lack of data protection or
significant performance penalties. Offloading the update and verification of
system-level redundancy to Tvarak, a hardware controller co-located with the
last-level cache, enables efficient protection of data from such bugs in memory
controller and NVM DIMM firmware. Simulation-based evaluation with seven
data-intensive applications shows Tvarak's performance and energy efficiency.
For example, Tvarak reduces Redis set-only performance by only 3%, compared to
50% reduction for a state-of-the-art software-only approach.
","['\nRajat Kateja\n', '\nNathan Beckmann\n', '\nGregory R. Ganger\n']",,,http://arxiv.org/abs/1908.09922v1,cs.AR,"['cs.AR', 'cs.OS']",,,[]
PAStime: Progress-aware Scheduling for Time-critical Computing,http://arxiv.org/abs/1908.06211v2,2019-08-17T00:24:07Z,2021-05-31T15:53:06Z,"  Over-estimation of worst-case execution times (WCETs) of real-time tasks
leads to poor resource utilization. In a mixed-criticality system (MCS), the
over-provisioning of CPU time to accommodate the WCETs of highly critical tasks
may lead to degraded service for less critical tasks. In this paper, we present
PAStime, a novel approach to monitor and adapt the runtime progress of highly
time-critical applications, to allow for improved service to lower criticality
tasks. In PAStime, CPU time is allocated to time-critical tasks according to
the delays they experience as they progress through their control flow graphs.
This ensures that as much time as possible is made available to improve the
Quality-of-Service of less critical tasks, while high-criticality tasks are
compensated after their delays.
  In this paper, we integrate PAStime with Adaptive Mixed-criticality (AMC)
scheduling. The LO-mode budget of a high-criticality task is adjusted according
to the delay observed at execution checkpoints. This is the first
implementation of AMC in the scheduling framework Using LITMUS-RT, which is
extended with our PAStime runtime policy and tested with real-time Linux
applications such as object classification and detection. We observe in our
experimental evaluation that AMC-PAStime significantly improves the utilization
of the low-criticality tasks while guaranteeing service to high-criticality
tasks.
","['\nSoham Sinha\n', '\nRichard West\n', '\nAhmad Golchin\n']",24 pages,,http://dx.doi.org/10.4230/LIPIcs.ECRTS.2020.3,cs.OS,"['cs.OS', 'C.3; D.4.7; D.4.1']",10.4230/LIPIcs.ECRTS.2020.3,,[]
"Good Motive but Bad Design: Why ARM MPU Has Become an Outcast in
  Embedded Systems",http://arxiv.org/abs/1908.03638v1,2019-08-09T21:33:33Z,2019-08-09T21:33:33Z,"  As more and more embedded devices are connected to the Internet, leading to
the emergence of Internet-of-Things (IoT), previously less tested (and
insecure) devices are exposed to miscreants. To prevent them from being
compromised, the memory protection unit (MPU), which is readily available on
many devices, has the potential to become a free lunch for the defenders. To
our surprise, the MPU is seldom used by real-world products. The reasons are
multi-fold. While there are non-technical reasons such as compatibility issues,
more importantly, we found that MPU brings virtually no security enhancement at
the expense of decreased performance and responsiveness. In this work, we
investigate the MPU adoption in major real-time operating systems (RTOSs), in
particular, the FreeRTOS, and try to pinpoint the fundamental reasons to
explain why MPU is not favored. We hope our findings can inspire new remedial
solutions to change the situation. We also review the latest MPU design and
provide technical suggestions to build more secure embedded systems.
","['\nWei Zhou\n', '\nLe Guan\n', '\nPeng Liu\n', '\nYuqing Zhang\n']",,,http://arxiv.org/abs/1908.03638v1,cs.CR,"['cs.CR', 'cs.OS']",,,[]
"EnclaveDom: Privilege Separation for Large-TCB Applications in Trusted
  Execution Environments",http://arxiv.org/abs/1907.13245v2,2019-07-30T22:07:59Z,2020-06-18T01:55:53Z,"  Trusted executions environments (TEEs) such as Intel(R) SGX provide
hardware-isolated execution areas in memory, called enclaves. By running only
the most trusted application components in the enclave, TEEs enable developers
to minimize the TCB of their applications thereby helping to protect sensitive
application data. However, porting existing applications to TEEs often requires
considerable refactoring efforts, as TEEs provide a restricted interface to
standard OS features. To ease development efforts, TEE application developers
often choose to run their unmodified application in a library OS container that
provides a full in-enclave OS interface. Yet, this large-TCB development
approach now leaves sensitive in-enclave data exposed to potential bugs or
vulnerabilities in third-party code imported into the application. Importantly,
because the TEE libOS and the application run in the same enclave address
space, even the libOS management data structures (e.g. file descriptor table)
may be vulnerable to attack, where in traditional OSes these data structures
may be protected via privilege isolation.
  We present EnclaveDom, a privilege separation system for large-TCB TEE
applications that partitions an enclave into tagged memory regions, and
enforces per-region access rules at the granularity of individual in-enclave
functions. EnclaveDom is implemented on Intel SGX using Memory Protection Keys
(MPK) for memory tagging. To evaluate the security and performance impact of
EnclaveDom, we integrated EnclaveDom with the Graphene-SGX library OS. While no
product or component can be absolutely secure, our prototype helps protect
internal libOS management data structures against tampering by
application-level code. At every libOS system call, EnclaveDom then only grants
access to those internal data structures which the syscall needs to perform its
task.
","['\nMarcela S. Melara\n', '\nMichael J. Freedman\n', '\nMic Bowman\n']",,,http://arxiv.org/abs/1907.13245v2,cs.CR,"['cs.CR', 'cs.OS']",,,[]
An Optimized Disk Scheduling Algorithm With Bad-Sector Management,http://arxiv.org/abs/1908.01167v1,2019-08-03T13:16:51Z,2019-08-03T13:16:51Z,"  In high performance computing, researchers try to optimize the CPU Scheduling
algorithms, for faster and efficient working of computers. But a process needs
both CPU bound and I/O bound for completion of its execution. With
modernization of computers the speed of processor, hard-disk, and I/O devices
increases gradually. Still the data access speed of hard-disk is much less than
the speed of the processor. So when processor receives a data from secondary
memory it executes immediately and again it have to wait for receiving another
data. So the slowness of the hard-disk becomes a bottleneck in the performance
of processor. Researchers try to develop and optimize the traditional disk
scheduling algorithms for faster data transfer to and from secondary data
storage devices. In this paper we try to evolve an optimized scheduling
algorithm by reducing the seek time, the rotational latency, and the data
transfer time in runtime. This algorithm has the feature to manage the
bad-sectors of the hard-disk. It also attempts to reduce power consumption and
heat reduction by minimizing bad sector reading time.
","['\nAmar Ranjan Dash\n', '\nSandipta Kumar Sahu\n', '\nB Kewal\n']","21 pages, 21 figures, 3 table, International Journal of Computer
  Science, Engineering and Applications (IJCSEA)","International Journal of Computer Science, Engineering and
  Applications (IJCSEA), AIRCC, 2019, Vol. 9(3), pp 1-21, DOI
  :10.5012/ijcsea.2019.9301",http://dx.doi.org/10.5012/ijcsea.2019.9301,cs.OS,"['cs.OS', 'cs.DS']",10.5012/ijcsea.2019.9301,,[]
"The Preliminary Evaluation of a Hypervisor-based Virtualization
  Mechanism for Intel Optane DC Persistent Memory Module",http://arxiv.org/abs/1907.12014v1,2019-07-28T05:06:24Z,2019-07-28T05:06:24Z,"  Non-volatile memory (NVM) technologies, being accessible in the same manner
as DRAM, are considered indispensable for expanding main memory capacities.
Intel Optane DCPMM is a long-awaited product that drastically increases main
memory capacities. However, a substantial performance gap exists between DRAM
and DCPMM. In our experiments, the read/write latencies of DCPMM were 400% and
407% higher than those of DRAM, respectively. The read/write bandwidths were
37% and 8% of those of DRAM. This performance gap in main memory presents a new
challenge to researchers; we need a new system software technology supporting
emerging hybrid memory architecture. In this paper, we present RAMinate, a
hypervisor-based virtualization mechanism for hybrid memory systems, and a key
technology to address the performance gap in main memory systems. It provides
great flexibility in memory management and maximizes the performance of virtual
machines (VMs) by dynamically optimizing memory mappings. Through experiments,
we confirmed that even though a VM has only 1% of DRAM in its RAM, the
performance degradation of the VM was drastically alleviated by memory mapping
optimization. The elapsed time to finish the build of Linux Kernel in the VM
was 557 seconds, which was only 13% increase from the 100% DRAM case (i.e., 495
seconds). When the optimization mechanism was disabled, the elapsed time
increased to 624 seconds (i.e. 26% increase from the 100% DRAM case).
","['\nTakahiro Hirofuchi\n', '\nRyousei Takano\n']",,,http://arxiv.org/abs/1907.12014v1,cs.OS,"['cs.OS', 'cs.AR', 'cs.PF', 'D.4; B.3']",,,[]
Keystone: An Open Framework for Architecting TEEs,http://arxiv.org/abs/1907.10119v2,2019-07-23T20:24:19Z,2019-09-07T05:54:07Z,"  Trusted execution environments (TEEs) are being used in all the devices from
embedded sensors to cloud servers and encompass a range of cost, power
constraints, and security threat model choices. On the other hand, each of the
current vendor-specific TEEs makes a fixed set of trade-offs with little room
for customization. We present Keystone -- the first open-source framework for
building customized TEEs. Keystone uses simple abstractions provided by the
hardware such as memory isolation and a programmable layer underneath untrusted
components (e.g., OS). We build reusable TEE core primitives from these
abstractions while allowing platform-specific modifications and application
features. We showcase how Keystone-based TEEs run on unmodified RISC-V hardware
and demonstrate the strengths of our design in terms of security, TCB size,
execution of a range of benchmarks, applications, kernels, and deployment
models.
","['\nDayeol Lee\n', '\nDavid Kohlbrenner\n', '\nShweta Shinde\n', '\nDawn Song\n', '\nKrste Asanović\n']",18 pages,,http://arxiv.org/abs/1907.10119v2,cs.CR,"['cs.CR', 'cs.OS']",,,[]
Reproducible Execution of POSIX Programs with DiOS,http://arxiv.org/abs/1907.03356v1,2019-07-07T22:26:02Z,2019-07-07T22:26:02Z,"  In this paper, we describe DiOS, a lightweight model operating system which
can be used to execute programs that make use of POSIX APIs. Such executions
are fully reproducible: running the same program with the same inputs twice
will result in two exactly identical instruction traces, even if the program
uses threads for parallelism.
  DiOS is implemented almost entirely in portable C and C++: although its
primary platform is DiVM, a verification-oriented virtual machine, it can be
configured to also run in KLEE, a symbolic executor. Finally, it can be
compiled into machine code to serve as a user-mode kernel.
  Additionally, DiOS is modular and extensible. Its various components can be
combined to match both the capabilities of the underlying platform and to
provide services required by a particular program. New components can be added
to cover additional system calls or APIs.
  The experimental evaluation has two parts. DiOS is first evaluated as a
component of a program verification platform based on DiVM. In the second part,
we consider its portability and modularity by combining it with the symbolic
executor KLEE.
","['\nPetr Ročkai\n', '\nZuzana Baranová\n', '\nJan Mrázek\n', '\nKatarína Kejstová\n', '\nJiří Barnat\n']",,,http://arxiv.org/abs/1907.03356v1,cs.OS,['cs.OS'],,,[]
Executable formal semantics for the POSIX shell,http://arxiv.org/abs/1907.05308v1,2019-07-11T15:33:34Z,2019-07-11T15:33:34Z,"  The POSIX shell is a widely deployed, powerful tool for managing computer
systems. The shell is the expert's control panel, a necessary tool for
configuring, compiling, installing, maintaining, and deploying systems. Even
though it is powerful, critical infrastructure, the POSIX shell is maligned and
misunderstood. Its power and its subtlety are a dangerous combination.
  We define a formal, mechanized, executable small-step semantics for the POSIX
shell, which we call Smoosh. We compared Smoosh against seven other shells that
aim for some measure of POSIX compliance (bash, dash, zsh, OSH, mksh, ksh93,
and yash). Using three test suites---the POSIX test suite, the Modernish test
suite and shell diagnosis, and a test suite of our own device---we found
Smoosh's semantics to be the most conformant to the POSIX standard. Modernish
judges Smoosh to have the fewest bugs (just one, from using dash's parser) and
no quirks. To show that our semantics is useful beyond yielding a conformant,
executable shell, we also implemented a symbolic stepper to illuminate the
subtle behavior of the shell.
  Smoosh will serve as a foundation for formal study of the POSIX shell,
supporting research on and development of new shells, new tooling for shells,
and new shell designs.
","['\nMichael Greenberg\n', '\nAustin J. Blatt\n']",,,http://arxiv.org/abs/1907.05308v1,cs.PL,"['cs.PL', 'cs.OS']",,,[]
HTS: A Hardware Task Scheduler for Heterogeneous Systems,http://arxiv.org/abs/1907.00271v1,2019-06-29T19:59:21Z,2019-06-29T19:59:21Z,"  As the Moore's scaling era comes to an end, application specific hardware
accelerators appear as an attractive way to improve the performance and power
efficiency of our computing systems. A massively heterogeneous system with a
large number of hardware accelerators along with multiple general purpose CPUs
is a promising direction, but pose several challenges in terms of the run-time
scheduling of tasks on the accelerators and design granularity of accelerators.
This paper addresses these challenges by developing an example heterogeneous
system to enable multiple applications to share the available accelerators. We
propose to design accelerators at a lower abstraction to enable applications to
be broken down into tasks that can be mapped on several accelerators. We
observe that several real-life workloads can be broken down into common
primitives that are shared across many workloads. Finally, we propose and
design a hardware task scheduler inspired by the hardware schedulers in
out-of-order superscalar processors to efficiently utilize the accelerators in
the system by scheduling tasks in out-of-order and even speculatively. We
evaluate the proposed system on both real-life and synthetic benchmarks based
on Digital Signal Processing~(DSP) applications. Compared to executing the
benchmark on a system with sequential scheduling, proposed scheduler achieves
up to 12x improvement in performance.
","['\nKartik Hegde\n', '\nAbhishek Srivastava\n', '\nRohit Agrawal\n']",,,http://arxiv.org/abs/1907.00271v1,cs.OS,"['cs.OS', 'cs.AR']",,,[]
"Lawn: an Unbound Low Latency Timer Data Structure for Large Scale, High
  Throughput Systems",http://arxiv.org/abs/1906.10860v2,2019-06-26T06:16:13Z,2019-07-15T09:50:24Z,"  As demand for Real-Time applications rises among the general public, the
importance of enabling large-scale, unbound algorithms to solve conventional
problems with low to no latency is critical for product viability. Timer
algorithms are prevalent in the core mechanisms behind operating systems,
network protocol implementation, stream processing, and several database
capabilities. This paper presents a field-tested algorithm for low latency,
unbound range timer structure, based upon the well excepted Timing Wheel
algorithm. Using a set of queues hashed by TTL, the algorithm allows for a
simpler implementation, minimal overhead no overflow and no performance
degradation in comparison to the current state of the algorithms under typical
use cases.
",['\nAdam Lev-Libfeld\n'],"6 pages, 2 figures, 4 tables",,http://arxiv.org/abs/1906.10860v2,cs.DS,"['cs.DS', 'cs.DC', 'cs.NI', 'cs.OS']",,,[]
Accelerator-level Parallelism,http://arxiv.org/abs/1907.02064v5,2019-07-02T21:04:47Z,2021-11-24T12:51:43Z,"  Future applications demand more performance, but technology advances have
been faltering. A promising approach to further improve computer system
performance under energy constraints is to employ hardware accelerators.
Already today, mobile systems concurrently employ multiple accelerators in what
we call accelerator-level parallelism (ALP). To spread the benefits of ALP more
broadly, we charge computer scientists to develop the science needed to best
achieve the performance and cost goals of ALP hardware and software.
","['\nMark D. Hill\n', '\nVijay Janapa Reddi\n']","6 pages, 3 figures, & 7 references",,http://arxiv.org/abs/1907.02064v5,cs.DC,"['cs.DC', 'cs.OS', 'cs.PF', 'cs.PL']",,,[]
Slicing the IO execution with ReLayTracer,http://arxiv.org/abs/1906.07124v1,2019-06-17T16:49:56Z,2019-06-17T16:49:56Z,"  Analyzing IO performance anomalies is a crucial task in various computing
environments, ranging from large-scale cloud applications to desktop
applications. However, the IO stack of modern operating systems is complicated,
making it hard to understand the performance anomalies with existing tools.
Kernel IO executions are frequently interrupted by internal kernel activities,
requiring a sophisticated IO profile tool to deal with the noises. Furthermore,
complicated interactions of concurrent IO requests cause different sources of
tail latencies in kernel IO stack. As a consequence, developers want to know
fine-grained latency profile across IO layers, which may differ in each IO
requests. To meet the requirements, this paper suggests ReLayTracer, a
per-request, per-layer IO profiler. ReLayTracer enables a detailed analysis to
identify root causes of IO performance anomalies by providing per-layer latency
distributions of each IO request, hardware performance behavior, and time spent
by kernel activities such as an interrupt.
","['\nGanguk Lee\n', '\nYeaseul Park\n', '\nJeongseob Ahn\n', '\nYoungjin Kwon\n']","8 pages, 7 figures",,http://arxiv.org/abs/1906.07124v1,cs.OS,['cs.OS'],,,[]
Cache Contention on Multicore Systems: An Ontology-based Approach,http://arxiv.org/abs/1906.00834v1,2019-06-03T14:32:03Z,2019-06-03T14:32:03Z,"  Multicore processors have proved to be the right choice for both desktop and
server systems because it can support high performance with an acceptable
budget expenditure. In this work, we have compared several works in cache
contention and found that such works have identified several techniques for
cache contention other than cache size including FSB, Memory Controller and
prefetching hardware. We found that Distributed Intensity Online (DIO) is a
very promising cache contention algorithm since it can achieve up to 2% from
the optimal technique. Moreover, we propose a new framework for cache
contention based on resource ontologies. In which ontologies instances will be
used for communication between diverse processes instead of grasping schedules
based on hardware.
",['\nMaruthi Rohit Ayyagari\n'],"6 pages, 3 figures. International Journal of Engineering Trends and
  Technology 2019",,http://dx.doi.org/10.14445/22312803/IJCTT-V67I5P110,cs.OS,"['cs.OS', 'cs.DC']",10.14445/22312803/IJCTT-V67I5P110,,[]
Neural Heterogeneous Scheduler,http://arxiv.org/abs/1906.03724v1,2019-06-09T22:15:19Z,2019-06-09T22:15:19Z,"  Access to parallel and distributed computation has enabled researchers and
developers to improve algorithms and performance in many applications. Recent
research has focused on next generation special purpose systems with multiple
kinds of coprocessors, known as heterogeneous system-on-chips (SoC). In this
paper, we introduce a method to intelligently schedule--and learn to
schedule--a stream of tasks to available processing elements in such a system.
We use deep reinforcement learning enabling complex sequential decision making
and empirically show that our reinforcement learning system provides for a
viable, better alternative to conventional scheduling heuristics with respect
to minimizing execution time.
","['\nTegg Taekyong Sung\n', '\nValliappa Chockalingam\n', '\nAlex Yahja\n', '\nBo Ryu\n']","7 pages. The first two authors contributed equally. ICML 2019
  Real-world Sequential Decision Making Workshop",,http://arxiv.org/abs/1906.03724v1,cs.LG,"['cs.LG', 'cs.OS']",,,[]
Avoiding Scalability Collapse by Restricting Concurrency,http://arxiv.org/abs/1905.10818v2,2019-05-26T15:38:30Z,2019-07-11T23:15:30Z,"  Saturated locks often degrade the performance of a multithreaded application,
leading to a so-called scalability collapse problem. This problem arises when a
growing number of threads circulating through a saturated lock causes the
overall application performance to fade or even drop abruptly. This problem is
particularly (but not solely) acute on oversubscribed systems (systems with
more threads than available hardware cores). In this paper, we introduce GCR
(generic concurrency restriction), a mechanism that aims to avoid the
scalability collapse. GCR, designed as a generic, lock-agnostic wrapper,
intercepts lock acquisition calls, and decides when threads would be allowed to
proceed with the acquisition of the underlying lock. Furthermore, we present
GCR-NUMA, a non-uniform memory access (NUMA)-aware extension of GCR, that
strives to ensure that threads allowed to acquire the lock are those that run
on the same socket. The extensive evaluation that includes more than two dozen
locks, three machines and three benchmarks shows that GCR brings substantial
speedup (in many cases, up to three orders of magnitude) in case of contention
and growing thread counts, while introducing nearly negligible slowdown when
the underlying lock is not contended. GCR-NUMA brings even larger performance
gains starting at even lighter lock contention.
","['\nDave Dice\n', '\nAlex Kogan\n']",,,http://arxiv.org/abs/1905.10818v2,cs.OS,['cs.OS'],,,[]
MemoryRanger Prevents Hijacking FILE_OBJECT Structures in Windows Kernel,http://arxiv.org/abs/1905.09543v1,2019-05-23T08:57:45Z,2019-05-23T08:57:45Z,"  Windows OS kernel memory is one of the main targets of cyber-attacks. By
launching such attacks, hackers are succeeding in process privilege escalation
and tampering with users data by accessing kernel mode memory. This paper
considers a new example of such an attack, which results in access to the files
opened in an exclusive mode. Windows built-in security features prevent such
legal access, but attackers can circumvent them by patching dynamically
allocated objects. The research shows that the Windows 10, version 1809 x64 is
vulnerable to this attack. The paper provides an example of using MemoryRanger,
a hypervisor-based solution to prevent such attack by running kernel-mode
drivers in isolated kernel memory enclaves.
",['\nIgor Korkin\n'],"10 pages, 5 figures. Korkin, I. (2019, May 15-16). MemoryRanger
  Prevents Hijacking FILE_OBJECT Structures in Windows Kernel. Paper presented
  at the Proceedings of the 14th annual Conference on Digital Forensics,
  Security and Law (CDFSL), Embry-Riddle Aeronautical University, Daytona
  Beach, Florida, USA. Retrieved from
  https://commons.erau.edu/adfsl/2019/paper-presentation/7/",,http://arxiv.org/abs/1905.09543v1,cs.CR,"['cs.CR', 'cs.OS']",,,[]
"$Δ$elta: Differential Energy-Efficiency, Latency, and Timing
  Analysis for Real-Time Networks",http://arxiv.org/abs/1905.11788v1,2019-05-28T13:09:06Z,2019-05-28T13:09:06Z,"  The continuously increasing degree of automation in many areas (e.g.
manufacturing engineering, public infrastructure) lead to the construction of
cyber-physical systems and cyber-physical networks. To both, time and energy
are the most critical operating resources. Considering for instance the Tactile
Internet specification, end-to-end latencies in these systems must be below
1ms, which means that both communication and system latencies are in the same
order of magnitude and must be predictably low. As control loops are commonly
handled over different variants of network infrastructure (e.g. mobile and
fibre links) particular attention must be payed to the design of reliable, yet
fast and energy-efficient data-transmission channels that are robust towards
unexpected transmission failures. As design goals are often conflicting (e.g.
high performance vs. low energy), it is necessary to analyze and investigate
trade-offs with regards to design decisions during the construction of
cyber-physical networks. In this paper, we present $\Delta$elta, an approach
towards a tool-supported construction process for cyber-physical networks.
$\Delta$elta extends the previously presented X-Lap tool by new analysis
features, but keeps the original measurements facilities unchanged.
$\Delta$elta jointly analyzes and correlates the runtime behavior (i.e.
performance, latency) and energy demand of individual system components. It
provides an automated analysis with precise thread-local time interpolation,
control-flow extraction, and examination of latency criticality. We further
demonstrate the applicability of $\Delta$elta with an evaluation of a
prototypical implementation.
","['\nStefan Reif\n', '\nAndreas Schmidt\n', '\nTimo Hönig\n', '\nThorsten Herfet\n', '\nWolfgang Schröder-Preikschat\n']",,,http://dx.doi.org/10.1145/3314206.3314211,cs.NI,"['cs.NI', 'cs.OS']",10.1145/3314206.3314211,,[]
"ExplFrame: Exploiting Page Frame Cache for Fault Analysis of Block
  Ciphers",http://arxiv.org/abs/1905.12974v3,2019-05-30T11:37:37Z,2020-02-12T10:51:12Z,"  Page Frame Cache (PFC) is a purely software cache, present in modern Linux
based operating systems (OS), which stores the page frames that are recently
being released by the processes running on a particular CPU. In this paper, we
show that the page frame cache can be maliciously exploited by an adversary to
steer the pages of a victim process to some pre-decided attacker-chosen
locations in the memory. We practically demonstrate an end-to-end attack,
ExplFrame, where an attacker having only user-level privilege is able to force
a victim process's memory pages to vulnerable locations in DRAM and
deterministically conduct Rowhammer to induce faults. We further show that
these faults can be exploited for extracting the secret key of table-based
block cipher implementations. As a case study, we perform a full-key recovery
on OpenSSL AES by Rowhammer-induced single bit faults in the T-tables. We
propose an improvised fault analysis technique which can exploit any
Rowhammer-induced bit-flips in the AES T-tables.
","['\nAnirban Chakraborty\n', '\nSarani Bhattacharya\n', '\nSayandeep Saha\n', '\nDebdeep Mukhopadhyay\n']","7 pages, 4 figues",,http://arxiv.org/abs/1905.12974v3,cs.CR,"['cs.CR', 'cs.OS']",,,[]
Scan-and-Pay on Android is Dangerous,http://arxiv.org/abs/1905.10141v1,2019-05-24T10:49:49Z,2019-05-24T10:49:49Z,"  Mobile payments have increased significantly in the recent years and
one-to-one money transfers are offered by a wide variety of smartphone
applications. These applications usually support scan-and-pay -- a technique
that allows a payer to easily scan the destination address of the payment
directly from the payee's smartphone screen. This technique is pervasive
because it does not require any particular hardware, only the camera, which is
present on all modern smartphones. However, in this work we show that a
malicious application can exploit the overlay feature on Android to compromise
the integrity of transactions that make use of the scan-and-pay technique. We
implement Malview, a proof-of-concept malicious application that runs in the
background on the payee's smartphone and show that it succeeds in redirecting
payments to a malicious wallet. We analyze the weaknesses of the current
defense mechanisms and discuss possible countermeasures against the attack.
","['\nEnis Ulqinaku\n', '\nJulinda Stefa\n', '\nAlessandro Mei\n']","Published in Infocom MobiSec Workshop 2019, Paris, France",,http://arxiv.org/abs/1905.10141v1,cs.CR,"['cs.CR', 'cs.HC', 'cs.OS']",,,[]
"Neverland: Lightweight Hardware Extensions for Enforcing Operating
  System Integrity",http://arxiv.org/abs/1905.05975v1,2019-05-15T06:54:55Z,2019-05-15T06:54:55Z,"  The security of applications hinges on the trustworthiness of the operating
system, as applications rely on the OS to protect code and data. As a result,
multiple protections for safeguarding the integrity of kernel code and data are
being continuously proposed and deployed. These existing protections, however,
are far from ideal as they either provide partial protection, or require
complex and high overhead hardware and software stacks.
  In this work, we present Neverland: a low-overhead, hardware-assisted, memory
protection scheme that safeguards the operating system from rootkits and
kernel-mode malware. Once the system is done booting, Neverland's hardware
takes away the operating system's ability to overwrite certain configuration
registers, as well as portions of its own physical address space that contain
kernel code and security-critical data. Furthermore, it prohibits the CPU from
fetching privileged code from any memory region lying outside the physical
addresses assigned to the OS kernel and drivers (regardless of virtual page
permissions). This combination of protections makes it extremely hard for an
attacker to tamper with the kernel or introduce new privileged code into the
system -- even in the presence of kernel vulnerabilities. Our evaluations show
that the extra hardware required to support these protections incurs minimal
silicon and energy overheads. Neverland enables operating systems to reduce
their attack surface without having to rely on complex integrity monitoring
software or hardware.
","['\nSalessawi Ferede Yitbarek\n', '\nTodd Austin\n']",,,http://arxiv.org/abs/1905.05975v1,cs.CR,"['cs.CR', 'cs.OS']",,,[]
Secure Extensibility for System State Extraction via Plugin Sandboxing,http://arxiv.org/abs/1905.08192v1,2019-05-20T16:16:17Z,2019-05-20T16:16:17Z,"  We introduce a new mechanism to securely extend systems data collection
software with potentially untrusted third-party code. Unlike existing tools
which run extension modules or plugins directly inside the monitored endpoint
(the guest), we run plugins inside a specially crafted sandbox, so as to
protect the guest as well as the software core. To get the right mix of
accessibility and constraints required for systems data extraction, we create
our sandbox by combining multiple features exported by an unmodified kernel. We
have tested its applicability by successfully sandboxing plugins of an
opensourced data collection software for containerized guest systems. We have
also verified its security posture in terms of successful containment of
several exploits, which would have otherwise directly impacted a guest, if
shipped inside third-party plugins.
","['\nSahil Suneja\n', '\nCanturk Isci\n']",,,http://arxiv.org/abs/1905.08192v1,cs.CR,"['cs.CR', 'cs.OS', 'cs.SE']",,,[]
Programming Unikernels in the Large via Functor Driven Development,http://arxiv.org/abs/1905.02529v1,2019-05-07T13:09:32Z,2019-05-07T13:09:32Z,"  Compiling applications as unikernels allows them to be tailored to diverse
execution environments. Dependency on a monolithic operating system is replaced
with linkage against libraries that provide specific services. Doing so in
practice has revealed a major barrier: managing the configuration matrix across
heterogenous execution targets. A realistic unikernel application depends on
hundreds of libraries, each of which may place different demands on the
different target execution platforms (e.g.,~cryptographic acceleration).
  We propose a modular approach to structuring large scale codebases that
cleanly separates configuration, application and operating system logic. Our
implementation is built on the \mirage unikernel framework, using the \ocaml
language's powerful abstraction and metaprogramming facilities. Leveraging
modules allows us to build many components independently, with only loose
coupling through a set of standardised signatures. Components can be
parameterized by other components and composed. Our approach accounts for
state, dependency ordering, and error management, and our usage over the years
has demonstrated significant efficiency benefits by leveraging compiler
features such as global link-time optimisation during the configuration
process. We describe our application architecture and experiences via some
practical applications of our approach, and discuss how library development in
\mirage can facilitate adoption in other unikernel frameworks and programming
languages.
","['\nGabriel Radanne\n', '\nThomas Gazagnaire\n', '\nAnil Madhavapeddy\n', '\nJeremy Yallop\n', '\nRichard Mortier\n', '\nHannes Mehnert\n', '\nMindy Preston\n', '\nDavid Scott\n']",,,http://arxiv.org/abs/1905.02529v1,cs.PL,"['cs.PL', 'cs.OS']",,,[]
IOArbiter: Dynamic Provisioning of Backend Block Storage in the Cloud,http://arxiv.org/abs/1904.09984v1,2019-04-23T14:14:49Z,2019-04-23T14:14:49Z,"  With the advent of virtualization technology, cloud computing realizes
on-demand computing. The capability of dynamic resource provisioning is a
fundamental driving factor for users to adopt the cloud technology. The aspect
is important for cloud service providers to optimize the expense for running
the infrastructure as well. Despite many technological advances in related
areas, however, it is still the case that the infrastructure providers must
decide hardware configuration before deploying a cloud infrastructure,
especially from the storage's perspective. This static nature of the storage
provisioning practice can cause many problems in meeting tenant requirements,
which often come later into the picture. In this paper, we propose a system
called IOArbiter that enables the dynamic creation of underlying storage
implementation in the cloud. IOArbiter defers storage provisioning to the time
at which a tenant actually requests a storage space. As a result, an underlying
storage implementation, e.g., RAID-5, 6 or Ceph storage pool with 6+3 erasure
coding, will be materialized at the volume creation time. Using our prototype
implementation with Openstack Cinder, we show that IOArbiter can simultaneously
satisfy a number of different tenant demands, which may not be possible with a
static configuration strategy. Additionally QoS mechanisms such as admission
control and dynamic throttling help the system mitigate a noisy neighbor
problem significantly.
","['\nMoo-Ryong Ra\n', '\nHee Won Lee\n']","7 pages, 3 figures",,http://arxiv.org/abs/1904.09984v1,cs.DC,"['cs.DC', 'cs.OS']",,,[]
A Survey on Tiering and Caching in High-Performance Storage Systems,http://arxiv.org/abs/1904.11560v1,2019-04-25T19:57:31Z,2019-04-25T19:57:31Z,"  Although every individual invented storage technology made a big step towards
perfection, none of them is spotless. Different data store essentials such as
performance, availability, and recovery requirements have not met together in a
single economically affordable medium, yet. One of the most influential factors
is price. So, there has always been a trade-off between having a desired set of
storage choices and the costs. To address this issue, a network of various
types of storing media is used to deliver the high performance of expensive
devices such as solid state drives and non-volatile memories, along with the
high capacity of inexpensive ones like hard disk drives. In software, caching
and tiering are long-established concepts for handling file operations and
moving data automatically within such a storage network and manage data backup
in low-cost media. Intelligently moving data around different devices based on
the needs is the key insight for this matter. In this survey, we discuss some
recent pieces of research that have been done to improve high-performance
storage systems with caching and tiering techniques.
",['\nMorteza Hoseinzadeh\n'],Ph.D. Research Exam Report,,http://arxiv.org/abs/1904.11560v1,cs.AR,"['cs.AR', 'cs.OS']",,,[]
MANA for MPI: MPI-Agnostic Network-Agnostic Transparent Checkpointing,http://arxiv.org/abs/1904.12595v1,2019-04-20T16:11:54Z,2019-04-20T16:11:54Z,"  Transparently checkpointing MPI for fault tolerance and load balancing is a
long-standing problem in HPC. The problem has been complicated by the need to
provide checkpoint-restart services for all combinations of an MPI
implementation over all network interconnects. This work presents MANA
(MPI-Agnostic Network-Agnostic transparent checkpointing), a single code base
which supports all MPI implementation and interconnect combinations. The
agnostic properties imply that one can checkpoint an MPI application under one
MPI implementation and perhaps over TCP, and then restart under a second MPI
implementation over InfiniBand on a cluster with a different number of CPU
cores per node. This technique is based on a novel ""split-process"" approach,
which enables two separate programs to co-exist within a single process with a
single address space. This work overcomes the limitations of the two most
widely adopted transparent checkpointing solutions, BLCR and DMTCP/InfiniBand,
which require separate modifications to each MPI implementation and/or
underlying network API. The runtime overhead is found to be insignificant both
for checkpoint-restart within a single host, and when comparing a local MPI
computation that was migrated to a remote cluster against an ordinary MPI
computation running natively on that same remote cluster.
","['\nRohan Garg\n', '\nGregory Price\n', '\nGene Cooperman\n']",24 pages; 9 figures; accepted at HPDC-2019,,http://arxiv.org/abs/1904.12595v1,cs.DC,"['cs.DC', 'cs.OS']",,,[]
KEY-SSD: Access-Control Drive to Protect Files from Ransomware Attacks,http://arxiv.org/abs/1904.05012v1,2019-04-10T06:15:28Z,2019-04-10T06:15:28Z,"  Traditional techniques to prevent damage from ransomware attacks are to
detect and block attacks by monitoring the known behaviors such as frequent
name changes, recurring access to cryptographic libraries and exchange keys
with remote servers. Unfortunately, intelligent ransomware can easily bypass
these techniques. Another prevention technique is to recover from the backup
copy when a file is infected with ransomware. However, the data backup
technique requires extra storage space and can be removed with ransomware. In
this paper, we propose to implement an access control mechanism on a disk
drive, called a KEY-SSD disk drive. KEY-SSD is the data store and the last
barrier to data protection. Unauthorized applications will not be able to read
file data even if they bypass the file system defense, thus denying the block
request without knowing the disk's registered block key and completely
eliminating the possibility of the file becoming hostage to ransomware. We have
prototyped KEY-SSD and validated the usefulness of KEY-SSD by demonstrating 1)
selective block access control, 2) unauthorized data access blocking and 3)
negligible performance overhead. Our comprehensive evaluation of KEY-SSD for
various workloads show the KEY-SSD performance is hardly degraded due to OS
lightweight key transmission and access control drive optimization. We also
confirmed that KEY-SSD successfully protects the files in the actual ransomware
sample.
","['\nJinwoo Ahn\n', '\nDonggyu Park\n', '\nChang-Gyu Lee\n', '\nDonghyun Min\n', '\nJunghee Lee\n', '\nSungyong Park\n', '\nQian Chen\n', '\nYoungjae Kim\n']","12 pages, 20 figures",,http://arxiv.org/abs/1904.05012v1,cs.CR,"['cs.CR', 'cs.OS']",,,[]
The Android Platform Security Model (2023),http://arxiv.org/abs/1904.05572v3,2019-04-11T08:20:07Z,2024-01-09T01:36:16Z,"  Android is the most widely deployed end-user focused operating system. With
its growing set of use cases encompassing communication, navigation, media
consumption, entertainment, finance, health, and access to sensors, actuators,
cameras, or microphones, its underlying security model needs to address a host
of practical threats in a wide variety of scenarios while being useful to
non-security experts. To support this flexibility, Android's security model
must strike a difficult balance between security, privacy, and usability for
end users; provide assurances for app developers; and maintain system
performance under tight hardware constraints. This paper aims to both document
the assumed threat model and discuss its implications, with a focus on the
ecosystem context in which Android exists. We analyze how different security
measures in past and current Android implementations work together to mitigate
these threats, and, where there are special cases in applying the security
model in practice; we discuss these deliberate deviations and examine their
impact.
","['\nRené Mayrhofer\n', '\nJeffrey Vander Stoep\n', '\nChad Brubaker\n', '\nDianne Hackborn\n', '\nBram Bonné\n', '\nGüliz Seray Tuncay\n', '\nRoger Piqueras Jover\n', '\nMichael A. Specter\n']",,"ACM Transactions on Privacy and Security, Volume 24, Issue 3,
  Article No. 19, 2021, pp 1-35",http://dx.doi.org/10.1145/3448609,cs.CR,"['cs.CR', 'cs.OS']",10.1145/3448609,,[]
"A WCET-aware cache coloring technique for reducing interference in
  real-time systems",http://arxiv.org/abs/1903.09310v4,2019-03-22T13:08:13Z,2019-05-20T16:35:52Z,"  The predictability of a system is the condition to give saferbound on worst
case execution timeof real-time tasks which are running on it. Commercial
off-the-shelf(COTS) processors are in-creasingly used in embedded systems and
contain shared cache memory. This component hasa hard predictable behavior
because its state depends of theexecution history of the systems.To increase
predictability of COTS component we use cache coloring, a technique widely
usedto partition cache memory. Our main contribution is a WCET aware heuristic
which parti-tion task according to the needs of each task. Our experiments are
made with CPLEX an ILPsolver with random tasks set generated running on
preemptive system scheduled with earliestdeadline first(EDF).
","['\nFabien Bouquillon\n', '\nClément Ballabriga\n', '\nGiuseppe Lipari\n', '\nSmail Niar\n']",,,http://arxiv.org/abs/1903.09310v4,cs.OS,"['cs.OS', 'cs.DC']",,,[]
"Understanding and taming SSD read performance variability: HDFS case
  study",http://arxiv.org/abs/1903.09347v1,2019-03-22T03:46:22Z,2019-03-22T03:46:22Z,"  In this paper we analyze the influence that lower layers (file system, OS,
SSD) have on HDFS' ability to extract maximum performance from SSDs on the read
path. We uncover and analyze three surprising performance slowdowns induced by
lower layers that result in HDFS read throughput loss. First, intrinsic
slowdown affects reads from every new file system extent for a variable amount
of time. Second, temporal slowdown appears temporarily and periodically and is
workload-agnostic. Third, in permanent slowdown, some files can individually
and permanently become slower after a period of time. We analyze the impact of
these slowdowns on HDFS and show significant throughput loss. Individually,
each of the slowdowns can cause a read throughput loss of 10-15%. However,
their effect is cumulative. When all slowdowns happen concurrently, read
throughput drops by as much as 30%. We further analyze mitigation techniques
and show that two of the three slowdowns could be addressed via increased IO
request parallelism in the lower layers. Unfortunately, HDFS cannot
automatically adapt to use such additional parallelism. Our results point to a
need for adaptability in storage stacks. The reason is that an access pattern
that maximizes performance in the common case is not necessarily the same one
that can mask performance fluctuations.
","['\nMaría F. Borge\n', '\nFlorin Dinu\n', '\nWilly Zwaenepoel\n']","13 pages, 16 figures",,http://arxiv.org/abs/1903.09347v1,cs.OS,"['cs.OS', 'cs.PF']",,,[]
"Processor in Non-Volatile Memory (PiNVSM): Towards to Data-centric
  Computing in Decentralized Environment",http://arxiv.org/abs/1903.03701v1,2019-03-08T23:58:52Z,2019-03-08T23:58:52Z,"  The AI problem has no solution in the environment of existing hardware stack
and OS architecture. CPU-centric model of computation has a huge number of
drawbacks that originate from memory hierarchy and obsolete architecture of the
computing core. The concept of mixing memory and logic has been around since
1960s. However, the concept of Processor-In-Memory (PIM) is unable to resolve
the critical issues of the CPU-centric computing model because of inevitable
replication of von Neumann architecture's drawbacks. The next generation of
NVM/SCM memory is able to give the second birth to the data-centric computing
paradigm. This paper presents a concept of Processor in Non-Volatile Memory
(PiNVSM) architecture. The basis of PiNVSM architecture is the concept of DPU
that contains the NVM memory and dedicated PU. All necessary PU's registers can
be implemented in the space of NVM memory. NVM memory of DPU is the single
space for storing and transformation of data. In the basis of PiNVSM
architecture lies the DPU array is able to overcome the limitations as Turing
machine model as von Neumann architecture. The DPU array hasn't a centralized
computing core. Every data portion has dedicated computing core that excludes
the necessity to transfer data to the place of data processing. Every DPU
contains data portion that is associated with the set of keywords. Any complex
data structure can be split on elementary items that can be stored into
independent DPU with dedicated computing core(s). One DPU is able to apply the
elementary transformation on one item. But the DPU array is able to make the
transformation of complex structure by means of concurrent execution of
elementary transformations in different DPUs. The PiNVSM architecture suggests
a principally new architecture of the computing core that creates a new
opportunity for data self-organization, data and code synthesis.
",['\nViacheslav Dubeyko\n'],arXiv admin note: text overlap with arXiv:1805.09612 by other authors,,http://arxiv.org/abs/1903.03701v1,cs.OS,['cs.OS'],,,[]
Nature of System Calls in CPU-centric Computing Paradigm,http://arxiv.org/abs/1903.04075v1,2019-03-10T23:34:14Z,2019-03-10T23:34:14Z,"  Modern operating systems are typically POSIX-compliant with major system
calls specified decades ago. The next generation of non-volatile memory (NVM)
technologies raise concerns about the efficiency of the traditional POSIX-based
systems. As one step toward building high performance NVM systems, we explore
the potential dependencies between system call performance and major hardware
components (e.g., CPU, memory, storage) under typical user cases (e.g.,
software compilation, installation, web browser, office suite) in this paper.
We build histograms for the most frequent and time-consuming system calls with
the goal to understand the nature of distribution on different platforms. We
find that there is a strong dependency between the system call performance and
the CPU architecture. On the other hand, the type of persistent storage plays a
less important role in affecting the performance.
","['\nViacheslav Dubeyko\n', '\nOm Rameshwar Gatla\n', '\nMai Zheng\n']",,,http://arxiv.org/abs/1903.04075v1,cs.OS,['cs.OS'],,,[]
MultiK: A Framework for Orchestrating Multiple Specialized Kernels,http://arxiv.org/abs/1903.06889v1,2019-03-16T07:17:47Z,2019-03-16T07:17:47Z,"  We present, MultiK, a Linux-based framework 1 that reduces the attack surface
for operating system kernels by reducing code bloat. MultiK ""orchestrates""
multiple kernels that are specialized for individual applications in a
transparent manner. This framework is flexible to accommodate different kernel
code reduction techniques and, most importantly, run the specialized kernels
with near-zero additional runtime overheads. MultiK avoids the overheads of
virtualization and runs natively on the system. For instance, an Apache
instance is shown to run on a kernel that has (a) 93.68% of its code reduced,
(b) 19 of 23 known kernel vulnerabilities eliminated and (c) with negligible
performance overheads (0.19%). MultiK is a framework that can integrate with
existing code reduction and OS security techniques. We demonstrate this by
using D-KUT and S-KUT -- two methods to profile and eliminate unwanted kernel
code. The whole process is transparent to the user applications because MultiK
does not require a recompilation of the application.
","['\nHsuan-Chi Kuo\n', '\nAkshith Gunasekaran\n', '\nYeongjin Jang\n', '\nSibin Mohan\n', '\nRakesh B. Bobba\n', '\nDavid Lie\n', '\nJesse Walker\n']",,,http://arxiv.org/abs/1903.06889v1,cs.OS,['cs.OS'],,,[]
Pyronia: Intra-Process Access Control for IoT Applications,http://arxiv.org/abs/1903.01950v2,2019-03-05T17:35:03Z,2019-11-20T21:28:47Z,"  Third-party code plays a critical role in IoT applications, which generate
and analyze highly privacy-sensitive data. Unlike traditional desktop and
server settings, IoT devices mostly run a dedicated, single application. As a
result, vulnerabilities in third-party libraries within a process pose a much
bigger threat than on traditional platforms.
  We present Pyronia, a fine-grained access control system for IoT applications
written in high-level languages. Pyronia exploits developers' coarse-grained
expectations about how imported third-party code operates to restrict access to
files, devices, and specific network destinations, at the granularity of
individual functions. To efficiently protect such sensitive OS resources,
Pyronia combines three techniques: system call interposition, stack inspection,
and memory domains. This design avoids the need for application refactoring, or
unintuitive data flow analysis, while enforcing the developer's access policy
at run time. Our Pyronia prototype for Python runs on a custom Linux kernel,
and incurs moderate performance overhead on unmodified Python applications.
","['\nMarcela S. Melara\n', '\nDavid H. Liu\n', '\nMichael J. Freedman\n']","19 pages, 3 figures",,http://arxiv.org/abs/1903.01950v2,cs.OS,['cs.OS'],,,[]
The Lustre Storage Architecture,http://arxiv.org/abs/1903.01955v1,2019-03-05T17:40:38Z,2019-03-05T17:40:38Z,"  This lengthy document often referred to as the ""Lustre Book"", contains a
detailed outline of Lustre file system architecture, as it was created between
2001 and 2005, in accordance with the requirements from various users. Now, in
2019, most features have been implemented, but some only recently, and some
along different lines of thought.
",['\nPeter Braam\n'],,,http://arxiv.org/abs/1903.01955v1,cs.OS,['cs.OS'],,,[]
"Denial-of-Service Attacks on Shared Cache in Multicore: Analysis and
  Prevention",http://arxiv.org/abs/1903.01314v1,2019-03-04T15:48:00Z,2019-03-04T15:48:00Z,"  In this paper we investigate the feasibility of denial-of-service (DoS)
attacks on shared caches in multicore platforms. With carefully engineered
attacker tasks, we are able to cause more than 300X execution time increases on
a victim task running on a dedicated core on a popular embedded multicore
platform, regardless of whether we partition its shared cache or not. Based on
careful experimentation on real and simulated multicore platforms, we identify
an internal hardware structure of a non-blocking cache, namely the cache
writeback buffer, as a potential target of shared cache DoS attacks. We propose
an OS-level solution to prevent such DoS attacks by extending a
state-of-the-art memory bandwidth regulation mechanism. We implement the
proposed mechanism in Linux on a real multicore platform and show its
effectiveness in protecting against cache DoS attacks.
","['\nMichael G Bechtel\n', '\nHeechul Yun\n']",To be published as a conference paper at RTAS 2019,,http://arxiv.org/abs/1903.01314v1,cs.AR,"['cs.AR', 'cs.OS']",,,[]
Dynamic Fault Tolerance Through Resource Pooling,http://arxiv.org/abs/1902.09493v1,2019-02-22T00:39:43Z,2019-02-22T00:39:43Z,"  Miniaturized satellites are currently not considered suitable for critical,
high-priority, and complex multi-phased missions, due to their low reliability.
As hardware-side fault tolerance (FT) solutions designed for larger spacecraft
can not be adopted aboard very small satellites due to budget, energy, and size
constraints, we developed a hybrid FT-approach based upon only COTS components,
commodity processor cores, library IP, and standard software. This approach
facilitates fault detection, isolation, and recovery in software, and utilizes
fault-coverage techniques across the embedded stack within an multiprocessor
system-on-chip (MPSoC). This allows our FPGA-based proof-of-concept
implementation to deliver strong fault-coverage even for missions with a long
duration, but also to adapt to varying performance requirements during the
mission. The operator of a spacecraft utilizing this approach can define
performance profiles, which allow an on-board computer (OBC) to trade between
processing capacity, fault coverage, and energy consumption using simple
heuristics. The software-side FT approach developed also offers advantages if
deployed aboard larger spacecraft through spare resource pooling, enabling an
OBC to more efficiently handle permanent faults. This FT approach in part
mimics a critical biological systems's way of tolerating and adjusting to
failures, enabling graceful ageing of an MPSoC.
","['\nChristian M. Fuchs\n', '\nNadia M. Murillo\n', '\nAske Plaat\n', '\nErik van der Kouwe\n', '\nTodor Stefanov\n']",,2018 NASA/ESA Conference on Adaptive Hardware and Systems (AHS),http://dx.doi.org/10.1109/AHS.2018.8541457,cs.DC,"['cs.DC', 'cs.OS', 'cs.SY']",10.1109/AHS.2018.8541457,,[]
Cloud Programming Simplified: A Berkeley View on Serverless Computing,http://arxiv.org/abs/1902.03383v1,2019-02-09T07:25:09Z,2019-02-09T07:25:09Z,"  Serverless cloud computing handles virtually all the system administration
operations needed to make it easier for programmers to use the cloud. It
provides an interface that greatly simplifies cloud programming, and represents
an evolution that parallels the transition from assembly language to high-level
programming languages. This paper gives a quick history of cloud computing,
including an accounting of the predictions of the 2009 Berkeley View of Cloud
Computing paper, explains the motivation for serverless computing, describes
applications that stretch the current limits of serverless, and then lists
obstacles and research opportunities required for serverless computing to
fulfill its full potential. Just as the 2009 paper identified challenges for
the cloud and predicted they would be addressed and that cloud use would
accelerate, we predict these issues are solvable and that serverless computing
will grow to dominate the future of cloud computing.
","['\nEric Jonas\n', '\nJohann Schleier-Smith\n', '\nVikram Sreekanti\n', '\nChia-Che Tsai\n', '\nAnurag Khandelwal\n', '\nQifan Pu\n', '\nVaishaal Shankar\n', '\nJoao Carreira\n', '\nKarl Krauth\n', '\nNeeraja Yadwadkar\n', '\nJoseph E. Gonzalez\n', '\nRaluca Ada Popa\n', '\nIon Stoica\n', '\nDavid A. Patterson\n']",,,http://arxiv.org/abs/1902.03383v1,cs.OS,['cs.OS'],,,[]
Fine-Grain Checkpointing with In-Cache-Line Logging,http://arxiv.org/abs/1902.00660v1,2019-02-02T07:22:25Z,2019-02-02T07:22:25Z,"  Non-Volatile Memory offers the possibility of implementing high-performance,
durable data structures. However, achieving performance comparable to
well-designed data structures in non-persistent (transient) memory is
difficult, primarily because of the cost of ensuring the order in which memory
writes reach NVM. Often, this requires flushing data to NVM and waiting a full
memory round-trip time.
  In this paper, we introduce two new techniques: Fine-Grained Checkpointing,
which ensures a consistent, quickly recoverable data structure in NVM after a
system failure, and In-Cache-Line Logging, an undo-logging technique that
enables recovery of earlier state without requiring cache-line flushes in the
normal case. We implemented these techniques in the Masstree data structure,
making it persistent and demonstrating the ease of applying them to a highly
optimized system and their low (5.9-15.4\%) runtime overhead cost.
","['\nNachshon Cohen\n', '\nDavid T. Aksun\n', '\nHillel Avni\n', '\nJames R. Larus\n']","In 2019 Architectural Support for Programming Languages and Operating
  Systems (ASPLOS 19), April 13, 2019, Providence, RI, USA",,http://dx.doi.org/10.1145/3297858.3304046,cs.OS,"['cs.OS', 'cs.PL']",10.1145/3297858.3304046,,[]
Can We Prove Time Protection?,http://arxiv.org/abs/1901.08338v1,2019-01-24T10:44:31Z,2019-01-24T10:44:31Z,"  Timing channels are a significant and growing security threat in computer
systems, with no established solution. We have recently argued that the OS must
provide time protection, in analogy to the established memory protection, to
protect applications from information leakage through timing channels. Based on
a recently-proposed implementation of time protection in the seL4 microkernel,
we investigate how such an implementation could be formally proved to prevent
timing channels. We postulate that this should be possible by reasoning about a
highly abstracted representation of the shared hardware resources that cause
timing channels.
","['\nGernot Heiser\n', '\nGerwin Klein\n', '\nToby Murray\n']","6 pages, 1 figure",,http://arxiv.org/abs/1901.08338v1,cs.OS,['cs.OS'],,,[]
"PINPOINT: Efficient and Effective Resource Isolation for Mobile Security
  and Privacy",http://arxiv.org/abs/1901.07732v1,2019-01-23T05:41:42Z,2019-01-23T05:41:42Z,"  Virtualization is frequently used to isolate untrusted processes and control
their access to sensitive resources. However, isolation usually carries a price
in terms of less resource sharing and reduced inter-process communication. In
an open architecture such as Android, this price and its impact on performance,
usability, and transparency must be carefully considered. Although previous
efforts in developing general-purpose isolation solutions have shown that some
of these negative side effects can be mitigated, doing so involves overcoming
significant design challenges by incorporating numerous additional platform
complexities not directly related to improved security. Thus, the general
purpose solutions become inefficient and burdensome if the end-user has only
specific security goals. In this paper, we present PINPOINT, a resource
isolation strategy that forgoes general-purpose solutions in favor of a
""building block"" approach that addresses specific end-user security goals.
PINPOINT embodies the concept of Linux Namespace lightweight isolation, but
does so in the Android Framework by guiding the security designer towards
isolation points that are contextually close to the resource(s) that need to be
isolated. This strategy allows the rest of the Framework to function fully as
intended, transparently. We demonstrate our strategy with a case study on
Android System Services, and show four applications of PINPOINTed system
services functioning with unmodified market apps. Our evaluation results show
that practical security and privacy advantages can be gained using our
approach, without inducing the problematic side-effects that other
general-purpose designs must address.
","['\nPaul Ratazzi\n', '\nAshok Bommisetti\n', '\nNian Ji\n', '\nWenliang Du\n']","Mobile Security Technologies (MoST) Workshop, May 21, 2015,
  http://www.ieee-security.org/TC/SPW2015/MoST/",,http://arxiv.org/abs/1901.07732v1,cs.OS,"['cs.OS', 'cs.CR']",,,[]
User Space Network Drivers,http://arxiv.org/abs/1901.10664v2,2019-01-30T04:10:01Z,2019-09-08T15:33:45Z,"  The rise of user space packet processing frameworks like DPDK and netmap
makes low-level code more accessible to developers and researchers. Previously,
driver code was hidden in the kernel and rarely modified, or even looked at, by
developers working at higher layers. These barriers are gone nowadays, yet
developers still treat user space drivers as black-boxes magically accelerating
applications. We want to change this: every researcher building high-speed
network applications should understand the intricacies of the underlying
drivers, especially if they impact performance. We present ixy, a user space
network driver designed for simplicity and educational purposes to show that
fast packet IO is not black magic but careful engineering. ixy focuses on the
bare essentials of user space packet processing: a packet forwarder including
the whole NIC driver uses less than 1,000 lines of C code.
  This paper is partially written in tutorial style on the case study of our
implementations of drivers for both the Intel 82599 family and for virtual
VirtIO NICs. The former allows us to reason about driver and framework
performance on a stripped-down implementation to assess individual
optimizations in isolation. VirtIO support ensures that everyone can run it in
a virtual machine.
  Our code is available as free and open source under the BSD license at
https://github.com/emmericp/ixy
","['\nPaul Emmerich\n', '\nMaximilian Pudelko\n', '\nSimon Bauer\n', '\nStefan Huber\n', '\nThomas Zwickl\n', '\nGeorg Carle\n']","in ACM/IEEE Symposium on Architectures for Networking and
  Communications Systems (ANCS 2019), 2019",,http://arxiv.org/abs/1901.10664v2,cs.NI,"['cs.NI', 'cs.OS']",,,[]
"Multiverse: Easy Conversion of Runtime Systems into OS Kernels via
  Automatic Hybridization",http://arxiv.org/abs/1901.06360v1,2019-01-18T18:01:38Z,2019-01-18T18:01:38Z,"  The hybrid runtime (HRT) model offers a path towards high performance and
efficiency. By integrating the OS kernel, runtime, and application, an HRT
allows the runtime developer to leverage the full feature set of the hardware
and specialize OS services to the runtime's needs. However, conforming to the
HRT model currently requires a port of the runtime to the kernel level, for
example to the Nautilus kernel framework, and this requires knowledge of kernel
internals. In response, we developed Multiverse, a system that bridges the gap
between a built-from-scratch HRT and a legacy runtime system. Multiverse allows
unmodified applications and runtimes to be brought into the HRT model without
any porting effort whatsoever by splitting the execution of the application
between the domains of a legacy OS and an HRT environment. We describe the
design and implementation of Multiverse and illustrate its capabilities using
the massive, widely-used Racket runtime system.
","['\nKyle C. Hale\n', '\nConor Hetland\n', '\nPeter Dinda\n']","Published in the Proceedings of the 14th International Conference on
  Autonomic Computing (ICAC '17)",,http://dx.doi.org/10.1109/ICAC.2017.24,cs.OS,['cs.OS'],10.1109/ICAC.2017.24,,[]
XOS: An Application-Defined Operating System for Data Center Servers,http://arxiv.org/abs/1901.00825v1,2019-01-03T17:27:14Z,2019-01-03T17:27:14Z,"  Rapid growth of datacenter (DC) scale, urgency of cost control, increasing
workload diversity, and huge software investment protection place unprecedented
demands on the operating system (OS) efficiency, scalability, performance
isolation, and backward-compatibility. The traditional OSes are not built to
work with deep-hierarchy software stacks, large numbers of cores, tail latency
guarantee, and increasingly rich variety of applications seen in modern DCs,
and thus they struggle to meet the demands of such workloads.
  This paper presents XOS, an application-defined OS for modern DC servers. Our
design moves resource management out of the OS kernel, supports customizable
kernel subsystems in user space, and enables elastic partitioning of hardware
resources. Specifically, XOS leverages modern hardware support for
virtualization to move resource management functionality out of the
conventional kernel and into user space, which lets applications achieve near
bare-metal performance. We implement XOS on top of Linux to provide backward
compatibility. XOS speeds up a set of DC workloads by up to 1.6X over our
baseline Linux on a 24-core server, and outperforms the state-of-the-art Dune
by up to 3.3X in terms of virtual memory management. In addition, XOS
demonstrates good scalability and strong performance isolation.
","['\nChen Zheng\n', '\nLei Wang\n', '\nSally A. McKee\n', '\nLixin Zhang\n', '\nHainan Ye\n', '\nJianfeng Zhan\n']","Accepted for publication in IEEE BigData 2018. 10 pages, 6 figures, 3
  tables",,http://arxiv.org/abs/1901.00825v1,cs.OS,"['cs.OS', 'D.4.0; D.4.7; D.4.8']",,,[]
"Efficient, Dynamic Multi-tenant Edge Computation in EdgeOS",http://arxiv.org/abs/1901.01222v1,2019-01-04T17:31:56Z,2019-01-04T17:31:56Z,"  In the future, computing will be immersed in the world around us -- from
augmented reality to autonomous vehicles to the Internet of Things. Many of
these smart devices will offer services that respond in real time to their
physical surroundings, requiring complex processing with strict performance
guarantees. Edge clouds promise a pervasive computational infrastructure a
short network hop away from end devices, but today's operating systems are a
poor fit to meet the goals of scalable isolation, dense multi-tenancy, and
predictable performance required by these emerging applications. In this paper
we present EdgeOS, a micro-kernel based operating system that meets these goals
by blending recent advances in real-time systems and network function
virtualization. EdgeOS introduces a Featherweight Process model that offers
lightweight isolation and supports extreme scalability even under high churn.
Our architecture provides efficient communication mechanisms, and low-overhead
per-client isolation. To achieve high performance networking, EdgeOS employs
kernel bypass paired with the isolation properties of Featherweight Processes.
We have evaluated our EdgeOS prototype for running high scale network
middleboxes using the Click software router and endpoint applications using
memcached. EdgeOS reduces startup latency by 170X compared to Linux processes
and over five orders of magnitude compared to containers, while providing three
orders of magnitude latency improvement when running 300 to 1000 edge-cloud
memcached instances on one server.
","['\nYuxin Ren\n', '\nVlad Nitu\n', '\nGuyue Liu\n', '\nGabriel Parmer\n', '\nTimothy Wood\n', '\nAlain Tchana\n', '\nRiley Kennedy\n']",,,http://arxiv.org/abs/1901.01222v1,cs.OS,['cs.OS'],,,[]
File System in Data-Centric Computing,http://arxiv.org/abs/1901.01340v1,2019-01-04T23:10:05Z,2019-01-04T23:10:05Z,"  The moving computation on the edge or near to data is the new trend that can
break the bandwidth wall and to unleash the power of next generation NVM or SCM
memory. File system is the important OS subsystem that plays the role of
mediator between the user-space application and storage device. The key goal of
the file system is to represent the file abstraction and to build the files'
namespace. In the current paradigm the file system needs to copy the metadata
and user data in the DRAM of the host with the goal to access and to modify the
user data on the host side. The DAX approach doesn't change the concept but to
build the way to bypass the page cache via the direct access to file's content
in persistent memory. Generally speaking, for the case of data-centric
computing, the file system needs to solve the opposite task not to copy data
into page cache but to deliver the processing activity near data on the storage
device side.
",['\nViacheslav Dubeyko\n'],,,http://arxiv.org/abs/1901.01340v1,cs.OS,['cs.OS'],,,[]
"A C-DAG task model for scheduling complex real-time tasks on
  heterogeneous platforms: preemption matters",http://arxiv.org/abs/1901.02450v1,2019-01-08T08:37:23Z,2019-01-08T08:37:23Z,"  Recent commercial hardware platforms for embedded real-time systems feature
heterogeneous processing units and computing accelerators on the same
System-on-Chip. When designing complex real-time application for such
architectures, the designer needs to make a number of difficult choices: on
which processor should a certain task be implemented? Should a component be
implemented in parallel or sequentially? These choices may have a great impact
on feasibility, as the difference in the processor internal architectures
impact on the tasks' execution time and preemption cost. To help the designer
explore the wide space of design choices and tune the scheduling parameters, in
this paper we propose a novel real-time application model, called C-DAG,
specifically conceived for heterogeneous platforms. A C-DAG allows to specify
alternative implementations of the same component of an application for
different processing engines to be selected off-line, as well as conditional
branches to model if-then-else statements to be selected at run-time. We also
propose a schedulability analysis for the C-DAG model and a heuristic
allocation algorithm so that all deadlines are respected. Our analysis takes
into account the cost of preempting a task, which can be non-negligible on
certain processors. We demonstrate the effectiveness of our approach on a large
set of synthetic experiments by comparing with state of the art algorithms in
the literature.
","['\nHoussam-Eddine Zahaf\nUNIMORE\n', '\nNicola Capodieci\nUNIMORE\n', '\nRoberto Cavicchioli\nUNIMORE\n', '\nMarko Bertogna\nUNIMORE\n', '\nGiuseppe Lipari\n']",,,http://arxiv.org/abs/1901.02450v1,cs.OS,['cs.OS'],,,"['UNIMORE', 'UNIMORE', 'UNIMORE', 'UNIMORE']"
MI6: Secure Enclaves in a Speculative Out-of-Order Processor,http://arxiv.org/abs/1812.09822v5,2018-12-24T03:55:32Z,2019-08-29T04:53:09Z,"  Recent attacks have broken process isolation by exploiting microarchitectural
side channels that allow indirect access to shared microarchitectural state.
Enclaves strengthen the process abstraction to restore isolation guarantees.
  We propose MI6, an aggressive, speculative out-of-order processor capable of
providing secure enclaves under a threat model that includes an untrusted OS
and an attacker capable of mounting any software attack currently considered
practical, including control flow speculation attacks. MI6 is inspired by
Sanctum [16] and extends its isolation guarantee to more realistic memory
hierarchies. It also introduces a purge instruction, which is used only when a
secure process is scheduled, and implements it for a complex processor
microarchitecture. We model the performance impact of enclaves in MI6 through
FPGA emulation on AWS F1 FPGAs by running SPEC CINT2006 benchmarks on top of an
untrusted Linux OS. Security comes at the cost of approximately 16.4% average
slowdown for protected programs.
","['\nThomas Bourgeat\n', '\nIlia Lebedev\n', '\nAndrew Wright\n', '\nSizhuo Zhang\n', '\n Arvind\n', '\nSrinivas Devadas\n']",15 pages,,http://arxiv.org/abs/1812.09822v5,cs.CR,"['cs.CR', 'cs.OS']",,,[]
Divide et Impera: MemoryRanger Runs Drivers in Isolated Kernel Spaces,http://arxiv.org/abs/1812.09920v1,2018-12-24T13:30:40Z,2018-12-24T13:30:40Z,"  One of the main issues in the OS security is to provide trusted code
execution in an untrusted environment. During executing, kernel-mode drivers
allocate and process memory data: OS internal structures, users private
information, and sensitive data of third-party drivers. All this data and the
drivers code can be tampered with by kernel-mode malware. Microsoft security
experts integrated new features to fill this gap, but they are not enough:
allocated data can be stolen and patched and the drivers code can be dumped
without any security reaction. The proposed hypervisor-based system
(MemoryRanger) tackles this issue by executing drivers in separate kernel
enclaves with specific memory attributes. MemoryRanger protects code and data
using Intel VT-x and EPT features with low performance degradation on Windows
10 x64.
",['\nIgor Korkin\n'],"Korkin, I. (2018, December 5-6). Divide et Impera: MemoryRanger Runs
  Drivers in Isolated Kernel Spaces. In Proceedings of the BlackHat Europe
  Conference, London, UK. 23 pages, 4 figures, 2 tables, 49 references.
  Retrieved from
  https://www.blackhat.com/eu-18/briefings/schedule/#divide-et-impera-memoryranger-runs-drivers-in-isolated-kernel-spaces-12668",,http://arxiv.org/abs/1812.09920v1,cs.CR,"['cs.CR', 'cs.OS']",,,[]
MiniOS: an instructional platform for teaching operating systems labs,http://arxiv.org/abs/1811.09792v2,2018-11-24T08:54:38Z,2018-12-01T04:45:49Z,"  Delivering hands-on practice laboratories for introductory courses on
operating systems is a difficult task. One of the main sources of the
difficulty is the sheer size and complexity of the operating systems software.
Consequently, some of the solutions adopted in the literature to teach
operating systems laboratory consider smaller and simpler systems, generally
referred to as instructional operating systems. This work continues in the same
direction and is threefold. First, it considers a simpler hardware platform.
Second, it argues that a minimal operating system is a viable option for
delivering laboratories. Third, it presents a laboratory teaching platform,
whereby students build a minimal operating system for an embedded hardware
platform. The proposed platform is called MiniOS. An important aspect of MiniOS
is that it is sufficiently supported with additional technical and pedagogic
material. Finally, the effectiveness of the proposed approach to teach
operating systems laboratories is illustrated through the experience of using
it to deliver laboratory projects in the Operating Systems course at the
University of Northern British Columbia. Finally, we discuss experimental
research in computing education and considered the qualitative results of this
work as part of a larger research endeavour.
","['\nRafael Roman Otero\n', '\nAlex Aravind\n']","32 pages, 11 figures",,http://arxiv.org/abs/1811.09792v2,cs.OS,['cs.OS'],,,[]
Transkernel: Bridging Monolithic Kernels to Peripheral Cores,http://arxiv.org/abs/1811.05000v3,2018-11-12T21:00:23Z,2019-06-05T19:27:33Z,"  Smart devices see a large number of ephemeral tasks driven by background
activities. In order to execute such a task, the OS kernel wakes up the
platform beforehand and puts it back to sleep afterwards. In doing so, the
kernel operates various IO devices and orchestrates their power state
transitions. Such kernel executions are inefficient as they mismatch typical
CPU hardware. They are better off running on a low-power, microcontroller-like
core, i.e., peripheral core, relieving CPU from the inefficiency.
  We therefore present a new OS structure, in which a lightweight virtual
executor called transkernel offloads specific phases from a monolithic kernel.
The transkernel translates stateful kernel execution through cross-ISA, dynamic
binary translation (DBT); it emulates a small set of stateless kernel services
behind a narrow, stable binary interface; it specializes for hot paths; it
exploits ISA similarities for lowering DBT cost.
  Through an ARM-based prototype, we demonstrate transkernel's feasibility and
benefit. We show that while cross-ISA DBT is typically used under the
assumption of efficiency loss, it can enable efficiency gain, even on
off-the-shelf hardware.
","['\nLiwei Guo\n', '\nShuang Zhai\n', '\nYi Qiao\n', '\nFelix Xiaozhu Lin\n']","The camera-ready version of this paper, will appear at USENIX ATC'19",,http://arxiv.org/abs/1811.05000v3,cs.OS,['cs.OS'],,,[]
DurableFS: A File System for Persistent Memory,http://arxiv.org/abs/1811.00757v1,2018-11-02T07:11:52Z,2018-11-02T07:11:52Z,"  With the availability of hybrid DRAM-NVRAM memory on the memory bus of CPUs,
a number of file systems on NVRAM have been designed and implemented. In this
paper we present the design and implementation of a file system on NVRAM called
DurableFS, which provides atomicity and durability of file operations to
applications. Due to the byte level random accessibility of memory, it is
possible to provide these guarantees without much overhead. We use standard
techniques like copy on write for data, and a redo log for metadata changes to
build an efficient file system which provides durability and atomicity
guarantees at the time a file is closed. Benchmarks on the implementation shows
that there is only a 7 %degradation in performance due to providing these
guarantees.
","['\nChandan Kalita\n', '\nGautam Barua\n', '\nPriya Sehgal\n']","6 Pages, 3 Figures",,http://arxiv.org/abs/1811.00757v1,cs.OS,['cs.OS'],,,[]
Finding Crash-Consistency Bugs with Bounded Black-Box Crash Testing,http://arxiv.org/abs/1810.02904v1,2018-10-05T23:04:23Z,2018-10-05T23:04:23Z,"  We present a new approach to testing file-system crash consistency: bounded
black-box crash testing (B3). B3 tests the file system in a black-box manner
using workloads of file-system operations. Since the space of possible
workloads is infinite, B3 bounds this space based on parameters such as the
number of file-system operations or which operations to include, and
exhaustively generates workloads within this bounded space. Each workload is
tested on the target file system by simulating power-loss crashes while the
workload is being executed, and checking if the file system recovers to a
correct state after each crash. B3 builds upon insights derived from our study
of crash-consistency bugs reported in Linux file systems in the last five
years. We observed that most reported bugs can be reproduced using small
workloads of three or fewer file-system operations on a newly-created file
system, and that all reported bugs result from crashes after fsync() related
system calls. We build two tools, CrashMonkey and ACE, to demonstrate the
effectiveness of this approach. Our tools are able to find 24 out of the 26
crash-consistency bugs reported in the last five years. Our tools also revealed
10 new crash-consistency bugs in widely-used, mature Linux file systems, seven
of which existed in the kernel since 2014. Our tools also found a
crash-consistency bug in a verified file system, FSCQ. The new bugs result in
severe consequences like broken rename atomicity and loss of persisted files.
","['\nJayashree Mohan\n', '\nAshlie Martinez\n', '\nSoujanya Ponnapalli\n', '\nPandian Raju\n', '\nVijay Chidambaram\n']",,,http://arxiv.org/abs/1810.02904v1,cs.OS,['cs.OS'],,,[]
Revitalizing Copybacks in Modern SSDs: Why and How,http://arxiv.org/abs/1810.04603v1,2018-10-10T15:49:52Z,2018-10-10T15:49:52Z,"  For modern flash-based SSDs, the performance overhead of internal data
migrations is dominated by the data transfer time, not by the flash program
time as in old SSDs. In order to mitigate the performance impact of data
migrations, we propose rCopyback, a restricted version of copyback. Rcopyback
works like the original copyback except that only n consecutive copybacks are
allowed. By limiting the number of successive copybacks, it guarantees that no
data reliability problem occurs when data is internally migrated using
rCopyback. In order to take a full advantage of rCopyback, we developed a
rCopyback-aware FTL, rcFTL, which intelligently decides whether rCopyback
should be used or not by exploiting varying host workloads. Our evaluation
results show that rcFTL can improve the overall I/O throughput by 54% on
average over an existing FTL which does not use copybacks.
","['\nDuwon Hong\n', '\nMyungsuk Kim\n', '\nJisung Park\n', '\nMyoungsoo Jung\n', '\nJihong Kim\n']","5 pages, 6 figures",,http://arxiv.org/abs/1810.04603v1,cs.OS,['cs.OS'],,,[]
"T-Visor: A Hypervisor for Mixed Criticality Embedded Real-time System
  with Hardware Virtualization Support",http://arxiv.org/abs/1810.05068v1,2018-10-11T15:05:44Z,2018-10-11T15:05:44Z,"  Recently, embedded systems have not only requirements for hard real-time
behavior and reliability, but also diversified functional demands, such as
network functions. To satisfy these requirements, virtualization using
hypervisors is promising for embedded systems. However, as most of existing
hypervisors are designed for general-purpose information processing systems,
they rely on large system stacks, so that they are not suitable for mixed
criticality embedded real-time systems. Even in hypervisors designed for
embedded systems, their schedulers do not consider the diversity of real-time
requirements and rapid change in scheduling theory.
  We present the design and implementation of T-Visor, a hypervisor specialized
for mixed criticality embedded real-time systems. T-Visor supports ARM
architecture and realizes full virtualization using ARM Virtualization
Extensions. To guarantee real-time behavior, T-Visor provides a flexible
scheduling framework so that developers can select the most suitable scheduling
algorithm for their systems. Our evaluation showed that it performed better
compared to Xen/ARM. From these results, we conclude that our design and
implementation are more suitable for embedded real-time systems than the
existing hypervisors.
","['\nTakumi Shimada\n', '\nTakeshi Yashiro\n', '\nKen Sakamura\n']",,,http://arxiv.org/abs/1810.05068v1,cs.OS,['cs.OS'],,,[]
Time Protection: the Missing OS Abstraction,http://arxiv.org/abs/1810.05345v2,2018-10-12T03:54:52Z,2018-10-15T23:38:53Z,"  Timing channels enable data leakage that threatens the security of computer
systems, from cloud platforms to smartphones and browsers executing untrusted
third-party code. Preventing unauthorised information flow is a core duty of
the operating system, however, present OSes are unable to prevent timing
channels. We argue that OSes must provide time protection in addition to the
established memory protection. We examine the requirements of time protection,
present a design and its implementation in the seL4 microkernel, and evaluate
its efficacy as well as performance overhead on Arm and x86 processors.
","['\nQian Ge\n', '\nYuval Yarom\n', '\nTom Chothia\n', '\nGernot Heiser\n']",,,http://arxiv.org/abs/1810.05345v2,cs.OS,['cs.OS'],,,[]
Compact NUMA-Aware Locks,http://arxiv.org/abs/1810.05600v2,2018-10-12T16:42:49Z,2019-03-01T05:13:22Z,"  Modern multi-socket architectures exhibit non-uniform memory access (NUMA)
behavior, where access by a core to data cached locally on a socket is much
faster than access to data cached on a remote socket. Prior work offers several
efficient NUMA-aware locks that exploit this behavior by keeping the lock
ownership on the same socket, thus reducing remote cache misses and
inter-socket communication. Virtually all those locks, however, are
hierarchical in their nature, thus requiring space proportional to the number
of sockets. The increased memory cost renders NUMA-aware locks unsuitable for
systems that are conscious to space requirements of their synchronization
constructs, with the Linux kernel being the chief example.
  In this work, we present a compact NUMA-aware lock that requires only one
word of memory, regardless of the number of sockets in the underlying machine.
The new lock is a variant of an efficient (NUMA-oblivious) MCS lock, and
inherits its performant features, such as local spinning and a single atomic
instruction in the acquisition path. Unlike MCS, the new lock organizes waiting
threads in two queues, one composed of threads running on the same socket as
the current lock holder, and another composed of threads running on a different
socket(s).
  We integrated the new lock in the Linux kernel's qspinlock, one of the major
synchronization constructs in the kernel. Our evaluation using both user-space
and kernel benchmarks shows that the new lock has a single-thread performance
of MCS, but significantly outperforms the latter under contention, achieving a
similar level of performance when compared to other, state-of-the-art
NUMA-aware locks that require substantially more space.
","['\nDave Dice\n', '\nAlex Kogan\n']",,,http://arxiv.org/abs/1810.05600v2,cs.OS,"['cs.OS', 'D.4.1; D.1.3']",,,[]
"Formalising Filesystems in the ACL2 Theorem Prover: an Application to
  FAT32",http://arxiv.org/abs/1810.04309v1,2018-10-10T00:35:05Z,2018-10-10T00:35:05Z,"  In this work, we present an approach towards constructing executable
specifications of existing filesystems and verifying their functional
properties in a theorem proving environment. We detail an application of this
approach to the FAT32 filesystem.
  We also detail the methodology used to build up this type of executable
specification through a series of models which incrementally add features of
the target filesystem. This methodology has the benefit of allowing the
verification effort to start from simple models which encapsulate features
common to many filesystems and which are thus suitable for reuse.
",['\nMihir Parang Mehta\nUT Austin\n'],"In Proceedings ACL2 2018, arXiv:1810.03762","EPTCS 280, 2018, pp. 18-29",http://dx.doi.org/10.4204/EPTCS.280.2,cs.LO,"['cs.LO', 'cs.OS']",10.4204/EPTCS.280.2,,['UT Austin']
Platform-Agnostic Steal-Time Measurement in a Guest Operating System,http://arxiv.org/abs/1810.01139v1,2018-10-02T09:44:32Z,2018-10-02T09:44:32Z,"  Steal time is a key performance metric for applications executed in a
virtualized environment. Steal time measures the amount of time the processor
is preempted by code outside the virtualized environment. This, in turn, allows
to compute accurately the execution time of an application inside a virtual
machine (i.e. it eliminates the time the virtual machine is suspended).
Unfortunately, this metric is only available in particular scenarios in which
the host and the guest OS are tightly coupled. Typical examples are the Xen
hypervisor and Linux-based guest OSes. In contrast, in scenarios where the
steal time is not available inside the virtualized environment, performance
measurements are, most often, incorrect.
  In this paper, we introduce a novel and platform agnostic approach to
calculate this steal time within the virtualized environment and without the
cooperation of the host OS. The theoretical execution time of a deterministic
microbenchmark is compared to its execution time in a virtualized environment.
When factoring in the virtual machine load, this solution -as simple as it is-
can compute the steal time. The preliminary results show that we are able to
compute the load of the physical processor within the virtual machine with high
accuracy.
","['\nJavier Verdu\n', '\nJuan Jose Costa\n', '\nBeatriz Otero\n', '\nEva Rodriguez\n', '\nAlex Pajuelo\n', '\nRamon Canal\n']","4 pages, 6 figures, technical report",,http://arxiv.org/abs/1810.01139v1,cs.OS,['cs.OS'],,,[]
"On the Fly Orchestration of Unikernels: Tuning and Performance
  Evaluation of Virtual Infrastructure Managers",http://arxiv.org/abs/1809.07701v1,2018-09-17T12:17:01Z,2018-09-17T12:17:01Z,"  Network operators are facing significant challenges meeting the demand for
more bandwidth, agile infrastructures, innovative services, while keeping costs
low. Network Functions Virtualization (NFV) and Cloud Computing are emerging as
key trends of 5G network architectures, providing flexibility, fast
instantiation times, support of Commercial Off The Shelf hardware and
significant cost savings. NFV leverages Cloud Computing principles to move the
data-plane network functions from expensive, closed and proprietary hardware to
the so-called Virtual Network Functions (VNFs). In this paper we deal with the
management of virtual computing resources (Unikernels) for the execution of
VNFs. This functionality is performed by the Virtual Infrastructure Manager
(VIM) in the NFV MANagement and Orchestration (MANO) reference architecture. We
discuss the instantiation process of virtual resources and propose a generic
reference model, starting from the analysis of three open source VIMs, namely
OpenStack, Nomad and OpenVIM. We improve the aforementioned VIMs introducing
the support for special-purpose Unikernels and aiming at reducing the duration
of the instantiation process. We evaluate some performance aspects of the VIMs,
considering both stock and tuned versions. The VIM extensions and performance
evaluation tools are available under a liberal open source licence.
","['\nPier Luigi Ventre\n', '\nPaolo Lungaroni\n', '\nGiuseppe Siracusano\n', '\nClaudio Pisa\n', '\nFlorian Schmidt\n', '\nFrancesco Lombardo\n', '\nStefano Salsano\n']",,,http://arxiv.org/abs/1809.07701v1,cs.DC,"['cs.DC', 'cs.NI', 'cs.OS', 'cs.PF']",,,[]
Dependency Graph Approach for Multiprocessor Real-Time Synchronization,http://arxiv.org/abs/1809.02892v1,2018-09-08T22:53:27Z,2018-09-08T22:53:27Z,"  Over the years, many multiprocessor locking protocols have been designed and
analyzed. However, the performance of these protocols highly depends on how the
tasks are partitioned and prioritized and how the resources are shared locally
and globally. This paper answers a few fundamental questions when real-time
tasks share resources in multiprocessor systems. We explore the fundamental
difficulty of the multiprocessor synchronization problem and show that a very
simplified version of this problem is ${\mathcal NP}$-hard in the strong sense
regardless of the number of processors and the underlying scheduling paradigm.
Therefore, the allowance of preemption or migration does not reduce the
computational complexity. For the positive side, we develop a dependency-graph
approach, that is specifically useful for frame-based real-time tasks, in which
all tasks have the same period and release their jobs always at the same time.
We present a series of algorithms with speedup factors between $2$ and $3$
under semi-partitioned scheduling. We further explore methodologies and
tradeoffs of preemptive against non-preemptive scheduling algorithms and
partitioned against semi-partitioned scheduling algorithms. The approach is
extended to periodic tasks under certain conditions.
","['\nJian-Jia Chen\n', '\nGeorg von der Brüggen\n', '\nJunjie Shi\n', '\nNiklas Uete\n']",accepted and to appear in IEEE Real-Time System Symposium (RTSS) 2018,,http://arxiv.org/abs/1809.02892v1,cs.OS,['cs.OS'],,,[]
"A Review of Software-Defined WLANs: Architectures and Central Control
  Mechanisms",http://arxiv.org/abs/1809.00121v1,2018-09-01T06:07:39Z,2018-09-01T06:07:39Z,"  The significant growth in the number of WiFi-enabled devices as well as the
increase in the traffic conveyed through wireless local area networks (WLANs)
necessitate the adoption of new network control mechanisms. Specifically, dense
deployment of access points, client mobility, and emerging QoS demands bring
about challenges that cannot be effectively addressed by distributed
mechanisms. Recent studies show that software-defined WLANs (SDWLANs) simplify
network control, improve QoS provisioning, and lower the deployment cost of new
network control mechanisms. In this paper, we present an overview of SDWLAN
architectures and provide a qualitative comparison in terms of features such as
programmability and virtualization. In addition, we classify and investigate
the two important classes of centralized network control mechanisms: (i)
association control (AsC) and (ii) channel assignment (ChA). We study the basic
ideas employed by these mechanisms, and in particular, we focus on the metrics
utilized and the problem formulation techniques proposed. We present a
comparison of these mechanisms and identify open research problems.
","['\nBehnam Dezfouli\n', '\nVahid Esmaeelzadeh\n', '\nJaykumar Sheth\n', '\nMarjan Radi\n']",,,http://arxiv.org/abs/1809.00121v1,cs.NI,"['cs.NI', 'cs.OS']",,,[]
"Real-time Linux communications: an evaluation of the Linux communication
  stack for real-time robotic applications",http://arxiv.org/abs/1808.10821v1,2018-08-30T15:20:56Z,2018-08-30T15:20:56Z,"  As robotics systems become more distributed, the communications between
different robot modules play a key role for the reliability of the overall
robot control. In this paper, we present a study of the Linux communication
stack meant for real-time robotic applications. We evaluate the real-time
performance of UDP based communications in Linux on multi-core embedded devices
as test platforms. We prove that, under an appropriate configuration, the Linux
kernel greatly enhances the determinism of communications using the UDP
protocol. Furthermore, we demonstrate that concurrent traffic disrupts the
bounded latencies and propose a solution by separating the real-time
application and the corresponding interrupt in a CPU.
","['\nCarlos San Vicente Gutiérrez\n', '\nLander Usategui San Juan\n', '\nIrati Zamalloa Ugarte\n', '\nVíctor Mayoral Vilches\n']",,,http://arxiv.org/abs/1808.10821v1,cs.OS,['cs.OS'],,,[]
"Profiling and Improving the Duty-Cycling Performance of Linux-based IoT
  Devices",http://arxiv.org/abs/1808.10097v2,2018-08-30T03:09:40Z,2019-01-03T20:36:43Z,"  Minimizing the energy consumption of Linux-based devices is an essential step
towards their wide deployment in various IoT scenarios. Energy saving methods
such as duty-cycling aim to address this constraint by limiting the amount of
time the device is powered on. In this work we study and improve the amount of
time a Linux-based IoT device is powered on to accomplish its tasks. We analyze
the processes of system boot up and shutdown on two platforms, the Raspberry Pi
3 and Raspberry Pi Zero Wireless, and enhance duty-cycling performance by
identifying and disabling time-consuming or unnecessary units initialized in
the userspace. We also study whether SD card speed and SD card capacity
utilization affect boot up duration and energy consumption. In addition, we
propose 'Pallex', a parallel execution framework built on top of the 'systemd
init' system to run a user application concurrently with userspace
initialization. We validate the performance impact of Pallex when applied to
various IoT application scenarios: (i) capturing an image, (ii) capturing and
encrypting an image, (iii) capturing and classifying an image using the the
k-nearest neighbor algorithm, and (iv) capturing images and sending them to a
cloud server. Our results show that system lifetime is increased by 18.3%,
16.8%, 13.9% and 30.2%, for these application scenarios, respectively.
","['\nImmanuel Amirtharaj\n', '\nTai Groot\n', '\nBehnam Dezfouli\n']",SCU's IoT Research Lab,,http://arxiv.org/abs/1808.10097v2,cs.OS,"['cs.OS', 'cs.DC', 'cs.NI', 'cs.PF']",,,[]
Runtime Analysis of Whole-System Provenance,http://arxiv.org/abs/1808.06049v2,2018-08-18T06:39:03Z,2018-08-25T05:50:43Z,"  Identifying the root cause and impact of a system intrusion remains a
foundational challenge in computer security. Digital provenance provides a
detailed history of the flow of information within a computing system,
connecting suspicious events to their root causes. Although existing
provenance-based auditing techniques provide value in forensic analysis, they
assume that such analysis takes place only retrospectively. Such post-hoc
analysis is insufficient for realtime security applications, moreover, even for
forensic tasks, prior provenance collection systems exhibited poor performance
and scalability, jeopardizing the timeliness of query responses.
  We present CamQuery, which provides inline, realtime provenance analysis,
making it suitable for implementing security applications. CamQuery is a Linux
Security Module that offers support for both userspace and in-kernel execution
of analysis applications. We demonstrate the applicability of CamQuery to a
variety of runtime security applications including data loss prevention,
intrusion detection, and regulatory compliance. In evaluation, we demonstrate
that CamQuery reduces the latency of realtime query mechanisms, while imposing
minimal overheads on system execution. CamQuery thus enables the further
deployment of provenance-based technologies to address central challenges in
computer security.
","['\nThomas Pasquier\n', '\nXueyuan Han\n', '\nThomas Moyer\n', '\nAdam Bates\n', '\nOlivier Hermant\n', '\nDavid Eyers\n', '\nJean Bacon\n', '\nMargo Seltzer\n']","16 pages, 12 figures, 25th ACM Conference on Computer and
  Communications Security 2018",,http://arxiv.org/abs/1808.06049v2,cs.CR,"['cs.CR', 'cs.OS']",,,[]
"Comparative Study of Virtual Machines and Containers for DevOps
  Developers",http://arxiv.org/abs/1808.08192v2,2018-08-17T22:35:15Z,2023-04-17T05:50:07Z,"  This paper presents a comparative study of virtual machines (VMs) and
containers for DevOps developers. The study explores the benefits and drawbacks
of each technology in terms of their functionality, performance, security, and
resource utilization. The paper examines the underlying architecture of VMs and
containers, and how they differ from each other. The study includes a series of
experiments that compare the performance and resource utilization of VMs and
containers in different scenarios. The experiments evaluate factors such as
startup time, memory usage, disk I/O, network latency, scalability, and
administrative overhead. Finally, the paper provides recommendations for DevOps
developers on which technology to choose based on their specific requirements
and use cases. Overall, the study aims to provide a comprehensive understanding
of the strengths and limitations of VMs and containers, helping developers to
make informed decisions when choosing between them.
","['\nSaurabh Deochake\n', '\nSumit Maheshwari\n', '\nRidip De\n', '\nAnish Grover\n']",16:198:553 Course Rutgers CS,,http://arxiv.org/abs/1808.08192v2,cs.OS,"['cs.OS', 'cs.NI']",,,[]
"New Analysis Techniques for Supporting Hard Real-Time Sporadic DAG Task
  Systems on Multiprocessors",http://arxiv.org/abs/1808.00017v1,2018-07-31T18:30:09Z,2018-07-31T18:30:09Z,"  The scheduling and schedulability analysis of real-time directed acyclic
graph (DAG) task systems have received much recent attention. The DAG model can
accurately represent intra-task parallelim and precedence constraints existing
in many application domains. Existing techniques show that analyzing the DAG
model is fundamentally more challenging compared to the ordinary sporadic task
model, due to the complex intra-DAG precedence constraints which may cause
rather pessimistic schedulability loss. However,such increased loss is
counter-intuitive because the DAG structure shall better exploit the
parallelism provided by the multiprocessor platform. Our observation is that
the intra-DAG precedence constraints, if not carefully considered by the
scheduling algorithm, may cause very unpredictable execution behaviors of
subtasks in a DAG and further cause pessimistic analysis. In this paper, we
present a set of novel scheduling and analysis techniques for better supporting
hard real-time sporadic DAG tasks on multiprocessors, through smartly defining
and analyzing the execution order of subtasks in each DAG. Evaluation
demonstrates that our developed utilization-based schedulability test is highly
efficient, which dramatically improves schedulability of existing
utilization-based tests by over 60% on average. Interestingly, when each DAG in
the system is an ordinary sporadic task, our test becomes identical to the
classical density test designed for the sporadic task model.
","['\nZheng Dong\n', '\nCong Liu\n']",,,http://arxiv.org/abs/1808.00017v1,cs.OS,['cs.OS'],,,[]
"A Spin-based model checking for the simple concurrent program on a
  preemptive RTOS",http://arxiv.org/abs/1808.04239v1,2018-08-07T08:21:58Z,2018-08-07T08:21:58Z,"  We adapt an existing preemptive scheduling model of RTOS kernel by eChronos
from machine-assisted proof to Spin-based model checker. The model we
constructed can be automatically verified rather than formulating proofs by
hand. Moreover, we look into the designs of a Linux-like real-time
kernel--Piko/RT and the specification of ARMv7-M architecture to reconstruct
the model, and use LTL to specify a simple concurrent
programs--consumer/producer problem during the development stage of the kernel.
We show that under the preemptive scheduling and the mechanism of ARMv7-M, the
program will not suffer from race condition, starvation, and deadlock.
","['\nChen-Kai Lin\nJim\n', '\n Ching-Chun\nJim\n', '\n Huang\n', '\nBow-Yaw Wang\n']","7 pages, 5 figures, The 24th Workshop on Compiler Techniques and
  System Software for High-Performance and Embedded Computing, 2018, Chiayi,
  Taiwan",,http://arxiv.org/abs/1808.04239v1,cs.OS,"['cs.OS', 'cs.FL', 'cs.LO']",,,"['Jim', 'Jim']"
StreamBox-TZ: Secure Stream Analytics at the Edge with TrustZone,http://arxiv.org/abs/1808.05078v2,2018-08-02T21:21:50Z,2019-06-05T20:52:44Z,"  While it is compelling to process large streams of IoT data on the cloud
edge, doing so exposes the data to a sophisticated, vulnerable software stack
on the edge and hence security threats. To this end, we advocate isolating the
data and its computations in a trusted execution environment (TEE) on the edge,
shielding them from the remaining edge software stack which we deem untrusted.
This approach faces two major challenges: (1) executing high-throughput,
low-delay stream analytics in a single TEE, which is constrained by a low
trusted computing base (TCB) and limited physical memory; (2) verifying
execution of stream analytics as the execution involves untrusted software
components on the edge. In response, we present StreamBox-TZ (SBT), a stream
analytics engine for an edge platform that offers strong data security,
verifiable results, and good performance. SBT contributes a data plane designed
and optimized for a TEE based on ARM TrustZone. It supports continuous remote
attestation for analytics correctness and result freshness while incurring low
overhead. SBT only adds 42.5 KB executable to the TCB (16% of the entire TCB).
On an octa core ARMv8 platform, it delivers the state-of-the-art performance by
processing input events up to 140 MB/sec (12M events/sec) with sub-second
delay. The overhead incurred by SBT's security mechanism is less than 25%.
","['\nHeejin Park\n', '\nShuang Zhai\n', '\nLong Lu\n', '\nFelix Xiaozhu Lin\n']",,,http://arxiv.org/abs/1808.05078v2,cs.CR,"['cs.CR', 'cs.DC', 'cs.OS']",,,[]
Regulating Access to System Sensors in Cooperating Programs,http://arxiv.org/abs/1808.05579v1,2018-08-02T21:37:29Z,2018-08-02T21:37:29Z,"  Modern operating systems such as Android, iOS, Windows Phone, and Chrome OS
support a cooperating program abstraction. Instead of placing all functionality
into a single program, programs cooperate to complete tasks requested by users.
However, untrusted programs may exploit interactions with other programs to
obtain unauthorized access to system sensors either directly or through
privileged services. Researchers have proposed that programs should only be
authorized to access system sensors on a user-approved input event, but these
methods do not account for possible delegation done by the program receiving
the user input event. Furthermore, proposed delegation methods do not enable
users to control the use of their input events accurately. In this paper, we
propose ENTRUST, a system that enables users to authorize sensor operations
that follow their input events, even if the sensor operation is performed by a
program different from the program receiving the input event. ENTRUST tracks
user input as well as delegation events and restricts the execution of such
events to compute unambiguous delegation paths to enable accurate and reusable
authorization of sensor operations. To demonstrate this approach, we implement
the ENTRUST authorization system for Android. We find, via a laboratory user
study, that attacks can be prevented at a much higher rate (54-64%
improvement); and via a field user study, that ENTRUST requires no more than
three additional authorizations per program with respect to the first-use
approach, while incurring modest performance (<1%) and memory overheads (5.5 KB
per program).
","['\nGiuseppe Petracca\n', '\nJens Grossklags\n', '\nPatrick McDaniel\n', '\nTrent Jaeger\n']",,,http://arxiv.org/abs/1808.05579v1,cs.CR,"['cs.CR', 'cs.HC', 'cs.OS']",,,[]
"Fast & Flexible IO : A Compositional Approach to Storage Construction
  for High-Performance Devices",http://arxiv.org/abs/1807.09696v1,2018-07-25T16:15:01Z,2018-07-25T16:15:01Z,"  Building storage systems has remained the domain of systems experts for many
years. They are complex and difficult to implement. Extreme care is needed to
ensure necessary guarantees of performance and operational correctness.
Furthermore, because of restrictions imposed by kernel-based designs, many
legacy implementations have traded software flexibility for performance. Their
implementation is restricted to compiled languages such as C and assembler, and
reuse tends to be difficult or constrained. Nevertheless, storage systems are
implicitly well-suited to software reuse and compositional software
construction. There are many logical functions, such as block allocation,
caching, partitioning, metadata management and so forth, that are common across
most variants of storage. In this paper, we present Comanche, an open-source
project that considers, as first-class concerns, both compositional design and
reuse, and the need for high-performance.
",['\nDaniel G. Waddington\n'],"7 pages, 4 figures",,http://arxiv.org/abs/1807.09696v1,cs.SE,"['cs.SE', 'cs.OS']",,,[]
User Manual for the Apple CoreCapture Framework,http://arxiv.org/abs/1808.07353v1,2018-07-17T16:34:43Z,2018-07-17T16:34:43Z,"  CoreCapture is Apple's primary logging and tracing framework for IEEE 802.11
on iOS and macOS. It allows users and developers to create comprehensive debug
output for analysis by Apple. In this manual, we provide an overview into the
concepts, show in detail how CoreCapture logs can be created on iOS and macOS,
and introduce the first CoreCapture dissector for Wireshark.
",['\nDavid Kreitschmann\n'],"12 pages, 12 figures",,http://arxiv.org/abs/1808.07353v1,cs.NI,"['cs.NI', 'cs.OS']",,,[]
"Parallel Architecture Hardware and General Purpose Operating System
  Co-design",http://arxiv.org/abs/1807.03546v1,2018-07-10T09:24:33Z,2018-07-10T09:24:33Z,"  Because most optimisations to achieve higher computational performance
eventually are limited, parallelism that scales is required. Parallelised
hardware alone is not sufficient, but software that matches the architecture is
required to gain best performance. For decades now, hardware design has been
guided by the basic design of existing software, to avoid the higher cost to
redesign the latter. In doing so, however, quite a variety of superior concepts
is excluded a priori. Consequently, co-design of both hardware and software is
crucial where highest performance is the goal. For special purpose application,
this co-design is common practice. For general purpose application, however, a
precondition for usability of a computer system is an operating system which is
both comprehensive and dynamic. As no such operating system has ever been
designed, a sketch for a comprehensive dynamic operating system is presented,
based on a straightforward hardware architecture to demonstrate how design
decisions regarding software and hardware do coexist and harmonise.
",['\nOskar Schirmer\n'],"66 pages, 30 figures and tables",,http://arxiv.org/abs/1807.03546v1,cs.DC,"['cs.DC', 'cs.OS']",,,[]
"TabulaROSA: Tabular Operating System Architecture for Massively Parallel
  Heterogeneous Compute Engines",http://arxiv.org/abs/1807.05308v1,2018-07-14T00:02:55Z,2018-07-14T00:02:55Z,"  The rise in computing hardware choices is driving a reevaluation of operating
systems. The traditional role of an operating system controlling the execution
of its own hardware is evolving toward a model whereby the controlling
processor is distinct from the compute engines that are performing most of the
computations. In this context, an operating system can be viewed as software
that brokers and tracks the resources of the compute engines and is akin to a
database management system. To explore the idea of using a database in an
operating system role, this work defines key operating system functions in
terms of rigorous mathematical semantics (associative array algebra) that are
directly translatable into database operations. These operations possess a
number of mathematical properties that are ideal for parallel operating systems
by guaranteeing correctness over a wide range of parallel operations. The
resulting operating system equations provide a mathematical specification for a
Tabular Operating System Architecture (TabulaROSA) that can be implemented on
any platform. Simulations of forking in TabularROSA are performed using an
associative array implementation and compared to Linux on a 32,000+ core
supercomputer. Using over 262,000 forkers managing over 68,000,000,000
processes, the simulations show that TabulaROSA has the potential to perform
operating system functions on a massively parallel scale. The TabulaROSA
simulations show 20x higher performance as compared to Linux while managing
2000x more processes in fully searchable tables.
","['\nJeremy Kepner\n', '\nRon Brightwell\n', '\nAlan Edelman\n', '\nVijay Gadepally\n', '\nHayden Jananthan\n', '\nMichael Jones\n', '\nSam Madden\n', '\nPeter Michaleas\n', '\nHamed Okhravi\n', '\nKevin Pedretti\n', '\nAlbert Reuther\n', '\nThomas Sterling\n', '\nMike Stonebraker\n']","8 pages, 6 figures, accepted at IEEE HPEC 2018",,http://dx.doi.org/10.1109/HPEC.2018.8547577,cs.DC,"['cs.DC', 'cs.DB', 'cs.OS', 'cs.PF']",10.1109/HPEC.2018.8547577,,[]
Integrating Proactive Mode Changes in Mixed Criticality Systems,http://arxiv.org/abs/1806.11431v1,2018-06-29T14:19:12Z,2018-06-29T14:19:12Z,"  In this work, we propose to integrate prediction algorithms to the scheduling
of mode changes under the Earliest-Deadline-First and Fixed-priority scheduling
in mixed-criticality real-time systems. The method proactively schedules a mode
change in the system based on state variables such as laxity, to the percentage
difference in the temporal distance between the completion time of the instance
of a task and its respective deadline, by the deadline (D) stipulated for the
task, in order to minimize deadline misses. The simulation model was validated
against an analytical model prior to the logical integration of the
Kalman-based prediction algorithm. Two study cases were presented, one covering
earliest-deadline first and the other the fixed-priority scheduling approach.
The results showed the gains in the adoption of the prediction approach for
both scheduling paradigms by presenting a significant reduction of the number
of missed deadlines for low-criticality tasks.
","['\nFlavio R Massaro Jr.\n', '\nPaulo S. Martins\n', '\nEdson L. Ursini\n']",,,http://arxiv.org/abs/1806.11431v1,cs.OS,['cs.OS'],,,[]
"LazyFP: Leaking FPU Register State using Microarchitectural
  Side-Channels",http://arxiv.org/abs/1806.07480v1,2018-06-19T21:59:59Z,2018-06-19T21:59:59Z,"  Modern processors utilize an increasingly large register set to facilitate
efficient floating point and SIMD computation. This large register set is a
burden for operating systems, as its content needs to be saved and restored
when the operating system context switches between tasks. As an optimization,
the operating system can defer the context switch of the FPU and SIMD register
set until the first instruction is executed that needs access to these
registers. Meanwhile, the old content is left in place with the hope that the
current task might not use these registers at all. This optimization is
commonly called lazy FPU context switching. To make it possible, a processor
offers the ability to toggle the availability of instructions utilizing
floating point and SIMD registers. If the instructions are turned off, any
attempt of executing them will generate a fault.
  In this paper, we present an attack that exploits lazy FPU context switching
and allows an adversary to recover the FPU and SIMD register set of arbitrary
processes or VMs. The attack works on processors that transiently execute FPU
or SIMD instructions that follow an instruction generating the fault indicating
the first use of FPU or SIMD instructions. On operating systems using lazy FPU
context switching, the FPU and SIMD register content of other processes or
virtual machines can then be reconstructed via cache side effects.
  With SIMD registers not only being used for cryptographic computation, but
also increasingly for simple operations, such as copying memory, we argue that
lazy FPU context switching is a dangerous optimization that needs to be turned
off in all operating systems, if there is a chance that they run on affected
processors.
","['\nJulian Stecklina\n', '\nThomas Prescher\n']",,,http://arxiv.org/abs/1806.07480v1,cs.OS,"['cs.OS', 'cs.AR', 'cs.CR']",,,[]
"Blocking time under basic priority inheritance: Polynomial bound and
  exact computation",http://arxiv.org/abs/1806.01589v2,2018-06-05T10:03:40Z,2018-06-11T14:25:54Z,"  The Priority Inheritance Protocol (PIP) is arguably the best-known protocol
for resource sharing under real-time constraints. Its importance in modern
applications is undisputed. Nevertheless, because jobs may be blocked under PIP
for a variety of reasons, determining a job's maximum blocking time could be
difficult, and thus far no exact method has been proposed that does it.
Existing analysis methods are inefficient, inaccurate, and of limited
applicability. This article proposes a new characterization of the problem,
thus allowing a polynomial method for bounding the blocking time, and an exact,
optimally efficient method for blocking time computation under priority
inheritance that have a general applicability.
","['\nPaolo Torroni\n', '\nZeynep Kiziltan\n', '\nEugenio Faldella\n']",,,http://arxiv.org/abs/1806.01589v2,cs.OS,['cs.OS'],,,[]
Datacenter RPCs can be General and Fast,http://arxiv.org/abs/1806.00680v2,2018-06-02T18:05:28Z,2019-01-15T01:47:04Z,"  It is commonly believed that datacenter networking software must sacrifice
generality to attain high performance. The popularity of specialized
distributed systems designed specifically for niche technologies such as RDMA,
lossless networks, FPGAs, and programmable switches testifies to this belief.
In this paper, we show that such specialization is not necessary. eRPC is a new
general-purpose remote procedure call (RPC) library that offers performance
comparable to specialized systems, while running on commodity CPUs in
traditional datacenter networks based on either lossy Ethernet or lossless
fabrics. eRPC performs well in three key metrics: message rate for small
messages; bandwidth for large messages; and scalability to a large number of
nodes and CPU cores. It handles packet loss, congestion, and background request
execution. In microbenchmarks, one CPU core can handle up to 10 million small
RPCs per second, or send large messages at 75 Gbps. We port a production-grade
implementation of Raft state machine replication to eRPC without modifying the
core Raft source code. We achieve 5.5 microseconds of replication latency on
lossy Ethernet, which is faster than or comparable to specialized replication
systems that use programmable switches, FPGAs, or RDMA.
","['\nAnuj Kalia\n', '\nMichael Kaminsky\n', '\nDavid G. Andersen\n']",Updated to NSDI 2019 version,,http://arxiv.org/abs/1806.00680v2,cs.OS,['cs.OS'],,,[]
"An Operating System Level Data Migration Scheme in Hybrid DRAM-NVM
  Memory Architecture",http://arxiv.org/abs/1805.02514v1,2018-05-04T07:36:09Z,2018-05-04T07:36:09Z,"  With the emergence of Non-Volatile Memories (NVMs) and their shortcomings
such as limited endurance and high power consumption in write requests, several
studies have suggested hybrid memory architecture employing both Dynamic Random
Access Memory (DRAM) and NVM in a memory system. By conducting a comprehensive
experiments, we have observed that such studies lack to consider very important
aspects of hybrid memories including the effect of: a) data migrations on
performance, b) data migrations on power, and c) the granularity of data
migration. This paper presents an efficient data migration scheme at the
Operating System level in a hybrid DRAMNVM memory architecture. In the proposed
scheme, two Least Recently Used (LRU) queues, one for DRAM section and one for
NVM section, are used for the sake of data migration. With careful
characterization of the workloads obtained from PARSEC benchmark suite, the
proposed scheme prevents unnecessary migrations and only allows migrations
which benefits the system in terms of power and performance. The experimental
results show that the proposed scheme can reduce the power consumption up to
79% compared to DRAM-only memory and up to 48% compared to the state-of-the art
techniques.
","['\nReza Salkhordeh\n', '\nHossein Asadi\n']",,DATE 2016,http://arxiv.org/abs/1805.02514v1,cs.OS,['cs.OS'],,,[]
"Mosaic: An Application-Transparent Hardware-Software Cooperative Memory
  Manager for GPUs",http://arxiv.org/abs/1804.11265v1,2018-04-30T15:08:54Z,2018-04-30T15:08:54Z,"  Modern GPUs face a trade-off on how the page size used for memory management
affects address translation and demand paging. Support for multiple page sizes
can help relax the page size trade-off so that address translation and demand
paging optimizations work together synergistically. However, existing page
coalescing and splintering policies require costly base page migrations that
undermine the benefits multiple page sizes provide. In this paper, we observe
that GPGPU applications present an opportunity to support multiple page sizes
without costly data migration, as the applications perform most of their memory
allocation en masse (i.e., they allocate a large number of base pages at once).
We show that this en masse allocation allows us to create intelligent memory
allocation policies which ensure that base pages that are contiguous in virtual
memory are allocated to contiguous physical memory pages. As a result,
coalescing and splintering operations no longer need to migrate base pages.
  We introduce Mosaic, a GPU memory manager that provides
application-transparent support for multiple page sizes. Mosaic uses base pages
to transfer data over the system I/O bus, and allocates physical memory in a
way that (1) preserves base page contiguity and (2) ensures that a large page
frame contains pages from only a single memory protection domain. This
mechanism allows the TLB to use large pages, reducing address translation
overhead. During data transfer, this mechanism enables the GPU to transfer only
the base pages that are needed by the application over the system I/O bus,
keeping demand paging overhead low.
","['\nRachata Ausavarungnirun\n', '\nJoshua Landgraf\n', '\nVance Miller\n', '\nSaugata Ghose\n', '\nJayneel Gandhi\n', '\nChristopher J. Rossbach\n', '\nOnur Mutlu\n']",,,http://arxiv.org/abs/1804.11265v1,cs.OS,"['cs.OS', 'cs.AR']",,,[]
"iReplayer: In-situ and Identical Record-and-Replay for Multithreaded
  Applications",http://arxiv.org/abs/1804.01226v1,2018-04-04T03:26:13Z,2018-04-04T03:26:13Z,"  Reproducing executions of multithreaded programs is very challenging due to
many intrinsic and external non-deterministic factors. Existing RnR systems
achieve significant progress in terms of performance overhead, but none targets
the in-situ setting, in which replay occurs within the same process as the
recording process. Also, most existing work cannot achieve identical replay,
which may prevent the reproduction of some errors.
  This paper presents iReplayer, which aims to identically replay multithreaded
programs in the original process (under the ""in-situ"" setting). The novel
in-situ and identical replay of iReplayer makes it more likely to reproduce
errors, and allows it to directly employ debugging mechanisms (e.g.
watchpoints) to aid failure diagnosis. Currently, iReplayer only incurs 3%
performance overhead on average, which allows it to be always enabled in the
production environment. iReplayer enables a range of possibilities, and this
paper presents three examples: two automatic tools for detecting buffer
overflows and use-after-free bugs, and one interactive debugging tool that is
integrated with GDB.
","['\nHongyu Liu\n', '\nSam Silvestro\n', '\nWei Wang\n', '\nChen Tian\n', '\nTongping Liu\n']","16 pages, 5 figures, to be published at PLDI'18",,http://dx.doi.org/10.1145/3192366.3192380,cs.OS,['cs.OS'],10.1145/3192366.3192380,,[]
"Reactive NaN Repair for Applying Approximate Memory to Numerical
  Applications",http://arxiv.org/abs/1804.00705v1,2018-03-26T03:52:05Z,2018-03-26T03:52:05Z,"  Applications in the AI and HPC fields require much memory capacity, and the
amount of energy consumed by main memory of server machines is ever increasing.
Energy consumption of main memory can be greatly reduced by applying
approximate computing in exchange for increased bit error rates. AI and HPC
applications are to some extent robust to bit errors because small numerical
errors are amortized by their iterative nature. However, a single occurrence of
a NaN due to bit-flips corrupts the whole calculation result. The issue is that
fixing every bit-flip using ECC incurs too much overhead because the bit error
rate is much higher than in normal environments. We propose a low-overhead
method to fix NaNs when approximate computing is applied to main memory. The
main idea is to reactively repair NaNs while leaving other non-fatal numerical
errors as-is to reduce the overhead. We implemented a prototype by leveraging
floating-point exceptions of x86 CPUs, and the preliminary evaluations showed
that our method incurs negligible overhead.
","['\nShinsuke Hamada\n', '\nSoramichi Akiyama\n', '\nMitaro Namiki\n']","Presented in the 8th Workshop on Systems for Multi-core and
  Heterogeneous Architectures (SFMA), co-located with EuroSys'18",,http://arxiv.org/abs/1804.00705v1,cs.DC,"['cs.DC', 'cs.ET', 'cs.OS']",,,[]
The Secure Machine: Efficient Secure Execution On Untrusted Platforms,http://arxiv.org/abs/1803.03951v1,2018-03-11T12:09:27Z,2018-03-11T12:09:27Z,"  In this work we present the Secure Machine, SeM for short, a CPU architecture
extension for secure computing. SeM uses a small amount of in-chip additional
hardware that monitors key communication channels inside the CPU chip, and only
acts when required. SeM provides confidentiality and integrity for a secure
program without trusting the platform software or any off-chip hardware. SeM
supports existing binaries of single- and multi-threaded applications running
on single- or multi-core, multi-CPU. The performance reduction caused by it is
only few percent, most of which is due to the memory encryption layer that is
commonly used in many secure architectures.
  We also developed SeM-Prepare, a software tool that automatically instruments
existing applications (binaries) with additional instructions so they can be
securely executed on our architecture without requiring any programming efforts
or the availability of the desired program`s source code.
  To enable secure data sharing in shared memory environments, we developed
Secure Distributed Shared Memory (SDSM), an efficient (time and memory)
algorithm for allowing thousands of compute nodes to share data securely while
running on an untrusted computing environment. SDSM shows a negligible
reduction in performance, and it requires negligible and hardware resources. We
developed Distributed Memory Integrity Trees, a method for enhancing single
node integrity trees for preserving the integrity of a distributed application
running on an untrusted computing environment. We show that our method is
applicable to existing single node integrity trees such as Merkle Tree, Bonsai
Merkle Tree, and Intel`s SGX memory integrity engine. All these building blocks
may be used together to form a practical secure system, and some can be used in
conjunction with other secure systems.
","['\nOfir Shwartz\n', '\nYitzhak Birk\n']","A PhD thesis, to appear at the Technion's library",,http://arxiv.org/abs/1803.03951v1,cs.CR,"['cs.CR', 'cs.AR', 'cs.DC', 'cs.OS']",,,[]
"Push Forward: Global Fixed-Priority Scheduling of Arbitrary-Deadline
  Sporadic Task Systems",http://arxiv.org/abs/1802.10376v1,2018-02-28T11:51:26Z,2018-02-28T11:51:26Z,"  The sporadic task model is often used to analyze recurrent execution of
identical tasks in real-time systems. A sporadic task defines an infinite
sequence of task instances, also called jobs, that arrive under the minimum
inter-arrival time constraint. To ensure the system safety, timeliness has to
be guaranteed in addition to functional correctness, i.e., all jobs of all
tasks have to be finished before the job deadlines. We focus on analyzing
arbitrary-deadline task sets on a homogeneous (identical) multiprocessor system
under any given global fixed-priority scheduling approach and provide a series
of schedulability tests with different tradeoffs between their time complexity
and their accuracy. Under the arbitrary-deadline setting, the relative deadline
of a task can be longer than the minimum inter-arrival time of the jobs of the
task. We show that global deadline-monotonic (DM) scheduling has a speedup
bound of $3-1/M$ against any optimal scheduling algorithms, where $M$ is the
number of identical processors, and prove that this bound is asymptotically
tight.
","['\nJian-Jia Chen\n', '\nGeorg von der Brüggen\n', '\nNiklas Ueter\n']",,,http://arxiv.org/abs/1802.10376v1,cs.OS,"['cs.OS', 'cs.DC']",,,[]
End-to-end Analysis and Design of a Drone Flight Controller,http://arxiv.org/abs/1802.05802v1,2018-02-15T23:38:27Z,2018-02-15T23:38:27Z,"  Timing guarantees are crucial to cyber-physical applications that must bound
the end-to-end delay between sensing, processing and actuation. For example, in
a flight controller for a multirotor drone, the data from a gyro or inertial
sensor must be gathered and processed to determine the attitude of the
aircraft. Sensor data fusion is followed by control decisions that adjust the
flight of a drone by altering motor speeds. If the processing pipeline between
sensor input and actuation is not bounded, the drone will lose control and
possibly fail to maintain flight.
  Motivated by the implementation of a multithreaded drone flight controller on
the Quest RTOS, we develop a composable pipe model based on the system's task,
scheduling and communication abstractions. This pipe model is used to analyze
two semantics of end-to-end time: reaction time and freshness time. We also
argue that end-to-end timing properties should be factored in at the early
stage of application design. Thus, we provide a mathematical framework to
derive feasible task periods that satisfy both a given set of end-to-end timing
constraints and the schedulability requirement. We demonstrate the
applicability of our design approach by using it to port the Cleanflight flight
controller firmware to Quest on the Intel Aero board. Experiments show that
Cleanflight ported to Quest is able to achieve end-to-end latencies within the
predicted time bounds derived by analysis.
","['\nZhuoqun Cheng\n', '\nRichard West\n', '\nCraig Einstein\n']",,,http://arxiv.org/abs/1802.05802v1,cs.SY,"['cs.SY', 'cs.OS']",,,[]
"KASR: A Reliable and Practical Approach to Attack Surface Reduction of
  Commodity OS Kernels",http://arxiv.org/abs/1802.07062v2,2018-02-20T11:08:31Z,2018-11-29T22:55:38Z,"  Commodity OS kernels have broad attack surfaces due to the large code base
and the numerous features such as device drivers. For a real-world use case
(e.g., an Apache Server), many kernel services are unused and only a small
amount of kernel code is used. Within the used code, a certain part is invoked
only at runtime while the rest are executed at startup and/or shutdown phases
in the kernel's lifetime run. In this paper, we propose a reliable and
practical system, named KASR, which transparently reduces attack surfaces of
commodity OS kernels at runtime without requiring their source code. The KASR
system, residing in a trusted hypervisor, achieves the attack surface reduction
through a two-step approach: (1) reliably depriving unused code of executable
permissions, and (2) transparently segmenting used code and selectively
activating them. We implement a prototype of KASR on Xen-4.8.2 hypervisor and
evaluate its security effectiveness on Linux kernel-4.4.0-87-generic. Our
evaluation shows that KASR reduces the kernel attack surface by 64% and trims
off 40% of CVE vulnerabilities. Besides, KASR successfully detects and blocks
all 6 real-world kernel rootkits. We measure its performance overhead with
three benchmark tools (i.e., SPECINT, httperf and bonnie++). The experimental
results indicate that KASR imposes less than 1% performance overhead (compared
to an unmodified Xen hypervisor) on all the benchmarks.
","['\nZhi Zhang\n', '\nYueqiang Cheng\n', '\nSurya Nepal\n', '\nDongxi Liu\n', '\nQingni Shen\n', '\nFethi Rabhi\n']","The work has been accepted at the 21st International Symposium on
  Research in Attacks, Intrusions, and Defenses 2018",,http://dx.doi.org/10.1007/978-3-030-00470-5_32,cs.CR,"['cs.CR', 'cs.OS']",10.1007/978-3-030-00470-5_32,,[]
vLibOS: Babysitting OS Evolution with a Virtualized Library OS,http://arxiv.org/abs/1801.07880v1,2018-01-24T07:11:22Z,2018-01-24T07:11:22Z,"  Many applications have service requirements that are not easily met by
existing operating systems. Real-time and security-critical tasks, for example,
often require custom OSes to meet their needs. However, development of special
purpose OSes is a time-consuming and difficult exercise. Drivers, libraries and
applications have to be written from scratch or ported from existing sources.
Many researchers have tackled this problem by developing ways to extend
existing systems with application-specific services. However, it is often
difficult to ensure an adequate degree of separation between legacy and new
services, especially when security and timing requirements are at stake.
Virtualization, for example, supports logical isolation of separate guest
services, but suffers from inadequate temporal isolation of time-critical code
required for real-time systems. This paper presents vLibOS, a master-slave
paradigm for new systems, whose services are built on legacy code that is
temporally and spatially isolated in separate VM domains. Existing OSes are
treated as sandboxed libraries, providing legacy services that are requested by
inter-VM calls, which execute with the time budget of the caller. We evaluate a
real-time implementation of vLibOS. Empirical results show that vLibOS achieves
as much as a 50\% reduction in performance slowdown for real-time threads, when
competing for a shared memory bus with a Linux VM.
","['\nYing Ye\n', '\nZhuoqun Cheng\n', '\nSoham Sinha\n', '\nRichard West\n']",,,http://arxiv.org/abs/1801.07880v1,cs.OS,['cs.OS'],,,[]
"Size-aware Sharding For Improving Tail Latencies in In-memory Key-value
  Stores",http://arxiv.org/abs/1802.00696v1,2018-02-02T14:23:00Z,2018-02-02T14:23:00Z,"  This paper introduces the concept of size-aware sharding to improve tail
latencies for in-memory key-value stores, and describes its implementation in
the Minos key-value store. Tail latencies are crucial in distributed
applications with high fan-out ratios, because overall response time is
determined by the slowest response. Size-aware sharding distributes requests
for keys to cores according to the size of the item associated with the key. In
particular, requests for small and large items are sent to disjoint subsets of
cores. Size-aware sharding improves tail latencies by avoiding head-of-line
blocking, in which a request for a small item gets queued behind a request for
a large item. Alternative size-unaware approaches to sharding, such as
keyhash-based sharding, request dispatching and stealing do not avoid
head-of-line blocking, and therefore exhibit worse tail latencies. The
challenge in implementing size-aware sharding is to maintain high throughput by
avoiding the cost of software dispatching and by achieving load balancing
between different cores. Minos uses hardware dispatch for all requests for
small items, which form the very large majority of all requests. It achieves
load balancing by adapting the number of cores handling requests for small and
large items to their relative presence in the workload. We compare Minos to
three state-of-the-art designs of in-memory KV stores. Compared to its closest
competitor, Minos achieves a 99th percentile latency that is up to two orders
of magnitude lower. Put differently, for a given value for the 99th percentile
latency equal to 10 times the mean service time, Minos achieves a throughput
that is up to 7.4 times higher.
","['\nDiego Didona\n', '\nWilly Zwaenepoel\n']",,,http://arxiv.org/abs/1802.00696v1,cs.DB,"['cs.DB', 'cs.OS']",,,[]
Virtual Breakpoints for x86/64,http://arxiv.org/abs/1801.09250v3,2018-01-28T17:09:14Z,2019-08-20T18:53:28Z,"  Efficient, reliable trapping of execution in a program at the desired
location is a linchpin technique for dynamic malware analysis. The progression
of debuggers and malware is akin to a game of cat and mouse - each are
constantly in a state of trying to thwart one another. At the core of most
efficient debuggers today is a combination of virtual machines and traditional
binary modification breakpoints (int3). In this paper, we present a design for
Virtual Breakpoints. a modification to the x86 MMU which brings breakpoint
management into hardware alongside page tables. In this paper we demonstrate
the fundamental abstraction failures of current trapping methods, and design a
new mechanism from the hardware up. This design incorporates lessons learned
from 50 years of virtualization and debugger design to deliver fast, reliable
trapping without the pitfalls of traditional binary modification.
",['\nGregory Michael Price\n'],"12 Pages, Presented at BSides Las Vegas 2019",,http://arxiv.org/abs/1801.09250v3,cs.OS,"['cs.OS', 'cs.CR']",,,[]
Representation Learning for Resource Usage Prediction,http://arxiv.org/abs/1802.00673v1,2018-02-02T13:21:13Z,2018-02-02T13:21:13Z,"  Creating a model of a computer system that can be used for tasks such as
predicting future resource usage and detecting anomalies is a challenging
problem. Most current systems rely on heuristics and overly simplistic
assumptions about the workloads and system statistics. These heuristics are
typically a one-size-fits-all solution so as to be applicable in a wide range
of applications and systems environments.
  With this paper, we present our ongoing work of integrating systems telemetry
ranging from standard resource usage statistics to kernel and library calls of
applications into a machine learning model. Intuitively, such a ML model
approximates, at any point in time, the state of a system and allows us to
solve tasks such as resource usage prediction and anomaly detection. To achieve
this goal, we leverage readily-available information that does not require any
changes to the applications run on the system. We train recurrent neural
networks to learn a model of the system under consideration. As a proof of
concept, we train models specifically to predict future resource usage of
running applications.
","['\nFlorian Schmidt\n', '\nMathias Niepert\n', '\nFelipe Huici\n']","3 pages, 2 figures, SysML 2018",,http://arxiv.org/abs/1802.00673v1,cs.DC,"['cs.DC', 'cs.LG', 'cs.OS']",,,[]
"Mirrored and Hybrid Disk Arrays: Organization, Scheduling, Reliability,
  and Performance",http://arxiv.org/abs/1801.08873v1,2018-01-26T15:59:51Z,2018-01-26T15:59:51Z,"  Basic mirroring (BM) classified as RAID level 1 replicates data on two disks,
thus doubling disk access bandwidth for read requests. RAID1/0 is an array of
BM pairs with balanced loads due to striping. When a disk fails the read load
on its pair is doubled, which results in halving the maximum attainable
bandwidth. We review RAID1 organizations which attain a balanced load upon disk
failure, but as shown by reliability analysis tend to be less reliable than
RAID1/0. Hybrid disk arrays which store XORed instead of replicated data tend
to have a higher reliability than mirrored disks, but incur a higher overhead
in updating data. Read request response time can be improved by processing them
at a higher priority than writes, since they have a direct effect on
application response time. Shortest seek distance and affinity based routing
both shorten seek time. Anticipatory arm placement places arms optimally to
minimize the seek distance. The analysis of RAID1 in normal, degraded, and
rebuild mode is provided to quantify RAID1/0 performance. We compare the
reliability of mirrored disk organizations against each other and hybrid disks
and erasure coded disk arrays.
",['\nAlexander Thomasian\n'],,,http://arxiv.org/abs/1801.08873v1,cs.DC,"['cs.DC', 'cs.OS', 'cs.PF']",,,[]
Elevating commodity storage with the SALSA host translation layer,http://arxiv.org/abs/1801.05637v2,2018-01-17T12:28:30Z,2019-01-10T12:39:27Z,"  To satisfy increasing storage demands in both capacity and performance,
industry has turned to multiple storage technologies, including Flash SSDs and
SMR disks. These devices employ a translation layer that conceals the
idiosyncrasies of their mediums and enables random access. Device translation
layers are, however, inherently constrained: resources on the drive are scarce,
they cannot be adapted to application requirements, and lack visibility across
multiple devices. As a result, performance and durability of many storage
devices is severely degraded.
  In this paper, we present SALSA: a translation layer that executes on the
host and allows unmodified applications to better utilize commodity storage.
SALSA supports a wide range of single- and multi-device optimizations and,
because is implemented in software, can adapt to specific workloads. We
describe SALSA's design, and demonstrate its significant benefits using
microbenchmarks and case studies based on three applications: MySQL, the Swift
object store, and a video server.
","['\nNikolas Ioannou\n', '\nKornilios Kourtis\n', '\nIoannis Koltsidas\n']","Presented at 2018 IEEE 26th International Symposium on Modeling,
  Analysis, and Simulation of Computer and Telecommunication Systems (MASCOTS)",,http://dx.doi.org/10.1109/MASCOTS.2018.00035,cs.OS,['cs.OS'],10.1109/MASCOTS.2018.00035,,[]
Shai: Enforcing Data-Specific Policies with Near-Zero Runtime Overhead,http://arxiv.org/abs/1801.04565v1,2018-01-14T14:31:46Z,2018-01-14T14:31:46Z,"  Data retrieval systems such as online search engines and online social
networks must comply with the privacy policies of personal and selectively
shared data items, regulatory policies regarding data retention and censorship,
and the provider's own policies regarding data use. Enforcing these policies is
difficult and error-prone. Systematic techniques to enforce policies are either
limited to type-based policies that apply uniformly to all data of the same
type, or incur significant runtime overhead.
  This paper presents Shai, the first system that systematically enforces
data-specific policies with near-zero overhead in the common case. Shai's key
idea is to push as many policy checks as possible to an offline, ahead-of-time
analysis phase, often relying on predicted values of runtime parameters such as
the state of access control lists or connected users' attributes. Runtime
interception is used sparingly, only to verify these predictions and to make
any remaining policy checks. Our prototype implementation relies on efficient,
modern OS primitives for sandboxing and isolation. We present the design of
Shai and quantify its overheads on an experimental data indexing and search
pipeline based on the popular search engine Apache Lucene.
","['\nEslam Elnikety\n', '\nDeepak Garg\n', '\nPeter Druschel\n']",,,http://arxiv.org/abs/1801.04565v1,cs.CR,"['cs.CR', 'cs.OS']",,,[]
"Connecting the World of Embedded Mobiles: The RIOT Approach to
  Ubiquitous Networking for the Internet of Things",http://arxiv.org/abs/1801.02833v1,2018-01-09T08:28:11Z,2018-01-09T08:28:11Z,"  The Internet of Things (IoT) is rapidly evolving based on low-power compliant
protocol standards that extend the Internet into the embedded world. Pioneering
implementations have proven it is feasible to inter-network very constrained
devices, but had to rely on peculiar cross-layered designs and offer a
minimalistic set of features. In the long run, however, professional use and
massive deployment of IoT devices require full-featured, cleanly composed, and
flexible network stacks.
  This paper introduces the networking architecture that turns RIOT into a
powerful IoT system, to enable low-power wireless scenarios. RIOT networking
offers (i) a modular architecture with generic interfaces for plugging in
drivers, protocols, or entire stacks, (ii) support for multiple heterogeneous
interfaces and stacks that can concurrently operate, and (iii) GNRC, its
cleanly layered, recursively composed default network stack. We contribute an
in-depth analysis of the communication performance and resource efficiency of
RIOT, both on a micro-benchmarking level as well as by comparing IoT
communication across different platforms. Our findings show that, though it is
based on significantly different design trade-offs, the networking subsystem of
RIOT achieves a performance equivalent to that of Contiki and TinyOS, the two
operating systems which pioneered IoT software platforms.
","['\nMartine Lenders\n', '\nPeter Kietzmann\n', '\nOliver Hahm\n', '\nHauke Petersen\n', '\nCenk Gündoğan\n', '\nEmmanuel Baccelli\n', '\nKaspar Schleiser\n', '\nThomas C. Schmidt\n', '\nMatthias Wählisch\n']",,,http://arxiv.org/abs/1801.02833v1,cs.NI,"['cs.NI', 'cs.OS']",,,[]
Protecting real-time GPU kernels on integrated CPU-GPU SoC platforms,http://arxiv.org/abs/1712.08738v3,2017-12-23T08:44:28Z,2018-04-27T02:19:42Z,"  Integrated CPU-GPU architecture provides excellent acceleration capabilities
for data parallel applications on embedded platforms while meeting the size,
weight and power (SWaP) requirements. However, sharing of main memory between
CPU applications and GPU kernels can severely affect the execution of GPU
kernels and diminish the performance gain provided by GPU. For example, in the
NVIDIA Tegra K1 platform which has the integrated CPU-GPU architecture, we
noticed that in the worst case scenario, the GPU kernels can suffer as much as
4X slowdown in the presence of co-running memory intensive CPU applications
compared to their solo execution. In this paper, we propose a software
mechanism, which we call BWLOCK++, to protect the performance of GPU kernels
from co-scheduled memory intensive CPU applications.
","['\nWaqar Ali\n', '\nHeechul Yun\n']",This paper will be published at ECRTS-2018,,http://arxiv.org/abs/1712.08738v3,cs.PF,"['cs.PF', 'cs.OS']",,,[]
"Migrate when necessary: toward partitioned reclaiming for soft real-time
  tasks",http://arxiv.org/abs/1712.06276v1,2017-12-18T07:32:19Z,2017-12-18T07:32:19Z,"  This paper presents a new strategy for scheduling soft real-time tasks on
multiple identical cores. The proposed approach is based on partitioned CPU
reservations and it uses a reclaiming mechanism to reduce the number of missed
deadlines. We introduce the possibility for a task to temporarily migrate to
another, less charged, CPU when it has exhausted the reserved bandwidth on its
allocated CPU. In addition, we propose a simple load balancing method to
decrease the number of deadlines missed by the tasks. The proposed algorithm
has been evaluated through simulations, showing its effectiveness (compared to
other multi-core reclaiming approaches) and comparing the performance of
different partitioning heuristics (Best Fit, Worst Fit and First Fit).
","['\nHoussam Eddine Zahaf\nCRIStAL\n', '\nGiuseppe Lipari\nCRIStAL\n', '\nLuca Abeni\nCRIStAL\n', '\nHoussam-Eddine Zahaf\nCRIStAL\n']",,"Proceedings of International Conference on Real-Time Networks and
  Systems, 2017, 10, pp.1-24",http://dx.doi.org/10.1145/3139258.3139280,cs.OS,['cs.OS'],10.1145/3139258.3139280,,"['CRIStAL', 'CRIStAL', 'CRIStAL', 'CRIStAL']"
EmLog: Tamper-Resistant System Logging for Constrained Devices with TEEs,http://arxiv.org/abs/1712.03943v3,2017-12-11T18:49:08Z,2017-12-19T03:20:19Z,"  Remote mobile and embedded devices are used to deliver increasingly impactful
services, such as medical rehabilitation and assistive technologies. Secure
system logging is beneficial in these scenarios to aid audit and forensic
investigations particularly if devices bring harm to end-users. Logs should be
tamper-resistant in storage, during execution, and when retrieved by a trusted
remote verifier. In recent years, Trusted Execution Environments (TEEs) have
emerged as the go-to root of trust on constrained devices for isolated
execution of sensitive applications. Existing TEE-based logging systems,
however, focus largely on protecting server-side logs and offer little
protection to constrained source devices. In this paper, we introduce EmLog --
a tamper-resistant logging system for constrained devices using the
GlobalPlatform TEE. EmLog provides protection against complex software
adversaries and offers several additional security properties over past
schemes. The system is evaluated across three log datasets using an
off-the-shelf ARM development board running an open-source,
GlobalPlatform-compliant TEE. On average, EmLog runs with low run-time memory
overhead (1MB heap and stack), 430--625 logs/second throughput, and five-times
persistent storage overhead versus unprotected logs.
","['\nCarlton Shepherd\n', '\nRaja Naeem Akram\n', '\nKonstantinos Markantonakis\n']","Accepted at the 11th IFIP International Conference on Information
  Security Theory and Practice (WISTP '17)",,http://arxiv.org/abs/1712.03943v3,cs.CR,"['cs.CR', 'cs.OS']",,,[]
Reservation-Based Federated Scheduling for Parallel Real-Time Tasks,http://arxiv.org/abs/1712.05040v1,2017-12-13T23:14:28Z,2017-12-13T23:14:28Z,"  This paper considers the scheduling of parallel real-time tasks with
arbitrary-deadlines. Each job of a parallel task is described as a directed
acyclic graph (DAG). In contrast to prior work in this area, where
decomposition-based scheduling algorithms are proposed based on the
DAG-structure and inter-task interference is analyzed as self-suspending
behavior, this paper generalizes the federated scheduling approach. We propose
a reservation-based algorithm, called reservation-based federated scheduling,
that dominates federated scheduling. We provide general constraints for the
design of such systems and prove that reservation-based federated scheduling
has a constant speedup factor with respect to any optimal DAG task scheduler.
Furthermore, the presented algorithm can be used in conjunction with any
scheduler and scheduling analysis suitable for ordinary arbitrary-deadline
sporadic task sets, i.e., without parallelism.
","['\nNiklas Ueter\n', '\nGeorg von der Brüggen\n', '\nJian-Jia Chen\n', '\nJing Li\n', '\nKunal Agrawal\n']",,,http://arxiv.org/abs/1712.05040v1,cs.OS,"['cs.OS', 'cs.DC']",,,[]
"Implementation of an Android Framework for USB storage access without
  root rights",http://arxiv.org/abs/1711.08139v1,2017-11-22T05:47:15Z,2017-11-22T05:47:15Z,"  This bachelor thesis describes the implementation of an Android framework to
access mass storage devices over the USB interface of a smartphone. First the
basics of USB (i.e. interfaces, endpoints and USB On the go) and accessing USB
devices via the official Android API are discussed. Next the USB mass storage
class is explained, which was de- signed by the USB-IF to access mobile mass
storage like USB pen drives or external HDDs. For communication with mass
storage devices, most important are the bulk-only transfer and the SCSI
transparent command set. Furthermore file systems, for accessing directo- ries
and files, are described. This thesis focuses on the FAT32 file system from
Microsoft, because it is the most commonly used file system on such devices.
After the theory part it is time to look at the implementation of the
framework. In this section, the first concern is the purpose in general. Then
the architecture of the framework and the actual implementation are presented.
Important parts are discussed in detail. The thesis finishes with an overview
of the test results on various Android devices, a short conclusion and an
outlook to future developments. Moreover the current status of the developed
framework is visualized.
",['\nMagnus Jahnen\n'],,,http://arxiv.org/abs/1711.08139v1,cs.OS,['cs.OS'],,,[]
Software Distribution Transparency and Auditability,http://arxiv.org/abs/1711.07278v1,2017-11-20T12:17:23Z,2017-11-20T12:17:23Z,"  A large user base relies on software updates provided through package
managers. This provides a unique lever for improving the security of the
software update process. We propose a transparency system for software updates
and implement it for a widely deployed Linux package manager, namely APT. Our
system is capable of detecting targeted backdoors without producing overhead
for maintainers. In addition, in our system, the availability of source code is
ensured, the binding between source and binary code is verified using
reproducible builds, and the maintainer responsible for distributing a specific
package can be identified. We describe a novel ""hidden version"" attack against
current software transparency systems and propose as well as integrate a
suitable defense. To address equivocation attacks by the transparency log
server, we introduce tree root cross logging, where the log's Merkle tree root
is submitted into a separately operated log server. This significantly relaxes
the inter-operator cooperation requirements compared to other systems. Our
implementation is evaluated by replaying over 3000 updates of the Debian
operating system over the course of two years, demonstrating its viability and
identifying numerous irregularities.
","['\nBenjamin Hof\n', '\nGeorg Carle\n']",,,http://arxiv.org/abs/1711.07278v1,cs.CR,"['cs.CR', 'cs.OS']",,,[]
"A Design-Space Exploration for Allocating Security Tasks in Multicore
  Real-Time Systems",http://arxiv.org/abs/1711.04808v1,2017-11-13T19:30:22Z,2017-11-13T19:30:22Z,"  The increased capabilities of modern real-time systems (RTS) expose them to
various security threats. Recently, frameworks that integrate security tasks
without perturbing the real-time tasks have been proposed, but they only target
single core systems. However, modern RTS are migrating towards multicore
platforms. This makes the problem of integrating security mechanisms more
complex, as designers now have multiple choices for where to allocate the
security tasks. In this paper we propose HYDRA, a design space exploration
algorithm that finds an allocation of security tasks for multicore RTS using
the concept of opportunistic execution. HYDRA allows security tasks to operate
with existing real-time tasks without perturbing system parameters or normal
execution patterns, while still meeting the desired monitoring frequency for
intrusion detection. Our evaluation uses a representative real-time control
system (along with synthetic task sets for a broader exploration) to illustrate
the efficacy of HYDRA.
","['\nMonowar Hasan\n', '\nSibin Mohan\n', '\nRodolfo Pellizzoni\n', '\nRakesh B. Bobba\n']","Accepted for publication, 21st DATE (Design, Automation & Test in
  Europe) conference, 2018",,http://arxiv.org/abs/1711.04808v1,cs.CR,"['cs.CR', 'cs.OS']",,,[]
"Seamless Resources Sharing in Wearable Networks by Application Function
  Virtualization",http://arxiv.org/abs/1711.08495v1,2017-11-14T21:28:19Z,2017-11-14T21:28:19Z,"  The prevalence of smart wearable devices is increasing exponentially and we
are witnessing a wide variety of fascinating new services that leverage the
capabilities of these wearables. Wearables are truly changing the way mobile
computing is deployed and mobile applications are being developed. It is
possible to leverage the capabilities such as connectivity, processing, and
sensing of wearable devices in an adaptive manner for efficient resource usage
and information accuracy within the personal area network. We show that
application developers are not yet taking advantage of these cross-device
capabilities, however, instead using wearables as passive sensors or simple end
displays to provide notifications to the user. We thus design AFV (Application
Function Virtualization), an architecture enabling automated dynamic function
virtualization and scheduling across devices in a personal area network,
simplifying the development of the apps that are adaptive to context changes.
AFV provides a simple set of APIs hiding complex architectural tasks from app
developers whilst continuously monitoring the user, device and network context,
to enable the adaptive invocation of functions across devices. We show the
feasibility of our design by implementing AFV on Android, and the benefits for
the user in terms of resource efficiency, especially in saving energy
consumption, and quality of experience with multiple use cases.
","['\nHarini Kolamunna\n', '\nKanchana Thilakarathna\n', '\nDiego Perino\n', '\nDwight Makaroff\n', '\nAruna Seneviratne\n']",,,http://arxiv.org/abs/1711.08495v1,cs.OS,"['cs.OS', 'cs.NI']",,,[]
Ocasta: Clustering Configuration Settings For Error Recovery,http://arxiv.org/abs/1711.04030v1,2017-11-02T15:45:05Z,2017-11-02T15:45:05Z,"  Effective machine-aided diagnosis and repair of configuration errors
continues to elude computer systems designers. Most of the literature targets
errors that can be attributed to a single erroneous configuration setting.
However, a recent study found that a significant amount of configuration errors
require fixing more than one setting together. To address this limitation,
Ocasta statistically clusters dependent configuration settings based on the
application's accesses to its configuration settings and utilizes the extracted
clustering of configuration settings to fix configuration errors involving more
than one configuration settings. Ocasta treats applications as black-boxes and
only relies on the ability to observe application accesses to their
configuration settings.
  We collected traces of real application usage from 24 Linux and 5 Windows
desktops computers and found that Ocasta is able to correctly identify clusters
with 88.6% accuracy. To demonstrate the effectiveness of Ocasta, we evaluated
it on 16 real-world configuration errors of 11 Linux and Windows applications.
Ocasta is able to successfully repair all evaluated configuration errors in 11
minutes on average and only requires the user to examine an average of 3
screenshots of the output of the application to confirm that the error is
repaired. A user study we conducted shows that Ocasta is easy to use by both
expert and non-expert users and is more efficient than manual configuration
error troubleshooting.
","['\nZhen Huang\n', '\nDavid Lie\n']","Published in Proceedings of the 44th Annual IEEE/IFIP International
  Conference on Dependable Systems and Networks (DSN 2014)","44th Annual IEEE/IFIP International Conference on Dependable
  Systems and Networks, 2014, pages={479-490}",http://dx.doi.org/10.1109/DSN.2014.51,cs.SE,"['cs.SE', 'cs.LG', 'cs.OS', 'B.8.1; I.5.3']",10.1109/DSN.2014.51,,[]
Towards Linux Kernel Memory Safety,http://arxiv.org/abs/1710.06175v1,2017-10-17T09:07:13Z,2017-10-17T09:07:13Z,"  The security of billions of devices worldwide depends on the security and
robustness of the mainline Linux kernel. However, the increasing number of
kernel-specific vulnerabilities, especially memory safety vulnerabilities,
shows that the kernel is a popular and practically exploitable target. Two
major causes of memory safety vulnerabilities are reference counter overflows
(temporal memory errors) and lack of pointer bounds checking (spatial memory
errors).
  To succeed in practice, security mechanisms for critical systems like the
Linux kernel must also consider performance and deployability as critical
design objectives. We present and systematically analyze two such mechanisms
for improving memory safety in the Linux kernel: (a) an overflow-resistant
reference counter data structure designed to accommodate typical reference
counter usage in kernel source code, and (b) runtime pointer bounds checking
using Intel MPX in the kernel.
","['\nElena Reshetova\n', '\nHans Liljestrand\n', '\nAndrew Paverd\n', '\nN. Asokan\n']",,,http://dx.doi.org/10.1002/spe.2638,cs.CR,"['cs.CR', 'cs.OS']",10.1002/spe.2638,,[]
Tails & Tor and other tools for Safeguarding Online Activities,http://arxiv.org/abs/1710.08705v1,2017-10-24T11:14:02Z,2017-10-24T11:14:02Z,"  There are not many known ways to break Tor anonymity, and they require an
enormous amount of computational power. Controlling both entrance and exit
nodes allows an attacker to compromise client IP with enough pattern analysis.
If an .onion or public website does not use SSL, information will not be
encrypted once it reaches the exit node. Tor has been successfully broken by
Carnegie Mellon, however they will not answer questions nor confirm their
method. This research paper investigates Tails & Tor and other tools for
Safeguarding Online Activities.
","['\nStephanie Abraham\n', '\nTyler Silva\n', '\nRobert Decourcy\n', '\nJim Cardon\n']",,,http://arxiv.org/abs/1710.08705v1,cs.CR,"['cs.CR', 'cs.OS']",,,[]
Exploiting Commutativity For Practical Fast Replication,http://arxiv.org/abs/1710.09921v1,2017-10-26T21:34:34Z,2017-10-26T21:34:34Z,"  Traditional approaches to replication require client requests to be ordered
before making them durable by copying them to replicas. As a result, clients
must wait for two round-trip times (RTTs) before updates complete. In this
paper, we show that this entanglement of ordering and durability is unnecessary
for strong consistency. Consistent Unordered Replication Protocol (CURP) allows
clients to replicate requests that have not yet been ordered, as long as they
are commutative. This strategy allows most operations to complete in 1 RTT (the
same as an unreplicated system). We implemented CURP in the Redis and RAMCloud
storage systems. In RAMCloud, CURP improved write latency by ~2x (13.8 us ->
7.3 us) and write throughput by 4x. Compared to unreplicated RAMCloud, CURP's
latency overhead for 3-way replication is just 0.4 us (6.9 us vs 7.3 us). CURP
transformed a non-durable Redis cache into a consistent and durable storage
system with only a small performance overhead.
","['\nSeo Jin Park\n', '\nJohn Ousterhout\n']","16 pages, 13 figures",,http://arxiv.org/abs/1710.09921v1,cs.DC,"['cs.DC', 'cs.OS']",,,[]
The Case for a Single System Image for Personal Devices,http://arxiv.org/abs/1710.03789v1,2017-10-10T19:10:40Z,2017-10-10T19:10:40Z,"  Computing technology has gotten cheaper and more powerful, allowing users to
have a growing number of personal computing devices at their disposal. While
this trend is beneficial for the user, it also creates a growing management
burden for the user. Each device must be managed independently and users must
repeat the same management tasks on the each device, such as updating software,
changing configurations, backup, and replicating data for availability. To
prevent the management burden from increasing with the number of devices, we
propose that all devices run a single system image called a personal computing
image. Personal computing images export a device-specific user interface on
each device, but provide a consistent view of application and operating state
across all devices. As a result, management tasks can be performed once on any
device and will be automatically propagated to all other devices belonging to
the user. We discuss evolutionary steps that can be taken to achieve personal
computing images for devices and elaborate on challenges that we believe
building such systems will face.
","['\nBeom Heyn Kim\n', '\nEyal de Lara\n', '\nDavid Lie\n']",,,http://arxiv.org/abs/1710.03789v1,cs.OS,['cs.OS'],,,[]
"Performance Evaluation of Container-based Virtualization for High
  Performance Computing Environments",http://arxiv.org/abs/1709.10140v1,2017-09-28T19:30:05Z,2017-09-28T19:30:05Z,"  Virtualization technologies have evolved along with the development of
computational environments since virtualization offered needed features at that
time such as isolation, accountability, resource allocation, resource fair
sharing and so on. Novel processor technologies bring to commodity computers
the possibility to emulate diverse environments where a wide range of
computational scenarios can be run. Along with processors evolution, system
developers have created different virtualization mechanisms where each new
development enhanced the performance of previous virtualized environments.
Recently, operating system-based virtualization technologies captured the
attention of communities abroad (from industry to academy and research) because
their important improvements on performance area.
  In this paper, the features of three container-based operating systems
virtualization tools (LXC, Docker and Singularity) are presented. LXC, Docker,
Singularity and bare metal are put under test through a customized single node
HPL-Benchmark and a MPI-based application for the multi node testbed. Also the
disk I/O performance, Memory (RAM) performance, Network bandwidth and GPU
performance are tested for the COS technologies vs bare metal. Preliminary
results and conclusions around them are presented and discussed.
","['\nCarlos Arango\n', '\nRémy Dernat\n', '\nJohn Sanabria\n']","Keywords: Container-based virtualization; Linux containers;
  Singularity-Containers; Docker; High performance computing",,http://arxiv.org/abs/1709.10140v1,cs.OS,"['cs.OS', 'cs.DC', 'cs.PF']",,,[]
FreeGuard: A Faster Secure Heap Allocator,http://arxiv.org/abs/1709.02746v2,2017-09-08T15:29:17Z,2017-09-25T16:10:02Z,"  In spite of years of improvements to software security, heap-related attacks
still remain a severe threat. One reason is that many existing memory
allocators fall short in a variety of aspects. For instance,
performance-oriented allocators are designed with very limited countermeasures
against attacks, but secure allocators generally suffer from significant
performance overhead, e.g., running up to 10x slower. This paper, therefore,
introduces FreeGuard, a secure memory allocator that prevents or reduces a wide
range of heap-related attacks, such as heap overflows, heap over-reads,
use-after-frees, as well as double and invalid frees. FreeGuard has similar
performance to the default Linux allocator, with less than 2% overhead on
average, but provides significant improvement to security guarantees. FreeGuard
also addresses multiple implementation issues of existing secure allocators,
such as the issue of scalability. Experimental results demonstrate that
FreeGuard is very effective in defending against a variety of heap-related
attacks.
","['\nSam Silvestro\n', '\nHongyu Liu\n', '\nCorey Crosser\n', '\nZhiqiang Lin\n', '\nTongping Liu\n']","15 pages, 4 figures, to be published at CCS'17",,http://dx.doi.org/10.1145/3133956.3133957,cs.OS,"['cs.OS', 'cs.CR']",10.1145/3133956.3133957,,[]
"Bringing Fault-Tolerant GigaHertz-Computing to Space: A Multi-Stage
  Software-Side Fault-Tolerance Approach for Miniaturized Spacecraft",http://arxiv.org/abs/1708.06931v1,2017-08-23T09:31:28Z,2017-08-23T09:31:28Z,"  Modern embedded technology is a driving factor in satellite miniaturization,
contributing to a massive boom in satellite launches and a rapidly evolving new
space industry. Miniaturized satellites, however, suffer from low reliability,
as traditional hardware-based fault-tolerance (FT) concepts are ineffective for
on-board computers (OBCs) utilizing modern systems-on-a-chip (SoC). Therefore,
larger satellites continue to rely on proven processors with large feature
sizes. Software-based concepts have largely been ignored by the space industry
as they were researched only in theory, and have not yet reached the level of
maturity necessary for implementation. We present the first integral,
real-world solution to enable fault-tolerant general-purpose computing with
modern multiprocessor-SoCs (MPSoCs) for spaceflight, thereby enabling their use
in future high-priority space missions. The presented multi-stage approach
consists of three FT stages, combining coarse-grained thread-level distributed
self-validation, FPGA reconfiguration, and mixed criticality to assure
long-term FT and excellent scalability for both resource constrained and
critical high-priority space missions. Early benchmark results indicate a
drastic performance increase over state-of-the-art radiation-hard OBC designs
and considerably lower software- and hardware development costs. This approach
was developed for a 4-year European Space Agency (ESA) project, and we are
implementing a tiled MPSoC prototype jointly with two industrial partners.
","['\nChristian M. Fuchs\n', '\nTodor Stefanov\n', '\nNadia Murillo\n', '\nAske Plaat\n']","26th IEEE Asian Test Symposium 2017, 27-30 Nov 2017, Taipei City,
  Taiwan",,http://arxiv.org/abs/1708.06931v1,cs.DC,"['cs.DC', 'cs.OS']",,,[]
Tug-of-War: Observations on Unified Content Handling,http://arxiv.org/abs/1708.09334v1,2017-08-29T10:24:40Z,2017-08-29T10:24:40Z,"  Modern applications and Operating Systems vary greatly with respect to how
they register and identify different types of content. These discrepancies lead
to exploits and inconsistencies in user experience. In this paper, we highlight
the issues arising in the modern content handling ecosystem, and examine how
the operating system can be used to achieve unified and consistent content
identification.
","['\nTheofilos Petsios\n', '\nAdrian Tang\n', '\nDimitris Mitropoulos\n', '\nSalvatore Stolfo\n', '\nAngelos D. Keromytis\n', '\nSuman Jana\n']",,,http://arxiv.org/abs/1708.09334v1,cs.CR,"['cs.CR', 'cs.OS']",,,[]
"Entirely protecting operating systems against transient errors in space
  environment",http://arxiv.org/abs/1708.06450v1,2017-08-21T23:10:59Z,2017-08-21T23:10:59Z,"  In this article, we propose a mainly-software hardening technique to totally
protect unmodified running operating systems on COTS hardware against transient
errors in heavily radiation - flooded environment like high altitude space. The
technique is currently being implemented in a hypervisor and allows to control
the upper layers of the software stack (operating system and applications). The
rest of the system, the hypervisor, will be protected by other means, thus
resulting in a completely protected system against transient errors. The
induced overhead turns around 200% but this is expected to decrease with future
improvements.
","['\nMahoukpégo Parfait Tokponnon\n', '\nMarc Lobelle\n', '\nEugene C. Ezin\n']","2 pages, 4 figures, Conference",,http://arxiv.org/abs/1708.06450v1,cs.OS,['cs.OS'],,,[]
Performance Measurements of Supercomputing and Cloud Storage Solutions,http://arxiv.org/abs/1708.00544v1,2017-08-01T22:48:06Z,2017-08-01T22:48:06Z,"  Increasing amounts of data from varied sources, particularly in the fields of
machine learning and graph analytics, are causing storage requirements to grow
rapidly. A variety of technologies exist for storing and sharing these data,
ranging from parallel file systems used by supercomputers to distributed block
storage systems found in clouds. Relatively few comparative measurements exist
to inform decisions about which storage systems are best suited for particular
tasks. This work provides these measurements for two of the most popular
storage technologies: Lustre and Amazon S3. Lustre is an open-source, high
performance, parallel file system used by many of the largest supercomputers in
the world. Amazon's Simple Storage Service, or S3, is part of the Amazon Web
Services offering, and offers a scalable, distributed option to store and
retrieve data from anywhere on the Internet. Parallel processing is essential
for achieving high performance on modern storage systems. The performance tests
used span the gamut of parallel I/O scenarios, ranging from single-client,
single-node Amazon S3 and Lustre performance to a large-scale, multi-client
test designed to demonstrate the capabilities of a modern storage appliance
under heavy load. These results show that, when parallel I/O is used correctly
(i.e., many simultaneous read or write processes), full network bandwidth
performance is achievable and ranged from 10 gigabits/s over a 10 GigE S3
connection to 0.35 terabits/s using Lustre on a 1200 port 10 GigE switch. These
results demonstrate that S3 is well-suited to sharing vast quantities of data
over the Internet, while Lustre is well-suited to processing large quantities
of data locally.
","['\nMichael Jones\n', '\nJeremy Kepner\n', '\nWilliam Arcand\n', '\nDavid Bestor\n', '\nBill Bergeron\n', '\nVijay Gadepally\n', '\nMichael Houle\n', '\nMatthew Hubbell\n', '\nPeter Michaleas\n', '\nAndrew Prout\n', '\nAlbert Reuther\n', '\nSiddharth Samsi\n', '\nPaul Monticiollo\n']","5 pages, 4 figures, to appear in IEEE HPEC 2017",,http://dx.doi.org/10.1109/HPEC.2017.8091073,cs.DC,"['cs.DC', 'astro-ph.IM', 'cs.NI', 'cs.OS', 'cs.PF']",10.1109/HPEC.2017.8091073,,[]
Optimizations of Management Algorithms for Multi-Level Memory Hierarchy,http://arxiv.org/abs/1707.07161v1,2017-07-22T13:29:01Z,2017-07-22T13:29:01Z,"  In the near future the SCM is predicted to modify the form of new programs,
the access form to storage, and the way that storage devices themselves are
built. Therefore, a combination between the SCM and a designated Memory
Allocation Manager (MAM) that will allow the programmer to manually control the
different memories in the memory hierarchy will be likely to achieve a new
level of performance for memory-aware data structures. Although the manual MAM
seems to be the optimal approach for multi-level memory hierarchy management,
this technique is still very far from being realistic, and the chances that it
would be implemented in current codes using High Performance Computing (HPC)
platforms is quite low. This premise means that the most reasonable way to
introduce the SCM into any usable and popular memory system would be by
implementing an automated version of the MAM using the fundamentals of paging
algorithms, as used for two-level memory hierarchy. Our hypothesis is that
achieving appropriate transferability between memory levels may be possible
using ideas of algorithms employed in current virtual memory systems, and that
the adaptation of those algorithms from a two-level memory hierarchy to an
N-level memory hierarchy is possible. In order to reach the conclusion that our
hypothesis is correct, we investigated various paging algorithms, and found the
ones that could be adapted successfully from two-level memory hierarchy to an
N-level memory hierarchy. We discovered that using an adaptation of the Aging
paging algorithm to an N-level memory hierarchy results in the best
performances in terms of Hit/Miss ratio. In order to verify our hypothesis we
build a simulator called ""DeMemory simulator"" for analyzing our algorithms as
well as for other algorithms that will be devised in the future.
",['\nGal Oren\n'],"Master's Thesis, Diss. The Open University (2015)",,http://arxiv.org/abs/1707.07161v1,cs.OS,['cs.OS'],,,[]
FluidMem: Memory as a Service for the Datacenter,http://arxiv.org/abs/1707.07780v1,2017-07-25T00:59:32Z,2017-07-25T00:59:32Z,"  Disaggregating resources in data centers is an emerging trend. Recent work
has begun to explore memory disaggregation, but suffers limitations including
lack of consideration of the complexity of cloud-based deployment, including
heterogeneous hardware and APIs for cloud users and operators. In this paper,
we present FluidMem, a complete system to realize disaggregated memory in the
datacenter. Going beyond simply demonstrating remote memory is possible, we
create an entire Memory as a Service. We define the requirements of Memory as a
Service and build its implementation in Linux as FluidMem. We present a
performance analysis of FluidMem and demonstrate that it transparently supports
remote memory for standard applications such as MongoDB and genome sequencing
applications.
","['\nBlake Caldwell\n', '\nYoungbin Im\n', '\nSangtae Ha\n', '\nRichard Han\n', '\nEric Keller\n']",University of Colorado Technical Report,,http://arxiv.org/abs/1707.07780v1,cs.OS,['cs.OS'],,,[]
Analyzing IO Amplification in Linux File Systems,http://arxiv.org/abs/1707.08514v1,2017-07-26T16:02:29Z,2017-07-26T16:02:29Z,"  We present the first systematic analysis of read, write, and space
amplification in Linux file systems. While many researchers are tackling write
amplification in key-value stores, IO amplification in file systems has been
largely unexplored. We analyze data and metadata operations on five widely-used
Linux file systems: ext2, ext4, XFS, btrfs, and F2FS. We find that data
operations result in significant write amplification (2-32X) and that metadata
operations have a large IO cost. For example, a single rename requires 648 KB
write IO in btrfs. We also find that small random reads result in read
amplification of 2-13X. Based on these observations, we present the CReWS
conjecture about the relationship between IO amplification, consistency, and
storage space utilization. We hope this paper spurs people to design future
file systems with less IO amplification, especially for non-volatile memory
technologies.
","['\nJayashree Mohan\n', '\nRohan Kadekodi\n', '\nVijay Chidambaram\n']",,,http://arxiv.org/abs/1707.08514v1,cs.OS,['cs.OS'],,,[]
Downgrade Attack on TrustZone,http://arxiv.org/abs/1707.05082v2,2017-07-17T10:38:13Z,2017-07-18T05:17:14Z,"  Security-critical tasks require proper isolation from untrusted software.
Chip manufacturers design and include trusted execution environments (TEEs) in
their processors to secure these tasks. The integrity and security of the
software in the trusted environment depend on the verification process of the
system.
  We find a form of attack that can be performed on the current implementations
of the widely deployed ARM TrustZone technology. The attack exploits the fact
that the trustlet (TA) or TrustZone OS loading verification procedure may use
the same verification key and may lack proper rollback prevention across
versions. If an exploit works on an out-of-date version, but the vulnerability
is patched on the latest version, an attacker can still use the same exploit to
compromise the latest system by downgrading the software to an older and
exploitable version.
  We did experiments on popular devices on the market including those from
Google, Samsung and Huawei, and found that all of them have the risk of being
attacked. Also, we show a real-world example to exploit Qualcomm's QSEE.
  In addition, in order to find out which device images share the same
verification key, pattern matching schemes for different vendors are analyzed
and summarized.
","['\nYue Chen\n', '\nYulong Zhang\n', '\nZhi Wang\n', '\nTao Wei\n']",,,http://arxiv.org/abs/1707.05082v2,cs.CR,"['cs.CR', 'cs.OS', 'D.4.6']",,,[]
Study and Analysis of MAC/IPAD Lab Configuration,http://arxiv.org/abs/1707.05405v1,2017-07-17T22:26:19Z,2017-07-17T22:26:19Z,"  This paper is about three virtualization modes: VMware, Parallels, and Boot
Camping. The trade off of their testing is the hardware requirements. The main
question is, among the three, which is the most suitable? The answer actually
varies from user to user. It depends on the user needs. Moreover, it is
necessary to consider its performance, graphics, efficiency and reliability,
and interoperability, and that is our major scope. In order to take the final
decision in choosing one of the modes it is important to run some tests, which
costs a lot in terms of money, complexity, and time consumption. Therefore, in
order to overcome this trade off, most of the research has been done through
online benchmarking and my own anticipation. The final solution was extracted
after comparing all previously mentioned above and after rigorous testing made
which will be introduced later in this document.
",['\nAyman Noor\n'],"11 pages, 15 figures",,http://arxiv.org/abs/1707.05405v1,cs.OS,"['cs.OS', 'cs.PF']",,,[]
"Deterministic Memory Abstraction and Supporting Multicore System
  Architecture",http://arxiv.org/abs/1707.05260v4,2017-07-17T16:12:15Z,2018-04-19T00:06:48Z,"  Poor time predictability of multicore processors has been a long-standing
challenge in the real-time systems community. In this paper, we make a case
that a fundamental problem that prevents efficient and predictable real-time
computing on multicore is the lack of a proper memory abstraction to express
memory criticality, which cuts across various layers of the system: the
application, OS, and hardware. We, therefore, propose a new holistic resource
management approach driven by a new memory abstraction, which we call
Deterministic Memory. The key characteristic of deterministic memory is that
the platform - the OS and hardware - guarantees small and tightly bounded
worst-case memory access timing. In contrast, we call the conventional memory
abstraction as best-effort memory in which only highly pessimistic worst-case
bounds can be achieved. We propose to utilize both abstractions to achieve high
time predictability but without significantly sacrificing performance. We
present deterministic memory-aware OS and architecture designs, including
OS-level page allocator, hardware-level cache, and DRAM controller designs. We
implement the proposed OS and architecture extensions on Linux and gem5
simulator. Our evaluation results, using a set of synthetic and real-world
benchmarks, demonstrate the feasibility and effectiveness of our approach.
","['\nFarzad Farshchi\n', '\nPrathap Kumar Valsan\n', '\nRenato Mancuso\n', '\nHeechul Yun\n']",,,http://arxiv.org/abs/1707.05260v4,cs.AR,"['cs.AR', 'cs.OS', 'cs.PF']",,,[]
"Comments on ""Gang EDF Schedulability Analysis""",http://arxiv.org/abs/1705.05798v1,2017-05-16T16:41:43Z,2017-05-16T16:41:43Z,"  This short report raises a correctness issue in the schedulability test
presented in Kato et al., ""Gang EDF Scheduling of Parallel Task Systems"", 30th
IEEE Real-Time Systems Symposium, 2009, pp. 459-468.
","['\nPascal Richard\n', '\nJoël Goossens\n', '\nShinpei Kato\n']",,,http://arxiv.org/abs/1705.05798v1,cs.OS,['cs.OS'],,,[]
"Look Mum, no VM Exits! (Almost)",http://arxiv.org/abs/1705.06932v1,2017-05-19T11:01:54Z,2017-05-19T11:01:54Z,"  Multi-core CPUs are a standard component in many modern embedded systems.
Their virtualisation extensions enable the isolation of services, and gain
popularity to implement mixed-criticality or otherwise split systems. We
present Jailhouse, a Linux-based, OS-agnostic partitioning hypervisor that uses
novel architectural approaches to combine Linux, a powerful general-purpose
system, with strictly isolated special-purpose components. Our design goals
favour simplicity over features, establish a minimal code base, and minimise
hypervisor activity.
  Direct assignment of hardware to guests, together with a deferred
initialisation scheme, offloads any complex hardware handling and bootstrapping
issues from the hypervisor to the general purpose OS. The hypervisor
establishes isolated domains that directly access physical resources without
the need for emulation or paravirtualisation. This retains, with negligible
system overhead, Linux's feature-richness in uncritical parts, while frugal
safety and real-time critical workloads execute in isolated, safe domains.
","['\nRalf Ramsauer\n', '\nJan Kiszka\n', '\nDaniel Lohmann\n', '\nWolfgang Mauerer\n']",,"Proceedings of the 13th Workshop on Operating Systems Platforms
  for Embedded Real-Time Applications (OSPERT 2017)",http://arxiv.org/abs/1705.06932v1,cs.OS,['cs.OS'],,,[]
GPU System Calls,http://arxiv.org/abs/1705.06965v2,2017-05-19T12:48:50Z,2017-05-24T20:48:00Z,"  GPUs are becoming first-class compute citizens and are being tasked to
perform increasingly complex work. Modern GPUs increasingly support
programmability- enhancing features such as shared virtual memory and hardware
cache coherence, enabling them to run a wider variety of programs. But a key
aspect of general-purpose programming where GPUs are still found lacking is the
ability to invoke system calls. We explore how to directly invoke generic
system calls in GPU programs. We examine how system calls should be meshed with
prevailing GPGPU programming models where thousands of threads are organized in
a hierarchy of execution groups: Should a system call be invoked at the
individual GPU task, or at different execution group levels? What are
reasonable ordering semantics for GPU system calls across these hierarchy of
execution groups? To study these questions, we implemented GENESYS -- a
mechanism to allow GPU pro- grams to invoke system calls in the Linux operating
system. Numerous subtle changes to Linux were necessary, as the existing kernel
assumes that only CPUs invoke system calls. We analyze the performance of
GENESYS using micro-benchmarks and three applications that exercise the
filesystem, networking, and memory allocation subsystems of the kernel. We
conclude by analyzing the suitability of all of Linux's system calls for the
GPU.
","['\nJán Veselý\n', '\nArkaprava Basu\n', '\nAbhishek Bhattacharjee\n', '\nGabriel Loh\n', '\nMark Oskin\n', '\nSteven K. Reinhardt\n']",,,http://arxiv.org/abs/1705.06965v2,cs.OS,"['cs.OS', 'D.4.4; D.4.7']",,,[]
MITHRIL: Mining Sporadic Associations for Cache Prefetching,http://arxiv.org/abs/1705.07400v1,2017-05-21T05:51:21Z,2017-05-21T05:51:21Z,"  The growing pressure on cloud application scalability has accentuated storage
performance as a critical bottle- neck. Although cache replacement algorithms
have been extensively studied, cache prefetching - reducing latency by
retrieving items before they are actually requested remains an underexplored
area. Existing approaches to history-based prefetching, in particular, provide
too few benefits for real systems for the resources they cost. We propose
MITHRIL, a prefetching layer that efficiently exploits historical patterns in
cache request associations. MITHRIL is inspired by sporadic association rule
mining and only relies on the timestamps of requests. Through evaluation of 135
block-storage traces, we show that MITHRIL is effective, giving an average of a
55% hit ratio increase over LRU and PROBABILITY GRAPH, a 36% hit ratio gain
over AMP at reasonable cost. We further show that MITHRIL can supplement any
cache replacement algorithm and be readily integrated into existing systems.
Furthermore, we demonstrate the improvement comes from MITHRIL being able to
capture mid-frequency blocks.
","['\nJuncheng Yang\n', '\nReza Karimi\n', '\nTrausti Sæmundsson\n', '\nAvani Wildani\n', '\nYmir Vigfusson\n']",,,http://arxiv.org/abs/1705.07400v1,cs.PF,"['cs.PF', 'cs.DC', 'cs.OS']",,,[]
IOTune: A G-states Driver for Elastic Performance of Block Storage,http://arxiv.org/abs/1705.03591v1,2017-05-10T02:30:06Z,2017-05-10T02:30:06Z,"  Imagining a disk which provides baseline performance at a relatively low
price during low-load periods, but when workloads demand more resources, the
disk performance is automatically promoted in situ and in real time. In a
hardware era, this is hardly achievable. However, this imagined disk is
becoming reality due to the technical advances of software-defined storage,
which enable volume performance to be adjusted on the fly. We propose IOTune, a
resource management middleware which employs software-defined storage
primitives to implement G-states of virtual block devices. G-states enable
virtual block devices to serve at multiple performance gears, getting rid of
conflicts between immutable resource reservation and dynamic resource demands,
and always achieving resource right-provisioning for workloads. Accompanying
G-states, we also propose a new block storage pricing policy for cloud
providers. Our case study for applying G-states to cloud block storage verifies
the effectiveness of the IOTune framework. Trace-replay based evaluations
demonstrate that storage volumes with G-states adapt to workload fluctuations.
For tenants, G-states enable volumes to provide much better QoS with a same
cost of ownership, comparing with static IOPS provisioning and the I/O credit
mechanism. G-states also reduce I/O tail latencies by one to two orders of
magnitude. From the standpoint of cloud providers, G-states promote storage
utilization, creating values and benefiting competitiveness. G-states supported
by IOTune provide a new paradigm for storage resource management and pricing in
multi-tenant clouds.
","['\nTao Lu\n', '\nPing Huang\n', '\nXubin He\n', '\nMatthew Welch\n', '\nSteven Gonzales\n', '\nMing Zhang\n']","15 pages, 10 figures",,http://arxiv.org/abs/1705.03591v1,cs.OS,"['cs.OS', 'D.4.2']",,,[]
A Reconnaissance Attack Mechanism for Fixed-Priority Real-Time Systems,http://arxiv.org/abs/1705.02561v1,2017-05-07T04:07:10Z,2017-05-07T04:07:10Z,"  In real-time embedded systems (RTS), failures due to security breaches can
cause serious damage to the system, the environment and/or injury to humans.
Therefore, it is very important to understand the potential threats and attacks
against these systems. In this paper we present a novel reconnaissance attack
that extracts the exact schedule of real-time systems designed using fixed
priority scheduling algorithms. The attack is demonstrated on both a real
hardware platform and a simulator, with a high success rate. Our evaluation
results show that the algorithm is robust even in the presence of execution
time variation.
","['\nChien-Ying Chen\n', '\nAmirEmad Ghassami\n', '\nSibin Mohan\n', '\nNegar Kiyavash\n', '\nRakesh B. Bobba\n', '\nRodolfo Pellizzoni\n', '\nMan-Ki Yoon\n']",,,http://arxiv.org/abs/1705.02561v1,cs.CR,"['cs.CR', 'cs.OS']",,,[]
"Improving the Performance and Endurance of Persistent Memory with
  Loose-Ordering Consistency",http://arxiv.org/abs/1705.03623v1,2017-05-10T06:47:40Z,2017-05-10T06:47:40Z,"  Persistent memory provides high-performance data persistence at main memory.
Memory writes need to be performed in strict order to satisfy storage
consistency requirements and enable correct recovery from system crashes.
Unfortunately, adhering to such a strict order significantly degrades system
performance and persistent memory endurance. This paper introduces a new
mechanism, Loose-Ordering Consistency (LOC), that satisfies the ordering
requirements at significantly lower performance and endurance loss. LOC
consists of two key techniques. First, Eager Commit eliminates the need to
perform a persistent commit record write within a transaction. We do so by
ensuring that we can determine the status of all committed transactions during
recovery by storing necessary metadata information statically with blocks of
data written to memory. Second, Speculative Persistence relaxes the write
ordering between transactions by allowing writes to be speculatively written to
persistent memory. A speculative write is made visible to software only after
its associated transaction commits. To enable this, our mechanism supports the
tracking of committed transaction ID and multi-versioning in the CPU cache. Our
evaluations show that LOC reduces the average performance overhead of memory
persistence from 66.9% to 34.9% and the memory write traffic overhead from
17.1% to 3.4% on a variety of workloads.
","['\nYouyou Lu\n', '\nJiwu Shu\n', '\nLong Sun\n', '\nOnur Mutlu\n']","This paper has been accepted by IEEE Transactions on Parallel and
  Distributed Systems",,http://dx.doi.org/10.1109/TPDS.2017.2701364,cs.AR,"['cs.AR', 'cs.OS']",10.1109/TPDS.2017.2701364,,[]
Mixed-criticality Scheduling with Dynamic Redistribution of Shared Cache,http://arxiv.org/abs/1704.08876v1,2017-04-28T11:17:51Z,2017-04-28T11:17:51Z,"  The design of mixed-criticality systems often involvespainful tradeoffs
between safety guarantees and performance.However, the use of more detailed
architectural modelsin the design and analysis of scheduling arrangements for
mixedcriticalitysystems can provide greater confidence in the analysis,but also
opportunities for better performance. Motivated by thisview, we propose an
extension of Vestal 19s model for mixedcriticalitymulticore systems that (i)
accounts for the per-taskpartitioning of the last-level cache and (ii) supports
the dynamicreassignment, for better schedulability, of cache portions
initiallyreserved for lower-criticality tasks to the higher-criticalitytasks,
when the system switches to high-criticality mode. Tothis model, we apply
partitioned EDF scheduling with Ekbergand Yi 19s deadline-scaling technique.
Our schedulability analysisand scalefactor calculation is cognisant of the
cache resourcesassigned to each task, by using WCET estimates that take
intoaccount these resources. It is hence able to leverage the
dynamicreconfiguration of the cache partitioning, at mode change, forbetter
performance, in terms of provable schedulability. We alsopropose heuristics for
partitioning the cache in low- and highcriticalitymode, that promote
schedulability. Our experimentswith synthetic task sets, indicate tangible
improvements inschedulability compared to a baseline cache-aware
arrangementwhere there is no redistribution of cache resources from low-
tohigh-criticality tasks in the event of a mode change.
","['\nMuhammad Ali Awan\n', '\nKonstantinos Bletsas\n', '\nPedro F. Souto\n', '\nBenny Akesson\n', '\nEduardo Tovar\n']","ECRTS 2017, 26 pages",,http://arxiv.org/abs/1704.08876v1,cs.OS,['cs.OS'],,,[]
"Contego: An Adaptive Framework for Integrating Security Tasks in
  Real-Time Systems",http://arxiv.org/abs/1705.00138v3,2017-04-29T06:22:32Z,2017-05-23T19:00:17Z,"  Embedded real-time systems (RTS) are pervasive. Many modern RTS are exposed
to unknown security flaws, and threats to RTS are growing in both number and
sophistication. However, until recently, cyber-security considerations were an
afterthought in the design of such systems. Any security mechanisms integrated
into RTS must (a) co-exist with the real- time tasks in the system and (b)
operate without impacting the timing and safety constraints of the control
logic. We introduce Contego, an approach to integrating security tasks into RTS
without affecting temporal requirements. Contego is specifically designed for
legacy systems, viz., the real-time control systems in which major alterations
of the system parameters for constituent tasks is not always feasible. Contego
combines the concept of opportunistic execution with hierarchical scheduling to
maintain compatibility with legacy systems while still providing flexibility by
allowing security tasks to operate in different modes. We also define a metric
to measure the effectiveness of such integration. We evaluate Contego using
synthetic workloads as well as with an implementation on a realistic embedded
platform (an open- source ARM CPU running real-time Linux).
","['\nMonowar Hasan\n', '\nSibin Mohan\n', '\nRodolfo Pellizzoni\n', '\nRakesh B. Bobba\n']","Accepted for publication, 29th Euromicro Conference on Real-Time
  Systems (ECRTS17)",,http://arxiv.org/abs/1705.00138v3,cs.CR,"['cs.CR', 'cs.OS']",,,[]
"TrustShadow: Secure Execution of Unmodified Applications with ARM
  TrustZone",http://arxiv.org/abs/1704.05600v2,2017-04-19T03:30:09Z,2017-06-23T02:43:41Z,"  The rapid evolution of Internet-of-Things (IoT) technologies has led to an
emerging need to make it smarter. A variety of applications now run
simultaneously on an ARM-based processor. For example, devices on the edge of
the Internet are provided with higher horsepower to be entrusted with storing,
processing and analyzing data collected from IoT devices. This significantly
improves efficiency and reduces the amount of data that needs to be transported
to the cloud for data processing, analysis and storage. However, commodity OSes
are prone to compromise. Once they are exploited, attackers can access the data
on these devices. Since the data stored and processed on the devices can be
sensitive, left untackled, this is particularly disconcerting.
  In this paper, we propose a new system, TrustShadow that shields legacy
applications from untrusted OSes. TrustShadow takes advantage of ARM TrustZone
technology and partitions resources into the secure and normal worlds. In the
secure world, TrustShadow constructs a trusted execution environment for
security-critical applications. This trusted environment is maintained by a
lightweight runtime system that coordinates the communication between
applications and the ordinary OS running in the normal world. The runtime
system does not provide system services itself. Rather, it forwards requests
for system services to the ordinary OS, and verifies the correctness of the
responses. To demonstrate the efficiency of this design, we prototyped
TrustShadow on a real chip board with ARM TrustZone support, and evaluated its
performance using both microbenchmarks and real-world applications. We showed
TrustShadow introduces only negligible overhead to real-world applications.
","['\nLe Guan\n', '\nPeng Liu\n', '\nXinyu Xing\n', '\nXinyang Ge\n', '\nShengzhi Zhang\n', '\nMeng Yu\n', '\nTrent Jaeger\n']",MobiSys 2017,,http://dx.doi.org/10.1145/3081333.3081349,cs.CR,"['cs.CR', 'cs.OS']",10.1145/3081333.3081349,,[]
Tackling Diversity and Heterogeneity by Vertical Memory Management,http://arxiv.org/abs/1704.01198v1,2017-04-04T21:43:04Z,2017-04-04T21:43:04Z,"  Existing memory management mechanisms used in commodity computing machines
typically adopt hardware based address interleaving and OS directed random
memory allocation to service generic application requests. These conventional
memory management mechanisms are challenged by contention at multiple memory
levels, a daunting variety of workload behaviors, and an increasingly
complicated memory hierarchy. Our ISCA-41 paper proposes vertical partitioning
to eliminate shared resource contention at multiple levels in the memory
hierarchy. Combined with horizontal memory management policies, our framework
supports a flexible policy space for tackling diverse application needs in
production environment and is suitable for future heterogeneous memory systems.
",['\nLei Liu\n'],,,http://arxiv.org/abs/1704.01198v1,cs.OS,['cs.OS'],,,[]
"A Backward Algorithm for the Multiprocessor Online Feasibility of
  Sporadic Tasks",http://arxiv.org/abs/1704.00999v1,2017-04-04T13:26:31Z,2017-04-04T13:26:31Z,"  The online feasibility problem (for a set of sporadic tasks) asks whether
there is a scheduler that always prevents deadline misses (if any), whatever
the sequence of job releases, which is a priori} unknown to the scheduler. In
the multiprocessor setting, this problem is notoriously difficult. The only
exact test for this problem has been proposed by Bonifaci and
Marchetti-Spaccamela: it consists in modelling all the possible behaviours of
the scheduler and of the tasks as a graph; and to interpret this graph as a
game between the tasks and the scheduler, which are seen as antagonistic
players. Then, computing a correct scheduler is equivalent to finding a winning
strategy for the `scheduler player', whose objective in the game is to avoid
deadline misses. In practice, however this approach is limited by the
intractable size of the graph. In this work, we consider the classical
attractor algorithm to solve such games, and introduce antichain techniques to
optimise its performance in practice and overcome the huge size of the game
graph. These techniques are inspired from results from the formal methods
community, and exploit the specific structure of the feasibility problem. We
demonstrate empirically that our approach allows to dramatically improve the
performance of the game solving algorithm.
","['\nGilles Geeraerts\n', '\nJoël Goossens\n', '\nThi-Van-Anh Nguyen\n']",Long version of a conference paper accepted to ACSD 2017,,http://arxiv.org/abs/1704.00999v1,cs.GT,"['cs.GT', 'cs.OS']",,,[]
Memos: Revisiting Hybrid Memory Management in Modern Operating System,http://arxiv.org/abs/1703.07725v1,2017-03-22T16:07:11Z,2017-03-22T16:07:11Z,"  The emerging hybrid DRAM-NVM architecture is challenging the existing memory
management mechanism in operating system. In this paper, we introduce memos,
which can schedule memory resources over the entire memory hierarchy including
cache, channels, main memory comprising DRAM and NVM simultaneously. Powered by
our newly designed kernel-level monitoring module and page migration engine,
memos can dynamically optimize the data placement at the memory hierarchy in
terms of the on-line memory patterns, current resource utilization and feature
of memory medium. Our experimental results show that memos can achieve high
memory utilization, contributing to system throughput by 19.1% and QoS by 23.6%
on average. Moreover, memos can reduce the NVM side memory latency by 3~83.3%,
energy consumption by 25.1~99%, and benefit the NVM lifetime significantly (40X
improvement on average).
","['\nLei Liu\n', '\nMengyao Xie\n', '\nHao Yang\n']",,,http://arxiv.org/abs/1703.07725v1,cs.OS,"['cs.OS', 'cs.AR']",,,[]
Virtualization technology for distributed time sensitive domains,http://arxiv.org/abs/1703.08469v1,2017-03-24T15:38:17Z,2017-03-24T15:38:17Z,"  This paper reports on the state of the art of virtualization technology for
both general purpose domains as well as real-time domains. There exits no
entirely instantaneous data transmission/transfer. There always exist a delay
while transmitting data, either in the processing or in the medium itself.
However most systems are designed to function appropriately with a delay
tolerance. This delay, inevitably, is affected when operating with an extra
layer, the virtualization. For real time systems it is crucial to know the
temporal limits in order not to surpass them. Introducing virtualization in the
real-time domain therefore requires deeper analysis by making use of techniques
that will offer results with deterministic execution times. The study of time
in systems and its behaviour under various possible circumstances is hence a
key for properly assessing this technology applied to both domains, especially
the real-time domain.
",['\nCarlos Antonio Perea-Gómez\n'],,,http://arxiv.org/abs/1703.08469v1,cs.DC,"['cs.DC', 'cs.OS']",,,[]
Formalizing Memory Accesses and Interrupts,http://arxiv.org/abs/1703.06571v1,2017-03-20T02:47:57Z,2017-03-20T02:47:57Z,"  The hardware/software boundary in modern heterogeneous multicore computers is
increasingly complex, and diverse across different platforms. A single memory
access by a core or DMA engine traverses multiple hardware translation and
caching steps, and the destination memory cell or register often appears at
different physical addresses for different cores. Interrupts pass through a
complex topology of interrupt controllers and remappers before delivery to one
or more cores, each with specific constraints on their configurations. System
software must not only correctly understand the specific hardware at hand, but
also configure it appropriately at runtime. We propose a formal model of
address spaces and resources in a system that allows us to express and verify
invariants of the system's runtime configuration, and illustrate (and motivate)
it with several real platforms we have encountered in the process of OS
implementation.
","['\nReto Achermann\nSystems Group, Department of Computer Science, ETH Zurich\n', '\nLukas Humbel\nSystems Group, Department of Computer Science, ETH Zurich\n', '\nDavid Cock\nSystems Group, Department of Computer Science, ETH Zurich\n', '\nTimothy Roscoe\nSystems Group, Department of Computer Science, ETH Zurich\n']","In Proceedings MARS 2017, arXiv:1703.05812","EPTCS 244, 2017, pp. 66-116",http://dx.doi.org/10.4204/EPTCS.244.4,cs.OS,"['cs.OS', 'cs.AR', 'D.4; D.4.2; D.4.7; D.3.4']",10.4204/EPTCS.244.4,,"['Systems Group, Department of Computer Science, ETH Zurich', 'Systems Group, Department of Computer Science, ETH Zurich', 'Systems Group, Department of Computer Science, ETH Zurich', 'Systems Group, Department of Computer Science, ETH Zurich']"
Adapting the DMTCP Plugin Model for Checkpointing of Hardware Emulation,http://arxiv.org/abs/1703.00897v1,2017-03-02T18:52:45Z,2017-03-02T18:52:45Z,"  Checkpoint-restart is now a mature technology. It allows a user to save and
later restore the state of a running process. The new plugin model for the
upcoming version 3.0 of DMTCP (Distributed MultiThreaded Checkpointing) is
described here. This plugin model allows a target application to disconnect
from the hardware emulator at checkpoint time and then re-connect to a possibly
different hardware emulator at the time of restart. The DMTCP plugin model is
important in allowing three distinct parties to seamlessly inter-operate. The
three parties are: the EDA designer, who is concerned with formal verification
of a circuit design; the DMTCP developers, who are concerned with providing
transparent checkpointing during the circuit emulation; and the hardware
emulator vendor, who provides a plugin library that responds to checkpoint,
restart, and other events.
  The new plugin model is an example of process-level virtualization:
virtualization of external abstractions from within a process. This capability
is motivated by scenarios for testing circuit models with the help of a
hardware emulator. The plugin model enables a three-way collaboration: allowing
a circuit designer and emulator vendor to each contribute separate proprietary
plugins while sharing an open source software framework from the DMTCP
developers. This provides a more flexible platform, where different fault
injection models based on plugins can be designed within the DMTCP
checkpointing framework. After initialization, one restarts from a checkpointed
state under the control of the desired plugin. This restart saves the time
spent in simulating the initialization phase, while enabling fault injection
exactly at the region of interest. Upon restart, one can inject faults or
otherwise modify the remainder of the simulation. The work concludes with a
brief survey of checkpointing and process-level virtualization.
","['\nRohan Garg\n', '\nKapil Arya\n', '\nJiajun Cao\n', '\nGene Cooperman\n', '\nJeff Evans\n', '\nAnkit Garg\n', '\nNeil A. Rosenberg\n', '\nK. Suresh\n']","5 pages, 11 figure, 1 listing; SELSE '17, March 21--22, 2017, Boston,
  MA, USA",,http://arxiv.org/abs/1703.00897v1,cs.OS,"['cs.OS', 'cs.AR', 'B.6.3']",,,[]
Assessing Code Authorship: The Case of the Linux Kernel,http://arxiv.org/abs/1703.02925v1,2017-03-08T17:26:02Z,2017-03-08T17:26:02Z,"  Code authorship is a key information in large-scale open source systems.
Among others, it allows maintainers to assess division of work and identify key
collaborators. Interestingly, open-source communities lack guidelines on how to
manage authorship. This could be mitigated by setting to build an empirical
body of knowledge on how authorship-related measures evolve in successful
open-source communities. Towards that direction, we perform a case study on the
Linux kernel. Our results show that: (a) only a small portion of developers (26
%) makes significant contributions to the code base; (b) the distribution of
the number of files per author is highly skewed --- a small group of top
authors (3 %) is responsible for hundreds of files, while most authors (75 %)
are responsible for at most 11 files; (c) most authors (62 %) have a specialist
profile; (d) authors with a high number of co-authorship connections tend to
collaborate with others with less connections.
","['\nGuilherme Avelino\n', '\nLeonardo Passos\n', '\nAndre Hora\n', '\nMarco Tulio Valente\n']","Accepted at 13th International Conference on Open Source Systems
  (OSS). 12 pages",,http://dx.doi.org/10.1007/978-3-319-57735-7_15,cs.SE,"['cs.SE', 'cs.OS', 'cs.SI']",10.1007/978-3-319-57735-7_15,,[]
Flashield: a Key-value Cache that Minimizes Writes to Flash,http://arxiv.org/abs/1702.02588v1,2017-02-08T19:21:13Z,2017-02-08T19:21:13Z,"  As its price per bit drops, SSD is increasingly becoming the default storage
medium for cloud application databases. However, it has not become the
preferred storage medium for key-value caches, even though SSD offers more than
10x lower price per bit and sufficient performance compared to DRAM. This is
because key-value caches need to frequently insert, update and evict small
objects. This causes excessive writes and erasures on flash storage, since
flash only supports writes and erasures of large chunks of data. These
excessive writes and erasures significantly shorten the lifetime of flash,
rendering it impractical to use for key-value caches. We present Flashield, a
hybrid key-value cache that uses DRAM as a ""filter"" to minimize writes to SSD.
Flashield performs light-weight machine learning profiling to predict which
objects are likely to be read frequently before getting updated; these objects,
which are prime candidates to be stored on SSD, are written to SSD in large
chunks sequentially. In order to efficiently utilize the cache's available
memory, we design a novel in-memory index for the variable-sized objects stored
on flash that requires only 4 bytes per object in DRAM. We describe Flashield's
design and implementation and, we evaluate it on a real-world cache trace.
Compared to state-of-the-art systems that suffer a write amplification of 2.5x
or more, Flashield maintains a median write amplification of 0.5x without any
loss of hit rate or throughput.
","['\nAssaf Eisenman\n', '\nAsaf Cidon\n', '\nEvgenya Pergament\n', '\nOr Haimovich\n', '\nRyan Stutsman\n', '\nMohammad Alizadeh\n', '\nSachin Katti\n']",,,http://arxiv.org/abs/1702.02588v1,cs.OS,['cs.OS'],,,[]
"Power and Execution Time Measurement Methodology for SDF Applications on
  FPGA-based MPSoCs",http://arxiv.org/abs/1701.03709v1,2017-01-13T16:15:53Z,2017-01-13T16:15:53Z,"  Timing and power consumption play an important role in the design of embedded
systems. Furthermore, both properties are directly related to the safety
requirements of many embedded systems. With regard to availability
requirements, power considerations are of uttermost importance for battery
operated systems. Validation of timing and power requires observability of
these properties. In many cases this is difficult, because the observability is
either not possible or requires big extra effort in the system validation
process. In this paper, we present a measurement-based approach for the joint
timing and power analysis of Synchronous Dataflow (SDF) applications running on
a shared memory multiprocessor systems-on-chip (MPSoC) architecture. As a
proof-of-concept, we implement an MPSoC system with configurable power and
timing measurement interfaces inside a Field Programmable Gate Array (FPGA).
Our experiments demonstrate the viability of our approach being able of
accurately analyzing different mappings of image processing applications (Sobel
filter and JPEG encoder) on an FPGA-based MPSoC implementation.
","['\nChristof Schlaak\n', '\nMaher Fakih\n', '\nRalf Stemmer\n']","Presented at HIP3ES, 2017 7 pages, 6 figures",,http://arxiv.org/abs/1701.03709v1,cs.DC,"['cs.DC', 'cs.AR', 'cs.OS', '68M20']",,,[]
"CannyFS: Opportunistically Maximizing I/O Throughput Exploiting the
  Transactional Nature of Batch-Mode Data Processing",http://arxiv.org/abs/1612.06830v1,2016-12-20T20:14:35Z,2016-12-20T20:14:35Z,"  We introduce a user mode file system, CannyFS, that hides latency by assuming
all I/O operations will succeed. The user mode process will in turn report
errors, allowing proper cleanup and a repeated attempt to take place. We
demonstrate benefits for the model tasks of extracting archives and removing
directory trees in a real-life HPC environment, giving typical reductions in
time use of over 80%.
  This approach can be considered a view of HPC jobs and their I/O activity as
transactions. In general, file systems lack clearly defined transaction
semantics. Over time, the competing trends to add cache and maintain data
integrity have resulted in different practical tradeoffs.
  High-performance computing is a special case where overall throughput demands
are high. Latency can also be high, with non-local storage. In addition, a
theoretically possible I/O error (like permission denied, loss of connection,
exceeding disk quota) will frequently warrant the resubmission of a full job or
task, rather than traditional error reporting or handling. Therefore,
opportunistically treating each I/O operation as successful, and part of a
larger transaction, can speed up some applications that do not leverage
asynchronous I/O.
","['\nJessica Nettelblad\n', '\nCarl Nettelblad\n']","8 pages, 3 figures, 1 table. Submitted",,http://arxiv.org/abs/1612.06830v1,cs.OS,['cs.OS'],,,[]
Browsix: Bridging the Gap Between Unix and the Browser,http://arxiv.org/abs/1611.07862v2,2016-11-23T16:23:40Z,2019-04-29T04:32:16Z,"  Applications written to run on conventional operating systems typically
depend on OS abstractions like processes, pipes, signals, sockets, and a shared
file system. Porting these applications to the web currently requires extensive
rewriting or hosting significant portions of code server-side because browsers
present a nontraditional runtime environment that lacks OS functionality.
  This paper presents Browsix, a framework that bridges the considerable gap
between conventional operating systems and the browser, enabling unmodified
programs expecting a Unix-like environment to run directly in the browser.
Browsix comprises two core parts: (1) a JavaScript-only system that makes core
Unix features (including pipes, concurrent processes, signals, sockets, and a
shared file system) available to web applications; and (2) extended JavaScript
runtimes for C, C++, Go, and Node.js that support running programs written in
these languages as processes in the browser. Browsix supports running a POSIX
shell, making it straightforward to connect applications together via pipes.
  We illustrate Browsix's capabilities via case studies that demonstrate how it
eases porting legacy applications to the browser and enables new functionality.
We demonstrate a Browsix-enabled LaTeX editor that operates by executing
unmodified versions of pdfLaTeX and BibTeX. This browser-only LaTeX editor can
render documents in seconds, making it fast enough to be practical. We further
demonstrate how Browsix lets us port a client-server application to run
entirely in the browser for disconnected operation. Creating these applications
required less than 50 lines of glue code and no code modifications,
demonstrating how easily Browsix can be used to build sophisticated web
applications from existing parts without modification.
","['\nBobby Powers\n', '\nJohn Vilk\n', '\nEmery D. Berger\n']","Final version published at
  https://dl.acm.org/citation.cfm?doid=3037697.3037727",ASPLOS 2017,http://dx.doi.org/10.1145/3037697.3037727,cs.OS,"['cs.OS', 'cs.PL']",10.1145/3037697.3037727,,[]
Memshare: a Dynamic Multi-tenant Memory Key-value Cache,http://arxiv.org/abs/1610.08129v1,2016-10-26T00:37:34Z,2016-10-26T00:37:34Z,"  Web application performance is heavily reliant on the hit rate of
memory-based caches. Current DRAM-based web caches statically partition their
memory across multiple applications sharing the cache. This causes under
utilization of memory which negatively impacts cache hit rates. We present
Memshare, a novel web memory cache that dynamically manages memory across
applications. Memshare provides a resource sharing model that guarantees
private memory to different applications while dynamically allocating the
remaining shared memory to optimize overall hit rate. Today's high cost of DRAM
storage and the availability of high performance CPU and memory bandwidth, make
web caches memory capacity bound. Memshare's log-structured design allows it to
provide significantly higher hit rates and dynamically partition memory among
applications at the expense of increased CPU and memory bandwidth consumption.
In addition, Memshare allows applications to use their own eviction policy for
their objects, independent of other applications. We implemented Memshare and
ran it on a week-long trace from a commercial memcached provider. We
demonstrate that Memshare increases the combined hit rate of the applications
in the trace by an 6.1% (from 84.7% hit rate to 90.8% hit rate) and reduces the
total number of misses by 39.7% without affecting system throughput or latency.
Even for single-tenant applications, Memshare increases the average hit rate of
the current state-of-the-art memory cache by an additional 2.7% on our
real-world trace.
","['\nAsaf Cidon\n', '\nDaniel Rushton\n', '\nStephen M. Rumble\n', '\nRyan Stutsman\n']","14 pages, 5 figures",,http://arxiv.org/abs/1610.08129v1,cs.OS,['cs.OS'],,,[]
"Verification of the Tree-Based Hierarchical Read-Copy Update in the
  Linux Kernel",http://arxiv.org/abs/1610.03052v3,2016-10-10T19:59:32Z,2018-11-22T17:40:49Z,"  Read-Copy Update (RCU) is a scalable, high-performance Linux-kernel
synchronization mechanism that runs low-overhead readers concurrently with
updaters. Production-quality RCU implementations for multi-core systems are
decidedly non-trivial. Giving the ubiquity of Linux, a rare ""million-year"" bug
can occur several times per day across the installed base. Stringent validation
of RCU's complex behaviors is thus critically important. Exhaustive testing is
infeasible due to the exponential number of possible executions, which suggests
use of formal verification.
  Previous verification efforts on RCU either focus on simple implementations
or use modeling languages, the latter requiring error-prone manual translation
that must be repeated frequently due to regular changes in the Linux kernel's
RCU implementation. In this paper, we first describe the implementation of Tree
RCU in the Linux kernel. We then discuss how to construct a model directly from
Tree RCU's source code in C, and use the CBMC model checker to verify its
safety and liveness properties. To our best knowledge, this is the first
verification of a significant part of RCU's source code, and is an important
step towards integration of formal verification into the Linux kernel's
regression test suite.
","['\nLihao Liang\n', '\nPaul E. McKenney\n', '\nDaniel Kroening\n', '\nTom Melham\n']","This is a long version of a conference paper published in the 2018
  Design, Automation and Test in Europe Conference (DATE)","Design, Automation and Test in Europe Conference (2018): 61-66",http://dx.doi.org/10.23919/DATE.2018.8341980,cs.LO,"['cs.LO', 'cs.DC', 'cs.OS', 'cs.SE', 'D.2.4; D.1.3']",10.23919/DATE.2018.8341980,,[]
An Evaluation of Coarse-Grained Locking for Multicore Microkernels,http://arxiv.org/abs/1609.08372v2,2016-09-27T12:21:02Z,2016-09-28T09:17:05Z,"  The trade-off between coarse- and fine-grained locking is a well understood
issue in operating systems. Coarse-grained locking provides lower overhead
under low contention, fine-grained locking provides higher scalability under
contention, though at the expense of implementation complexity and re- duced
best-case performance.
  We revisit this trade-off in the context of microkernels and tightly-coupled
cores with shared caches and low inter-core migration latencies. We evaluate
performance on two architectures: x86 and ARM MPCore, in the former case also
utilising transactional memory (Intel TSX). Our thesis is that on such
hardware, a well-designed microkernel, with short system calls, can take
advantage of coarse-grained locking on modern hardware, avoid the run-time and
complexity cost of multiple locks, enable formal verification, and still
achieve scalability comparable to fine-grained locking.
","['\nKevin Elphinstone\n', '\nAmirreza Zarrabi\n', '\nAdrian Danis\n', '\nYanyan Shen\n', '\nGernot Heiser\n']","11 pages, 7 figures, 28 references",,http://arxiv.org/abs/1609.08372v2,cs.OS,"['cs.OS', 'D.4.1; D.4.7; D.4.8']",,,[]
Practical Data Compression for Modern Memory Hierarchies,http://arxiv.org/abs/1609.02067v1,2016-09-07T16:53:40Z,2016-09-07T16:53:40Z,"  In this thesis, we describe a new, practical approach to integrating
hardware-based data compression within the memory hierarchy, including on-chip
caches, main memory, and both on-chip and off-chip interconnects. This new
approach is fast, simple, and effective in saving storage space. A key insight
in our approach is that access time (including decompression latency) is
critical in modern memory hierarchies. By combining inexpensive hardware
support with modest OS support, our holistic approach to compression achieves
substantial improvements in performance and energy efficiency across the memory
hierarchy. Using this new approach, we make several major contributions in this
thesis. First, we propose a new compression algorithm, Base-Delta-Immediate
Compression (BDI), that achieves high compression ratio with very low
compression/decompression latency. BDI exploits the existing low dynamic range
of values present in many cache lines to compress them to smaller sizes using
Base+Delta encoding. Second, we observe that the compressed size of a cache
block can be indicative of its reuse. We use this observation to develop a new
cache insertion policy for compressed caches, the Size-based Insertion Policy
(SIP), which uses the size of a compressed block as one of the metrics to
predict its potential future reuse. Third, we propose a new main memory
compression framework, Linearly Compressed Pages (LCP), that significantly
reduces the complexity and power cost of supporting main memory compression. We
demonstrate that any compression algorithm can be adapted to fit the
requirements of LCP, and that LCP can be efficiently integrated with the
existing cache compression designs, avoiding extra compression/decompression.
",['\nGennady Pekhimenko\n'],PhD Thesis,,http://arxiv.org/abs/1609.02067v1,cs.AR,"['cs.AR', 'cs.OS']",,,[]
"Compatible and Usable Mandatory Access Control for Good-enough OS
  Security",http://arxiv.org/abs/1609.00875v1,2016-09-03T23:09:11Z,2016-09-03T23:09:11Z,"  OS compromise is one of the most serious computer security problems today,
but still not being resolved. Although people proposed different kinds of
methods, they could not be accepted by most users who are non-expert due to the
lack of compatibility and usability. In this paper, we introduce a kind of new
mandatory access control model, named CUMAC, that aims to achieve good-enough
security, high compatibility and usability. It has two novel features. One is
access control based on tracing potential intrusion that can reduce false
negatives and facilitate security configuration, in order to improve both
compatibility and usability; the other is automatically figuring out all of the
compatibility exceptions that usually incurs incompatible problems. The
experiments performed on the prototype show that CUMAC can defense attacks from
network, mobile disk and local untrustable users while keeping good
compatibility and usability.
",['\nZhiyong Shan\n'],"Electronic Commerce and Security, 2009. ISECS '09. Second
  International Symposium on",,http://dx.doi.org/10.1109/ISECS.2009.29,cs.OS,['cs.OS'],10.1109/ISECS.2009.29,,[]
"Suspicious-Taint-Based Access Control for Protecting OS from Network
  Attacks",http://arxiv.org/abs/1609.00100v2,2016-09-01T03:45:39Z,2016-09-03T22:40:31Z,"  Today, security threats to operating systems largely come from network.
Traditional discretionary access control mechanism alone can hardly defeat
them. Although traditional mandatory access control models can effectively
protect the security of OS, they have problems of being incompatible with
application software and complex in administration. In this paper, we propose a
new model, Suspicious-Taint-Based Access Control (STBAC) model, for defeating
network attacks while being compatible, simple and maintaining good system
performance. STBAC regards the processes using Non-Trustable-Communications as
the starting points of suspicious taint, traces the activities of the
suspiciously tainted processes by taint rules, and forbids the suspiciously
tainted processes to illegally access vital resources by protection rules. Even
in the cases when some privileged processes are subverted, STBAC can still
protect vital resources from being compromised by the intruder. We implemented
the model in the Linux kernel and evaluated it through experiments. The
evaluation showed that STBAC could protect vital resources effectively without
significant impact on compatibility and performance.
",['\nZhiyong Shan\n'],,,http://arxiv.org/abs/1609.00100v2,cs.CR,"['cs.CR', 'cs.OS']",,,[]
POLYPATH: Supporting Multiple Tradeoffs for Interaction Latency,http://arxiv.org/abs/1608.05654v1,2016-08-19T16:23:24Z,2016-08-19T16:23:24Z,"  Modern mobile systems use a single input-to-display path to serve all
applications. In meeting the visual goals of all applications, the path has a
latency inadequate for many important interactions. To accommodate the
different latency requirements and visual constraints by different
interactions, we present POLYPATH, a system design in which application
developers (and users) can choose from multiple path designs for their
application at any time. Because a POLYPATH system asks for two or more path
designs, we present a novel fast path design, called Presto. Presto reduces
latency by judiciously allowing frame drops and tearing.
  We report an Android 5-based prototype of POLYPATH with two path designs:
Android legacy and Presto. Using this prototype, we quantify the effectiveness,
overhead, and user experience of POLYPATH, especially Presto, through both
objective measurements and subjective user assessment. We show that Presto
reduces the latency of legacy touchscreen drawing applications by almost half;
and more importantly, this reduction is orthogonal to that of other popular
approaches and is achieved without any user-noticeable negative visual effect.
When combined with touch prediction, Presto is able to reduce the touch latency
below 10 ms, a remarkable achievement without any hardware support.
","['\nMin Hong Yun\n', '\nSongtao He\n', '\nLin Zhong\n']",,,http://arxiv.org/abs/1608.05654v1,cs.OS,['cs.OS'],,,[]
SandBlaster: Reversing the Apple Sandbox,http://arxiv.org/abs/1608.04303v1,2016-08-15T15:26:22Z,2016-08-15T15:26:22Z,"  In order to limit the damage of malware on Mac OS X and iOS, Apple uses
sandboxing, a kernel-level security layer that provides tight constraints for
system calls. Particularly used for Apple iOS, sandboxing prevents apps from
executing potentially dangerous actions, by defining rules in a sandbox
profile. Investigating Apple's built-in sandbox profiles is difficult as they
are compiled and stored in binary format. We present SandBlaster, a software
bundle that is able to reverse/decompile Apple binary sandbox profiles to their
original human readable SBPL (SandBox Profile Language) format. We use
SandBlaster to reverse all built-in Apple iOS binary sandbox profiles for iOS
7, 8 and 9. Our tool is, to the best of our knowledge, the first to provide a
full reversing of the Apple sandbox, shedding light into the inner workings of
Apple sandbox profiles and providing essential support for security researchers
and professionals interested in Apple security mechanisms.
","['\nRăzvan Deaconescu\n', '\nLuke Deshotels\n', '\nMihai Bucicoiu\n', '\nWilliam Enck\n', '\nLucas Davi\n', '\nAhmad-Reza Sadeghi\n']","25 pages, 9 figures, 14 listings This report is an auxiliary document
  to the paper ""SandScout: Automatic Detection of Flaws in iOS Sandbox
  Profiles"", to be presented at the ACM Conference on Computer and
  Communications Security (CCS) 2016",,http://arxiv.org/abs/1608.04303v1,cs.CR,"['cs.CR', 'cs.OS', 'D.4.6']",,,[]
System-level Scalable Checkpoint-Restart for Petascale Computing,http://arxiv.org/abs/1607.07995v2,2016-07-27T07:46:13Z,2016-09-24T01:49:03Z,"  Fault tolerance for the upcoming exascale generation has long been an area of
active research. One of the components of a fault tolerance strategy is
checkpointing. Petascale-level checkpointing is demonstrated through a new
mechanism for virtualization of the InfiniBand UD (unreliable datagram) mode,
and for updating the remote address on each UD-based send, due to lack of a
fixed peer. Note that InfiniBand UD is required to support modern MPI
implementations. An extrapolation from the current results to future SSD-based
storage systems provides evidence that the current approach will remain
practical in the exascale generation. This transparent checkpointing approach
is evaluated using a framework of the DMTCP checkpointing package. Results are
shown for HPCG (linear algebra), NAMD (molecular dynamics), and the NAS NPB
benchmarks. In tests up to 32,752 MPI processes on 32,752 CPU cores,
checkpointing of a computation with a 38 TB memory footprint in 11 minutes is
demonstrated. Runtime overhead is reduced to less than 1%. The approach is also
evaluated across three widely used MPI implementations.
","['\nJiajun Cao\n', '\nKapil Arya\n', '\nRohan Garg\n', '\nShawn Matott\n', '\nDhabaleswar K. Panda\n', '\nHari Subramoni\n', '\nJérôme Vienne\n', '\nGene Cooperman\n']","18 pages, 5 figures, to be published in ICPADS 2016",,http://arxiv.org/abs/1607.07995v2,cs.DC,"['cs.DC', 'cs.OS', 'C.4; D.1.3; D.2.11']",,,[]
"TREES: A CPU/GPU Task-Parallel Runtime with Explicit Epoch
  Synchronization",http://arxiv.org/abs/1608.00571v1,2016-08-01T15:33:14Z,2016-08-01T15:33:14Z,"  We have developed a task-parallel runtime system, called TREES, that is
designed for high performance on CPU/GPU platforms. On platforms with multiple
CPUs, Cilk's ""work-first"" principle underlies how task-parallel applications
can achieve performance, but work-first is a poor fit for GPUs. We build upon
work-first to create the ""work-together"" principle that addresses the specific
strengths and weaknesses of GPUs. The work-together principle extends
work-first by stating that (a) the overhead on the critical path should be paid
by the entire system at once and (b) work overheads should be paid
co-operatively. We have implemented the TREES runtime in OpenCL, and we
experimentally evaluate TREES applications on a CPU/GPU platform.
","['\nBlake A. Hechtman\n', '\nAndrew D. Hilton\n', '\nDaniel J. Sorin\n']",,,http://arxiv.org/abs/1608.00571v1,cs.DC,"['cs.DC', 'cs.OS', 'cs.PL']",,,[]
"Energy-Efficient Real-Time Scheduling for Two-Type Heterogeneous
  Multiprocessors",http://arxiv.org/abs/1607.07763v1,2016-07-15T14:52:57Z,2016-07-15T14:52:57Z,"  We propose three novel mathematical optimization formulations that solve the
same two-type heterogeneous multiprocessor scheduling problem for a real-time
taskset with hard constraints. Our formulations are based on a global
scheduling scheme and a fluid model. The first formulation is a mixed-integer
nonlinear program, since the scheduling problem is intuitively considered as an
assignment problem. However, by changing the scheduling problem to first
determine a task workload partition and then to find the execution order of all
tasks, the computation time can be significantly reduced. Specifically, the
workload partitioning problem can be formulated as a continuous nonlinear
program for a system with continuous operating frequency, and as a continuous
linear program for a practical system with a discrete speed level set. The task
ordering problem can be solved by an algorithm with a complexity that is linear
in the total number of tasks. The work is evaluated against existing global
energy/feasibility optimal workload allocation formulations. The results
illustrate that our algorithms are both feasibility optimal and energy optimal
for both implicit and constrained deadline tasksets. Specifically, our
algorithm can achieve up to 40% energy saving for some simulated tasksets with
constrained deadlines. The benefit of our formulation compared with existing
work is that our algorithms can solve a more general class of scheduling
problems due to incorporating a scheduling dynamic model in the formulations
and allowing for a time-varying speed profile. Moreover, our algorithms can be
applied to both online and offline scheduling schemes.
","['\nMason Thammawichai\n', '\nEric C. Kerrigan\n']",,"Real-Time Systems, 2017",http://dx.doi.org/10.1007/s11241-017-9291-6,cs.DC,"['cs.DC', 'cs.OS', 'cs.SY', 'math.OC']",10.1007/s11241-017-9291-6,,[]
A Note on the Period Enforcer Algorithm for Self-Suspending Tasks,http://arxiv.org/abs/1606.04386v1,2016-06-14T14:22:25Z,2016-06-14T14:22:25Z,"  The period enforcer algorithm for self-suspending real-time tasks is a
technique for suppressing the ""back-to-back"" scheduling penalty associated with
deferred execution. Originally proposed in 1991, the algorithm has attracted
renewed interest in recent years. This note revisits the algorithm in the light
of recent developments in the analysis of self-suspending tasks, carefully
re-examines and explains its underlying assumptions and limitations, and points
out three observations that have not been made in the literature to date: (i)
period enforcement is not strictly superior (compared to the base case without
enforcement) as it can cause deadline misses in self-suspending task sets that
are schedulable without enforcement; (ii) to match the assumptions underlying
the analysis of the period enforcer, a schedulability analysis of
self-suspending tasks subject to period enforcement requires a task set
transformation for which no solution is known in the general case, and which is
subject to exponential time complexity (with current techniques) in the limited
case of a single self-suspending task; and (iii) the period enforcer algorithm
is incompatible with all existing analyses of suspension-based locking
protocols, and can in fact cause ever-increasing suspension times until a
deadline is missed.
","['\nJian-Jia Chen\n', '\nBjörn B. Brandenburg\n']",,,http://dx.doi.org/10.4230/LITES-v004-i001-a001,cs.DC,"['cs.DC', 'cs.OS', 'C.3; D.4.1']",10.4230/LITES-v004-i001-a001,,[]
Scalability of VM Provisioning Systems,http://arxiv.org/abs/1606.05794v1,2016-06-18T18:55:42Z,2016-06-18T18:55:42Z,"  Virtual machines and virtualized hardware have been around for over half a
century. The commoditization of the x86 platform and its rapidly growing
hardware capabilities have led to recent exponential growth in the use of
virtualization both in the enterprise and high performance computing (HPC). The
startup time of a virtualized environment is a key performance metric for high
performance computing in which the runtime of any individual task is typically
much shorter than the lifetime of a virtualized service in an enterprise
context. In this paper, a methodology for accurately measuring the startup
performance on an HPC system is described. The startup performance overhead of
three of the most mature, widely deployed cloud management frameworks
(OpenStack, OpenNebula, and Eucalyptus) is measured to determine their
suitability for workloads typically seen in an HPC environment. A 10x
performance difference is observed between the fastest (Eucalyptus) and the
slowest (OpenNebula) framework. This time difference is primarily due to delays
in waiting on networking in the cloud-init portion of the startup. The
methodology and measurements presented should facilitate the optimization of
startup across a variety of virtualization environments.
","['\nMike Jones\n', '\nBill Arcand\n', '\nBill Bergeron\n', '\nDavid Bestor\n', '\nChansup Byun\n', '\nLauren Milechin\n', '\nVijay Gadepally\n', '\nMatt Hubbell\n', '\nJeremy Kepner\n', '\nPete Michaleas\n', '\nJulie Mullen\n', '\nAndy Prout\n', '\nTony Rosa\n', '\nSiddharth Samsi\n', '\nCharles Yee\n', '\nAlbert Reuther\n']","5 pages; 6 figures; accepted to the IEEE High Performance Extreme
  Computing (HPEC) conference 2016",,http://dx.doi.org/10.1109/HPEC.2016.7761629,cs.DC,"['cs.DC', 'cs.CY', 'cs.OS', 'cs.SY']",10.1109/HPEC.2016.7761629,,[]
It's Time: OS Mechanisms for Enforcing Asymmetric Temporal Integrity,http://arxiv.org/abs/1606.00111v2,2016-06-01T04:29:58Z,2016-06-02T05:32:46Z,"  Mixed-criticality systems combine real-time components of different levels of
criticality, i.e. severity of failure, on the same processor, in order to
obtain good resource utilisation. They must guarantee deadlines of
highly-critical tasks at the expense of lower-criticality ones in the case of
overload. Present operating systems provide inadequate support for this kind of
system, which is of growing importance in avionics and other verticals. We
present an approach that provides the required asymmetric integrity and its
implementation in the high-assurance seL4 microkernel.
","['\nAnna Lyons\n', '\nGernot Heiser\n']",Paper submitted to OSDI 2016,,http://arxiv.org/abs/1606.00111v2,cs.OS,['cs.OS'],,,[]
The Design of the NetBSD I/O Subsystems,http://arxiv.org/abs/1605.05810v1,2016-05-19T05:03:00Z,2016-05-19T05:03:00Z,"  This book describes the source code of the NetBSD Operating System Release
1.6 in SUN UltraSPARC 64-bit platform by annotating related excerpts from
references and user manuals on the NetBSD Operating System. The goal of this
book is to provide necessary information to understand the operation and the
implementation of I/O subsystems in the kernel as well as to design and
implement a new filesystem on the NetBSD platform.
",['\nSungWon Chung\n'],"This arXiv archival version is the same as the initial 2002 release
  of this publication except updates in preface and few corrections on typos
  and contact information",,http://arxiv.org/abs/1605.05810v1,cs.OS,['cs.OS'],,,[]
"An optimized round robin cpu scheduling algorithm with dynamic time
  quantum",http://arxiv.org/abs/1605.00362v1,2016-05-02T06:24:43Z,2016-05-02T06:24:43Z,"  CPU scheduling is one of the most crucial operations performed by operating
system. Different algorithms are available for CPU scheduling amongst them RR
(Round Robin) is considered as optimal in time shared environment. The
effectiveness of Round Robin completely depends on the choice of time quantum.
In this paper a new CPU scheduling algorithm has been proposed, named as DABRR
(Dynamic Average Burst Round Robin). That uses dynamic time quantum instead of
static time quantum used in RR. The performance of the proposed algorithm is
experimentally compared with traditional RR and some existing variants of RR.
The results of our approach presented in this paper demonstrate improved
performance in terms of average waiting time, average turnaround time, and
context switching.
","['\nAmar Ranjan Dash\n', '\nSandipta kumar Sahu\n', '\nSanjay Kumar Samantra\n']","20 pages, 7 figures, 16 Tables. arXiv admin note: text overlap with
  arXiv:1511.02498","International Journal of Computer Science, Engineering and
  Information Technology (IJCSEIT), Vol. 5,No.1, February 2015",http://dx.doi.org/10.5121/ijcseit.2015.5102,cs.OS,['cs.OS'],10.5121/ijcseit.2015.5102,,[]
"A Qualitative Comparison of MPSoC Mobile and Embedded Virtualization
  Techniques",http://arxiv.org/abs/1605.01168v1,2016-05-04T07:36:46Z,2016-05-04T07:36:46Z,"  Virtualization is generally adopted in server and desktop environments to
provide for fault tolerance, resource management, and energy efficiency.
Virtualization enables parallel execution of multiple operating systems (OSs)
while sharing the hardware resources. Virtualization was previously not deemed
as feasible technology for mobile and embedded devices due to their limited
processing and memory resource. However, the enterprises are advocating Bring
Your Own Device (BYOD) applications that enable co-existence of heterogeneous
OSs on a single mobile device. Moreover, embedded device require virtualization
for logical isolation of secure and general purpose OSs on a single device. In
this paper, we investigate the processor architectures in the mobile and
embedded space while examining their formal visualizability. We also compare
the virtualization solutions enabling coexistence of multiple OSs in Multicore
Processor System-on-Chip (MPSoC) mobile and embedded systems. We advocate that
virtualization is necessary to manage resource in MPSoC designs and to enable
BYOD, security, and logical isolation use cases.
","['\nJunaid Shuja\n', '\nAbdullah Gani\n', '\nSajjad A. Madani\n']","International Conference of Global Network for Innovative Technology
  (IGNITE-2014), Penang, Malaysia",,http://arxiv.org/abs/1605.01168v1,cs.OS,['cs.OS'],,,[]
"Isolate First, Then Share: a New OS Architecture for Datacenter
  Computing",http://arxiv.org/abs/1604.01378v5,2016-04-05T19:33:27Z,2017-10-26T03:27:40Z,"  This paper presents the ""isolate first, then share"" OS model in which the
processor cores, memory, and devices are divided up between disparate OS
instances and a new abstraction, subOS, is proposed to encapsulate an OS
instance that can be created, destroyed, and resized on-the-fly. The intuition
is that this avoids shared kernel states between applications, which in turn
reduces performance loss caused by contention. We decompose the OS into the
supervisor and several subOSes running at the same privilege level: a subOS
directly manages physical resources, while the supervisor can create, destroy,
resize a subOS on-the-fly. The supervisor and subOSes have few state sharing,
but fast inter-subOS communication mechanisms are provided on demand.
  We present the first implementation, RainForest, which supports unmodified
Linux binaries. Our comprehensive evaluation shows RainForest outperforms Linux
with four different kernels, LXC, and Xen in terms of worst-case and average
performance most of time when running a large number of benchmarks. The source
code is available soon.
","['\nGang Lu\n', '\nJianfeng Zhan\n', '\nChongkang Tan\n', '\nXinlong Lin\n', '\nDefei Kong\n', '\nChen Zheng\n', '\nFei Tang\n', '\nCheng Huang\n', '\nLei Wang\n', '\nTianshu Hao\n']","14 pages, 13 figures, 5 tables",,http://arxiv.org/abs/1604.01378v5,cs.OS,"['cs.OS', 'D.4.0; D.4.6; D.4.7']",,,[]
Aware: Controlling App Access to I/O Devices on Mobile Platforms,http://arxiv.org/abs/1604.02171v1,2016-04-07T20:38:51Z,2016-04-07T20:38:51Z,"  Smartphones' cameras, microphones, and device displays enable users to
capture and view memorable moments of their lives. However, adversaries can
trick users into authorizing malicious apps that exploit weaknesses in current
mobile platforms to misuse such on-board I/O devices to stealthily capture
photos, videos, and screen content without the users' consent. Contemporary
mobile operating systems fail to prevent such misuse of I/O devices by
authorized apps due to lack of binding between users' interactions and accesses
to I/O devices performed by these apps. In this paper, we propose Aware, a
security framework for authorizing app requests to perform operations using I/O
devices, which binds app requests with user intentions to make all uses of
certain I/O devices explicit. We evaluate our defense mechanisms through
laboratory-based experimentation and a user study, involving 74 human subjects,
whose ability to identify undesired operations targeting I/O devices increased
significantly. Without Aware, only 18% of the participants were able to
identify attacks from tested RAT apps. Aware systematically blocks all the
attacks in absence of user consent and supports users in identifying 82% of
social-engineering attacks tested to hijack approved requests, including some
more sophisticated forms of social engineering not yet present in available
RATs. Aware introduces only 4.79% maximum performance overhead over operations
targeting I/O devices. Aware shows that a combination of system defenses and
user interface can significantly strengthen defenses for controlling the use of
on-board I/O devices.
","['\nGiuseppe Petracca\n', '\nAhmad Atamli\n', '\nYuqiong Sun\n', '\nJens Grossklags\n', '\nTrent Jaeger\n']",,,http://arxiv.org/abs/1604.02171v1,cs.OS,['cs.OS'],,,[]
AuDroid: Preventing Attacks on Audio Channels in Mobile Devices,http://arxiv.org/abs/1604.00320v1,2016-04-01T16:32:47Z,2016-04-01T16:32:47Z,"  Voice control is a popular way to operate mobile devices, enabling users to
communicate requests to their devices. However, adversaries can leverage voice
control to trick mobile devices into executing commands to leak secrets or to
modify critical information. Contemporary mobile operating systems fail to
prevent such attacks because they do not control access to the speaker at all
and fail to control when untrusted apps may use the microphone, enabling
authorized apps to create exploitable communication channels. In this paper, we
propose a security mechanism that tracks the creation of audio communication
channels explicitly and controls the information flows over these channels to
prevent several types of attacks.We design and implement AuDroid, an extension
to the SELinux reference monitor integrated into the Android operating system
for enforcing lattice security policies over the dynamically changing use of
system audio resources. To enhance flexibility, when information flow errors
are detected, the device owner, system apps and services are given the
opportunity to resolve information flow errors using known methods, enabling
AuDroid to run many configurations safely. We evaluate our approach on 17
widely-used apps that make extensive use of the microphone and speaker, finding
that AuDroid prevents six types of attack scenarios on audio channels while
permitting all 17 apps to run effectively. AuDroid shows that it is possible to
prevent attacks using audio channels without compromising functionality or
introducing significant performance overhead.
","['\nGiuseppe Petracca\n', '\nYuqiong Sun\n', '\nAhmad Atamli\n', '\nTrent Jaeger\n']",2015 Annual Computer Security Applications Conference,,http://arxiv.org/abs/1604.00320v1,cs.CR,"['cs.CR', 'cs.OS']",,,[]
"An Implementation and Analysis of a Kernel Network Stack in Go with the
  CSP Style",http://arxiv.org/abs/1603.05636v1,2016-03-17T19:43:36Z,2016-03-17T19:43:36Z,"  Modern operating system kernels are written in lower-level languages such as
C. Although the low-level functionalities of C are often useful within kernels,
they also give rise to several classes of bugs. Kernels written in higher level
languages avoid many of these potential problems, at the possible cost of
decreased performance. This research evaluates the advantages and disadvantages
of a kernel written in a higher level language. To do this, the network stack
subsystem of the kernel was implemented in Go with the Communicating Sequential
Processes (CSP) style. Go is a high-level programming language that supports
the CSP style, which recommends splitting large tasks into several smaller ones
running in independent ""threads"". Modules for the major networking protocols,
including Ethernet, ARP, IPv4, ICMP, UDP, and TCP, were implemented. In this
study, the implemented Go network stack, called GoNet, was compared to a
representative network stack written in C. The GoNet code is more readable and
generally performs better than that of its C stack counterparts. From this, it
can be concluded that Go with CSP style is a viable alternative to C for the
language of kernel implementations.
","['\nHarshal Sheth\n', '\nAashish Welling\n']",10 pages,,http://arxiv.org/abs/1603.05636v1,cs.OS,['cs.OS'],,,[]
Powering the Internet of Things with RIOT: Why? How? What is RIOT?,http://arxiv.org/abs/1603.03635v1,2016-03-11T14:10:18Z,2016-03-11T14:10:18Z,"  The crucial importance of software platforms was highlighted by recent events
both at the political level (e.g. renewed calls for digital data and operating
system ""sovereignty"", following E. Snowden's revelations) and at the business
level (e.g. Android generated a new industry worth tens of billions of euros
yearly). In the Internet of Things, which is expected to generate business at
very large scale, but also to threaten even more individual privacy, such
aspects will be exacerbated. The need for an operating system like RIOT stems
from this context, and this short article outlines RIOT's main non-technical
aspects, as well as its key technical characteristics.
","['\nEmmanuel Baccelli\n', '\nKaspar Schleiser\n']",4 pages,,http://arxiv.org/abs/1603.03635v1,cs.CY,"['cs.CY', 'cs.NI', 'cs.OS']",,,[]
HyBIS: Windows Guest Protection through Advanced Memory Introspection,http://arxiv.org/abs/1601.05851v1,2016-01-22T01:22:53Z,2016-01-22T01:22:53Z,"  Effectively protecting the Windows OS is a challenging task, since most
implementation details are not publicly known. Windows has always been the main
target of malwares that have exploited numerous bugs and vulnerabilities.
Recent trusted boot and additional integrity checks have rendered the Windows
OS less vulnerable to kernel-level rootkits. Nevertheless, guest Windows
Virtual Machines are becoming an increasingly interesting attack target. In
this work we introduce and analyze a novel Hypervisor-Based Introspection
System (HyBIS) we developed for protecting Windows OSes from malware and
rootkits. The HyBIS architecture is motivated and detailed, while targeted
experimental results show its effectiveness. Comparison with related work
highlights main HyBIS advantages such as: effective semantic introspection,
support for 64-bit architectures and for latest Windows (8.x and 10), advanced
malware disabling capabilities. We believe the research effort reported here
will pave the way to further advances in the security of Windows OSes.
","['\nRoberto di Pietro\n', '\nFederico Franzoni\n', '\nFlavio Lombardi\n']",,,http://dx.doi.org/10.1007/978-3-319-58469-0_13,cs.OS,"['cs.OS', 'cs.CR']",10.1007/978-3-319-58469-0_13,,[]
Open Mobile API: Accessing the UICC on Android Devices,http://arxiv.org/abs/1601.03027v1,2016-01-12T20:46:07Z,2016-01-12T20:46:07Z,"  This report gives an overview of secure element integration into Android
devices. It focuses on the Open Mobile API as an open interface to access
secure elements from Android applications. The overall architecture of the Open
Mobile API is described and current Android devices are analyzed with regard to
the availability of this API. Moreover, this report summarizes our efforts of
reverse engineering the stock ROM of a Samsung Galaxy S3 in order to analyze
the integration of the Open Mobile API and the interface that is used to
perform APDU-based communication with the UICC (Universal Integrated Circuit
Card). It further provides a detailed explanation on how to integrate this
functionality into CyanogenMod (an after-market firmware for Android devices).
","['\nMichael Roland\n', '\nMichael Hölzl\n']","University of Applied Sciences Upper Austria, JR-Center u'smile,
  Technical report, 76 pages, 12 figures",,http://arxiv.org/abs/1601.03027v1,cs.CR,"['cs.CR', 'cs.OS', 'C.2.0; C.3; C.5.3; D.2.7; D.4.6']",,,[]
Research on Scalability of Operating Systems on Multicore Processors,http://arxiv.org/abs/1512.06908v1,2015-12-21T23:35:42Z,2015-12-21T23:35:42Z,"  Large number of cores and hardware resource sharing are two characteristics
on multicore processors, which bring new challenges for the design of operating
systems. How to locate and analyze the speedup restrictive factors in operating
systems, how to simulate and avoid the phenomenon that speedup decreases with
the number of cores because of lock contention (i.e., lock thrashing) and how
to avoid the contention of shared resources such as the last level cache are
key challenges for the operating system scalability research on multicore
systems.
",['\nYan Cui\n'],,,http://arxiv.org/abs/1512.06908v1,cs.OS,['cs.OS'],,,[]
Energy-aware Fixed-Priority Multi-core Scheduling for Real-time Systems,http://arxiv.org/abs/1512.07351v1,2015-12-23T04:39:00Z,2015-12-23T04:39:00Z,"  Multi-core processors are becoming more and more popular in embedded and
real-time systems. While fixed-priority scheduling with task-splitting in
real-time systems are widely applied, current approaches have not taken into
consideration energy-aware aspects such as dynamic voltage/frequency scheduling
(DVS). In this paper, we propose two strategies to apply dynamic voltage
scaling (DVS) to fixed-priority scheduling algorithms with task-splitting for
periodic real-time tasks on multi-core processors. The first strategy
determines voltage scales for each processor after scheduling (Static DVS),
which ensures all tasks meet the timing requirements on synchronization. The
second strategy adaptively determines the frequency of each task before
scheduling (Adaptive DVS) according to the total utilization of task-set and
number of cores available. The combination of frequency pre-allocation and
task-splitting makes it possible to maximize energy savings with DVS.
Simulation results show that it is possible to achieve significant energy
savings with DVS while preserving the schedulability requirements of real-time
schedulers for multi-core processors.
","['\nYao Guo\n', '\nJunyang Lu\n']",conference extension,,http://arxiv.org/abs/1512.07351v1,cs.OS,"['cs.OS', 'C.3']",,,[]
Mixed-Criticality Scheduling with I/O,http://arxiv.org/abs/1512.07654v3,2015-12-23T22:41:04Z,2016-03-12T18:13:31Z,"  This paper addresses the problem of scheduling tasks with different
criticality levels in the presence of I/O requests. In mixed-criticality
scheduling, higher criticality tasks are given precedence over those of lower
criticality when it is impossible to guarantee the schedulability of all tasks.
While mixed-criticality scheduling has gained attention in recent years, most
approaches typically assume a periodic task model. This assumption does not
always hold in practice, especially for real-time and embedded systems that
perform I/O. For example, many tasks block on I/O requests until devices signal
their completion via interrupts; both the arrival of interrupts and the waking
of blocked tasks can be aperiodic. In our prior work, we developed a scheduling
technique in the Quest real-time operating system, which integrates the
time-budgeted management of I/O operations with Sporadic Server scheduling of
tasks. This paper extends our previous scheduling approach with support for
mixed-criticality tasks and I/O requests on the same processing core. Results
show the effective schedulability of different task sets in the presence of I/O
requests is superior in our approach compared to traditional methods that
manage I/O using techniques such as Sporadic Servers.
","['\nEric Missimer\n', '\nKatherine Zhao\n', '\nRichard West\n']","Second version has replaced simulation experiments with real machine
  experiments, third version fixed minor error in Equation 5 (missing a plus
  sign)",,http://arxiv.org/abs/1512.07654v3,cs.OS,['cs.OS'],,,[]
Real-Time scheduling: from hard to soft real-time systems,http://arxiv.org/abs/1512.01978v1,2015-12-07T11:05:05Z,2015-12-07T11:05:05Z,"  Real-time systems are traditionally classified into hard real-time and soft
real-time: in the first category we have safety critical real-time systems
where missing a deadline can have catastrophic consequences, whereas in the
second class we find systems or which we need to optimise the Quality of
service provided to the user. However, the frontier between these two classes
is thinner than one may think, and many systems that were considered as hard
real-time in the past should now be reconsidered under a different light. In
this paper we shall first recall the fundamental notion of time-predictability
and criticality, in order to understand where the real-time deadlines that we
use in our theoretical models come from. We shall then introduce the model of a
soft real-time system and present one popular method for scheduling hard and
soft real-time tasks, the resource reservation framework. Finally, we shall
show how resource reservation techniques can be successfully applied to the
design of classical control systems, thus adding robustness to the system and
increasing resource utilisation and performance.
","['\nGiuseppe Lipari\n', '\nLuigi Palopoli\n']",,,http://arxiv.org/abs/1512.01978v1,cs.OS,['cs.OS'],,,[]
"Parallel and sequential reclaiming in multicore real-time global
  scheduling",http://arxiv.org/abs/1512.01984v2,2015-12-07T11:16:43Z,2016-03-10T11:04:54Z,"  When integrating hard, soft and non-real-time tasks in general purpose
operating systems, it is necessary to provide temporal isolation so that the
timing properties of one task do not depend on the behaviour of the others.
However, strict budget enforcement can lead to inefficient use of the
computational resources in the presence of tasks with variable workload. Many
resource reclaiming algorithms have been proposed in the literature for single
processor scheduling, but not enough work exists for global scheduling in
multiprocessor systems. In this report, we propose two reclaiming algorithms
for multiprocessor global scheduling and we prove their correctness.
","['\nLuca Abeni\n', '\nGiuseppe Lipari\n', '\nAndrea Parri\n', '\nYoucheng Sun\n']",,,http://arxiv.org/abs/1512.01984v2,cs.OS,['cs.OS'],,,[]
TinyLFU: A Highly Efficient Cache Admission Policy,http://arxiv.org/abs/1512.00727v2,2015-12-02T15:05:46Z,2015-12-03T10:38:11Z,"  This paper proposes to use a frequency based cache admission policy in order
to boost the effectiveness of caches subject to skewed access distributions.
Given a newly accessed item and an eviction candidate from the cache, our
scheme decides, based on the recent access history, whether it is worth
admitting the new item into the cache at the expense of the eviction candidate.
  Realizing this concept is enabled through a novel approximate LFU structure
called TinyLFU, which maintains an approximate representation of the access
frequency of a large sample of recently accessed items. TinyLFU is very compact
and light-weight as it builds upon Bloom filter theory.
  We study the properties of TinyLFU through simulations of both synthetic
workloads as well as multiple real traces from several sources. These
simulations demonstrate the performance boost obtained by enhancing various
replacement policies with the TinyLFU eviction policy. Also, a new combined
replacement and eviction policy scheme nicknamed W-TinyLFU is presented.
W-TinyLFU is demonstrated to obtain equal or better hit-ratios than other state
of the art replacement policies on these traces. It is the only scheme to
obtain such good results on all traces.
","['\nGil Einziger\n', '\nRoy Friedman\n', '\nBen Manes\n']","A much earlier and shorter version of this work appeared in the
  Euromicro PDP 2014 conference",,http://arxiv.org/abs/1512.00727v2,cs.OS,['cs.OS'],,,[]
Specifying a Realistic File System,http://arxiv.org/abs/1511.04169v1,2015-11-13T06:27:39Z,2015-11-13T06:27:39Z,"  We present the most interesting elements of the correctness specification of
BilbyFs, a performant Linux flash file system. The BilbyFs specification
supports asynchronous writes, a feature that has been overlooked by several
file system verification projects, and has been used to verify the correctness
of BilbyFs's fsync() C implementation. It makes use of nondeterminism to be
concise and is shallowly-embedded in higher-order logic.
","['\nSidney Amani\nNICTA and University of New South Wales, Australia\n', '\nToby Murray\nNICTA and University of New South Wales, Australia\n']","In Proceedings MARS 2015, arXiv:1511.02528","EPTCS 196, 2015, pp. 1-9",http://dx.doi.org/10.4204/EPTCS.196.1,cs.LO,"['cs.LO', 'cs.OS']",10.4204/EPTCS.196.1,,"['NICTA and University of New South Wales, Australia', 'NICTA and University of New South Wales, Australia']"
"Controlled Owicki-Gries Concurrency: Reasoning about the Preemptible
  eChronos Embedded Operating System",http://arxiv.org/abs/1511.04170v1,2015-11-13T06:27:48Z,2015-11-13T06:27:48Z,"  We introduce a controlled concurrency framework, derived from the
Owicki-Gries method, for describing a hardware interface in detail sufficient
to support the modelling and verification of small, embedded operating systems
(OS's) whose run-time responsiveness is paramount. Such real-time systems run
with interrupts mostly enabled, including during scheduling. That differs from
many other successfully modelled and verified OS's that typically reduce the
complexity of concurrency by running on uniprocessor platforms and by switching
interrupts off as much as possible. Our framework builds on the traditional
Owicki-Gries method, for its fine-grained concurrency is needed for
high-performance system code. We adapt it to support explicit concurrency
control, by providing a simple, faithful representation of the hardware
interface that allows software to control the degree of interleaving between
user code, OS code, interrupt handlers and a scheduler that controls context
switching. We then apply this framework to model the interleaving behavior of
the eChronos OS, a preemptible real-time OS for embedded micro-controllers. We
discuss the accuracy and usability of our approach when instantiated to model
the eChronos OS. Both our framework and the eChronos model are formalised in
the Isabelle/HOL theorem prover, taking advantage of the high level of
automation in modern reasoning tools.
","['\nJune Andronick\nNICTA and UNSW\n', '\nCorey Lewis\nNICTA\n', '\nCarroll Morgan\nNICTA and UNSW\n']","In Proceedings MARS 2015, arXiv:1511.02528","EPTCS 196, 2015, pp. 10-24",http://dx.doi.org/10.4204/EPTCS.196.2,cs.LO,"['cs.LO', 'cs.OS']",10.4204/EPTCS.196.2,,"['NICTA and UNSW', 'NICTA', 'NICTA and UNSW']"
"Characteristic specific prioritized dynamic average burst round robin
  scheduling for uniprocessor and multiprocessor environment",http://arxiv.org/abs/1511.02498v1,2015-11-08T15:44:59Z,2015-11-08T15:44:59Z,"  CPU scheduling is one of the most crucial operations performed by operating
systems. Different conventional algorithms like FCFS, SJF, Priority, and RR
(Round Robin) are available for CPU Scheduling. The effectiveness of Priority
and Round Robin scheduling algorithm completely depends on selection of
priority features of processes and on the choice of time quantum. In this paper
a new CPU scheduling algorithm has been proposed, named as CSPDABRR
(Characteristic specific Prioritized Dynamic Average Burst Round Robin), that
uses seven priority features for calculating priority of processes and uses
dynamic time quantum instead of static time quantum used in RR. The performance
of the proposed algorithm is experimentally compared with traditional RR and
Priority scheduling algorithm in both uni-processor and multi-processor
environment. The results of our approach presented in this paper demonstrate
improved performance in terms of average waiting time, average turnaround time,
and optimal priority feature.
","['\nAmar Ranjan Dash\n', '\nSandipta Kumar Sahu\n', '\nSanjay Kumar Samantra\n', '\nSradhanjali Sabat\n']","20 Pages, 10 Figures, 18 Tables, 20 References, International Journal
  of Computer Science, Engineering and Applications (IJCSEA) Vol.5, No.4/5,
  October 2015",,http://dx.doi.org/10.5121/ijcsea.2015.5501,cs.OS,['cs.OS'],10.5121/ijcsea.2015.5501,,[]
Proceedings Workshop on Models for Formal Analysis of Real Systems,http://arxiv.org/abs/1511.02528v1,2015-11-08T21:12:17Z,2015-11-08T21:12:17Z,"  This volume contains the proceedings of MARS 2015, the first workshop on
Models for Formal Analysis of Real Systems, held on November 23, 2015 in Suva,
Fiji, as an affiliated workshop of LPAR 2015, the 20th International Conference
on Logic for Programming, Artificial Intelligence and Reasoning.
  The workshop emphasises modelling over verification. It aims at discussing
the lessons learned from making formal methods for the verification and
analysis of realistic systems. Examples are:
  (1) Which formalism is chosen, and why?
  (2) Which abstractions have to be made and why?
  (3) How are important characteristics of the system modelled?
  (4) Were there any complications while modelling the system?
  (5) Which measures were taken to guarantee the accuracy of the model?
  We invited papers that present full models of real systems, which may lay the
basis for future comparison and analysis. An aim of the workshop is to present
different modelling approaches and discuss pros and cons for each of them.
Alternative formal descriptions of the systems presented at this workshop are
encouraged, which should foster the development of improved specification
formalisms.
","['\nRob van Glabbeek\n', '\nJan Friso Groote\n', '\nPeter Höfner\n']",,"EPTCS 196, 2015",http://dx.doi.org/10.4204/EPTCS.196,cs.LO,"['cs.LO', 'cs.CR', 'cs.OS', 'cs.SY']",10.4204/EPTCS.196,,[]
Energy-Efficient Scheduling for Homogeneous Multiprocessor Systems,http://arxiv.org/abs/1510.05567v2,2015-10-19T16:26:47Z,2015-11-12T12:16:11Z,"  We present a number of novel algorithms, based on mathematical optimization
formulations, in order to solve a homogeneous multiprocessor scheduling
problem, while minimizing the total energy consumption. In particular, for a
system with a discrete speed set, we propose solving a tractable linear
program. Our formulations are based on a fluid model and a global scheduling
scheme, i.e. tasks are allowed to migrate between processors. The new methods
are compared with three global energy/feasibility optimal workload allocation
formulations. Simulation results illustrate that our methods achieve both
feasibility and energy optimality and outperform existing methods for
constrained deadline tasksets. Specifically, the results provided by our
algorithm can achieve up to an 80% saving compared to an algorithm without a
frequency scaling scheme and up to 70% saving compared to a constant frequency
scaling scheme for some simulated tasksets. Another benefit is that our
algorithms can solve the scheduling problem in one step instead of using a
recursive scheme. Moreover, our formulations can solve a more general class of
scheduling problems, i.e. any periodic real-time taskset with arbitrary
deadline. Lastly, our algorithms can be applied to both online and offline
scheduling schemes.
","['\nMason Thammawichai\n', '\nEric C. Kerrigan\n']","Corrected typos: definition of J_i in Section 2.1; (3b)-(3c);
  definition of \Phi_A and \Phi_D in paragraph after (6b). Previous equations
  were correct only for special case of p_i=d_i",,http://arxiv.org/abs/1510.05567v2,cs.OS,"['cs.OS', 'cs.SY', 'math.OC']",,,[]
Multitasking Programming of OBDH Satellite Based On PC-104,http://arxiv.org/abs/1510.02552v1,2015-10-09T02:58:40Z,2015-10-09T02:58:40Z,"  On Board Data Handling (OBDH) has functions to monitor, control, acquire,
analyze, take a decision, and execute the command. OBDH should organize the
task between sub system. OBDH like a heart which has a vital function. Because
the function is seriously important therefore designing and implementing the
OBDH should be carefully, in order to have a good reliability. Many OBDHs have
been made to support the satellite mission using primitive programming. In
handling the data from various input, OBDH should always be available to all
sub systems, when the tasks are many, it is not easy to program using primitive
programming. Sometimes the data become corrupt because the data which come to
the OBDH is in the same time. Therefore it is required to have a way to handle
the data safely and also easy in programming perspective. In this research,
OBDH is programmed using multi tasking programming perspective has been
created. The Operating System (OS) has been implemented so that can run the
tasks simultaneously. The OS is prepared by configuring the Linux Kernel for
the specific processor, creating Root File System (RFS), installing the
BusyBox. In order to do the above method, preparing the environment in our
machine has been done, they are installing the Cross Tool Chain, U-Boot,
GNU-Linux Kernel Source etc. After that, programming using c code with
multitasking programming can be implemented. By using above method, it is found
that programming is easier and the corruption data because of reentrancy can be
minimized. Keywords- Operating System, PC-104, Kernel, C Programming
",['\nHaryono Haryono\n'],8 pages,"International Journal of advanced studies in Computer Science and
  Engineering IJASCSE Volume 4, Issue 8, 2015",http://arxiv.org/abs/1510.02552v1,cs.OS,['cs.OS'],,,[]
Folding a Tree into a Map,http://arxiv.org/abs/1509.07694v1,2015-09-25T12:37:09Z,2015-09-25T12:37:09Z,"  Analysis of the retrieval architecture of the highly influential UNIX file
system (\cite{Ritchie}\cite{multicsfs}) provides insight into design methods,
constraints, and possible alternatives. The basic architecture can be
understood in terms of function composition and recursion by anyone with some
mathematical maturity. Expertise in operating system coding or in any
specialized ""formal method"" is not required.
",['\nVictor Yodaiken\n'],,,http://arxiv.org/abs/1509.07694v1,cs.OS,['cs.OS'],,,[]
Virtualization Architecture for NoC-based Reconfigurable Systems,http://arxiv.org/abs/1508.07127v1,2015-08-28T08:45:35Z,2015-08-28T08:45:35Z,"  We propose a virtualization architecture for NoC-based reconfigurable
systems. The motivation of this work is to develop a service-oriented
architecture that includes Partial Reconfigurable Region as a Service (PRRaaS)
and Processing Element as a Service (PEaaS) for software applications.
According to the requirements of software applications, new PEs can be created
on-demand by (re)configuring the logic resource of the PRRs in the FPGA, while
the configured PEs can also be virtualized to support multiple application
tasks at the same time. As a result, such a two-level virtualization mechanism,
including the gate-level virtualization and the PE-level virtualization,
enables an SoC to be dynamically adapted to changing application requirements.
Therefore, more software applications can be performed, and system performance
can be further enhanced.
","['\nChun-Hsian Huang\n', '\nKwuan-Wei Tseng\n', '\nChih-Cheng Lin\n', '\nFang-Yu Lin\n', '\nPao-Ann Hsiung\n']","Presented at Second International Workshop on FPGAs for Software
  Programmers (FSP 2015) (arXiv:1508.06320)",,http://arxiv.org/abs/1508.07127v1,cs.AR,"['cs.AR', 'cs.OS']",,,[]
"EOS: Automatic In-vivo Evolution of Kernel Policies for Better
  Performance",http://arxiv.org/abs/1508.06356v2,2015-08-26T03:43:38Z,2015-10-18T04:50:29Z,"  Today's monolithic kernels often implement a small, fixed set of policies
such as disk I/O scheduling policies, while exposing many parameters to let
users select a policy or adjust the specific setting of the policy. Ideally,
the parameters exposed should be flexible enough for users to tune for good
performance, but in practice, users lack domain knowledge of the parameters and
are often stuck with bad, default parameter settings.
  We present EOS, a system that bridges the knowledge gap between kernel
developers and users by automatically evolving the policies and parameters in
vivo on users' real, production workloads. It provides a simple policy
specification API for kernel developers to programmatically describe how the
policies and parameters should be tuned, a policy cache to make in-vivo tuning
easy and fast by memorizing good parameter settings for past workloads, and a
hierarchical search engine to effectively search the parameter space.
Evaluation of EOS on four main Linux subsystems shows that it is easy to use
and effectively improves each subsystem's performance.
","['\nYan Cui\n', '\nQuan Chen\n', '\nJunfeng Yang\n']","14 pages, technique report",,http://arxiv.org/abs/1508.06356v2,cs.OS,['cs.OS'],,,[]
A Software-only Mechanism for Device Passthrough and Sharing,http://arxiv.org/abs/1508.06367v2,2015-08-26T05:20:50Z,2016-09-22T10:38:29Z,"  Network processing elements in virtual machines, also known as Network
Function Virtualization (NFV) often face CPU bottlenecks at the virtualization
interface. Even highly optimized paravirtual device interfaces fall short of
the throughput requirements of modern devices. Passthrough devices, together
with SR-IOV support for multiple device virtual functions (VF) and IOMMU
support, mitigate this problem somewhat, by allowing a VM to directly control a
device partition bypassing the virtualization stack. However, device
passthrough requires high-end (expensive and power-hungry) hardware, places
scalability limits on consolidation ratios, and does not support efficient
switching between multiple VMs on the same host.
  We present a paravirtual interface that securely exposes an I/O device
directly to the guest OS running inside the VM, and yet allows that device to
be securely shared among multiple VMs and the host. Compared to the best-known
paravirtualization interfaces, our paravirtual interface supports up to 2x
higher throughput, and is closer in performance to device passthrough. Unlike
device passthrough however, we do not require SR-IOV or IOMMU support, and
allow fine-grained dynamic resource allocation, significantly higher
consolidation ratios, and seamless VM migration. Our security mechanism is
based on a novel approach called dynamic binary opcode subtraction.
","['\nPiyus Kedia\n', '\nSorav Bansal\n']",,,http://arxiv.org/abs/1508.06367v2,cs.OS,['cs.OS'],,,[]
Cold Object Identification in the Java Virtual Machine,http://arxiv.org/abs/1508.04753v1,2015-08-18T19:50:17Z,2015-08-18T19:50:17Z,"  Many Java applications instantiate objects within the Java heap that are
persistent but seldom if ever referenced by the application. Examples include
strings, such as error messages, and collections of value objects that are
preloaded for fast access but they may include objects that are seldom
referenced. This paper describes a stack-based framework for detecting these
""cold"" objects at runtime, with a view to marshaling and sequestering them in
designated regions of the heap where they may be preferentially paged out to a
backing store, thereby freeing physical memory pages for occupation by more
active objects. Furthermore, we evaluate the correctness and efficiency of
stack-based approach with an Access Barrier. The experimental results from a
series of SPECjvm2008 benchmarks are presented.
","['\nKim T. Briggs\n', '\nBaoguo Zhou\n', '\nGerhard W. Dueck\n']",For submission to `Software: Practice and Experience',,http://arxiv.org/abs/1508.04753v1,cs.PL,"['cs.PL', 'cs.OS']",,,[]
"A Case Study on Covert Channel Establishment via Software Caches in
  High-Assurance Computing Systems",http://arxiv.org/abs/1508.05228v1,2015-08-21T09:59:27Z,2015-08-21T09:59:27Z,"  Covert channels can be utilized to secretly deliver information from high
privileged processes to low privileged processes in the context of a
high-assurance computing system. In this case study, we investigate the
possibility of covert channel establishment via software caches in the context
of a framework for component-based operating systems. While component-based
operating systems offer security through the encapsulation of system service
processes, complete isolation of these processes is not reasonably feasible.
This limitation is practically demonstrated with our concept of a specific
covert timing channel based on file system caching. The stability of the covert
channel is evaluated and a methodology to disrupt the covert channel
transmission is presented. While these kinds of attacks are not limited to
high-assurance computing systems, our study practically demonstrates that even
security-focused computing systems with a minimal trusted computing base are
vulnerable for such kinds of attacks and careful design decisions are necessary
for secure operating system architectures.
","['\nWolfgang Schmidt\n', '\nMichael Hanspach\n', '\nJörg Keller\n']","12 pages, based upon the master's thesis of Schmidt",,http://arxiv.org/abs/1508.05228v1,cs.CR,"['cs.CR', 'cs.OS']",,,[]
Optimize Unsynchronized Garbage Collection in an SSD Array,http://arxiv.org/abs/1506.07566v1,2015-06-24T21:14:27Z,2015-06-24T21:14:27Z,"  Solid state disks (SSDs) have advanced to outperform traditional hard drives
significantly in both random reads and writes. However, heavy random writes
trigger fre- quent garbage collection and decrease the performance of SSDs. In
an SSD array, garbage collection of individ- ual SSDs is not synchronized,
leading to underutilization of some of the SSDs.
  We propose a software solution to tackle the unsyn- chronized garbage
collection in an SSD array installed in a host bus adaptor (HBA), where
individual SSDs are exposed to an operating system. We maintain a long I/O
queue for each SSD and flush dirty pages intelligently to fill the long I/O
queues so that we hide the performance imbalance among SSDs even when there are
few parallel application writes. We further define a policy of select- ing
dirty pages to flush and a policy of taking out stale flush requests to reduce
the amount of data written to SSDs. We evaluate our solution in a real system.
Experi- ments show that our solution fully utilizes all SSDs in an array under
random write-heavy workloads. It improves I/O throughput by up to 62% under
random workloads of mixed reads and writes when SSDs are under active garbage
collection. It causes little extra data writeback and increases the cache hit
rate.
","['\nDa Zheng\n', '\nRandal Burns\n', '\nAlexander S. Szalay\n']",,,http://arxiv.org/abs/1506.07566v1,cs.OS,['cs.OS'],,,[]
Defending against malicious peripherals with Cinch,http://arxiv.org/abs/1506.01449v4,2015-06-04T02:11:27Z,2016-06-29T20:42:04Z,"  Malicious peripherals designed to attack their host computers are a growing
problem. Inexpensive and powerful peripherals that attach to plug-and-play
buses have made such attacks easy to mount. Making matters worse, commodity
operating systems lack coherent defenses, and users are often unaware of the
scope of the problem. We present Cinch, a pragmatic response to this threat.
Cinch uses virtualization to attach peripheral devices to a logically separate,
untrusted machine, and includes an interposition layer between the untrusted
machine and the protected one. This layer regulates interaction with devices
according to user-configured policies. Cinch integrates with existing OSes,
enforces policies that thwart real-world attacks, and has low overhead.
","['\nSebastian Angel\n', '\nRiad S. Wahby\n', '\nMax Howald\n', '\nJoshua B. Leners\n', '\nMichael Spilo\n', '\nZhen Sun\n', '\nAndrew J. Blumberg\n', '\nMichael Walfish\n']","18 pages, 7 figures","Proc. USENIX Security (2016), 397--414",http://arxiv.org/abs/1506.01449v4,cs.OS,"['cs.OS', 'cs.CR']",,,[]
Reproducible and User-Controlled Software Environments in HPC with Guix,http://arxiv.org/abs/1506.02822v2,2015-06-09T08:30:23Z,2015-07-26T18:50:32Z,"  Support teams of high-performance computing (HPC) systems often find
themselves between a rock and a hard place: on one hand, they understandably
administrate these large systems in a conservative way, but on the other hand,
they try to satisfy their users by deploying up-to-date tool chains as well as
libraries and scientific software. HPC system users often have no guarantee
that they will be able to reproduce results at a later point in time, even on
the same system-software may have been upgraded, removed, or recompiled under
their feet, and they have little hope of being able to reproduce the same
software environment elsewhere. We present GNU Guix and the functional package
management paradigm and show how it can improve reproducibility and sharing
among researchers with representative use cases.
","['\nLudovic Courtès\nINRIA Bordeaux - Sud-Ouest\n', '\nRicardo Wurmus\n']","2nd International Workshop on Reproducibility in Parallel Computing
  (RepPar), Aug 2015, Vienne, Austria. http://reppar.org/",,http://arxiv.org/abs/1506.02822v2,cs.DC,"['cs.DC', 'cs.OS', 'cs.SE']",,,['INRIA Bordeaux - Sud-Ouest']
Taking back control of HPC file systems with Robinhood Policy Engine,http://arxiv.org/abs/1505.01448v1,2015-05-06T18:14:56Z,2015-05-06T18:14:56Z,"  Today, the largest Lustre file systems store billions of entries. On such
systems, classic tools based on namespace scanning become unusable. Operations
such as managing file lifetime, scheduling data copies, and generating overall
filesystem statistics become painful as they require collecting, sorting and
aggregating information for billions of records. Robinhood Policy Engine is an
open source software developed to address these challenges. It makes it
possible to schedule automatic actions on huge numbers of filesystem entries.
It also gives a synthetic understanding of file systems contents by providing
overall statistics about data ownership, age and size profiles. Even if it can
be used with any POSIX filesystem, Robinhood supports Lustre specific features
like OSTs, pools, HSM, ChangeLogs, and DNE. It implements specific support for
these features, and takes advantage of them to manage Lustre file systems
efficiently.
",['\nThomas Leibovici\n'],"International Workshop on the Lustre Ecosystem: Challenges and
  Opportunities, March 2015, Annapolis MD",,http://arxiv.org/abs/1505.01448v1,cs.DC,"['cs.DC', 'cs.OS']",,,[]
Development of a Burst Buffer System for Data-Intensive Applications,http://arxiv.org/abs/1505.01765v1,2015-05-07T16:31:21Z,2015-05-07T16:31:21Z,"  Modern parallel filesystems such as Lustre are designed to provide high,
scalable I/O bandwidth in response to growing I/O requirements; however, the
bursty I/O characteristics of many data-intensive scientific applications make
it difficult for back-end parallel filesystems to efficiently handle I/O
requests. A burst buffer system, through which data can be temporarily buffered
via high-performance storage mediums, allows for gradual flushing of data to
back-end filesystems. In this paper, we explore issues surrounding the
development of a burst buffer system for data-intensive scientific
applications. Our initial results demonstrate that utilizing a burst buffer
system on top of the Lustre filesystem shows promise for dealing with the
intense I/O traffic generated by application checkpointing.
","['\nTeng Wang\n', '\nSarp Oral\n', '\nMichael Pritchard\n', '\nKevin Vasko\n', '\nWeikuan Yu\n']","International Workshop on the Lustre Ecosystem: Challenges and
  Opportunities, March 2015, Annapolis MD",,http://arxiv.org/abs/1505.01765v1,cs.DC,"['cs.DC', 'cs.OS']",,,[]
"Evaluate and Compare Two Utilization-Based Schedulability-Test
  Frameworks for Real-Time Systems",http://arxiv.org/abs/1505.02155v3,2015-05-08T19:52:46Z,2016-09-23T07:13:59Z,"  This report summarizes two general frameworks, namely k2Q and k2U, that have
been recently developed by us. The purpose of this report is to provide
detailed evaluations and comparisons of these two frameworks. These two
frameworks share some similar characteristics, but they are useful for
different application cases. These two frameworks together provide
comprehensive means for the users to automatically convert the pseudo
polynomial-time tests (or even exponential-time tests) into polynomial-time
tests with closed mathematical forms. With the quadratic and hyperbolic forms,
k2Q and k2U frameworks can be used to provide many quantitive features to be
measured and evaluated, like the total utilization bounds, speed-up factors,
etc., not only for uniprocessor scheduling but also for multiprocessor
scheduling. These frameworks can be viewed as ""blackbox"" interfaces for
providing polynomial-time schedulability tests and response time analysis for
real-time applications. We have already presented their advantages for being
applied in some models in the previous papers. However, it was not possible to
present a more comprehensive comparison between these two frameworks. We hope
this report can help the readers and users clearly understand the difference of
these two frameworks, their unique characteristics, and their advantages. We
demonstrate their differences and properties by using the traditional sporadic
realtime task models in uniprocessor scheduling and multiprocessor global
scheduling.
","['\nJian-Jia Chen\n', '\nWen-Hung Huang\n', '\nCong Liu\n']",arXiv admin note: text overlap with arXiv:1501.07084,,http://arxiv.org/abs/1505.02155v3,cs.DS,"['cs.DS', 'cs.OS']",,,[]
Deterministically Deterring Timing Attacks in Deterland,http://arxiv.org/abs/1504.07070v2,2015-04-27T13:08:26Z,2016-05-30T13:35:25Z,"  The massive parallelism and resource sharing embodying today's cloud business
model not only exacerbate the security challenge of timing channels, but also
undermine the viability of defenses based on resource partitioning. We propose
hypervisor-enforced timing mitigation to control timing channels in cloud
environments. This approach closes ""reference clocks"" internal to the cloud by
imposing a deterministic view of time on guest code, and uses timing mitigators
to pace I/O and rate-limit potential information leakage to external observers.
Our prototype hypervisor is the first system to mitigate timing-channel leakage
across full-scale existing operating systems such as Linux and applications in
arbitrary languages. Mitigation incurs a varying performance cost, depending on
workload and tunable leakage-limiting parameters, but this cost may be
justified for security-critical cloud applications and data.
","['\nWeiyi Wu\n', '\nBryan Ford\n']","15 pages, 15 figures",,http://arxiv.org/abs/1504.07070v2,cs.OS,['cs.OS'],,,[]
Evaluating Dynamic File Striping For Lustre,http://arxiv.org/abs/1504.06833v1,2015-04-26T14:44:00Z,2015-04-26T14:44:00Z,"  We define dynamic striping as the ability to assign different Lustre striping
characteristics to contiguous segments of a file as it grows. In this paper, we
evaluate the effects of dynamic striping using a watermark-based strategy where
the stripe count or width is increased once a file's size exceeds one of the
chosen watermarks. To measure the performance of this strategy we used a
modified version of the IOR benchmark, a netflow analysis workload, and the
blastn algorithm from NCBI BLAST. The results indicate that dynamic striping is
beneficial to tasks with unpredictable data file size and large sequential
reads, but are less conclusive for workloads with significant random read
phases.
","['\nJoel Reed\n', '\nJeremy Archuleta\n', '\nMichael J. Brim\n', '\nJoshua Lothian\n']","International Workshop on the Lustre Ecosystem: Challenges and
  Opportunities, March 2015, Annapolis MD",,http://arxiv.org/abs/1504.06833v1,cs.OS,"['cs.OS', 'cs.DC']",,,[]
Monitoring Extreme-scale Lustre Toolkit,http://arxiv.org/abs/1504.06836v1,2015-04-26T14:57:05Z,2015-04-26T14:57:05Z,"  We discuss the design and ongoing development of the Monitoring Extreme-scale
Lustre Toolkit (MELT), a unified Lustre performance monitoring and analysis
infrastructure that provides continuous, low-overhead summary information on
the health and performance of Lustre, as well as on-demand, in- depth problem
diagnosis and root-cause analysis. The MELT infrastructure leverages a
distributed overlay network to enable monitoring of center-wide Lustre
filesystems where clients are located across many network domains. We preview
interactive command-line utilities that help administrators and users to
observe Lustre performance at various levels of resolution, from individual
servers or clients to whole filesystems, including job-level reporting.
Finally, we discuss our future plans for automating the root-cause analysis of
common Lustre performance problems.
","['\nMichael J. Brim\n', '\nJoshua K. Lothian\n']","International Workshop on the Lustre Ecosystem: Challenges and
  Opportunities, March 2015, Annapolis MD",,http://arxiv.org/abs/1504.06836v1,cs.DC,"['cs.DC', 'cs.OS']",,,[]
Survey of Operating Systems for the IoT Environment,http://arxiv.org/abs/1504.02517v2,2015-04-09T23:17:01Z,2015-04-13T13:39:37Z,"  This paper is a comprehensive survey of the various operating systems
available for the Internet of Things environment. At first the paper introduces
the various aspects of the operating systems designed for the IoT environment
where resource constraint poses a huge problem for the operation of the general
OS designed for the various computing devices. The latter part of the paper
describes the various OS available for the resource constraint IoT environment
along with the various platforms each OS supports, the software development
kits available for the development of applications in the respective OS along
with the various protocols implemented in these OS for the purpose of
communication and networking.
","['\nTuhin Borgohain\n', '\nUday Kumar\n', '\nSugata Sanyal\n']","5 pages, 7 tables",,http://arxiv.org/abs/1504.02517v2,cs.OS,['cs.OS'],,,[]
Garbage Collection Techniques for Flash-Resident Page-Mapping FTLs,http://arxiv.org/abs/1504.01666v1,2015-04-07T16:58:31Z,2015-04-07T16:58:31Z,"  Storage devices based on flash memory have replaced hard disk drives (HDDs)
due to their superior performance, increasing density, and lower power
consumption. Unfortunately, flash memory is subject to challenging
idiosyncrasies like erase-before-write and limited block lifetime. These
constraints are handled by a flash translation layer (FTL), which performs
out-of-place updates, wear-leveling and garbage-collection behind the scene,
while offering the application a virtualization of the physical address space.
  A class of relevant FTLs employ a flash-resident page-associative mapping
table from logical to physical addresses, with a smaller RAM-resident cache for
frequently mapped entries. In this paper, we address the problem of performing
garbage-collection under such FTLs. We observe two problems. Firstly,
maintaining the metadata needed to perform garbage-collection under these
schemes is problematic, because at write-time we do not necessarily know the
physical address of the before-image. Secondly, the size of this metadata must
remain small, because it makes RAM unavailable for caching frequently accessed
entries. We propose two complementary techniques, called Lazy Gecko and
Logarithmic Gecko, which address these issues. Lazy Gecko works well when RAM
is plentiful enough to store the GC metadata. Logarithmic Gecko works well when
RAM isn't plentiful and efficiently stores the GC metadata in flash. Thus,
these techniques are applicable to a wide range of flash devices with varying
amounts of embedded RAM.
","['\nNiv Dayan\n', '\nPhilippe Bonnet\n']",,,http://arxiv.org/abs/1504.01666v1,cs.DB,"['cs.DB', 'cs.OS']",,,[]
"RIOT OS Paves the Way for Implementation of High-Performance MAC
  Protocols",http://arxiv.org/abs/1504.03875v1,2015-04-15T12:05:59Z,2015-04-15T12:05:59Z,"  Implementing new, high-performance MAC protocols requires real-time features,
to be able to synchronize correctly between different unrelated devices. Such
features are highly desirable for operating wireless sensor networks (WSN) that
are designed to be part of the Internet of Things (IoT). Unfortunately, the
operating systems commonly used in this domain cannot provide such features. On
the other hand, ""bare-metal"" development sacrifices portability, as well as the
mul-titasking abilities needed to develop the rich applications that are useful
in the domain of the Internet of Things. We describe in this paper how we
helped solving these issues by contributing to the development of a port of
RIOT OS on the MSP430 microcontroller, an architecture widely used in
IoT-enabled motes. RIOT OS offers rich and advanced real-time features,
especially the simultaneous use of as many hardware timers as the underlying
platform (microcontroller) can offer. We then demonstrate the effectiveness of
these features by presenting a new implementation, on RIOT OS, of S-CoSenS, an
efficient MAC protocol that uses very low processing power and energy.
","['\nKévin Roussel\nINRIA Nancy - Grand Est / LORIA\n', '\nYe-Qiong Song\nINRIA Nancy - Grand Est / LORIA\n', '\nOlivier Zendra\nINRIA Nancy - Grand Est / LORIA\n']","SCITEPRESS. SENSORNETS 2015, Feb 2015, Angers, France.
  http://www.scitepress.org",,http://dx.doi.org/10.5220/0005237600050014,cs.NI,"['cs.NI', 'cs.OS', 'cs.PF']",10.5220/0005237600050014,,"['INRIA Nancy - Grand Est / LORIA', 'INRIA Nancy - Grand Est / LORIA', 'INRIA Nancy - Grand Est / LORIA']"
OS-level Failure Injection with SystemTap,http://arxiv.org/abs/1502.01509v1,2015-02-05T11:36:45Z,2015-02-05T11:36:45Z,"  Failure injection in distributed systems has been an important issue to
experiment with robust, resilient distributed systems. In order to reproduce
real-life conditions, parts of the application must be killed without letting
the operating system close the existing network communications in a ""clean""
way. When a process is simply killed, the OS closes them. SystemTap is a an
infrastructure that probes the Linux kernel's internal calls. If processes are
killed at kernel-level, they can be destroyed without letting the OS do
anything else. In this paper, we present a kernel-level failure injection
system based on SystemTap. We present how it can be used to implement
deterministic and probabilistic failure scenarios.
","['\nCamille Coti\n', '\nNicolas Greneche\n']",,,http://arxiv.org/abs/1502.01509v1,cs.OS,['cs.OS'],,,[]
"Protecting Memory-Performance Critical Sections in Soft Real-Time
  Applications",http://arxiv.org/abs/1502.02287v1,2015-02-08T19:09:36Z,2015-02-08T19:09:36Z,"  Soft real-time applications such as multimedia applications often show bursty
memory access patterns---regularly requiring a high memory bandwidth for a
short duration of time. Such a period is often critical for timely data
processing. Hence, we call it a memory-performance critical section.
Unfortunately, in multicore architecture, non-real-time applications on
different cores may also demand high memory bandwidth at the same time, which
can substantially increase the time spent on the memory performance critical
sections.
  In this paper, we present BWLOCK, user-level APIs and a memory bandwidth
control mechanism that can protect such memory performance critical sections of
soft real-time applications. BWLOCK provides simple lock like APIs to declare
memory-performance critical sections. If an application enters a
memory-performance critical section, the memory bandwidth control system then
dynamically limit other cores' memory access rates to protect memory
performance of the application until the critical section finishes.
  From case studies with real-world soft real-time applications, we found (1)
such memory-performance critical sections do exist and are often easy to
identify; and (2) applying BWLOCK for memory critical sections significantly
improve performance of the soft real-time applications at a small or no cost in
throughput of non real-time applications.
","['\nHeechul Yun\n', '\nSantosh Gondi\n', '\nSiddhartha Biswas\n']",technical report,,http://arxiv.org/abs/1502.02287v1,cs.OS,['cs.OS'],,,[]
"k2U: A General Framework from k-Point Effective Schedulability Analysis
  to Utilization-Based Tests",http://arxiv.org/abs/1501.07084v3,2015-01-28T12:38:20Z,2015-09-14T21:25:55Z,"  To deal with a large variety of workloads in different application domains in
real-time embedded systems, a number of expressive task models have been
developed. For each individual task model, researchers tend to develop
different types of techniques for deriving schedulability tests with different
computation complexity and performance. In this paper, we present a general
schedulability analysis framework, namely the k2U framework, that can be
potentially applied to analyze a large set of real-time task models under any
fixed-priority scheduling algorithm, on both uniprocessor and multiprocessor
scheduling. The key to k2U is a k-point effective schedulability test, which
can be viewed as a ""blackbox"" interface. For any task model, if a corresponding
k-point effective schedulability test can be constructed, then a sufficient
utilization-based test can be automatically derived. We show the generality of
k2U by applying it to different task models, which results in new and improved
tests compared to the state-of-the-art.
  Analogously, a similar concept by testing only k points with a different
formulation has been studied by us in another framework, called k2Q, which
provides quadratic bounds or utilization bounds based on a different
formulation of schedulability test. With the quadratic and hyperbolic forms,
k2Q and k2U frameworks can be used to provide many quantitive features to be
measured, like the total utilization bounds, speed-up factors, etc., not only
for uniprocessor scheduling but also for multiprocessor scheduling. These
frameworks can be viewed as a ""blackbox"" interface for schedulability tests and
response-time analysis.
","['\nJian-Jia Chen\n', '\nWen-Hung Huang\n', '\nCong Liu\n']",,,http://arxiv.org/abs/1501.07084v3,cs.OS,['cs.OS'],,,[]
"A Case Study: Task Scheduling Methodologies for High Speed Computing
  Systems",http://arxiv.org/abs/1501.01370v1,2015-01-07T05:51:19Z,2015-01-07T05:51:19Z,"  High Speed computing meets ever increasing real-time computational demands
through the leveraging of flexibility and parallelism. The flexibility is
achieved when computing platform designed with heterogeneous resources to
support multifarious tasks of an application where as task scheduling brings
parallel processing. The efficient task scheduling is critical to obtain
optimized performance in heterogeneous computing Systems (HCS). In this paper,
we brought a review of various application scheduling models which provide
parallelism for homogeneous and heterogeneous computing systems. In this paper,
we made a review of various scheduling methodologies targeted to high speed
computing systems and also prepared summary chart. The comparative study of
scheduling methodologies for high speed computing systems has been carried out
based on the attributes of platform & application as well. The attributes are
execution time, nature of task, task handling capability, type of host &
computing platform. Finally a summary chart has been prepared and it
demonstrates that the need of developing scheduling methodologies for
Heterogeneous Reconfigurable Computing Systems (HRCS) which is an emerging high
speed computing platform for real time applications.
","['\nMahendra Vucha\n', '\nArvind Rajawat\n']",12 pages,,http://dx.doi.org/10.5121/ijesa.2014.4401,cs.OS,"['cs.OS', 'cs.PF', 'cs.SY']",10.5121/ijesa.2014.4401,,[]
Sprobes: Enforcing Kernel Code Integrity on the TrustZone Architecture,http://arxiv.org/abs/1410.7747v1,2014-10-28T19:20:34Z,2014-10-28T19:20:34Z,"  Many smartphones now deploy conventional operating systems, so the rootkit
attacks so prevalent on desktop and server systems are now a threat to
smartphones. While researchers have advocated using virtualization to detect
and prevent attacks on operating systems (e.g., VM introspection and trusted
virtual domains), virtualization is not practical on smartphone systems due to
the lack of virtualization support and/or the expense of virtualization.
Current smartphone processors do have hardware support for running a protected
environment, such as the ARM TrustZone extensions, but such hardware does not
control the operating system operations sufficiently to enable VM
introspection. In particular, a conventional operating system running with
TrustZone still retains full control of memory management, which a rootkit can
use to prevent traps on sensitive instructions or memory accesses necessary for
effective introspection. In this paper, we present SPROBES, a novel primitive
that enables introspection of operating systems running on ARM TrustZone
hardware. Using SPROBES, an introspection mechanism protected by TrustZone can
instrument individual operating system instructions of its choice, receiving an
unforgeable trap whenever any SPROBE is executed. The key challenge in
designing SPROBES is preventing the rootkit from removing them, but we identify
a set of five invariants whose enforcement is sufficient to restrict rootkits
to execute only approved, SPROBE-injected kernel code. We implemented a
proof-of-concept version of SPROBES for the ARM Fast Models emulator,
demonstrating that in Linux kernel 2.6.38, only 12 SPROBES are sufficient to
enforce all five of these invariants. With SPROBES we show that it is possible
to leverage the limited TrustZone extensions to limit conventional kernel
execution to approved code comprehensively.
","['\nXinyang Ge\n', '\nHayawardh Vijayakumar\n', '\nTrent Jaeger\n']","In Proceedings of the Third Workshop on Mobile Security Technologies
  (MoST) 2014 (http://arxiv.org/abs/1410.6674)",,http://arxiv.org/abs/1410.7747v1,cs.CR,"['cs.CR', 'cs.OS']",,,[]
A First Look at Firefox OS Security,http://arxiv.org/abs/1410.7754v1,2014-10-28T19:23:26Z,2014-10-28T19:23:26Z,"  With Firefox OS, Mozilla is making a serious push for an HTML5-based mobile
platform. In order to assuage security concerns over providing hardware access
to web applications, Mozilla has introduced a number of mechanisms that make
the security landscape of Firefox OS distinct from both the desktop web and
other mobile operating systems. From an application security perspective, the
two most significant of these mechanisms are the the introduction of a default
Content Security Policy and code review in the market. This paper describes how
lightweight static analysis can augment these mechanisms to find
vulnerabilities which have otherwise been missed. We provide examples of
privileged applications in the market that contain vulnerabilities that can be
automatically detected.
  In addition to these findings, we show some of the challenges that occur when
desktop software is repurposed for a mobile operating system. In particular, we
argue that the caching of certificate overrides across applications--a known
problem in Firefox OS--generates a counter-intuitive user experience that
detracts from the security of the system.
","['\nDaniel Defreez\n', '\nBhargava Shastry\n', '\nHao Chen\n', '\nJean-Pierre Seifert\n']","In Proceedings of the Third Workshop on Mobile Security Technologies
  (MoST) 2014 (http://arxiv.org/abs/1410.6674)",,http://arxiv.org/abs/1410.7754v1,cs.CR,"['cs.CR', 'cs.OS']",,,[]
"Proceedings 2014 International Workshop on Advanced Intrusion Detection
  and Prevention",http://arxiv.org/abs/1410.3226v1,2014-10-13T08:51:40Z,2014-10-13T08:51:40Z,"  This volume contains the proceedings of the 2014 International Advanced
Intrusion Detection and Prevention (AIDP'14) Workshop, held in Marrakesh,
Morocco, on the 5th of June 2014, in conjunction with the 29th IFIP TC-11 SEC
2014 International Conference. It includes a revised version of the papers
selected for presentation at the work- shop.
","['\nJoaquin Garcia-Alfaro\nInstitut Mines-Télécom, Télécom SudParis\n', '\nGürkan Gür\nProvus\n']",,"EPTCS 165, 2014",http://dx.doi.org/10.4204/EPTCS.165,cs.CR,"['cs.CR', 'cs.NI', 'cs.OS', 'Security']",10.4204/EPTCS.165,,"['Institut Mines-Télécom, Télécom SudParis', 'Provus']"
"Mining Block I/O Traces for Cache Preloading with Sparse Temporal
  Non-parametric Mixture of Multivariate Poisson",http://arxiv.org/abs/1410.3463v1,2014-10-13T14:26:28Z,2014-10-13T14:26:28Z,"  Existing caching strategies, in the storage domain, though well suited to
exploit short range spatio-temporal patterns, are unable to leverage long-range
motifs for improving hitrates. Motivated by this, we investigate novel Bayesian
non-parametric modeling(BNP) techniques for count vectors, to capture long
range correlations for cache preloading, by mining Block I/O traces. Such
traces comprise of a sequence of memory accesses that can be aggregated into
high-dimensional sparse correlated count vector sequences.
  While there are several state of the art BNP algorithms for clustering and
their temporal extensions for prediction, there has been no work on exploring
these for correlated count vectors. Our first contribution addresses this gap
by proposing a DP based mixture model of Multivariate Poisson (DP-MMVP) and its
temporal extension(HMM-DP-MMVP) that captures the full covariance structure of
multivariate count data. However, modeling full covariance structure for count
vectors is computationally expensive, particularly for high dimensional data.
Hence, we exploit sparsity in our count vectors, and as our main contribution,
introduce the Sparse DP mixture of multivariate Poisson(Sparse-DP-MMVP),
generalizing our DP-MMVP mixture model, also leading to more efficient
inference. We then discuss a temporal extension to our model for cache
preloading.
  We take the first step towards mining historical data, to capture long range
patterns in storage traces for cache preloading. Experimentally, we show a
dramatic improvement in hitrates on benchmark traces and lay the groundwork for
further research in storage domain to reduce latencies using data mining
techniques to capture long range motifs.
","['\nLavanya Sita Tekumalla\n', '\nChiranjib Bhattacharyya\n']",,,http://arxiv.org/abs/1410.3463v1,cs.OS,"['cs.OS', 'cs.LG', 'cs.SY']",,,[]
"Rank-Aware Dynamic Migrations and Adaptive Demotions for DRAM Power
  Management",http://arxiv.org/abs/1409.5567v1,2014-09-19T09:30:49Z,2014-09-19T09:30:49Z,"  Modern DRAM architectures allow a number of low-power states on individual
memory ranks for advanced power management. Many previous studies have taken
advantage of demotions on low-power states for energy saving. However, most of
the demotion schemes are statically performed on a limited number of
pre-selected low-power states, and are suboptimal for different workloads and
memory architectures. Even worse, the idle periods are often too short for
effective power state transitions, especially for memory intensive
applications. Wrong decisions on power state transition incur significant
energy and delay penalties. In this paper, we propose a novel memory system
design named RAMZzz with rank-aware energy saving optimizations including
dynamic page migrations and adaptive demotions. Specifically, we group the
pages with similar access locality into the same rank with dynamic page
migrations. Ranks have their hotness: hot ranks are kept busy for high
utilization and cold ranks can have more lengthy idle periods for power state
transitions. We further develop adaptive state demotions by considering all
low-power states for each rank and a prediction model to estimate the
power-down timeout among states. We experimentally compare our algorithm with
other energy saving policies with cycle-accurate simulation. Experiments with
benchmark workloads show that RAMZzz achieves significant improvement on
energy-delay2 and energy consumption over other energy saving techniques.
","['\nYanchao Lu\n', '\nDonghong Wu\n', '\nBingsheng He\n', '\nXueyan Tang\n', '\nJianliang Xu\n', '\nMinyi Guo\n']",19 pages,,http://arxiv.org/abs/1409.5567v1,cs.PF,"['cs.PF', 'cs.AR', 'cs.OS', 'B.3.2']",,,[]
"Making FPGAs Accessible to Scientists and Engineers as Domain Expert
  Software Programmers with LabVIEW",http://arxiv.org/abs/1408.4715v1,2014-08-20T16:39:15Z,2014-08-20T16:39:15Z,"  In this paper we present a graphical programming framework, LabVIEW, and
associated language and libraries, as well as programming techniques and
patterns that we have found useful in making FPGAs accessible to scientists and
engineers as domain expert software programmers.
","['\nHugo A. Andrade\n', '\nSimon Hogg\n', '\nStephan Ahrends\n']","Presented at First International Workshop on FPGAs for Software
  Programmers (FSP 2014) (arXiv:1408.4423)",,http://arxiv.org/abs/1408.4715v1,cs.SE,"['cs.SE', 'cs.DC', 'cs.OS', 'cs.PL']",,,[]
Assessment of Response Time for New Multi Level Feedback Queue Scheduler,http://arxiv.org/abs/1408.0990v1,2014-08-02T19:56:24Z,2014-08-02T19:56:24Z,"  Response time is one of the characteristics of scheduler, happens to be a
prominent attribute of any CPU scheduling algorithm. The proposed New Multi
Level Feedback Queue [NMLFQ] Scheduler is compared with dynamic, real time,
Dependent Activity Scheduling Algorithm (DASA) and Lockes Best Effort
Scheduling Algorithm (LBESA). We abbreviated beneficial result of NMLFQ
scheduler in comparison with dynamic best effort schedulers with respect to
response time.
","['\nM. V. Panduranga Rao\n', '\nK. C. Shet\n']","7 pages, 5 figures","International Journal of Computer Trends and Technology (IJCTT),
  Volume 13, Number 3, Pages: 113-119, July 2014. ISSN:2231-2803",http://dx.doi.org/10.14445/22312803/IJCTT-V13P124,cs.OS,"['cs.OS', '68U20', 'D.3.2']",10.14445/22312803/IJCTT-V13P124,,[]
"Effects of Hard Real-Time Constraints in Implementing the Myopic
  Scheduling Algorithm",http://arxiv.org/abs/1407.0122v1,2014-07-01T07:23:10Z,2014-07-01T07:23:10Z,"  Myopic is a hard real-time process scheduling algorithm that selects a
suitable process based on a heuristic function from a subset (Window)of all
ready processes instead of choosing from all available processes, like original
heuristic scheduling algorithm. Performance of the algorithm significantly
depends on the chosen heuristic function that assigns weight to different
parameters like deadline, earliest starting time, processing time etc. and the
sizeof the Window since it considers only k processes from n processes (where,
k<= n). This research evaluates the performance of the Myopic algorithm for
different parameters to demonstrate the merits and constraints of the
algorithm. A comparative performance of the impact of window size in
implementing the Myopic algorithm is presented and discussed through a set of
experiments.
","['\nKazi Sakib\n', '\nM. S. Hasan\n', '\nM. A. Hossain\n']","9 pages, Journal of Computer Science (JCS), Bangladesh, Vol. 1, No.
  2, December, 2007",,http://arxiv.org/abs/1407.0122v1,cs.OS,"['cs.OS', 'cs.DC']",,,[]
"Preemptive Thread Block Scheduling with Online Structural Runtime
  Prediction for Concurrent GPGPU Kernels",http://arxiv.org/abs/1406.6037v1,2014-06-23T19:44:03Z,2014-06-23T19:44:03Z,"  Recent NVIDIA Graphics Processing Units (GPUs) can execute multiple kernels
concurrently. On these GPUs, the thread block scheduler (TBS) uses the FIFO
policy to schedule their thread blocks. We show that FIFO leaves performance to
chance, resulting in significant loss of performance and fairness. To improve
performance and fairness, we propose use of the preemptive Shortest Remaining
Time First (SRTF) policy instead. Although SRTF requires an estimate of runtime
of GPU kernels, we show that such an estimate of the runtime can be easily
obtained using online profiling and exploiting a simple observation on GPU
kernels' grid structure. Specifically, we propose a novel Structural Runtime
Predictor. Using a simple Staircase model of GPU kernel execution, we show that
the runtime of a kernel can be predicted by profiling only the first few thread
blocks. We evaluate an online predictor based on this model on benchmarks from
ERCBench, and find that it can estimate the actual runtime reasonably well
after the execution of only a single thread block. Next, we design a thread
block scheduler that is both concurrent kernel-aware and uses this predictor.
We implement the SRTF policy and evaluate it on two-program workloads from
ERCBench. SRTF improves STP by 1.18x and ANTT by 2.25x over FIFO. When compared
to MPMax, a state-of-the-art resource allocation policy for concurrent kernels,
SRTF improves STP by 1.16x and ANTT by 1.3x. To improve fairness, we also
propose SRTF/Adaptive which controls resource usage of concurrently executing
kernels to maximize fairness. SRTF/Adaptive improves STP by 1.12x, ANTT by
2.23x and Fairness by 2.95x compared to FIFO. Overall, our implementation of
SRTF achieves system throughput to within 12.64% of Shortest Job First (SJF, an
oracle optimal scheduling policy), bridging 49% of the gap between FIFO and
SJF.
","['\nSreepathi Pai\n', '\nR. Govindarajan\n', '\nMatthew J. Thazhuthaveetil\n']","14 pages, full pre-review version of PACT 2014 poster",,http://arxiv.org/abs/1406.6037v1,cs.AR,"['cs.AR', 'cs.OS', 'D.3.3; C.1.2']",,,[]
On the Reverse Engineering of the Citadel Botnet,http://arxiv.org/abs/1406.5569v1,2014-06-21T02:04:56Z,2014-06-21T02:04:56Z,"  Citadel is an advanced information-stealing malware which targets financial
information. This malware poses a real threat against the confidentiality and
integrity of personal and business data. A joint operation was recently
conducted by the FBI and the Microsoft Digital Crimes Unit in order to take
down Citadel command-and-control servers. The operation caused some disruption
in the botnet but has not stopped it completely. Due to the complex structure
and advanced anti-reverse engineering techniques, the Citadel malware analysis
process is both challenging and time-consuming. This allows cyber criminals to
carry on with their attacks while the analysis is still in progress. In this
paper, we present the results of the Citadel reverse engineering and provide
additional insight into the functionality, inner workings, and open source
components of the malware. In order to accelerate the reverse engineering
process, we propose a clone-based analysis methodology. Citadel is an offspring
of a previously analyzed malware called Zeus; thus, using the former as a
reference, we can measure and quantify the similarities and differences of the
new variant. Two types of code analysis techniques are provided in the
methodology, namely assembly to source code matching and binary clone
detection. The methodology can help reduce the number of functions requiring
manual analysis. The analysis results prove that the approach is promising in
Citadel malware analysis. Furthermore, the same approach is applicable to
similar malware analysis scenarios.
","['\nAshkan Rahimian\n', '\nRaha Ziarati\n', '\nStere Preda\n', '\nMourad Debbabi\n']","10 pages, 17 figures. This is an updated / edited version of a paper
  appeared in FPS 2013","LNCS 8352, 2014, pp 408-425",http://dx.doi.org/10.1007/978-3-319-05302-8_25,cs.CR,"['cs.CR', 'cs.NI', 'cs.OS', 'cs.SE', 'D.4.6; K.6.5; E.3; D.2.7']",10.1007/978-3-319-05302-8_25,,[]
Timing Analysis for DAG-based and GFP Scheduled Tasks,http://arxiv.org/abs/1406.1133v1,2014-06-04T18:18:37Z,2014-06-04T18:18:37Z,"  Modern embedded systems have made the transition from single-core to
multi-core architectures, providing performance improvement via parallelism
rather than higher clock frequencies. DAGs are considered among the most
generic task models in the real-time domain and are well suited to exploit this
parallelism. In this paper we provide a schedulability test using response-time
analysis exploiting exploring and bounding the self interference of a DAG task.
Additionally we bound the interference a high priority task has on lower
priority ones.
","['\nJosé Marinho\n', '\nStefan M. Petters\n']",,,http://arxiv.org/abs/1406.1133v1,cs.DC,"['cs.DC', 'cs.OS']",,,[]
"Supporting Soft Real-Time Sporadic Task Systems on Heterogeneous
  Multiprocessors with No Utilization Loss",http://arxiv.org/abs/1405.7322v1,2014-05-28T18:28:45Z,2014-05-28T18:28:45Z,"  Heterogeneous multicore architectures are becoming increasingly popular due
to their potential of achieving high performance and energy efficiency compared
to the homogeneous multicore architectures. In such systems, the real-time
scheduling problem becomes more challenging in that processors have different
speeds. A job executing on a processor with speed $x$ for $t$ time units
completes $(x \cdot t)$ units of execution. Prior research on heterogeneous
multiprocessor real-time scheduling has focused on hard real-time systems,
where, significant processing capacity may have to be sacrificed in the
worst-case to ensure that all deadlines are met. As meeting hard deadlines is
overkill for many soft real-time systems in practice, this paper shows that on
soft real-time heterogeneous multiprocessors, bounded response times can be
ensured for globally-scheduled sporadic task systems with no utilization loss.
A GEDF-based scheduling algorithm, namely GEDF-H, is presented and response
time bounds are established under both preemptive and non-preemptive GEDF-H
scheduling. Extensive experiments show that the magnitude of the derived
response time bound is reasonable, often smaller than three task periods. To
the best of our knowledge, this paper is the first to show that soft real-time
sporadic task systems can be supported on heterogeneous multiprocessors without
utilization loss, and with reasonable predicted response time.
","['\nGuangmo Tong\n', '\nCong Liu\n']",,,http://arxiv.org/abs/1405.7322v1,cs.OS,['cs.OS'],,,[]
"Heterogeneity-aware Fault Tolerance using a Self-Organizing Runtime
  System",http://arxiv.org/abs/1405.2912v1,2014-05-12T16:41:23Z,2014-05-12T16:41:23Z,"  Due to the diversity and implicit redundancy in terms of processing units and
compute kernels, off-the-shelf heterogeneous systems offer the opportunity to
detect and tolerate faults during task execution in hardware as well as in
software. To automatically leverage this diversity, we introduce an extension
of an online-learning runtime system that combines the benefits of the existing
performance-oriented task mapping with task duplication, a diversity-oriented
mapping strategy and heterogeneity-aware majority voter. This extension uses a
new metric to dynamically rate the remaining benefit of unreliable processing
units and a memory management mechanism for automatic data transfers and
checkpointing in the host and device memories.
","['\nMario Kicherer\n', '\nWolfgang Karl\n']","Presented at 1st Workshop on Resource Awareness and Adaptivity in
  Multi-Core Computing (Racing 2014) (arXiv:1405.2281)",,http://arxiv.org/abs/1405.2912v1,cs.OS,['cs.OS'],,,[]
"Resource-Aware Replication on Heterogeneous Multicores: Challenges and
  Opportunities",http://arxiv.org/abs/1405.2913v1,2014-05-12T16:41:49Z,2014-05-12T16:41:49Z,"  Decreasing hardware feature sizes and increasing heterogeneity in multicore
hardware require software that can adapt to these platforms' properties. We
implemented ROMAIN, an OS service providing redundant multithreading on top of
the FIASCO.OC microkernel to address the increasing unreliability of hardware.
In this paper we review challenges and opportunities for ROMAIN to adapt to
such multicore platforms in order to decrease execution overhead, resource
requirements, and vulnerability against faults.
","['\nBjörn Döbel\n', '\nRobert Muschner\n', '\nHermann Härtig\n']","Presented at 1st Workshop on Resource Awareness and Adaptivity in
  Multi-Core Computing (Racing 2014) (arXiv:1405.2281)",,http://arxiv.org/abs/1405.2913v1,cs.DC,"['cs.DC', 'cs.OS']",,,[]
"Proceedings of the First Workshop on Resource Awareness and Adaptivity
  in Multi-Core Computing (Racing 2014)",http://arxiv.org/abs/1405.2281v1,2014-05-08T07:09:55Z,2014-05-08T07:09:55Z,"  This volume contains the papers accepted at the 1st Workshop on Resource
Awareness and Adaptivity in Multi-Core Computing (Racing 2014), held in
Paderborn, Germany, May 29-30, 2014. Racing 2014 was co-located with the IEEE
European Test Symposium (ETS).
","['\nFrank Hannig\n', '\nJürgen Teich\n']",Website of the workshop: http://www12.cs.fau.de/racing2014/,,http://arxiv.org/abs/1405.2281v1,cs.DC,"['cs.DC', 'cs.OS', 'cs.RO', 'cs.SE']",,,[]
An Effective Round Robin Algorithm using Min-Max Dispersion Measure,http://arxiv.org/abs/1404.5869v1,2014-04-23T15:55:50Z,2014-04-23T15:55:50Z,"  Round Robin (RR) scheduling algorithm is a preemptive scheduling algorithm.
It is designed especially for time sharing Operating System (OS). In RR
scheduling algorithm the CPU switches between the processes when the static
Time Quantum (TQ) expires. RR scheduling algorithm is considered as the most
widely used scheduling algorithm in research because the TQ is equally shared
among the processes. In this paper a newly proposed variant of RR algorithm
called Min-Max Round Robin (MMRR) scheduling algorithm is presented. The idea
of this MMRR is to make the TQ repeatedly adjusted using Min-Max dispersion
measure in accordance with remaining CPU burst time. Our experimental analysis
shows that MMRR performs much better than RR algorithm in terms of average
turnaround time, average waiting time and number of context switches.
","['\nSanjaya Kumar Panda\n', '\nSourav Kumar Bhoi\n']","9 pages, 15 figures. International Journal on Computer Science and
  Engineering (IJCSE), 2012",,http://arxiv.org/abs/1404.5869v1,cs.OS,['cs.OS'],,,[]
"Enhancing CPU Performance using Subcontrary Mean Dynamic Round Robin
  (SMDRR) Scheduling Algorithm",http://arxiv.org/abs/1404.6087v1,2014-04-24T11:06:06Z,2014-04-24T11:06:06Z,"  Round Robin (RR) Algorithm is considered as optimal in time shared
environment because the static time is equally shared among the processes. If
the time quantum taken is static then it undergoes degradation of the CPU
performance and leads to so many context switches. In this paper, we have
proposed a new effective dynamic RR algorithm SMDRR (Subcontrary Mean Dynamic
Round Robin) based on dynamic time quantum where we use the subcontrary mean or
harmonic mean to find the time quantum. The idea of this approach is to make
the time quantum repeatedly adjusted according to the burst time of the
currently running processes. Our experimental analysis shows that SMDRR
performs better than RR algorithm in terms of reducing the number of context
switches, average turnaround time and average waiting time.
","['\nSourav Kumar Bhoi\n', '\nSanjaya Kumar Panda\n', '\nDebashee Tarai\n']","5 pages, 13 figures. Journal of Global Research in Computer Science
  2011. arXiv admin note: text overlap with arXiv:1103.3832 by other authors",,http://arxiv.org/abs/1404.6087v1,cs.OS,['cs.OS'],,,[]
Toward Parametric Timed Interfaces for Real-Time Components,http://arxiv.org/abs/1404.0088v1,2014-04-01T00:40:01Z,2014-04-01T00:40:01Z,"  We propose here a framework to model real-time components consisting of
concurrent real-time tasks running on a single processor, using parametric
timed automata. Our framework is generic and modular, so as to be easily
adapted to different schedulers and more complex task models. We first perform
a parametric schedulability analysis of the components using the inverse
method. We show that the method unfortunately does not provide satisfactory
results when the task periods are consid- ered as parameters. After identifying
and explaining the problem, we present a solution adapting the model by making
use of the worst-case scenario in schedulability analysis. We show that the
analysis with the inverse method always converges on the modified model when
the system load is strictly less than 100%. Finally, we show how to use our
parametric analysis for the generation of timed interfaces in compositional
system design.
","['\nYoucheng Sun\n', '\nGiuseppe Lipari\n', '\nÉtienne André\n', '\nLaurent Fribourg\n']","In Proceedings SynCoP 2014, arXiv:1403.7841","EPTCS 145, 2014, pp. 49-64",http://dx.doi.org/10.4204/EPTCS.145.6,cs.OS,['cs.OS'],10.4204/EPTCS.145.6,,[]
"An Enhanced Multi-Pager Environment Support for Second Generation
  Microkernels",http://arxiv.org/abs/1404.1637v1,2014-04-06T23:53:01Z,2014-04-06T23:53:01Z,"  The main objective of this paper is to present a mechanism of enhanced paging
support for the second generation microkernels in the form of explicit support
of multi-pager environment for the tasks running in the system. Proposed
mechanism is based on the intra-kernel high granularity pagers assignments per
virtual address space, which allow efficient and simple dispatching of page
faults to the appropriate pagers. The paging is one of the major features of
the virtual memory, which is extensively used by advanced operating systems to
provide an illusion of elastic memory. Original and present second generation
microkernels provide only limited, inflexible and unnatural support for paging.
Furthermore, facilities provided by current solutions for multi-pager support
on the runtime level introduce an overhead in terms of mode switches and thread
context switches which can be significantly reduced. Limited paging support
limits the attractiveness of the second generation microkernel based systems
use in real-life applications, in which processes usually have concurrent
servicing of multiple paging servers. The purpose of this paper is to present a
facilities for the efficient and flexible support of multi-pager environments
for the second generation microkernels. A comparison of the proposed solution
to the present architecture L4 + L4Re has been made and overhead of the page
fault handling critical path has been evaluated. Proposed solution is simple
enough and provides a natural and flexible support of multi-pager environments
for second generation microkernels in efficient way. It introduces a third less
overhead in terms of the mode switches and thread context switches in
comparison to the present L4 + L4Re solution implemented in the Fiasco.OC.
",['\nYauhen Klimiankou\n'],"7 pages, 4 figures, 1 table","International Journal of Computer Science and Information
  Security, Vol. 12 No. 1 JAN 2014",http://arxiv.org/abs/1404.1637v1,cs.OS,['cs.OS'],,,[]
"Task & Resource Self-adaptive Embedded Real-time Operating System
  Microkernel for Wireless Sensor Nodes",http://arxiv.org/abs/1403.5010v1,2014-03-20T00:34:00Z,2014-03-20T00:34:00Z,"  Wireless Sensor Networks (WSNs) are used in many application fields, such as
military, healthcare, environment surveillance, etc. The WSN OS based on
event-driven model doesn't support real-time and multi-task application types
and the OSs based on thread-driven model consume much energy because of
frequent context switch. Due to the high-dense and large-scale deployment of
sensor nodes, it is very difficult to collect sensor nodes to update their
software. Furthermore, the sensor nodes are vulnerable to security attacks
because of the characteristics of broadcast communication and unattended
application. This paper presents a task and resource self-adaptive embedded
real-time microkernel, which proposes hybrid programming model and offers a
two-level scheduling strategy to support real-time multi-task correspondingly.
A communication scheme, which takes the ""tuple"" space and ""IN/OUT"" primitives
from ""LINDA"", is proposed to support some collaborative and distributed tasks.
In addition, this kernel implements a run-time over-the-air updating mechanism
and provides a security policy to avoid the attacks and ensure the reliable
operation of nodes. The performance evaluation is proposed and the experiential
results show this kernel is task-oriented and resource-aware and can be used
for the applications of event-driven and real-time multi-task.
","['\nKexing Xing\n', '\nDecheng Zuo\n', '\nHaiying Zhou\n', '\nHou Kun-Mean\n']",,,http://arxiv.org/abs/1403.5010v1,cs.OS,['cs.OS'],,,[]
File System Design Approaches,http://arxiv.org/abs/1403.5976v1,2014-03-21T05:31:33Z,2014-03-21T05:31:33Z,"  In this article, the file system development design approaches are discussed.
The selection of the file system design approach is done according to the needs
of the developers what are the needed requirements and specifications for the
new design. It allowed us to identify where our proposal fitted in with
relation to current and past file system development. Our experience with file
system development is limited so the research served to identify the different
techniques that can be used. The variety of file systems encountered show what
an active area of research file system development is. The file systems may be
from one of the two fundamental categories. In one category, the file system is
developed in user space and runs as a user process. Another file system may be
developed in the kernel space and runs as a privileged process. Another one is
the mixed approach in which we can take the advantages of both aforesaid
approaches. Each development option has its own pros and cons. In this article,
these design approaches are discussed.
",['\nBrijender Kahanwal\n'],"5 pages, 6 figures","International Journal of Advances in Engineering Sciences,2014,
  Vol. 4(1), PP 16-20",http://arxiv.org/abs/1403.5976v1,cs.OS,['cs.OS'],,,[]
"Design and Performance Evaluation of an Optimized Disk Scheduling
  Algorithm (ODSA)",http://arxiv.org/abs/1403.0334v1,2014-03-03T08:13:29Z,2014-03-03T08:13:29Z,"  Management of disk scheduling is a very important aspect of operating system.
Performance of the disk scheduling completely depends on how efficient is the
scheduling algorithm to allocate services to the request in a better manner.
Many algorithms (FIFO, SSTF, SCAN, C-SCAN, LOOK, etc.) are developed in the
recent years in order to optimize the system disk I/O performance. By reducing
the average seek time and transfer time, we can improve the performance of disk
I/O operation. In our proposed algorithm, Optimize Disk Scheduling Algorithm
(ODSA) is taking less average seek time and transfer time as compare to other
disk scheduling algorithms (FIFO, SSTF, SCAN, C-SCAN, LOOK, etc.), which
enhances the efficiency of the disk performance in a better manner.
","['\nSourav Kumar Bhoi\n', '\nSanjaya Kumar Panda\n', '\nImran Hossain Faruk\n']","8 pages, 26 figures",International Journal of Computer Applications 2012,http://dx.doi.org/10.5120/5010-7329,cs.OS,['cs.OS'],"10.5120/5010-7329 10.5120/5010-7329 10.5120/5010-7329 10.5120/5010-7329
  10.5120/5010-7329",,[]
"A Group based Time Quantum Round Robin Algorithm using Min-Max Spread
  Measure",http://arxiv.org/abs/1403.0335v1,2014-03-03T08:27:29Z,2014-03-03T08:27:29Z,"  Round Robin (RR) Scheduling is the basis of time sharing environment. It is
the combination of First Come First Served (FCFS) scheduling algorithm and
preemption among processes. It is basically used in a time sharing operating
system. It switches from one process to another process in a time interval. The
time interval or Time Quantum (TQ) is fixed for all available processes. So,
the larger process suffers from Context Switches (CS). To increase efficiency,
we have to select different TQ for processes. The main objective of RR is to
reduce the CS, maximize the utilization of CPU and minimize the turn around and
the waiting time. In this paper, we have considered different TQ for a group of
processes. It reduces CS as well as enhancing the performance of RR algorithm.
TQ can be calculated using min-max dispersion measure. Our experimental
analysis shows that Group Based Time Quantum (GBTQ) RR algorithm performs
better than existing RR algorithm with respect to Average Turn Around Time
(ATAT), Average Waiting Time (AWT) and CS.
","['\nSanjaya Kumar Panda\n', '\nDebasis Dash\n', '\nJitendra Kumar Rout\n']","7 pages, 16 figures",International Journal of Computer Applications 2013,http://dx.doi.org/10.5120/10667-5445,cs.OS,['cs.OS'],10.5120/10667-5445,,[]
"A Taxonomy for Attack Patterns on Information Flows in Component-Based
  Operating Systems",http://arxiv.org/abs/1403.1165v1,2014-03-05T15:43:57Z,2014-03-05T15:43:57Z,"  We present a taxonomy and an algebra for attack patterns on component-based
operating systems. In a multilevel security scenario, where isolation of
partitions containing data at different security classifications is the primary
security goal and security breaches are mainly defined as undesired disclosure
or modification of classified data, strict control of information flows is the
ultimate goal. In order to prevent undesired information flows, we provide a
classification of information flow types in a component-based operating system
and, by this, possible patterns to attack the system. The systematic
consideration of informations flows reveals a specific type of operating system
covert channel, the covert physical channel, which connects two former isolated
partitions by emitting physical signals into the computer's environment and
receiving them at another interface.
","['\nMichael Hanspach\n', '\nJörg Keller\n']",9 pages,"In Proceedings of the 7th Layered Assurance Workshop, New Orleans,
  LA, USA, December 2013",http://arxiv.org/abs/1403.1165v1,cs.CR,"['cs.CR', 'cs.OS', 'cs.SD']",,,[]
Formal Description of Components in Operating Systems,http://arxiv.org/abs/1402.4929v1,2014-02-20T08:33:02Z,2014-02-20T08:33:02Z,"  The contemporary development of hardware components is a prerequisite for
increasing the concentration of computing power. System software is developing
at a much slower pace. To use available resources efficiently modeling is
required. Formalization of elements, present in the material, provides the
basis for modeling. Examples are presented to demonstrate the efficiency of the
concept.
",['\nAsen Petkov Iliev\n'],,"IJITEE (ISSN: 2278 - 3075, Volume-3, Issue-4, September 2013) 96 -
  98",http://arxiv.org/abs/1402.4929v1,cs.OS,['cs.OS'],,,[]
"LWRP: Low Power Consumption Weighting Replacement Policy using Buffer
  Memory",http://arxiv.org/abs/1402.0631v1,2014-02-04T06:26:34Z,2014-02-04T06:26:34Z,"  As the performance gap between memory and processors has increased, then it
leads to the poor performance. Efficient virtual memory can overcome this
problem. And the efficiency of virtual memory depends on the replacement policy
used for cache. In this paper, our algorithm not only based on the time to last
access and frequency index but, we also consider the power consumption. We show
that Low Power Consumption Weighting Replacement Policy (LWRP) has better
performance and low power consumption.
","['\nS. R. Bhalgama\n', '\nC. C. Kavar\n', '\nS. S. Parmar\n']","4 pages, 2 figures, Published with International Journal of Computer
  Trends and Technology (IJCTT)","IJCTT 7(3):147-150, January 2014. Published by Seventh Sense
  Research Group",http://dx.doi.org/10.14445/22312803/IJCTT-V7P142,cs.OS,['cs.OS'],10.14445/22312803/IJCTT-V7P142,,[]
Anception: Application Virtualization For Android,http://arxiv.org/abs/1401.6726v1,2014-01-27T03:01:33Z,2014-01-27T03:01:33Z,"  The problem of malware has become significant on Android devices. Library
operating systems and application virtualization are both possible solutions
for confining malware. Unfortunately, such solutions do not exist for Android.
Designing mechanisms for application virtualization is a significant chal-
lenge for several reasons: (1) graphics performance is important due to
popularity of games and (2) applications with the same UID can share state.
This paper presents Anception, the first flexible application virtualization
framework for Android. It is imple- mented as a modification to the Android
kernel and supports application virtualization that addresses the above
requirements. Anception is able to confine many types of malware while
supporting unmodified Android applications. Our Anception- based system
exhibits up to 3.9% overhead on various 2D/3D benchmarks, and 1.8% overhead on
the SunSpider benchmark.
","['\nEarlence Fernandes\n', '\nAlexander Crowell\n', '\nAjit Aluri\n', '\nAtul Prakash\n']","University of Michigan, Technical Report CSE-TR-583-13",,http://arxiv.org/abs/1401.6726v1,cs.CR,"['cs.CR', 'cs.OS']",,,[]
"Performance Impact of Lock-Free Algorithms on Multicore Communication
  APIs",http://arxiv.org/abs/1401.6100v1,2014-01-09T00:04:41Z,2014-01-09T00:04:41Z,"  Data race conditions in multi-tasking software applications are prevented by
serializing access to shared memory resources, ensuring data consistency and
deterministic behavior. Traditionally tasks acquire and release locks to
synchronize operations on shared memory. Unfortunately, lock management can add
significant processing overhead especially for multicore deployments where
tasks on different cores convoy in queues waiting to acquire a lock.
Implementing more than one lock introduces the risk of deadlock and using
spinlocks constrains which cores a task can run on. The better alternative is
to eliminate locks and validate that real-time properties are met, which is not
directly considered in many embedded applications. Removing the locks is
non-trivial and packaging lock-free algorithms for developers reduces the
possibility of concurrency defects. This paper details how a multicore
communication API implementation is enhanced to support lock-free messaging and
the impact this has on data exchange latency between tasks. Throughput and
latency are compared on Windows and Linux between lock-based and lock-free
implementations for data exchange of messages, packets, and scalars. A model of
the lock-free exchange predicts performance at the system architecture level
and provides a stop criterion for the refactoring. The results show that
migration from single to multicore hardware architectures degrades lock-based
performance, and increases lock-free performance.
","['\nK. Eric Harper\n', '\nThijmen de Gooijer\n']","17 pages, 8 figures, 36 references, Embedded World Conference 2013",,http://arxiv.org/abs/1401.6100v1,cs.DC,"['cs.DC', 'cs.OS', 'cs.PF', 'cs.SE']",,,[]
Transparent Checkpoint-Restart for Hardware-Accelerated 3D Graphics,http://arxiv.org/abs/1312.6650v2,2013-12-23T19:19:40Z,2014-02-03T23:01:55Z,"  Providing fault-tolerance for long-running GPU-intensive jobs requires
application-specific solutions, and often involves saving the state of complex
data structures spread among many graphics libraries. This work describes a
mechanism for transparent GPU-independent checkpoint-restart of 3D graphics.
The approach is based on a record-prune-replay paradigm: all OpenGL calls
relevant to the graphics driver state are recorded; calls not relevant to the
internal driver state as of the last graphics frame prior to checkpoint are
discarded; and the remaining calls are replayed on restart. A previous approach
for OpenGL 1.5, based on a shadow device driver, required more than 78,000
lines of OpenGL-specific code. In contrast, the new approach, based on
record-prune-replay, is used to implement the same case in just 4,500 lines of
code. The speed of this approach varies between 80 per cent and nearly 100 per
cent of the speed of the native hardware acceleration for OpenGL 1.5, as
measured when running the ioquake3 game under Linux. This approach has also
been extended to demonstrate checkpointing of OpenGL 3.0 for the first time,
with a demonstration for PyMol, for molecular visualization.
","['\nSamaneh Kazemi Nafchi\n', '\nRohan Garg\n', '\nGene Cooperman\n']","20 pages, 6 figures, 4 tables",,http://arxiv.org/abs/1312.6650v2,cs.OS,"['cs.OS', 'D.4.5']",,,[]
Support for Error Tolerance in the Real-Time Transport Protocol,http://arxiv.org/abs/1312.5892v1,2013-12-20T11:11:54Z,2013-12-20T11:11:54Z,"  Streaming applications often tolerate bit errors in their received data well.
This is contrasted by the enforcement of correctness of the packet headers and
payload by network protocols. We investigate a solution for the Real-time
Transport Protocol (RTP) that is tolerant to errors by accepting erroneous
data. It passes potentially corrupted stream data payloads to the codecs. If
errors occur in the header, our solution recovers from these by leveraging the
known state and expected header values for each stream. The solution is fully
receiver-based and incrementally deployable, and as such requires neither
support from the sender nor changes to the RTP specification. Evaluations show
that our header error recovery scheme can recover from almost all errors, with
virtually no erroneous recoveries, up to bit error rates of about 10%.
","['\nFlorian Schmidt\n', '\nDavid Orlea\n', '\nKlaus Wehrle\n']","18 pages, 9 figures, published as technical report of the Department
  of Computer Science of RWTH Aachen University",,http://arxiv.org/abs/1312.5892v1,cs.NI,"['cs.NI', 'cs.IT', 'cs.OS', 'math.IT', 'C.2.2']",,,[]
"Cache-aware static scheduling for hard real-time multicore systems based
  on communication affinities",http://arxiv.org/abs/1312.4509v1,2013-12-16T20:32:52Z,2013-12-16T20:32:52Z,"  The growing need for continuous processing capabilities has led to the
development of multicore systems with a complex cache hierarchy. Such multicore
systems are generally designed for improving the performance in average case,
while hard real-time systems must consider worst-case scenarios. An open
challenge is therefore to efficiently schedule hard real-time tasks on a
multicore architecture. In this work, we propose a mathematical formulation for
computing a static scheduling that minimize L1 data cache misses between hard
real-time tasks on a multicore architecture using communication affinities.
","['\nLilia Zaourar\nLIST\n', '\nMathieu Jan\nLIST\n', '\nMaurice Pitel\n']",,"34th IEEE Real-Time Systems Symposium (RTSS'13), WiP session,
  Vancouver : Canada (2013)",http://arxiv.org/abs/1312.4509v1,cs.OS,['cs.OS'],,,"['LIST', 'LIST']"
Rio: A System Solution for Sharing I/O between Mobile Systems,http://arxiv.org/abs/1312.4931v1,2013-12-17T20:43:33Z,2013-12-17T20:43:33Z,"  Mobile systems are equipped with a diverse collection of I/O devices,
including cameras, microphones, sensors, and modems. There exist many novel use
cases for allowing an application on one mobile system to utilize I/O devices
from another. This paper presents Rio, an I/O sharing solution that supports
unmodified applications and exposes all the functionality of an I/O device for
sharing. Rio's design is common to many classes of I/O devices, thus
significantly reducing the engineering effort to support new I/O devices. Our
implementation of Rio on Android consists of 6700 total lines of code and
supports four I/O classes with fewer than 450 class-specific lines of code. Rio
also supports I/O sharing between mobile systems of different form factors,
including smartphones and tablets. We show that Rio achieves performance close
to that of local I/O for audio, sensors, and modems, but suffers noticeable
performance degradation for camera due to network throughput limitations
between the two systems, which is likely to be alleviated by emerging wireless
standards.
","['\nArdalan Amiri Sani\n', '\nKevin Boos\n', '\nMin Hong Yun\n', '\nLin Zhong\n']",,,http://arxiv.org/abs/1312.4931v1,cs.OS,['cs.OS'],,,[]
Managing NymBoxes for Identity and Tracking Protection,http://arxiv.org/abs/1312.3665v2,2013-12-12T22:38:08Z,2014-05-05T16:05:35Z,"  Despite the attempts of well-designed anonymous communication tools to
protect users from tracking or identification, flaws in surrounding software
(such as web browsers) and mistakes in configuration may leak the user's
identity. We introduce Nymix, an anonymity-centric operating system
architecture designed ""top-to-bottom"" to strengthen identity- and
tracking-protection. Nymix's core contribution is OS support for nym-browsing:
independent, parallel, and ephemeral web sessions. Each web session, or
pseudonym, runs in a unique virtual machine (VM) instance evolving from a
common base state with support for long-lived sessions which can be anonymously
stored to the cloud, avoiding de-anonymization despite potential confiscation
or theft. Nymix allows a user to safely browse the Web using various different
transports simultaneously through a pluggable communication model that supports
Tor, Dissent, and a private browsing mode. In evaluations, Nymix consumes 600
MB per nymbox and loads within 15 to 25 seconds.
","['\nDavid Isaac Wolinsky\n', '\nBryan Ford\n']","16 pages, 7 figure, 1 table",,http://arxiv.org/abs/1312.3665v2,cs.OS,"['cs.OS', 'cs.CR']",,,[]
Transparent Checkpoint-Restart over InfiniBand,http://arxiv.org/abs/1312.3938v3,2013-12-13T20:53:39Z,2014-01-30T21:39:39Z,"  InfiniBand is widely used for low-latency, high-throughput cluster computing.
Saving the state of the InfiniBand network as part of distributed checkpointing
has been a long-standing challenge for researchers. Because of a lack of a
solution, typical MPI implementations have included custom checkpoint-restart
services that ""tear down"" the network, checkpoint each node as if the node were
a standalone computer, and then re-connect the network again. We present the
first example of transparent, system-initiated checkpoint-restart that directly
supports InfiniBand. The new approach is independent of any particular Linux
kernel, thus simplifying the current practice of using a kernel-based module,
such as BLCR. This direct approach results in checkpoints that are found to be
faster than with the use of a checkpoint-restart service. The generality of
this approach is shown not only by checkpointing an MPI computation, but also a
native UPC computation (Berkeley Unified Parallel C), which does not use MPI.
Scalability is shown by checkpointing 2,048 MPI processes across 128 nodes
(with 16 cores per node). In addition, a cost-effective debugging approach is
also enabled, in which a checkpoint image from an InfiniBand-based production
cluster is copied to a local Ethernet-based cluster, where it can be restarted
and an interactive debugger can be attached to it. This work is based on a
plugin that extends the DMTCP (Distributed MultiThreaded CheckPointing)
checkpoint-restart package.
","['\nJiajun Cao\n', '\nGregory Kerr\n', '\nKapil Arya\n', '\nGene Cooperman\n']","22 pages, 2 figures, 9 tables",,http://arxiv.org/abs/1312.3938v3,cs.OS,"['cs.OS', 'cs.DC', 'D.0']",,,[]
File System - A Component of Operating System,http://arxiv.org/abs/1312.1810v1,2013-12-06T09:18:04Z,2013-12-06T09:18:04Z,"  The file system provides the mechanism for online storage and access to file
contents, including data and programs. This paper covers the high-level details
of file systems, as well as related topics such as the disk cache, the file
system interface to the kernel, and the user-level APIs that use the features
of the file system. It will give you a thorough understanding of how a file
system works in general. The main component of the operating system is the file
system. It is used to create, manipulate, store, and retrieve data. At the
highest level, a file system is a way to manage information on a secondary
storage medium. There are so many layers under and above the file system. All
the layers are to be fully described here. This paper will give the explanatory
knowledge of the file system designers and the researchers in the area. The
complete path from the user process to secondary storage device is to be
mentioned. File system is the area where the researchers are doing lot of job
and there is always a need to do more work. The work is going on for the
efficient, secure, energy saving techniques for the file systems. As we know
that the hardware is going to be fast in performance and low-priced day by day.
The software is not built to comeback with the hardware technology. So there is
a need to do research in this area to bridge the technology gap.
","['\nBrijender Kahanwal\n', '\nTejinder Pal Singh\n', '\nRuchira Bhargava\n', '\nGirish Pal Singh\n']","5 pages, 3 figures, 1 table","Asian Journal of Computer Science and Information Technology 2(5),
  pp. 124-128, 2012",http://arxiv.org/abs/1312.1810v1,cs.OS,['cs.OS'],,,[]
"Towards the Framework of the File Systems Performance Evaluation
  Techniques and the Taxonomy of Replay Traces",http://arxiv.org/abs/1312.1822v1,2013-12-06T10:20:38Z,2013-12-06T10:20:38Z,"  This is the era of High Performance Computing (HPC). There is a great demand
of the best performance evaluation techniques for the file and storage systems.
The task of evaluation is both necessary and hard. It gives in depth analysis
of the target system and that becomes the decision points for the users. That
is also helpful for the inventors or developers to find out the bottleneck in
their systems. In this paper many performance evaluation techniques are
described for file and storage system evaluation and the main stress is given
on the important one that is replay traces. A survey has been done for the
performance evaluation techniques used by the researchers and on the replay
traces. And the taxonomy of the replay traces is described. The some of the
popular replay traces are just like, Tracefs [1], //Trace [2], Replayfs [3] and
VFS Interceptor [12]. At last we have concluded all the features that must be
considered when we are going to develop the new tool for the replay traces. The
complete work of this paper shows that the storage system developers must care
about all the techniques which can be used for the performance evaluation of
the file systems. So they can develop highly efficient future file and storage
systems.
","['\nBrijender Kahanwal\n', '\nTejinder Pal Singh\n']",7 pages,"International Journal of Advanced Research in Computer Science,
  2(6) pp. 224-229, 2011",http://arxiv.org/abs/1312.1822v1,cs.OS,['cs.OS'],,,[]
Impact of Limpware on HDFS: A Probabilistic Estimation,http://arxiv.org/abs/1311.3322v1,2013-11-13T22:05:58Z,2013-11-13T22:05:58Z,"  With the advent of cloud computing, thousands of machines are connected and
managed collectively. This era is confronted with a new challenge: performance
variability, primarily caused by large-scale management issues such as hardware
failures, software bugs, and configuration mistakes. In our previous work we
highlighted one overlooked cause: limpware - hardware whose performance
degrades significantly compared to its specification. We showed that limpware
can cause severe impact in current scale-out systems. In this report, we
quantify how often these scenarios happen in Hadoop Distributed File System.
","['\nThanh Do\n', '\nHaryadi S. Gunawi\n']","9 pages, 6 figures, detailed probability calculation for SOCC 13
  limplock paper",,http://arxiv.org/abs/1311.3322v1,cs.OS,"['cs.OS', 'cs.DC']",,,[]
"Efficient Runtime Monitoring with Metric Temporal Logic: A Case Study in
  the Android Operating System",http://arxiv.org/abs/1311.2362v1,2013-11-11T06:21:29Z,2013-11-11T06:21:29Z,"  We present a design and an implementation of a security policy specification
language based on metric linear-time temporal logic (MTL). MTL features
temporal operators that are indexed by time intervals, allowing one to specify
timing-dependent security policies. The design of the language is driven by the
problem of runtime monitoring of applications in mobile devices. A main case
the study is the privilege escalation attack in the Android operating system,
where an app gains access to certain resource or functionalities that are not
explicitly granted to it by the user, through indirect control flow. To capture
these attacks, we extend MTL with recursive definitions, that are used to
express call chains betwen apps. We then show how the metric operators of MTL,
in combination with recursive definitions, can be used to specify policies to
detect privilege escalation, under various fine grained constraints. We present
a new algorithm, extending that of linear time temporal logic, for monitoring
safety policies written in our specification language. The monitor does not
need to store the entire history of events generated by the apps, something
that is crucial for practical implementations. We modified the Android OS
kernel to allow us to insert our generated monitors modularly. We have tested
the modified OS on an actual device, and show that it is effective in detecting
policy violations.
","['\nHendra Gunadi\n', '\nAlwen Tiu\n']",,,http://arxiv.org/abs/1311.2362v1,cs.LO,"['cs.LO', 'cs.CR', 'cs.OS', 'F.3.1; D.4.6']",,,[]
Performance Evaluation of Java File Security System (JFSS),http://arxiv.org/abs/1311.3686v1,2013-11-13T10:53:02Z,2013-11-13T10:53:02Z,"  Security is a critical issue of the modern file and storage systems, it is
imperative to protect the stored data from unauthorized access. We have
developed a file security system named as Java File Security System (JFSS) [1]
that guarantee the security to files on the demand of all users. It has been
developed on Java platform. Java has been used as programming language in order
to provide portability, but it enforces some performance limitations. It is
developed in FUSE (File System in User space) [3]. Many efforts have been done
over the years for developing file systems in user space (FUSE). All have their
own merits and demerits. In this paper we have evaluated the performance of
Java File Security System (JFSS). Over and over again, the increased security
comes at the expense of user convenience, performance or compatibility with
other systems. JFSS system performance evaluations show that encryption
overheads are modest as compared to security.
","['\nBrijender Kahanwal\n', '\nDr. Tejinder Pal Singh\n', '\nDr. R. K. Tuteja\n']","7 pages, 5 figures, journal","Pelagia Research Library Advances in Applied Science Research,
  Vol. 2(6), pp. 254-260, 2011",http://arxiv.org/abs/1311.3686v1,cs.OS,"['cs.OS', 'cs.CR', 'cs.PF']",,,[]
The Quest-V Separation Kernel for Mixed Criticality Systems,http://arxiv.org/abs/1310.6298v1,2013-10-23T17:14:25Z,2013-10-23T17:14:25Z,"  Multi- and many-core processors are becoming increasingly popular in embedded
systems. Many of these processors now feature hardware virtualization
capabilities, such as the ARM Cortex A15, and x86 processors with Intel VT-x or
AMD-V support. Hardware virtualization offers opportunities to partition
physical resources, including processor cores, memory and I/O devices amongst
guest virtual machines. Mixed criticality systems and services can then
co-exist on the same platform in separate virtual machines. However,
traditional virtual machine systems are too expensive because of the costs of
trapping into hypervisors to multiplex and manage machine physical resources on
behalf of separate guests. For example, hypervisors are needed to schedule
separate VMs on physical processor cores. In this paper, we discuss the design
of the Quest-V separation kernel, that partitions services of different
criticalities in separate virtual machines, or sandboxes. Each sandbox
encapsulates a subset of machine physical resources that it manages without
requiring intervention of a hypervisor. Moreover, a hypervisor is not needed
for normal operation, except to bootstrap the system and establish
communication channels between sandboxes.
","['\nYe Li\n', '\nRichard West\n', '\nEric Missimer\n']",6 pages,,http://arxiv.org/abs/1310.6298v1,cs.OS,['cs.OS'],,,[]
Predictable Migration and Communication in the Quest-V Multikernel,http://arxiv.org/abs/1310.6301v1,2013-10-23T17:29:42Z,2013-10-23T17:29:42Z,"  Quest-V is a system we have been developing from the ground up, with
objectives focusing on safety, predictability and efficiency. It is designed to
work on emerging multicore processors with hardware virtualization support.
Quest-V is implemented as a ""distributed system on a chip"" and comprises
multiple sandbox kernels. Sandbox kernels are isolated from one another in
separate regions of physical memory, having access to a subset of processing
cores and I/O devices. This partitioning prevents system failures in one
sandbox affecting the operation of other sandboxes. Shared memory channels
managed by system monitors enable inter-sandbox communication.
  The distributed nature of Quest-V means each sandbox has a separate physical
clock, with all event timings being managed by per-core local timers. Each
sandbox is responsible for its own scheduling and I/O management, without
requiring intervention of a hypervisor.
  In this paper, we formulate bounds on inter-sandbox communication in the
absence of a global scheduler or global system clock. We also describe how
address space migration between sandboxes can be guaranteed without violating
service constraints. Experimental results on a working system show the
conditions under which Quest-V performs real-time communication and migration.
","['\nYe Li\n', '\nEric Missimer\n', '\nRichard West\n']",10 pages,,http://arxiv.org/abs/1310.6301v1,cs.OS,['cs.OS'],,,[]
Quest-V: A Virtualized Multikernel for Safety-Critical Real-Time Systems,http://arxiv.org/abs/1310.6349v1,2013-10-23T17:35:03Z,2013-10-23T17:35:03Z,"  Modern processors are increasingly featuring multiple cores, as well as
support for hardware virtualization. While these processors are common in
desktop and server-class computing, they are less prevalent in embedded and
real-time systems. However, smartphones and tablet PCs are starting to feature
multicore processors with hardware virtualization. If the trend continues, it
is possible that future real-time systems will feature more sophisticated
processor architectures. Future automotive or avionics systems, for example,
could replace complex networks of uniprocessors with consolidated services on a
smaller number of multicore processors. Likewise, virtualization could be used
to isolate services and increase the availability of a system even when
failures occur.
  This paper investigates whether advances in modern processor technologies
offer new opportunities to rethink the design of real-time operating systems.
We describe some of the design principles behind Quest-V, which is being used
as an exploratory vehicle for real-time system design on multicore processors
with hardware virtualization capabilities. While not all embedded systems
should assume such features, a case can be made that more robust,
safety-critical systems can be built to use hardware virtualization without
incurring significant overheads.
","['\nRichard West\n', '\nYe Li\n', '\nEric Missimer\n']","12 pages. arXiv admin note: text overlap with arXiv:1112.5136,
  arXiv:1310.6301",,http://arxiv.org/abs/1310.6349v1,cs.OS,['cs.OS'],,,[]
Impacting the bioscience progress by backporting software for Bio-Linux,http://arxiv.org/abs/1310.1588v1,2013-10-06T14:27:59Z,2013-10-06T14:27:59Z,"  In year 2006 Bio-Linux with the work of Tim Booth and team gives its rising
and provide an operating system that was and still specialized in providing a
bioinformatic specific software environment for the working needs in this
corner of bioscience. It is shown that Bio-Linux is affected by a 2 year
release cycle and with this the final releases of Bio-Linux will not have the
latest bioinformatic software on board. The paper shows how to get around this
huge time gap and bring new software for Bio-Linux on board through a process
that is called backporting. A summary of within the work to this paper just
backported bioinformatic tools is given. A describtion of a workflow for
continuously integration of the newest bioinformatic tools gives an outlook to
further concrete planned developments and the influence of speeding up
scientific progress.
",['\nSasa Paporovic\n'],"10 pages,2 Figures, 1 Table and 1 notice",,http://arxiv.org/abs/1310.1588v1,cs.OS,"['cs.OS', 'cs.SE']",,,[]
"Simulation of an Optimum Multilevel Dynamic Round Robin Scheduling
  Algorithm",http://arxiv.org/abs/1309.3096v1,2013-09-12T10:26:10Z,2013-09-12T10:26:10Z,"  CPU scheduling has valiant effect on resource utilization as well as overall
quality of the system. Round Robin algorithm performs optimally in time shared
systems, but it performs more number of context switches, larger waiting time
and larger response time. In order to simulate the behavior of various CPU
scheduling algorithms and to improve Round Robin scheduling algorithm using
dynamic time slice concept, in this paper we produce the implementation of new
CPU scheduling algorithm called An Optimum Multilevel Dynamic Round Robin
Scheduling (OMDRRS), which calculates intelligent time slice and warps after
every round of execution. The results display the robustness of this software,
especially for academic, research and experimental use, as well as proving the
desirability and efficiency of the probabilistic algorithm over the other
existing techniques and it is observed that this OMDRRS projects good
performance as compared to the other existing CPU scheduling algorithms.
","['\nNeetu Goel\n', '\nR. B. Garg\n']","International Journal of Computer Applications, Aug 2013",,http://dx.doi.org/10.5120/13263-0743,cs.OS,['cs.OS'],10.5120/13263-0743,,[]
"Flashmon V2: Monitoring Raw NAND Flash Memory I/O Requests on Embedded
  Linux",http://arxiv.org/abs/1309.1714v1,2013-09-06T18:14:04Z,2013-09-06T18:14:04Z,"  This paper presents Flashmon version 2, a tool for monitoring embedded Linux
NAND flash memory I/O requests. It is designed for embedded boards based
devices containing raw flash chips. Flashmon is a kernel module and stands for
""flash monitor"". It traces flash I/O by placing kernel probes at the NAND
driver level. It allows tracing at runtime the 3 main flash operations: page
reads / writes and block erasures. Flashmon is (1) generic as it was
successfully tested on the three most widely used flash file systems that are
JFFS2, UBIFS and YAFFS, and several NAND chip models. Moreover, it is (2) non
intrusive, (3) has a controllable memory footprint, and (4) exhibits a low
overhead (<6%) on the traced system. Finally, it is (5) simple to integrate and
used as a standalone module or as a built-in function / module in existing
kernel sources. Monitoring flash memory operations allows a better
understanding of existing flash management systems by studying and analyzing
their behavior. Moreover it is useful in development phase for prototyping and
validating new solutions.
","['\nPierre Olivier\nLab-STICC\n', '\nJalil Boukhobza\nLab-STICC\n', '\nEric Senn\nLab-STICC\n']","EWiLi, the Embedded Operating Systems Workshop, Toulouse : France
  (2013)",,http://arxiv.org/abs/1309.1714v1,cs.OS,"['cs.OS', 'cs.PF']",,,"['Lab-STICC', 'Lab-STICC', 'Lab-STICC']"
Opacity of Memory Management in Software Transactional Memory,http://arxiv.org/abs/1308.2881v1,2013-08-13T14:37:51Z,2013-08-13T14:37:51Z,"  Opacity of Transactional Memory is proposed to be established by incremental
validation. Quiescence in terms of epoch-based memory reclamation is applied to
deal with doomed transactions causing memory access violations. This method
unfortunately involves increased memory consumption and does not cover
reclamations outside of transactions. This paper introduces a different method
which combines incremental validation with elements of sandboxing to solve
these issues.
","['\nHolger Machens\n', '\nVolker Turau\n']","Keywords: transactional memory, opacity, privatization, memory
  reclamation",,http://arxiv.org/abs/1308.2881v1,cs.DC,"['cs.DC', 'cs.OS', '65Y05']",,,[]
Intensional view of General Single Processor Operating Systems,http://arxiv.org/abs/1308.1199v1,2013-08-06T07:54:19Z,2013-08-06T07:54:19Z,"  Operating systems are currently viewed ostensively. As a result they mean
different things to different people. The ostensive character makes it is hard
to understand OSes formally. An intensional view can enable better formal work,
and also offer constructive support for some important problems, e.g. OS
architecture. This work argues for an intensional view of operating systems. It
proposes to overcome the current ostensive view by defining an OS based on
formal models of computation, and also introduces some principles. Together
these are used to develop a framework of algorithms of single processor OS
structure using an approach similar to function level programming. In this
abridged paper we illustrate the essential approach, discuss some advantages
and limitations and point out some future possibilities.
",['\nAbhijat Vichare\n'],"17 pages, 3 figures. Condensed and improved version of
  http://arxiv.org/abs/1112.4451 for submission to SOSP'13. arXiv admin note:
  substantial text overlap with arXiv:1112.4451",,http://arxiv.org/abs/1308.1199v1,cs.OS,"['cs.OS', 'D.4.7']",,,[]
An Improving Method for Loop Unrolling,http://arxiv.org/abs/1308.0698v1,2013-08-03T14:09:23Z,2013-08-03T14:09:23Z,"  In this paper we review main ideas mentioned in several other papers which
talk about optimization techniques used by compilers. Here we focus on loop
unrolling technique and its effect on power consumption, energy usage and also
its impact on program speed up by achieving ILP (Instruction-level
parallelism). Concentrating on superscalar processors, we discuss the idea of
generalized loop unrolling presented by J.C. Hang and T. Leng and then we
present a new method to traverse a linked list to get a better result of loop
unrolling in that case. After that we mention the results of some experiments
carried out on a Pentium 4 processor (as an instance of super scalar
architecture). Furthermore, the results of some other experiments on
supercomputer (the Alliat FX/2800 System) containing superscalar node
processors would be mentioned. These experiments show that loop unrolling has a
slight measurable effect on energy usage as well as power consumption. But it
could be an effective way for program speed up.
","['\nMeisam Booshehri\n', '\nAbbas Malekpour\n', '\nPeter Luksch\n']","4 pages, International Journal of Computer Science and Information
  Security","International Journal of Computer Science and Information
  Security, Vol. 11, No. 5, pp. 73-76 , 2013",http://arxiv.org/abs/1308.0698v1,cs.PL,"['cs.PL', 'cs.OS', '68Nxx', 'D.3.4']",,,[]
A Comparative Study of CPU Scheduling Algorithms,http://arxiv.org/abs/1307.4165v1,2013-07-16T05:21:34Z,2013-07-16T05:21:34Z,"  Developing CPU scheduling algorithms and understanding their impact in
practice can be difficult and time consuming due to the need to modify and test
operating system kernel code and measure the resulting performance on a
consistent workload of real applications. As processor is the important
resource, CPU scheduling becomes very important in accomplishing the operating
system (OS) design goals. The intention should be allowed as many as possible
running processes at all time in order to make best use of CPU. This paper
presents a state diagram that depicts the comparative study of various
scheduling algorithms for a single CPU and shows which algorithm is best for
the particular situation. Using this representation, it becomes much easier to
understand what is going on inside the system and why a different set of
processes is a candidate for the allocation of the CPU at different time. The
objective of the study is to analyze the high efficient CPU scheduler on design
of the high quality scheduling algorithms which suits the scheduling goals. Key
Words:-Scheduler, State Diagrams, CPU-Scheduling, Performance
","['\nNeetu Goel\n', '\nR. B. Garg\n']",,"International Journal of Graphics & Image Processing |Vol 2|issue
  4|November 2012",http://arxiv.org/abs/1307.4165v1,cs.OS,['cs.OS'],,,[]
An Optimum Multilevel Dynamic Round Robin Scheduling Algorithm,http://arxiv.org/abs/1307.4167v1,2013-07-16T05:25:16Z,2013-07-16T05:25:16Z,"  The main objective of this paper is to improve the Round Robin scheduling
algorithm using the dynamic time slice concept. CPU scheduling becomes very
important in accomplishing the operating system (OS) design goals. The
intention should be allowed as many as possible running processes at all time
in order to make best use of CPU. CPU scheduling has strong effect on resource
utilization as well as overall performance of the system. Round Robin algorithm
performs optimally in time-shared systems, but it is not suitable for soft real
time systems, because it gives more number of context switches, larger waiting
time and larger response time. In this paper, a new CPU scheduling algorithm
called An Optimum Multilevel Dynamic Round Robin Scheduling Algorithm is
proposed, which calculates intelligent time slice and changes after every round
of execution. The suggested algorithm was evaluated on some CPU scheduling
objectives and it was observed that this algorithm gave good performance as
compared to the other existing CPU scheduling algorithms.
","['\nNeetu Goel\n', '\nR. B. Garg\n']",,"Published in National Conference on Information Communication &
  Networks, Dated: April 6,2013 organized by Tecnia Institute of Advanced
  Studies",http://arxiv.org/abs/1307.4167v1,cs.OS,['cs.OS'],,,[]
"Partitioned scheduling of multimode multiprocessor real-time systems
  with temporal isolation",http://arxiv.org/abs/1306.1316v1,2013-06-06T06:57:57Z,2013-06-06T06:57:57Z,"  We consider the partitioned scheduling problem of multimode real-time systems
upon identical multiprocessor platforms. During the execution of a multimode
system, the system can change from one mode to another such that the current
task set is replaced with a new one. In this paper, we consider a synchronous
transition protocol in order to take into account mode-independent tasks, i.e.,
tasks of which the execution pattern must not be jeopardized by the mode
changes. We propose two methods for handling mode changes in partitioned
scheduling. The first method is offline/optimal and computes a static
allocation of tasks schedulable and respecting both tasks and transition
deadlines (if any). The second approach is subject to a sufficient condition in
order to ensure online First Fit based allocation to satisfy the timing
constraints.
","['\nJoël Goossens\nULB\n', '\nPascal Richard\nLIAS/Ensma and Université de Poitiers\n']",,,http://arxiv.org/abs/1306.1316v1,cs.OS,['cs.OS'],,,"['ULB', 'LIAS/Ensma and Université de Poitiers']"
Augmenting Operating Systems With the GPU,http://arxiv.org/abs/1305.3345v1,2013-05-15T02:53:19Z,2013-05-15T02:53:19Z,"  The most popular heterogeneous many-core platform, the CPU+GPU combination,
has received relatively little attention in operating systems research. This
platform is already widely deployed: GPUs can be found, in some form, in most
desktop and laptop PCs. Used for more than just graphics processing, modern
GPUs have proved themselves versatile enough to be adapted to other
applications as well. Though GPUs have strengths that can be exploited in
systems software, this remains a largely untapped resource. We argue that
augmenting the OS kernel with GPU computing power opens the door to a number of
new opportunities. GPUs can be used to speed up some kernel functions, make
other scale better, and make it feasible to bring some computation-heavy
functionality into the kernel. We present our framework for using the GPU as a
co-processor from an OS kernel, and demonstrate a prototype in Linux.
","['\nWeibin Sun\n', '\nRobert Ricci\n']","5 pages, 2 figures, old white paper submitted for KGPU citation",,http://arxiv.org/abs/1305.3345v1,cs.OS,"['cs.OS', 'D.4.7']",,,[]
"On the periodic behavior of real-time schedulers on identical
  multiprocessor platforms",http://arxiv.org/abs/1305.3849v1,2013-05-16T15:54:12Z,2013-05-16T15:54:12Z,"  This paper is proposing a general periodicity result concerning any
deterministic and memoryless scheduling algorithm (including
non-work-conserving algorithms), for any context, on identical multiprocessor
platforms. By context we mean the hardware architecture (uniprocessor,
multicore), as well as task constraints like critical sections, precedence
constraints, self-suspension, etc. Since the result is based only on the
releases and deadlines, it is independent from any other parameter. Note that
we do not claim that the given interval is minimal, but it is an upper bound
for any cycle of any feasible schedule provided by any deterministic and
memoryless scheduler.
","['\nEmmanuel Grolleau\nLIAS-ISAE/ENSMA\n', '\nJoël Goossens\nULB\n', '\nLiliana Cucu-Grosjean\nINRIA\n']",,,http://arxiv.org/abs/1305.3849v1,cs.OS,['cs.OS'],,,"['LIAS-ISAE/ENSMA', 'ULB', 'INRIA']"
"Practical Fine-grained Privilege Separation in Multithreaded
  Applications",http://arxiv.org/abs/1305.2553v1,2013-05-12T02:44:15Z,2013-05-12T02:44:15Z,"  An inherent security limitation with the classic multithreaded programming
model is that all the threads share the same address space and, therefore, are
implicitly assumed to be mutually trusted. This assumption, however, does not
take into consideration of many modern multithreaded applications that involve
multiple principals which do not fully trust each other. It remains challenging
to retrofit the classic multithreaded programming model so that the security
and privilege separation in multi-principal applications can be resolved.
  This paper proposes ARBITER, a run-time system and a set of security
primitives, aimed at fine-grained and data-centric privilege separation in
multithreaded applications. While enforcing effective isolation among
principals, ARBITER still allows flexible sharing and communication between
threads so that the multithreaded programming paradigm can be preserved. To
realize controlled sharing in a fine-grained manner, we created a novel
abstraction named ARBITER Secure Memory Segment (ASMS) and corresponding OS
support. Programmers express security policies by labeling data and principals
via ARBITER's API following a unified model. We ported a widely-used, in-memory
database application (memcached) to ARBITER system, changing only around 100
LOC. Experiments indicate that only an average runtime overhead of 5.6% is
induced to this security enhanced version of application.
","['\nJun Wang\n', '\nXi Xiong\n', '\nPeng Liu\n']",,,http://arxiv.org/abs/1305.2553v1,cs.OS,"['cs.OS', 'cs.CR', 'D.4.6']",,,[]
"EURETILE 2010-2012 summary: first three years of activity of the
  European Reference Tiled Experiment",http://arxiv.org/abs/1305.1459v1,2013-05-07T10:22:31Z,2013-05-07T10:22:31Z,"  This is the summary of first three years of activity of the EURETILE FP7
project 247846. EURETILE investigates and implements brain-inspired and
fault-tolerant foundational innovations to the system architecture of massively
parallel tiled computer architectures and the corresponding programming
paradigm. The execution targets are a many-tile HW platform, and a many-tile
simulator. A set of SW process - HW tile mapping candidates is generated by the
holistic SW tool-chain using a combination of analytic and bio-inspired
methods. The Hardware dependent Software is then generated, providing OS
services with maximum efficiency/minimal overhead. The many-tile simulator
collects profiling data, closing the loop of the SW tool chain. Fine-grain
parallelism inside processes is exploited by optimized intra-tile compilation
techniques, but the project focus is above the level of the elementary tile.
The elementary HW tile is a multi-processor, which includes a fault tolerant
Distributed Network Processor (for inter-tile communication) and ASIP
accelerators. Furthermore, EURETILE investigates and implements the innovations
for equipping the elementary HW tile with high-bandwidth, low-latency
brain-like inter-tile communication emulating 3 levels of connection hierarchy,
namely neural columns, cortical areas and cortex, and develops a dedicated
cortical simulation benchmark: DPSNN-STDP (Distributed Polychronous Spiking
Neural Net with synaptic Spiking Time Dependent Plasticity). EURETILE leverages
on the multi-tile HW paradigm and SW tool-chain developed by the FET-ACA SHAPES
Integrated Project (2006-2009).
","['\nPier Stanislao Paolucci\n', '\nIuliana Bacivarov\n', '\nGert Goossens\n', '\nRainer Leupers\n', '\nFrédéric Rousseau\n', '\nChristoph Schumacher\n', '\nLothar Thiele\n', '\nPiero Vicini\n']",56 pages,,http://dx.doi.org/10.12837/2013T01,cs.DC,"['cs.DC', 'cs.AR', 'cs.NE', 'cs.OS', 'cs.PL', 'C.1.4; C.3; B.7.2; F.2.2']",10.12837/2013T01,,[]
Invasive Computing - Common Terms and Granularity of Invasion,http://arxiv.org/abs/1304.6067v1,2013-04-22T19:29:05Z,2013-04-22T19:29:05Z,"  Future MPSoCs with 1000 or more processor cores on a chip require new means
for resource-aware programming in order to deal with increasing imperfections
such as process variation, fault rates, aging effects, and power as well as
thermal problems. On the other hand, predictable program executions are
threatened if not impossible if no proper means of resource isolation and
exclusive use may be established on demand. In view of these problems and
menaces, invasive computing enables an application programmer to claim for
processing resources and spread computations to claimed processors dynamically
at certain points of the program execution.
  Such decisions may be depending on the degree of application parallelism and
the state of the underlying resources such as utilization, load, and
temperature, but also with the goal to provide predictable program execution on
MPSoCs by claiming processing resources exclusively as the default and thus
eliminating interferences and creating the necessary isolation between multiple
concurrently running applications. For achieving this goal, invasive computing
introduces new programming constructs for resource-aware programming that
meanwhile, for testing purpose, have been embedded into the parallel computing
language X10 as developed by IBM using a library-based approach.
  This paper presents major ideas and common terms of invasive computing as
investigated by the DFG Transregional Collaborative Research Centre TR89.
Moreoever, a reflection is given on the granularity of resources that may be
requested by invasive programs.
","['\nJürgen Teich\n', '\nWolfgang Schröder-Preikschat\n', '\nAndreas Herkersdorf\n']",,,http://arxiv.org/abs/1304.6067v1,cs.OS,['cs.OS'],,,[]
Network Control Systems RTAI framework A Review,http://arxiv.org/abs/1304.7001v1,2013-04-25T11:10:38Z,2013-04-25T11:10:38Z,"  With the advancement in the automation industry, to perform complex remote
operations is required. Advancements in the networking technology has led to
the development of different architectures to implement control from a large
distance. In various control applications of the modern industry, the agents,
such as sensors, actuators, and controllers are basically geographically
distributed. For efficient working of a control application, all of the agents
have to exchange information through a communication media. At present, an
increasing number of distributed control systems are based on platforms made up
of conventional PCs running open-source real-time operating systems. Often,
these systems needed to have networked devices supporting synchronized
operations with respect to each node. A framework is studied that relies on
standard software and protocol as RTAI, EtherCAT, RTnet and IEEE 1588. RTAI and
its various protocols are studied in network control systems environment.
","['\nDeepika Bhatia\n', '\nUrmila Shrawankar\n']",Pages: 4 Figures : 1,"International Journal of Computer Science and Information
  Technologies (IJCSIT),Vol. 2(5) , 2011, 2380-2383",http://arxiv.org/abs/1304.7001v1,cs.OS,['cs.OS'],,,[]
Paging with dynamic memory capacity,http://arxiv.org/abs/1304.6007v1,2013-04-22T16:23:24Z,2013-04-22T16:23:24Z,"  We study a generalization of the classic paging problem that allows the
amount of available memory to vary over time - capturing a fundamental property
of many modern computing realities, from cloud computing to multi-core and
energy-optimized processors. It turns out that good performance in the
""classic"" case provides no performance guarantees when memory capacity
fluctuates: roughly speaking, moving from static to dynamic capacity can mean
the difference between optimality within a factor 2 in space and time, and
suboptimality by an arbitrarily large factor. More precisely, adopting the
competitive analysis framework, we show that some online paging algorithms,
despite having an optimal (h,k)-competitive ratio when capacity remains
constant, are not (3,k)-competitive for any arbitrarily large k in the presence
of minimal capacity fluctuations. In this light it is surprising that several
classic paging algorithms perform remarkably well even if memory capacity
changes adversarially - even without taking those changes into explicit
account! In particular, we prove that LFD still achieves the minimum number of
faults, and that several classic online algorithms such as LRU have a ""dynamic""
(h,k)-competitive ratio that is the best one can achieve without knowledge of
future page requests, even if one had perfect knowledge of future capacity
fluctuations (an exact characterization of this ratio shows it is almost,
albeit not quite, equal to the ""classic"" ratio k/(k-h+1)). In other words, with
careful management, knowing/predicting future memory resources appears far less
crucial to performance than knowing/predicting future data accesses.
",['\nEnoch Peserico\n'],,,http://arxiv.org/abs/1304.6007v1,cs.DS,"['cs.DS', 'cs.OS', 'cs.PF']",,,[]
Survey of Server Virtualization,http://arxiv.org/abs/1304.3557v1,2013-04-12T07:33:42Z,2013-04-12T07:33:42Z,"  Virtualization is a term that refers to the abstraction of computer
resources. The purpose of virtual computing environment is to improve resource
utilization by providing a unified integrated operating platform for users and
applications based on aggregation of heterogeneous and autonomous resources.
More recently, virtualization at all levels (system, storage, and network)
became important again as a way to improve system security, reliability and
availability, reduce costs, and provide greater flexibility. Virtualization has
rapidly become a go-to technology for increasing efficiency in the data center.
With virtualization technologies providing tremendous flexibility, even
disparate architectures may be deployed on a single machine without
interference This paper explains the basics of server virtualization and
addresses pros and cons of virtualization
","['\nRadhwan Y Ameen\n', '\nAsmaa Y. Hamo\n']","10 pages 14 figures. arXiv admin note: text overlap with
  arXiv:1010.3233 by other authors",,http://arxiv.org/abs/1304.3557v1,cs.OS,['cs.OS'],,,[]
Making I/O Virtualization Easy with Device Files,http://arxiv.org/abs/1304.3771v1,2013-04-13T04:04:08Z,2013-04-13T04:04:08Z,"  Personal computers have diverse and fast-evolving I/O devices, making their
I/O virtualization different from that of servers and data centers. In this
paper, we present our recent endeavors in simplifying I/O virtualization for
personal computers. Our key insight is that many operating systems, including
Unix-like ones, abstract I/O devices as device files. There is a small and
stable set of operations on device files, therefore, I/O virtualization at the
device file boundary requires a one-time effort to support various I/O devices.
  We present devirtualization, our design of I/O virtualization at the device
file boundary and its implementation for Linux/x86 systems. We are able to
virtualize various GPUs, input devices, cameras, and audio devices with fewer
than 4900 LoC, of which only about 300 are specific to I/O device classes. Our
measurements show that devirtualized devices achieve interactive performance
indistinguishable from native ones by human users, even when running 3D HD
games.
","['\nArdalan Amiri Sani\n', '\nSreekumar Nair\n', '\nLin Zhong\n', '\nQuinn Jacobson\n']",,,http://arxiv.org/abs/1304.3771v1,cs.OS,['cs.OS'],,,[]
Capturing Information Flows inside Android and Qemu Environments,http://arxiv.org/abs/1302.5109v1,2013-02-20T15:50:29Z,2013-02-20T15:50:29Z,"  The smartphone market has grown so wide that it assumed a strategic
relevance. Today the most common smartphone OSs are Google's Android and
Apple's iOS. The former is particularly interesting due to its open source
nature, that allows everyone to deeply inspect every aspect of the OS. Android
source code is also bundled with an hardware emulator, based on the open source
software Qemu, that allows the user to run the Android OS without the need of a
physical device. We first present a procedure to extract information flows from
a generic system. We then focus on Android and Qemu architectures and their
logging infrastructures. Finally, we detail what happens inside an Android
device in a particular scenario: the system boot.
","['\nMarco Sironi\n', '\nFrancesco Tisato\n']",13 pages,,http://arxiv.org/abs/1302.5109v1,cs.OS,['cs.OS'],,,[]
LFTL: A multi-threaded FTL for a Parallel IO Flash Card under Linux,http://arxiv.org/abs/1302.5502v1,2013-02-22T07:32:48Z,2013-02-22T07:32:48Z,"  New PCI-e flash cards and SSDs supporting over 100,000 IOPs are now
available, with several usecases in the design of a high performance storage
system. By using an array of flash chips, arranged in multiple banks, large
capacities are achieved. Such multi-banked architecture allow parallel read,
write and erase operations. In a raw PCI-e flash card, such parallelism is
directly available to the software layer. In addition, the devices have
restrictions such as, pages within a block can only be written sequentially.
The devices also have larger minimum write sizes (greater than 4KB). Current
flash translation layers (FTLs) in Linux are not well suited for such devices
due to the high device speeds, architectural restrictions as well as other
factors such as high lock contention. We present a FTL for Linux that takes
into account the hardware restrictions, that also exploits the parallelism to
achieve high speeds. We also consider leveraging the parallelism for garbage
collection by scheduling the garbage collection activities on idle banks. We
propose and evaluate an adaptive method to vary the amount of garbage
collection according to the current I/O load on the device.
","['\n Srimugunthan\n', '\nK. Gopinath\n', '\nGiridhar Appaji Nag Yasa\n']",,,http://arxiv.org/abs/1302.5502v1,cs.OS,['cs.OS'],,,[]
"Towards Python-based Domain-specific Languages for Self-reconfigurable
  Modular Robotics Research",http://arxiv.org/abs/1302.5521v1,2013-02-22T09:01:24Z,2013-02-22T09:01:24Z,"  This paper explores the role of operating system and high-level languages in
the development of software and domain-specific languages (DSLs) for
self-reconfigurable robotics. We review some of the current trends in
self-reconfigurable robotics and describe the development of a software system
for ATRON II which utilizes Linux and Python to significantly improve software
abstraction and portability while providing some basic features which could
prove useful when using Python, either stand-alone or via a DSL, on a
self-reconfigurable robot system. These features include transparent socket
communication, module identification, easy software transfer and reliable
module-to-module communication. The end result is a software platform for
modular robots that where appropriate builds on existing work in operating
systems, virtual machines, middleware and high-level languages.
","['\nMikael Moghadam\n', '\nDavid Johan Christensen\n', '\nDavid Brandt\n', '\nUlrik Pagh Schultz\n']",Presented at DSLRob 2011 (arXiv:1212.3308),,http://arxiv.org/abs/1302.5521v1,cs.RO,"['cs.RO', 'cs.OS', 'cs.PL', 'cs.SE']",,,[]
"Energy Minimization for Parallel Real-Time Systems with Malleable Jobs
  and Homogeneous Frequencies",http://arxiv.org/abs/1302.1747v1,2013-02-07T13:52:09Z,2013-02-07T13:52:09Z,"  In this work, we investigate the potential utility of parallelization for
meeting real-time constraints and minimizing energy. We consider malleable Gang
scheduling of implicit-deadline sporadic tasks upon multiprocessors. We first
show the non-necessity of dynamic voltage/frequency regarding optimality of our
scheduling problem. We adapt the canonical schedule for DVFS multiprocessor
platforms and propose a polynomial-time optimal processor/frequency-selection
algorithm. We evaluate the performance of our algorithm via simulations using
parameters obtained from a hardware testbed implementation. Our algorithm has
up to a 60 watt decrease in power consumption over the optimal non-parallel
approach.
","['\nNathan Fisher\n', '\nJoël Goossens\n', '\nPradeep M. Hettiarachchi\n', '\nAntonio Paolillo\n']",,,http://arxiv.org/abs/1302.1747v1,cs.OS,['cs.OS'],,,[]
"RevDedup: A Reverse Deduplication Storage System Optimized for Reads to
  Latest Backups",http://arxiv.org/abs/1302.0621v3,2013-02-04T09:09:38Z,2013-06-27T06:04:17Z,"  Scaling up the backup storage for an ever-increasing volume of virtual
machine (VM) images is a critical issue in virtualization environments. While
deduplication is known to effectively eliminate duplicates for VM image
storage, it also introduces fragmentation that will degrade read performance.
We propose RevDedup, a deduplication system that optimizes reads to latest VM
image backups using an idea called reverse deduplication. In contrast with
conventional deduplication that removes duplicates from new data, RevDedup
removes duplicates from old data, thereby shifting fragmentation to old data
while keeping the layout of new data as sequential as possible. We evaluate our
RevDedup prototype using microbenchmark and real-world workloads. For a 12-week
span of real-world VM images from 160 users, RevDedup achieves high
deduplication efficiency with around 97% of saving, and high backup and read
throughput on the order of 1GB/s. RevDedup also incurs small metadata overhead
in backup/read operations.
","['\nChun-Ho Ng\n', '\nPatrick P. C. Lee\n']",A 7-page version appeared in APSys'13,,http://arxiv.org/abs/1302.0621v3,cs.DC,"['cs.DC', 'cs.OS']",,,[]
"Parametric Schedulability Analysis of Fixed Priority Real-Time
  Distributed Systems",http://arxiv.org/abs/1302.1306v1,2013-02-06T10:06:39Z,2013-02-06T10:06:39Z,"  Parametric analysis is a powerful tool for designing modern embedded systems,
because it permits to explore the space of design parameters, and to check the
robustness of the system with respect to variations of some uncontrollable
variable. In this paper, we address the problem of parametric schedulability
analysis of distributed real-time systems scheduled by fixed priority. In
particular, we propose two different approaches to parametric analysis: the
first one is a novel technique based on classical schedulability analysis,
whereas the second approach is based on model checking of Parametric Timed
Automata (PTA).
  The proposed analytic method extends existing sensitivity analysis for single
processors to the case of a distributed system, supporting preemptive and
non-preemptive scheduling, jitters and unconstrained deadlines. Parametric
Timed Automata are used to model all possible behaviours of a distributed
system, and therefore it is a necessary and sufficient analysis. Both
techniques have been implemented in two software tools, and they have been
compared with classical holistic analysis on two meaningful test cases. The
results show that the analytic method provides results similar to classical
holistic analysis in a very efficient way, whereas the PTA approach is slower
but covers the entire space of solutions.
","['\nYoucheng Sun\n', '\nRomain Soulat\n', '\nGiuseppe Lipari\n', '\nÉtienne André\n', '\nLaurent Fribourg\n']",Submitted to ECRTS 2013 (http://ecrts.eit.uni-kl.de/ecrts13),,http://arxiv.org/abs/1302.1306v1,cs.DC,"['cs.DC', 'cs.OS']",,,[]
"Schedulability Analysis of Distributed Real-Time Applications under
  Dependence and Several Latency Constraints",http://arxiv.org/abs/1301.4800v1,2013-01-21T10:00:22Z,2013-01-21T10:00:22Z,"  This paper focuses on the analysis of real-time non preemptive multiprocessor
scheduling with precedence and several latency constraints. It aims to specify
a schedulability condition which enables a designer to check a priori -without
executing or simulating- if its scheduling of tasks will hold the precedences
between tasks as well as several latency constraints imposed on determined
pairs of tasks. It is shown that the required analysis is closely linked to the
topological structure of the application graph. More precisely, it depends on
the configuration of tasks paths subject to latency constraints. As a result of
the study, a sufficient schedulability condition is introduced for precedences
and latency constraints in the hardest configuration in term of complexity with
an optimal number of processors in term of applications parallelism. In
addition, the proposed conditions provides a practical lower bounds for general
cases. Performances results and comparisons with an optimal approach
demonstrate the effectiveness of the proposed approach.
",['\nOmar Kermia\n'],"8 pages, 6 figures, Published with International Journal of Computer
  Applications (IJCA)","International Journal of Computer Applications 62(14):1-7, January
  2013. Published by Foundation of Computer Science, New York, USA",http://dx.doi.org/10.5120/10145-4978,cs.OS,"['cs.OS', 'cs.DC']",10.5120/10145-4978,,[]
LNOS - Live Network Operating System,http://arxiv.org/abs/1212.6354v1,2012-12-27T11:55:43Z,2012-12-27T11:55:43Z,"  Operating Systems exists since existence of computers, and have been evolving
continuously from time to time. In this paper we have reviewed a relatively new
or unexplored topic of Live OS. From networking perspective, Live OS is used
for establishing Clusters, Firewalls and as Network security assessment tool
etc. Our proposed concept is that a Live OS can be established or configured
for an organizations specific network requirements with respect to their
servers. An important server failure due to hardware or software could take
time for remedy of the problem, so for that situation a preconfigured server in
the form of Live OS on CD/DVD/USB can be used as an immediate solution. In a
network of ten nodes, we stopped the server machine and with necessary
adjustments, Live OS replaced the server in less than five minutes. Live OS in
a network environment is a quick replacement of the services that are failed
due to server failure (hardware or software). It is a cost effective solution
for low budget networks. The life of Live OS starts when we boot it from
CD/DVD/USB and remains in action for that session. As soon as the machine is
rebooted, any work done for that session is gone, (in case we do not store any
information on permanent storage media). Live CD/DVD/USB is normally used on
systems where we do not have Operating Systems installed. A Live OS can also be
used on systems where we already have an installed OS. On the basis of
functionality a Live OS can be used for many purposes and has some typical
advantages that are not available on other operating systems. Vendors are
releasing different distributions of Live OS and is becoming their sole
identity in a particular domain like Networks, Security, Education or
Entertainment etc. There can be many aspects of Live OS, but Linux based Live
OS and their use in the field of networks is the main focus of this paper.
","['\nSajjad Haider\n', '\nMehboob Yasin\n', '\nNaveed Hussain\n', '\nMuhammad Imran\n']","6th CIIT Workshop on Research in Computing (CWRC, 2007)",,http://arxiv.org/abs/1212.6354v1,cs.NI,"['cs.NI', 'cs.OS']",,,[]
Adaptive Scheduling in Real-Time Systems Through Period Adjustment,http://arxiv.org/abs/1212.3502v1,2012-12-14T15:33:45Z,2012-12-14T15:33:45Z,"  Real time system technology traditionally developed for safety critical
systems, has now been extended to support multimedia systems and virtual
reality. A large number of real-time application, related to multimedia and
adaptive control system, require more flexibility than classical real-time
theory usually permits. This paper proposes an efficient adaptive scheduling
framework in real-time systems based on period adjustment. Under this model
periodic task can change their execution rates based on their importance value
to keep the system underloaded. We propose Period_Adjust algorithm, which
consider the tasks whose periods are bounded as well as the tasks whose periods
are not bounded.
",['\nShri Prakash Dwivedi\n'],"8 pages, 5 figures",,http://arxiv.org/abs/1212.3502v1,cs.OS,['cs.OS'],,,[]
"Feasibility Tests for Recurrent Real-Time Tasks in the Sporadic DAG
  Model",http://arxiv.org/abs/1212.2778v1,2012-12-12T11:37:48Z,2012-12-12T11:37:48Z,"  A model has been proposed in [Baruah et al., in Proceedings of the IEEE
Real-Time Systems Symposium 2012] for representing recurrent
precedence-constrained tasks to be executed on multiprocessor platforms, where
each recurrent task is modeled by a directed acyclic graph (DAG), a period, and
a relative deadline. Each vertex of the DAG represents a sequential job, while
the edges of the DAG represent precedence constraints between these jobs. All
the jobs of the DAG are released simultaneously and have to be completed within
some specified relative deadline. The task may release jobs in this manner an
unbounded number of times, with successive releases occurring at least the
specified period apart. The feasibility problem is to determine whether such a
recurrent task can be scheduled to always meet all deadlines on a specified
number of dedicated processors.
  The case of a single task has been considered in [Baruah et al., 2012]. The
main contribution of this paper is to consider the case of multiple tasks. We
show that EDF has a speedup bound of 2-1/m, where m is the number of
processors. Moreover, we present polynomial and pseudopolynomial schedulability
tests, of differing effectiveness, for determining whether a set of sporadic
DAG tasks can be scheduled by EDF to meet all deadlines on a specified number
of processors.
","['\nVincenzo Bonifaci\n', '\nAlberto Marchetti-Spaccamela\n', '\nSebastian Stiller\n', '\nAndreas Wiese\n']",,"Proceedings of the 2013 25th Euromicro Conference on Real-Time
  Systems",http://dx.doi.org/10.1109/ECRTS.2013.32,cs.DS,"['cs.DS', 'cs.OS', 'D.4.1; F.2.2']",10.1109/ECRTS.2013.32,,[]
A Generic Checkpoint-Restart Mechanism for Virtual Machines,http://arxiv.org/abs/1212.1787v1,2012-12-08T12:56:49Z,2012-12-08T12:56:49Z,"  It is common today to deploy complex software inside a virtual machine (VM).
Snapshots provide rapid deployment, migration between hosts, dependability
(fault tolerance), and security (insulating a guest VM from the host). Yet, for
each virtual machine, the code for snapshots is laboriously developed on a
per-VM basis. This work demonstrates a generic checkpoint-restart mechanism for
virtual machines. The mechanism is based on a plugin on top of an unmodified
user-space checkpoint-restart package, DMTCP. Checkpoint-restart is
demonstrated for three virtual machines: Lguest, user-space QEMU, and KVM/QEMU.
The plugins for Lguest and KVM/QEMU require just 200 lines of code. The Lguest
kernel driver API is augmented by 40 lines of code. DMTCP checkpoints
user-space QEMU without any new code. KVM/QEMU, user-space QEMU, and DMTCP need
no modification. The design benefits from other DMTCP features and plugins.
Experiments demonstrate checkpoint and restart in 0.2 seconds using forked
checkpointing, mmap-based fast-restart, and incremental Btrfs-based snapshots.
","['\nRohan Garg\n', '\nKomal Sodha\n', '\nGene Cooperman\n']",,,http://arxiv.org/abs/1212.1787v1,cs.OS,['cs.OS'],,,[]
"Multicore Dynamic Kernel Modules Attachment Technique for Kernel
  Performance Enhancement",http://arxiv.org/abs/1211.4840v1,2012-11-20T19:38:00Z,2012-11-20T19:38:00Z,"  Traditional monolithic kernels dominated kernel structures for long time
along with small sized kernels,few hardware companies and limited kernel
functionalities. Monolithic kernel structure was not applicable when the number
of hardware companies increased and kernel services consumed by different users
for many purposes. One of the biggest disadvantages of the monolithic kernels
is the inflexibility due to the need to include all the available modules in
kernel compilation causing high time consuming. Lately, new kernel structure
was introduced through multicore operating systems. Unfortunately, many
multicore operating systems such as barrelfish and FOS are experimental. This
paper aims to simulate the performance of multicore hybrid kernels through
dynamic kernel module customized attachment/ deattachment for multicore
machines. In addition, this paper proposes a new technique for loading dynamic
kernel modules based on the user needs and machine capabilities.
",['\nMohamed Farag\n'],"13 pages, International Journal of Computer Science & Information
  Technology (IJCSIT) Vol 4, No 4, August 2012",,http://dx.doi.org/10.5121/ijcsit.2012.4405,cs.OS,['cs.OS'],10.5121/ijcsit.2012.4405,,[]
An Insight View of Kernel Visual Debugger in System Boot up,http://arxiv.org/abs/1211.4839v1,2012-11-20T19:33:08Z,2012-11-20T19:33:08Z,"  For many years, developers could not figure out the mystery of OS kernels.
The main source of this mystery is the interaction between operating systems
and hardware while system's boot up and kernel initialization. In addition,
many operating system kernels differ in their behavior toward many situations.
For instance, kernels act differently in racing conditions, kernel
initialization and process scheduling. For such operations, kernel debuggers
were designed to help in tracing kernel behavior and solving many kernel bugs.
The importance of kernel debuggers is not limited to kernel code tracing but
also, they can be used in verification and performance comparisons. However,
developers had to be aware of debugger commands thus introducing some
difficulties to non-expert programmers. Later, several visual kernel debuggers
were presented to make it easier for programmers to trace their kernel code and
analyze kernel behavior. Nowadays, several kernel debuggers exist for solving
this mystery but only very few support line-by-line debugging at run-time. In
this paper, a generic approach for operating system source code debugging in
graphical mode with line-by-line tracing support is proposed. In the context of
this approach, system boot up and evaluation of two operating system schedulers
from several points of views will be discussed.
",['\nMohamed Farag\n'],"10 pages, International Journal","International Journal of Computer Science & Information Technology
  (IJCSIT) Vol 4, No 5, October 2012",http://dx.doi.org/10.5121/ijcsit.2012.4510,cs.OS,"['cs.OS', 'cs.SY']",10.5121/ijcsit.2012.4510,,[]
Automatic Verification of Message-Based Device Drivers,http://arxiv.org/abs/1211.6185v1,2012-11-27T02:36:10Z,2012-11-27T02:36:10Z,"  We develop a practical solution to the problem of automatic verification of
the interface between device drivers and the OS. Our solution relies on a
combination of improved driver architecture and verification tools. It supports
drivers written in C and can be implemented in any existing OS, which sets it
apart from previous proposals for verification-friendly drivers. Our
Linux-based evaluation shows that this methodology amplifies the power of
existing verification tools in detecting driver bugs, making it possible to
verify properties beyond the reach of traditional techniques.
","['\nSidney Amani\nNICTA and UNSW\n', '\nPeter Chubb\nNICTA and UNSW\n', '\nAlastair F. Donaldson\nImperial College London\n', '\nAlexander Legg\nNICTA and UNSW\n', '\nLeonid Ryzhyk\nNICTA and UNSW\n', '\nYanjin Zhu\nNICTA and UNSW\n']","In Proceedings SSV 2012, arXiv:1211.5873","EPTCS 102, 2012, pp. 4-17",http://dx.doi.org/10.4204/EPTCS.102.3,cs.OS,"['cs.OS', 'cs.SE', 'D.4.4; B.4.2; D.2.4']",10.4204/EPTCS.102.3,,"['NICTA and UNSW', 'NICTA and UNSW', 'Imperial College London', 'NICTA and UNSW', 'NICTA and UNSW', 'NICTA and UNSW']"
"Chiefly Symmetric: Results on the Scalability of Probabilistic Model
  Checking for Operating-System Code",http://arxiv.org/abs/1211.6196v1,2012-11-27T02:37:28Z,2012-11-27T02:37:28Z,"  Reliability in terms of functional properties from the safety-liveness
spectrum is an indispensable requirement of low-level operating-system (OS)
code. However, with evermore complex and thus less predictable hardware,
quantitative and probabilistic guarantees become more and more important.
Probabilistic model checking is one technique to automatically obtain these
guarantees. First experiences with the automated quantitative analysis of
low-level operating-system code confirm the expectation that the naive
probabilistic model checking approach rapidly reaches its limits when
increasing the numbers of processes. This paper reports on our work-in-progress
to tackle the state explosion problem for low-level OS-code caused by the
exponential blow-up of the model size when the number of processes grows. We
studied the symmetry reduction approach and carried out our experiments with a
simple test-and-test-and-set lock case study as a representative example for a
wide range of protocols with natural inter-process dependencies and long-run
properties. We quickly see a state-space explosion for scenarios where
inter-process dependencies are insignificant. However, once inter-process
dependencies dominate the picture models with hundred and more processes can be
constructed and analysed.
","['\nChristel Baier\nTU Dresden\n', '\nMarcus Daum\nTU Dresden\n', '\nBenjamin Engel\nTU Dresden\n', '\nHermann Härtig\nTU Dresden\n', '\nJoachim Klein\nTU Dresden\n', '\nSascha Klüppelholz\nTU Dresden\n', '\nSteffen Märcker\nTU Dresden\n', '\nHendrik Tews\nTU Dresden\n', '\nMarcus Völp\nTU Dresden\n']","In Proceedings SSV 2012, arXiv:1211.5873","EPTCS 102, 2012, pp. 156-166",http://dx.doi.org/10.4204/EPTCS.102.14,cs.LO,"['cs.LO', 'cs.OS']",10.4204/EPTCS.102.14,,"['TU Dresden', 'TU Dresden', 'TU Dresden', 'TU Dresden', 'TU Dresden', 'TU Dresden', 'TU Dresden', 'TU Dresden', 'TU Dresden']"
A Formal Model of a Virtual Filesystem Switch,http://arxiv.org/abs/1211.6187v1,2012-11-27T02:36:24Z,2012-11-27T02:36:24Z,"  This work presents a formal model that is part of our effort to construct a
verified file system for Flash memory. To modularize the verification we factor
out generic aspects into a common component that is inspired by the Linux
Virtual Filesystem Switch (VFS) and provides POSIX compatible operations. It
relies on an abstract specification of its internal interface to concrete file
system implementations (AFS). We proved that preconditions of AFS are respected
and that the state is kept consistent. The model can be made executable and
mounted into the Linux directory tree using FUSE.
","['\nGidon Ernst\nUniversity of Augsburg\n', '\nGerhard Schellhorn\nUniversity of Augsburg\n', '\nDominik Haneberg\nUniversity of Augsburg\n', '\nJörg Pfähler\nUniversity of Augsburg\n', '\nWolfgang Reif\nUniversity of Augsburg\n']","In Proceedings SSV 2012, arXiv:1211.5873","EPTCS 102, 2012, pp. 33-45",http://dx.doi.org/10.4204/EPTCS.102.5,cs.LO,"['cs.LO', 'cs.OS', 'cs.SE']",10.4204/EPTCS.102.5,,"['University of Augsburg', 'University of Augsburg', 'University of Augsburg', 'University of Augsburg', 'University of Augsburg']"
"On the Use of Underspecified Data-Type Semantics for Type Safety in
  Low-Level Code",http://arxiv.org/abs/1211.6190v1,2012-11-27T02:36:45Z,2012-11-27T02:36:45Z,"  In recent projects on operating-system verification, C and C++ data types are
often formalized using a semantics that does not fully specify the precise byte
encoding of objects. It is well-known that such an underspecified data-type
semantics can be used to detect certain kinds of type errors. In general,
however, underspecified data-type semantics are unsound: they assign
well-defined meaning to programs that have undefined behavior according to the
C and C++ language standards.
  A precise characterization of the type-correctness properties that can be
enforced with underspecified data-type semantics is still missing. In this
paper, we identify strengths and weaknesses of underspecified data-type
semantics for ensuring type safety of low-level systems code. We prove
sufficient conditions to detect certain classes of type errors and, finally,
identify a trade-off between the complexity of underspecified data-type
semantics and their type-checking capabilities.
","['\nHendrik Tews\nTU Dresden\n', '\nMarcus Völp\nTU Dresden\n', '\nTjark Weber\nUppsala University\n']","In Proceedings SSV 2012, arXiv:1211.5873","EPTCS 102, 2012, pp. 73-87",http://dx.doi.org/10.4204/EPTCS.102.8,cs.LO,"['cs.LO', 'cs.OS', 'cs.PL']",10.4204/EPTCS.102.8,,"['TU Dresden', 'TU Dresden', 'Uppsala University']"
Disk Scheduling: Selection of Algorithm,http://arxiv.org/abs/1210.6447v1,2012-10-24T07:56:48Z,2012-10-24T07:56:48Z,"  The objective of this paper is to take some aspects of disk scheduling and
scheduling algorithms. The disk scheduling is discussed with a sneak peak in
general and selection of algorithm in particular.
","['\nS. Yashvir\n', '\nOm Prakash\n']",9 pages; http://www.ijascse.in/publications-2012--2,,http://arxiv.org/abs/1210.6447v1,cs.OS,['cs.OS'],,,[]
"Online Adaptive Fault Tolerant based Feedback Control Scheduling
  Algorithm for Multiprocessor Embedded Systems",http://arxiv.org/abs/1210.2882v1,2012-10-10T12:13:30Z,2012-10-10T12:13:30Z,"  Since some years ago, use of Feedback Control Scheduling Algorithm (FCSA) in
the control scheduling co-design of multiprocessor embedded system has
increased. FCSA provides Quality of Service (QoS) in terms of overall system
performance and resource allocation in open and unpredictable environment. FCSA
uses quality control feedback loop to keep CPU utilization under desired
unitization bound by avoiding overloading and deadline miss ratio. Integrated
Fault tolerance (FT) based FCSA design methodology guarantees that the Safety
Critical (SC) tasks will meet their deadlines in the presence of faults.
However, current FCSA design model does not provide the optimal solution with
dynamic load fluctuation. This paper presented a novel methodology of designing
an online adaptive fault tolerant based feedback control algorithm for
multiprocessor embedded systems. This procedure is important for control
scheduling co-design for multiprocessor embedded systems.
","['\nOumair Naseer\n', '\nRana Atif Ali Khan\n']",9 pages,,http://arxiv.org/abs/1210.2882v1,cs.SY,"['cs.SY', 'cs.OS']",,,[]
"JooFlux: Hijacking Java 7 InvokeDynamic To Support Live Code
  Modifications",http://arxiv.org/abs/1210.1039v1,2012-10-03T09:15:19Z,2012-10-03T09:15:19Z,"  Changing functional and non-functional software implementation at runtime is
useful and even sometimes critical both in development and production
environments. JooFlux is a JVM agent that allows both the dynamic replacement
of method implementations and the application of aspect advices. It works by
doing bytecode transformation to take advantage of the new invokedynamic
instruction added in Java SE 7 to help implementing dynamic languages for the
JVM. JooFlux can be managed using a JMX agent so as to operate dynamic
modifications at runtime, without resorting to a dedicated domain-specific
language. We compared JooFlux with existing AOP platforms and dynamic
languages. Results demonstrate that JooFlux performances are close to the Java
ones --- with most of the time a marginal overhead, and sometimes a gain ---
where AOP platforms and dynamic languages present significant overheads. This
paves the way for interesting future evolutions and applications of JooFlux.
","['\nJulien Ponge\nCITI\n', '\nFrédéric Le Mouël\nCITI\n']",,,http://arxiv.org/abs/1210.1039v1,cs.OS,"['cs.OS', 'cs.PL']",,,"['CITI', 'CITI']"
Classification Of Heterogeneous Operating System,http://arxiv.org/abs/1209.4600v1,2012-09-19T03:59:12Z,2012-09-19T03:59:12Z,"  Operating system is a bridge between system and user. An operating system
(OS) is a software program that manages the hardware and software resources of
a computer. The OS performs basic tasks, such as controlling and allocating
memory, prioritizing the processing of instructions, controlling input and
output devices, facilitating networking, and managing files. It is difficult to
present a complete as well as deep account of operating systems developed till
date. So, this paper tries to overview only a subset of the available operating
systems and its different categories. OS are being developed by a large number
of academic and commercial organizations for the last several decades. This
paper, therefore, concentrates on the different categories of OS with special
emphasis to those that had deep impact on the evolution process. The aim of
this paper is to provide a brief timely commentary on the different categories
important operating systems available today.
","['\nKamlesh Sharma\n', '\nT. V. Prasad\n']",,,http://arxiv.org/abs/1209.4600v1,cs.OS,['cs.OS'],,,[]
