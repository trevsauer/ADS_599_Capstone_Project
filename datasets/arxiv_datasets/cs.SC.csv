Title,ID,Published,Updated,Summary,Author,Comments,Journal_Ref,Link,Primary_Category,Categories,DOI,License,Affiliation
Creative Telescoping for Hypergeometric Double Sums,http://arxiv.org/abs/2401.16314v1,2024-01-29T17:18:47Z,2024-01-29T17:18:47Z,"  We present efficient methods for calculating linear recurrences of
hypergeometric double sums and, more generally, of multiple sums. In
particular, we supplement this approach with the algorithmic theory of
contiguous relations, which guarantees the applicability of our method for many
input sums. In addition, we elaborate new techniques to optimize the underlying
key task of our method to compute rational solutions of parameterized linear
recurrences.
","['\nPeter Paule\n', '\nCarsten Schneider\n']",,,http://arxiv.org/abs/2401.16314v1,cs.SC,['cs.SC'],,,[]
3D Space Trajectories and beyond: Abstract Art Creation with 3D Printing,http://arxiv.org/abs/2401.11909v1,2024-01-22T12:52:38Z,2024-01-22T12:52:38Z,"  We present simple models of trajectories in space, both in 2D and in 3D. The
first examples, which model bicircular moves in the same direction, are
classical curves (epicycloids, etc.). Then, we explore bicircular moves in
reverse direction and tricircular moves in 2D and 3D, to explore complex
visualisations of extraplanetary movements. These moves are studied in a plane
setting. Then, adding increasing complexity, we explore them in a non planar
setting (which is a closer model of the real situation). The exploration is
followed by using these approaches for creating mathematical art in 2D and 3D
printed objects, providing new ways of mathematical representations. Students'
activities are organized around this exploration.
","['\nThierry Dana-Picard\nJerusalem College of Technology\n', '\nMatias Tejera\nJohannes Kepler University Linz, Austria\n', '\nEva Ulbrich\nJohannes Kepler University Linz, Austria\n']","In Proceedings ADG 2023, arXiv:2401.10725","EPTCS 398, 2024, pp. 142-152",http://dx.doi.org/10.4204/EPTCS.398.17,cs.CG,"['cs.CG', 'cs.SC']",10.4204/EPTCS.398.17,,"['Jerusalem College of Technology', 'Johannes Kepler University Linz, Austria', 'Johannes Kepler University Linz, Austria']"
Computation of classical and $v$-adic $L$-series of $t$-motives,http://arxiv.org/abs/2401.12618v1,2024-01-23T10:16:47Z,2024-01-23T10:16:47Z,"  We design an algorithm for computing the $L$-series associated to an Anderson
$t$-motives, exhibiting quasilinear complexity with respect to the target
precision. Based on experiments, we conjecture that the order of vanishing at
$T=1$ of the $v$-adic $L$-series of a given Anderson $t$-motive with good
reduction does not depend on the finite place $v$.
","['\nXavier Caruso\nIMB, CANARI\n', '\nQuentin Gazda\nCMLS\n']",,,http://arxiv.org/abs/2401.12618v1,cs.SC,"['cs.SC', 'math.NT']",,,"['IMB, CANARI', 'CMLS']"
"Local Hamiltonian decomposition and classical simulation of parametrized
  quantum circuits",http://arxiv.org/abs/2401.13156v2,2024-01-24T00:30:31Z,2024-01-31T18:20:04Z,"  In this paper we develop a classical algorithm of complexity $O(K \, 2^n)$ to
simulate parametrized quantum circuits (PQCs) of $n$ qubits, where $K$ is the
total number of one-qubit and two-qubit control gates. The algorithm is
developed by finding $2$-sparse unitary matrices of order $2^n$ explicitly
corresponding to any single-qubit and two-qubit control gates in an $n$-qubit
system. Finally, we determine analytical expression of Hamiltonians for any
such gate and consequently a local Hamiltonian decomposition of any PQC is
obtained. All results are validated with numerical simulations.
","['\nBibhas Adhikari\n', '\nAryan Jha\n']",,,http://arxiv.org/abs/2401.13156v2,quant-ph,"['quant-ph', 'cs.SC']",,,[]
"Lessons on Datasets and Paradigms in Machine Learning for Symbolic
  Computation: A Case Study on CAD",http://arxiv.org/abs/2401.13343v1,2024-01-24T10:12:43Z,2024-01-24T10:12:43Z,"  Symbolic Computation algorithms and their implementation in computer algebra
systems often contain choices which do not affect the correctness of the output
but can significantly impact the resources required: such choices can benefit
from having them made separately for each problem via a machine learning model.
This study reports lessons on such use of machine learning in symbolic
computation, in particular on the importance of analysing datasets prior to
machine learning and on the different machine learning paradigms that may be
utilised. We present results for a particular case study, the selection of
variable ordering for cylindrical algebraic decomposition, but expect that the
lessons learned are applicable to other decisions in symbolic computation.
  We utilise an existing dataset of examples derived from applications which
was found to be imbalanced with respect to the variable ordering decision. We
introduce an augmentation technique for polynomial systems problems that allows
us to balance and further augment the dataset, improving the machine learning
results by 28\% and 38\% on average, respectively. We then demonstrate how the
existing machine learning methodology used for the problem $-$ classification
$-$ might be recast into the regression paradigm. While this does not have a
radical change on the performance, it does widen the scope in which the
methodology can be applied to make choices.
","['\nTereso del Río\n', '\nMatthew England\n']",,,http://arxiv.org/abs/2401.13343v1,cs.SC,"['cs.SC', 'cs.LG', '68W30, 68T05, 03C10', 'I.2.6; I.1.0']",,,[]
Symbolic Equation Solving via Reinforcement Learning,http://arxiv.org/abs/2401.13447v1,2024-01-24T13:42:24Z,2024-01-24T13:42:24Z,"  Machine-learning methods are gradually being adopted in a great variety of
social, economic, and scientific contexts, yet they are notorious for
struggling with exact mathematics. A typical example is computer algebra, which
includes tasks like simplifying mathematical terms, calculating formal
derivatives, or finding exact solutions of algebraic equations. Traditional
software packages for these purposes are commonly based on a huge database of
rules for how a specific operation (e.g., differentiation) transforms a certain
term (e.g., sine function) into another one (e.g., cosine function). Thus far,
these rules have usually needed to be discovered and subsequently programmed by
humans. Focusing on the paradigmatic example of solving linear equations in
symbolic form, we demonstrate how the process of finding elementary
transformation rules and step-by-step solutions can be automated using
reinforcement learning with deep neural networks.
","['\nLennart Dabelow\n', '\nMasahito Ueda\n']","12 pages, 4 figures + appendices 17 pages, 1 figure, 16 tables",,http://arxiv.org/abs/2401.13447v1,cs.LG,"['cs.LG', 'cs.SC']",,,[]
Towards Automatic Transformations of Coq Proof Scripts,http://arxiv.org/abs/2401.11897v1,2024-01-22T12:48:34Z,2024-01-22T12:48:34Z,"  Proof assistants like Coq are increasingly popular to help mathematicians
carry out proofs of the results they conjecture. However, formal proofs remain
highly technical and are especially difficult to reuse. In this paper, we
present a framework to carry out a posteriori script transformations. These
transformations are meant to be applied as an automated post-processing step,
once the proof has been completed. As an example, we present a transformation
which takes an arbitrary large proof script and produces an equivalent
single-line proof script, which can be executed by Coq in one single step.
Other applications, such as fully expanding a proof script (for debugging
purposes), removing all named hypotheses, etc. could be developed within this
framework. We apply our tool to various Coq proof scripts, including some from
the GeoCoq library.
","['\nNicolas Magaud\nLab. ICube CNRS Université de Strasbourg, France\n']","In Proceedings ADG 2023, arXiv:2401.10725","EPTCS 398, 2024, pp. 4-10",http://dx.doi.org/10.4204/EPTCS.398.4,cs.LO,"['cs.LO', 'cs.SC', 'cs.SE']",10.4204/EPTCS.398.4,,"['Lab. ICube CNRS Université de Strasbourg, France']"
"Showing Proofs, Assessing Difficulty with GeoGebra Discovery",http://arxiv.org/abs/2401.11900v1,2024-01-22T12:50:12Z,2024-01-22T12:50:12Z,"  In our contribution we describe some on-going improvements concerning the
Automated Reasoning Tools developed in GeoGebra Discovery, providing different
examples of the performance of these new features. We describe the new
ShowProof command, that outputs both the sequence of the different steps
performed by GeoGebra Discovery to confirm a certain statement, as well as a
number intending to grade the difficulty or interest of the assertion. The
proposal of this assessment measure, involving the comparison of the expression
of the thesis (or conclusion) as a combination of the hypotheses, will be
developed.
","['\nZoltán Kovács\nThe Private University College of Education of the Diocese of Linz, Austria\n', '\nTomás Recio\nEscuela Politécnica Superior, Universidad Antonio de Nebrija, Madrid, Spain\n', '\nM. Pilar Vélez\nEscuela Politécnica Superior, Universidad Antonio de Nebrija, Madrid, Spain\n']","In Proceedings ADG 2023, arXiv:2401.10725","EPTCS 398, 2024, pp. 43-52",http://dx.doi.org/10.4204/EPTCS.398.8,cs.SC,"['cs.SC', 'cs.AI', 'cs.CG']",10.4204/EPTCS.398.8,,"['The Private University College of Education of the Diocese of Linz, Austria', 'Escuela Politécnica Superior, Universidad Antonio de Nebrija, Madrid, Spain', 'Escuela Politécnica Superior, Universidad Antonio de Nebrija, Madrid, Spain']"
"Solving with GeoGebra Discovery an Austrian Mathematics Olympiad
  problem: Lessons Learned",http://arxiv.org/abs/2401.11906v1,2024-01-22T12:51:35Z,2024-01-22T12:51:35Z,"  We address, through the automated reasoning tools in GeoGebra Discovery, a
problem from a regional phase of the Austrian Mathematics Olympiad 2023. Trying
to solve this problem gives rise to four different kind of feedback: the almost
instantaneous, automated solution of the proposed problem; the measure of its
complexity, according to some recent proposals; the automated discovery of a
generalization of the given assertion, showing that the same statement is true
over more general polygons than those mentioned in the problem; and the
difficulties associated to the analysis of the surprising and involved high
number of degenerate cases that appear when using the LocusEquation command in
this problem. In our communication we will describe and reflect on these
diverse issues, enhancing its exemplar role for showing some of the advantages,
problems, and current fields of development of GeoGebra Discovery.
","['\nBelén Ariño-Morera\nDepartamento de Economía Financiera y Contabilidad, Universidad Rey Juan Carlos, Madrid, Spain\n', '\nZoltán Kovács\nThe Private University College of Education of the Diocese of Linz, Austria\n', '\nTomás Recio\nEscuela Politécnica Superior, Universidad Antonio de Nebrija, Madrid, Spain\n', '\nPiedad Tolmos\nDepartamento de Economía Financiera y Contabilidad, Universidad Rey Juan Carlos, Madrid, Spain\n']","In Proceedings ADG 2023, arXiv:2401.10725","EPTCS 398, 2024, pp. 101-109",http://dx.doi.org/10.4204/EPTCS.398.13,cs.SC,"['cs.SC', 'cs.AI', 'cs.CG']",10.4204/EPTCS.398.13,,"['Departamento de Economía Financiera y Contabilidad, Universidad Rey Juan Carlos, Madrid, Spain', 'The Private University College of Education of the Diocese of Linz, Austria', 'Escuela Politécnica Superior, Universidad Antonio de Nebrija, Madrid, Spain', 'Departamento de Economía Financiera y Contabilidad, Universidad Rey Juan Carlos, Madrid, Spain']"
"The Locus Story of a Rocking Camel in a Medical Center in the City of
  Freistadt",http://arxiv.org/abs/2401.11908v1,2024-01-22T12:52:22Z,2024-01-22T12:52:22Z,"  We give an example of automated geometry reasoning for an imaginary classroom
project by using the free software package GeoGebra Discovery. The project is
motivated by a publicly available toy, a rocking camel, installed at a medical
center in Upper Austria. We explain how the process of a false conjecture,
experimenting, modeling, a precise mathematical setup, and then a proof by
automated reasoning could help extend mathematical knowledge at secondary
school level and above.
","['\nAnna Käferböck\nThe Private University College of Education of the Diocese of Linz, Austria\n', '\nZoltán Kovács\nThe Private University College of Education of the Diocese of Linz, Austria\n']","In Proceedings ADG 2023, arXiv:2401.10725","EPTCS 398, 2024, pp. 132-141",http://dx.doi.org/10.4204/EPTCS.398.16,cs.RO,"['cs.RO', 'cs.CG', 'cs.SC']",10.4204/EPTCS.398.16,,"['The Private University College of Education of the Diocese of Linz, Austria', 'The Private University College of Education of the Diocese of Linz, Austria']"
Open Source Prover in the Attic,http://arxiv.org/abs/2401.13702v1,2024-01-22T12:50:29Z,2024-01-22T12:50:29Z,"  The well known JGEX program became open source a few years ago, but
seemingly, further development of the program can only be done without the
original authors. In our project, we are looking at whether it is possible to
continue such a large project as a newcomer without the involvement of the
original authors. Is there a way to internationalize, fix bugs, improve the
code base, add new features? In other words, to save a relic found in the attic
and polish it into a useful everyday tool.
","['\nZoltán Kovács\nThe Private University College of Education of the Diocese of Linz, Austria\n', '\nAlexander Vujic\nThe Private University College of Education of the Diocese of Linz, Austria\n']","In Proceedings ADG 2023, arXiv:2401.10725","EPTCS 398, 2024, pp. 53-61",http://dx.doi.org/10.4204/EPTCS.398.9,cs.PL,"['cs.PL', 'cs.MS', 'cs.SC', 'cs.SE']",10.4204/EPTCS.398.9,,"['The Private University College of Education of the Diocese of Linz, Austria', 'The Private University College of Education of the Diocese of Linz, Austria']"
"Solving Some Geometry Problems of the Náboj 2023 Contest with
  Automated Deduction in GeoGebra Discovery",http://arxiv.org/abs/2401.13703v1,2024-01-22T12:51:51Z,2024-01-22T12:51:51Z,"  In this article, we solve some of the geometry problems of the N\'aboj 2023
competition with the help of a computer, using examples that the software tool
GeoGebra Discovery can calculate. In each case, the calculation requires
symbolic computations. We analyze the difficulty of feeding the problem into
the machine and set further goals to make the problems of this type of contests
even more tractable in the future.
","['\nAmela Hota\nThe Private University College of Education of the Diocese of Linz, Austria\n', '\nZoltán Kovács\nThe Private University College of Education of the Diocese of Linz, Austria\n', '\nAlexander Vujic\nThe Private University College of Education of the Diocese of Linz, Austria\n']","In Proceedings ADG 2023, arXiv:2401.10725","EPTCS 398, 2024, pp. 110-123",http://dx.doi.org/10.4204/EPTCS.398.14,math.HO,"['math.HO', 'cs.AI', 'cs.CG', 'cs.SC']",10.4204/EPTCS.398.14,,"['The Private University College of Education of the Diocese of Linz, Austria', 'The Private University College of Education of the Diocese of Linz, Austria', 'The Private University College of Education of the Diocese of Linz, Austria']"
"Using Java Geometry Expert as Guide in the Preparations for Math
  Contests",http://arxiv.org/abs/2401.13704v1,2024-01-22T12:52:07Z,2024-01-22T12:52:07Z,"  We give an insight into Java Geometry Expert (JGEX) in use in a school
context, focusing on the Austrian school system. JGEX can offer great support
in some classroom situations, especially for solving mathematical competition
tasks. Also, we discuss some limitations of the program.
","['\nInes Ganglmayr\nThe Private University College of Education of the Diocese of Linz, Austria\n', '\nZoltán Kovács\nThe Private University College of Education of the Diocese of Linz, Austria\n']","In Proceedings ADG 2023, arXiv:2401.10725","EPTCS 398, 2024, pp. 124-131",http://dx.doi.org/10.4204/EPTCS.398.15,cs.CY,"['cs.CY', 'cs.AI', 'cs.CG', 'cs.SC']",10.4204/EPTCS.398.15,,"['The Private University College of Education of the Diocese of Linz, Austria', 'The Private University College of Education of the Diocese of Linz, Austria']"
"On the Algorithmic Verification of Nonlinear Superposition for Systems
  of First Order Ordinary Differential Equations",http://arxiv.org/abs/2401.17012v1,2024-01-30T13:47:46Z,2024-01-30T13:47:46Z,"  This paper belongs to a group of work in the intersection of symbolic
computation and group analysis aiming for the symbolic analysis of differential
equations. The goal is to extract important properties without finding the
explicit general solution. In this contribution, we introduce the algorithmic
verification of nonlinear superposition properties and its implementation. More
exactly, for a system of nonlinear ordinary differential equations of first
order with a polynomial right-hand side, we check if the differential system
admits a general solution by means of a superposition rule and a certain number
of particular solutions. It is based on the theory of Newton polytopes and
associated symbolic computation. The developed method provides the basis for
the identification of nonlinear superpositions within a given system and for
the construction of numerical methods which preserve important algebraic
properties at the numerical level.
","['\nVeronika Treumova\n', '\nDmitry A. Lyakhov\n', '\nDominik L. Michels\n']",,,http://arxiv.org/abs/2401.17012v1,cs.SC,"['cs.SC', 'cs.NA', 'math.NA', 'math.RA']",,,[]
Validated numerics for algebraic path tracking,http://arxiv.org/abs/2401.17973v1,2024-01-31T16:28:17Z,2024-01-31T16:28:17Z,"  Using validated numerical methods, interval arithmetic and Taylor models, we
propose a certified predictor-corrector loop for tracking zeros of polynomial
systems with a parameter. We provide a Rust implementation which shows
tremendous improvement over existing software for certified path tracking.
","['\nAlexandre Guillemot\n', '\nPierre Lairez\n']",,,http://arxiv.org/abs/2401.17973v1,math.NA,"['math.NA', 'cs.NA', 'cs.SC']",,,[]
"Symbolic-numeric algorithm for parameter estimation in discrete-time
  models with $\exp$",http://arxiv.org/abs/2401.16220v1,2024-01-29T15:19:16Z,2024-01-29T15:19:16Z,"  Determining unknown parameter values in dynamic models is crucial for
accurate analysis of the dynamics across the different scientific disciplines.
Discrete-time dynamic models are widely used to model biological processes, but
it is often difficult to determine these parameters. In this paper, we propose
a robust symbolic-numeric approach for parameter estimation in discrete-time
models that involve non-algebraic functions such as exp. We illustrate the
performance (precision) of our approach by applying our approach to the flour
beetle (LPA) model, an archetypal discrete-time model in biology. Unlike
optimization-based methods, our algorithm guarantees to find all solutions of
the parameter values given time-series data for the measured variables.
","['\nYosef Berman\n', '\nJoshua Forrest\n', '\nMatthew Grote\n', '\nAlexey Ovchinnikov\n', '\nSonia Rueda\n']",,,http://arxiv.org/abs/2401.16220v1,q-bio.QM,"['q-bio.QM', 'cs.SC', 'cs.SY', 'eess.SY', 'math.AC', 'math.DS', '92B05, 68W30, 14Q20, 39A60, 13P15']",,,[]
Submodule approach to creative telescoping,http://arxiv.org/abs/2401.08455v1,2024-01-16T15:59:40Z,2024-01-16T15:59:40Z,"  This paper proposes ideas to speed up the process of creative telescoping,
particularly when the telescoper is reducible. One can interpret telescoping as
computing an annihilator $L \in D$ for an element $m$ in a $D$-module $M$. The
main idea is to look for submodules of $M$. If $N$ is a non-trivial submodule
of $M$, constructing the minimal operator $R$ of the image of $m$ in $M/N$
gives a right-factor of $L$ in $D$. Then $L = L' R$ where the left-factor $L'$
is the telescoper of $R(m) \in N$. To expedite computing $L'$, compute the
action of $D$ on a natural basis of $N$, then obtain $L'$ with a cyclic vector
computation.
  The next main idea is that when $N$ has automorphisms, use them to construct
submodules. An automorphism with distinct eigenvalues can be used to decompose
$N$ as a direct sum $N_1 \oplus \cdots \oplus N_k$. Then $L'$ is the LCLM
(Least Common Left Multiple) of $L_1, \ldots, L_k$ where $L_i$ is the
telescoper of the projection of $R(m)$ on $N_i$. An LCLM can greatly increase
the degrees of coefficients, so $L'$ and $L$ can be much larger expressions
than the factors $L_1,\ldots,L_k$ and $R$. Examples show that computing each
factor $L_i$ and $R$ seperately can save a lot of CPU time compared to
computing $L$ in expanded form with standard creative telescoping.
",['\nMark van Hoeij\n'],10 pages,,http://arxiv.org/abs/2401.08455v1,cs.SC,"['cs.SC', '39A04', 'G.2.1']",,,[]
Hypergeometric Solutions of Linear Difference Systems,http://arxiv.org/abs/2401.08470v1,2024-01-16T16:19:09Z,2024-01-16T16:19:09Z,"  We extend Petkov\v{s}ek's algorithm for computing hypergeometric solutions of
scalar difference equations to the case of difference systems $\tau(Y) = M Y$,
with $M \in {\rm GL}_n(C(x))$, where $\tau$ is the shift operator.
Hypergeometric solutions are solutions of the form $\gamma P$ where $P \in
C(x)^n$ and $\gamma$ is a hypergeometric term over $C(x)$, i.e.
${\tau(\gamma)}/{\gamma} \in C(x)$. Our contributions concern efficient
computation of a set of candidates for ${\tau(\gamma)}/{\gamma}$ which we write
as $\lambda = c\frac{A}{B}$ with monic $A, B \in C[x]$, $c \in C^*$. Factors of
the denominators of $M^{-1}$ and $M$ give candidates for $A$ and $B$, while
another algorithm is needed for $c$. We use the super-reduction algorithm to
compute candidates for $c$, as well as other ingredients to reduce the list of
candidates for $A/B$. To further reduce the number of candidates $A/B$, we
bound the so-called type of $A/B$ by bounding local types. Our algorithm has
been implemented in Maple and experiments show that our implementation can
handle systems of high dimension, which is useful for factoring operators.
","['\nMoulay Barkatou\n', '\nMark van Hoeij\n', '\nJohannes Middeke\n', '\nYi Zhou\n']",24 pages,,http://arxiv.org/abs/2401.08470v1,cs.SC,"['cs.SC', '39A04', 'G.2.1']",,,[]
Interpretable Concept Bottlenecks to Align Reinforcement Learning Agents,http://arxiv.org/abs/2401.05821v2,2024-01-11T10:38:22Z,2024-02-01T13:36:04Z,"  Goal misalignment, reward sparsity and difficult credit assignment are only a
few of the many issues that make it difficult for deep reinforcement learning
(RL) agents to learn optimal policies. Unfortunately, the black-box nature of
deep neural networks impedes the inclusion of domain experts for inspecting the
model and revising suboptimal policies. To this end, we introduce *Successive
Concept Bottleneck Agents* (SCoBots), that integrate consecutive concept
bottleneck (CB) layers. In contrast to current CB models, SCoBots do not just
represent concepts as properties of individual objects, but also as relations
between objects which is crucial for many RL tasks. Our experimental results
provide evidence of SCoBots' competitive performances, but also of their
potential for domain experts to understand and regularize their behavior. Among
other things, SCoBots enabled us to identify a previously unknown misalignment
problem in the iconic video game, Pong, and resolve it. Overall, SCoBots thus
result in more human-aligned RL agents. Our code is available at
https://github.com/k4ntz/SCoBots .
","['\nQuentin Delfosse\n', '\nSebastian Sztwiertnia\n', '\nMark Rothermel\n', '\nWolfgang Stammer\n', '\nKristian Kersting\n']","20 pages, 8 of main text, 8 of appendix, 3 main figures",,http://arxiv.org/abs/2401.05821v2,cs.LG,"['cs.LG', 'cs.SC']",,,[]
Universal Analytic Gr{ö}bner Bases and Tropical Geometry,http://arxiv.org/abs/2401.05759v1,2024-01-11T09:08:34Z,2024-01-11T09:08:34Z,"  A universal analytic Gr{\""o}bner basis (UAGB) of an ideal of a Tate algebra
is a set containing a local Gr{\""o}bner basis for all suitable convergence
radii. In a previous article, the authors proved the existence of finite UAGB's
for polynomial ideals, leaving open the question of how to compute them. In
this paper, we provide an algorithm computing a UAGB for a given polynomial
ideal, by traversing the Gr{\""o}bner fan of the ideal. As an application, it
offers a new point of view on algorithms for computing tropical varieties of
homogeneous polynomial ideals, which typically rely on lifting the computations
to an algebra of power series. Motivated by effective computations in tropical
analytic geometry, we also examine local bases for more general convergence
conditions, constraining the radii to a convex polyhedron. In this setting, we
provide an algorithm to compute local Gr{\""o}bner bases and discuss obstacles
towards proving the existence of finite UAGBs. CCS CONCEPTS $\bullet$ Computing
methodologies $\rightarrow$ Algebraic algorithms.
","['\nTristan Vaccon\nUNILIM\n', '\nThibaut Verron\nJKU\n']",,"ISSAC 2023: International Symposium on Symbolic and Algebraic
  Computation 2023, Jul 2023, Troms{{\o}}, Norway. pp.517-525",http://dx.doi.org/10.1145/3597066.3597110,cs.SC,"['cs.SC', 'math.AG', 'math.NT']",10.1145/3597066.3597110,,"['UNILIM', 'JKU']"
"On Hilbert-Poincaré series of affine semi-regular polynomial
  sequences and related Gröbner bases",http://arxiv.org/abs/2401.07768v2,2024-01-15T15:26:52Z,2024-03-03T16:31:16Z,"  Gr\""{o}bner bases are nowadays central tools for solving various problems in
commutative algebra and algebraic geometry. A typical use of Gr\""{o}bner bases
is the multivariate polynomial system solving, which enables us to construct
algebraic attacks against post-quantum cryptographic protocols. Therefore, the
determination of the complexity of computing Gr\""{o}bner bases is very
important both in theory and in practice: One of the most important cases is
the case where input polynomials compose an (overdetermined) affine
semi-regular sequence. The first part of this paper aims to present a survey on
Gr\""{o}bner basis computation and its complexity. In the second part, we shall
give an explicit formula on the (truncated) Hilbert-Poincar\'{e} series
associated to the homogenization of an affine semi-regular sequence. Based on
the formula, we also study (reduced) Gr\""{o}bner bases of the ideals generated
by an affine semi-regular sequence and its homogenization. Some of our results
are considered to give mathematically rigorous proofs of the correctness of
methods for computing Gr\""{o}bner bases of the ideal generated by an affine
semi-regular sequence.
","['\nMomonari Kudo\n', '\nKazuhiro Yokoyama\n']","25 pages, Comments are welcome!","Mathematical Foundations for Post-Quantum Cryptography (T. Takagi
  et al. eds), Mathematics for Industry, Springer, 2024",http://arxiv.org/abs/2401.07768v2,cs.SC,"['cs.SC', 'math.AC', 'math.AG']",,,[]
"Bootstrapping OTS-Funcimg Pre-training Model (Botfip) -- A Comprehensive
  Symbolic Regression Framework",http://arxiv.org/abs/2401.09748v1,2024-01-18T06:19:05Z,2024-01-18T06:19:05Z,"  In the field of scientific computing, many problem-solving approaches tend to
focus only on the process and final outcome, even in AI for science, there is a
lack of deep multimodal information mining behind the data, missing a
multimodal framework akin to that in the image-text domain. In this paper, we
take Symbolic Regression(SR) as our focal point and, drawing inspiration from
the BLIP model in the image-text domain, propose a scientific computing
multimodal framework based on Function Images (Funcimg) and Operation Tree
Sequence (OTS), named Bootstrapping OTS-Funcimg Pre-training Model (Botfip). In
SR experiments, we validate the advantages of Botfip in low-complexity SR
problems, showcasing its potential. As a MED framework, Botfip holds promise
for future applications in a broader range of scientific computing problems.
","['\nTianhao Chen\n', '\nPengbo Xu\n', '\nHaibiao Zheng\n']",,,http://arxiv.org/abs/2401.09748v1,cs.SC,"['cs.SC', 'cs.AI', 'cs.LG']",,,[]
EFO: the Emotion Frame Ontology,http://arxiv.org/abs/2401.10751v1,2024-01-19T15:20:57Z,2024-01-19T15:20:57Z,"  Emotions are a subject of intense debate in various disciplines. Despite the
proliferation of theories and definitions, there is still no consensus on what
emotions are, and how to model the different concepts involved when we talk
about - or categorize - them. In this paper, we propose an OWL frame-based
ontology of emotions: the Emotion Frames Ontology (EFO). EFO treats emotions as
semantic frames, with a set of semantic roles that capture the different
aspects of emotional experience. EFO follows pattern-based ontology design, and
is aligned to the DOLCE foundational ontology. EFO is used to model multiple
emotion theories, which can be cross-linked as modules in an Emotion Ontology
Network. In this paper, we exemplify it by modeling Ekman's Basic Emotions (BE)
Theory as an EFO-BE module, and demonstrate how to perform automated inferences
on the representation of emotion situations. EFO-BE has been evaluated by
lexicalizing the BE emotion frames from within the Framester knowledge graph,
and implementing a graph-based emotion detector from text. In addition, an EFO
integration of multimodal datasets, including emotional speech and emotional
face expressions, has been performed to enable further inquiry into crossmodal
emotion semantics.
","['\nStefano De Giorgis\n', '\nAldo Gangemi\n']",,,http://arxiv.org/abs/2401.10751v1,cs.AI,"['cs.AI', 'cs.CY', 'cs.SC']",,,[]
"Constraint-Generation Policy Optimization (CGPO): Nonlinear Programming
  for Policy Optimization in Mixed Discrete-Continuous MDPs",http://arxiv.org/abs/2401.12243v1,2024-01-20T07:12:57Z,2024-01-20T07:12:57Z,"  We propose Constraint-Generation Policy Optimization (CGPO) for optimizing
policy parameters within compact and interpretable policy classes for mixed
discrete-continuous Markov Decision Processes (DC-MDPs). CGPO is not only able
to provide bounded policy error guarantees over an infinite range of initial
states for many DC-MDPs with expressive nonlinear dynamics, but it can also
provably derive optimal policies in cases where it terminates with zero error.
Furthermore, CGPO can generate worst-case state trajectories to diagnose policy
deficiencies and provide counterfactual explanations of optimal actions. To
achieve such results, CGPO proposes a bi-level mixed-integer nonlinear
optimization framework for optimizing policies within defined expressivity
classes (i.e. piecewise (non)-linear) and reduces it to an optimal constraint
generation methodology that adversarially generates worst-case state
trajectories. Furthermore, leveraging modern nonlinear optimizers, CGPO can
obtain solutions with bounded optimality gap guarantees. We handle stochastic
transitions through explicit marginalization (where applicable) or
chance-constraints, providing high-probability policy performance guarantees.
We also present a road-map for understanding the computational complexities
associated with different expressivity classes of policy, reward, and
transition dynamics. We experimentally demonstrate the applicability of CGPO in
diverse domains, including inventory control, management of a system of water
reservoirs, and physics control. In summary, we provide a solution for deriving
structured, compact, and explainable policies with bounded performance
guarantees, enabling worst-case scenario generation and counterfactual policy
diagnostics.
","['\nMichael Gimelfarb\n', '\nAyal Taitler\n', '\nScott Sanner\n']",,,http://arxiv.org/abs/2401.12243v1,math.OC,"['math.OC', 'cs.LG', 'cs.RO', 'cs.SC', 'cs.SY', 'eess.SY']",,,[]
"Computing greatest common divisor of several parametric univariate
  polynomials via generalized subresultant polynomials",http://arxiv.org/abs/2401.00408v1,2023-12-31T06:32:54Z,2023-12-31T06:32:54Z,"  In this paper, we tackle the following problem: compute the gcd for several
univariate polynomials with parametric coefficients. It amounts to partitioning
the parameter space into ``cells'' so that the gcd has a uniform expression
over each cell and constructing a uniform expression of gcd in each cell. We
tackle the problem as follows. We begin by making a natural and obvious
extension of subresultant polynomials of two polynomials to several
polynomials. Then we develop the following structural theories about them.
  1. We generalize Sylvester's theory to several polynomials, in order to
obtain an elegant relationship between generalized subresultant polynomials and
the gcd of several polynomials, yielding an elegant algorithm.
  2. We generalize Habicht's theory to several polynomials, in order to obtain
a systematic relationship between generalized subresultant polynomials and
pseudo-remainders, yielding an efficient algorithm.
  Using the generalized theories, we present a simple (structurally elegant)
algorithm which is significantly more efficient (both in the output size and
computing time) than algorithms based on previous approaches.
","['\nHoon Hong\n', '\nJing Yang\n']",,,http://arxiv.org/abs/2401.00408v1,cs.SC,['cs.SC'],,,[]
Persistent components in Canny's resultant,http://arxiv.org/abs/2401.01948v1,2024-01-03T19:17:55Z,2024-01-03T19:17:55Z,"  When using resultants for elimination, one standard issue is that the
resultant vanishes if the variety contains components of dimension larger than
the expected dimension. J. Canny proposed an elegant construction, generalized
characteristic polynomial, to address this issue by symbolically perturbing the
system before the resultant computation. Such perturbed resultant would
typically involve artefact components only loosely related to the geometry of
the variety of interest. For removing these components, J.M. Rojas proposed to
take the greatest common divisor of the results of two different perturbations.
In this paper, we investigate this construction, and show that the extra
components persistent under taking different perturbations must come either
from singularities or from positive-dimensional fibers.
",['\nGleb Pogudin\n'],,,http://arxiv.org/abs/2401.01948v1,cs.SC,"['cs.SC', 'math.AG']",,,[]
Algorithm for globally identifiable reparametrizions of ODEs,http://arxiv.org/abs/2401.00762v1,2024-01-01T14:04:32Z,2024-01-01T14:04:32Z,"  Structural global parameter identifiability indicates whether one can
determine a parameter's value in an ODE model from given inputs and outputs. If
a given model has parameters for which there is exactly one value, such
parameters are called identifiable. We present a procedure for replacing, if
possible, a given ODE model involving not identifiable parameters by an
equivalent one such that the new set of parameters is identifiable. We first
derive this as an algorithm for one-dimensional ODE models and then reuse this
approach for higher-dimensional models.
","['\nSebastian Falkensteiner\n', '\nAlexey Ovchinnikov\n', '\nJ. Rafael Sendra\n']",,,http://arxiv.org/abs/2401.00762v1,eess.SY,"['eess.SY', 'cs.SC', 'cs.SY', 'math.AP', '93C15, 93B25, 93B30, 34A55, 14E08, 14M20, 14Q20, 12H05, 92B05']",,,[]
Finite Expression Method for Learning Dynamics on Complex Networks,http://arxiv.org/abs/2401.03092v2,2024-01-05T23:47:37Z,2024-02-12T16:09:48Z,"  Complex network data pervades various real-world domains, including physical,
technological, and biological systems. Despite the prevalence of such data,
predicting trends and understanding behavioral patterns in complex systems
remains challenging due to poorly understood underlying mechanisms. While
data-driven methods have made strides in uncovering governing equations from
time series data, efforts to extract physical laws from network data are
limited and often struggle with incomplete or noisy data. To address these
challenges, we introduce a novel approach called the Finite Expression Method
(FEX) and its fast algorithm for this learning problem on complex networks. FEX
represents dynamics on complex networks using binary trees composed of finite
mathematical operators. The nodes within these trees are trained through a
combinatorial optimization process guided by reinforcement learning techniques.
This unique configuration allows FEX to capture complex dynamics with minimal
prior knowledge of the system and a small dictionary of mathematical operators.
Our extensive numerical experiments demonstrate that FEX excels in accurately
identifying dynamics across diverse network topologies and dynamic behaviors.
","['\nZezheng Song\n', '\nChunmei Wang\n', '\nHaizhao Yang\n']",,,http://arxiv.org/abs/2401.03092v2,cs.SC,"['cs.SC', 'cs.NA', 'math.NA', 'physics.app-ph']",,,[]
"Symbolic Security Verification of Mesh Commissioning Protocol in Thread
  (extended version)",http://arxiv.org/abs/2312.12958v1,2023-12-20T12:00:06Z,2023-12-20T12:00:06Z,"  The Thread protocol (or simply Thread ) is a popular networking protocol for
the Internet of Things (IoT). It allows seamless integration of a set of
applications and protocols, hence reducing the risk of incompatibility among
different applications or user protocols. Thread has been deployed in many
popular smart home products by the majority of IoT manufacturers, such as Apple
TV, Apple HomePod mini, eero 6, Nest Hub, and Nest Wifi. Despite a few
empirical analyses on the security of Thread, there is still a lack of formal
analysis on this infrastructure of the booming IoT ecosystem. In this work, we
performed a formal symbolic analysis of the security properties of Thread. Our
main focus is on MeshCoP (Mesh Commissioning Protocol), the main subprotocol in
Thread for secure authentication and commissioning of new, untrusted devices
inside an existing Thread network. This case study presents the challenges and
proposed solutions in modeling MeshCoP. We use ProVerif, a symbolic
verification tool of {\pi}-calculus models, for verifying the security
properties of MeshCoP.
","['\nPankaj Upadhyay\n', '\nSubodh Sharma\n', '\nGuangdong Bai\n']",18 pages,,http://arxiv.org/abs/2312.12958v1,cs.CR,"['cs.CR', 'cs.SC', '68Q60', 'I.6.5']",,,[]
$p$-adic algorithm for bivariate Gröbner bases,http://arxiv.org/abs/2312.14116v1,2023-12-21T18:40:50Z,2023-12-21T18:40:50Z,"  We present a $p$-adic algorithm to recover the lexicographic Gr\""obner basis
$\mathcal G$ of an ideal in $\mathbb Q[x,y]$ with a generating set in $\mathbb
Z[x,y]$, with a complexity that is less than cubic in terms of the dimension of
$\mathbb Q[x,y]/\langle \mathcal G \rangle$ and softly linear in the height of
its coefficients. We observe that previous results of Lazard's that use Hermite
normal forms to compute Gr\""obner bases for ideals with two generators can be
generalized to a set of $t\in \mathbb N^+$ generators. We use this result to
obtain a bound on the height of the coefficients of $\mathcal G$, and to
control the probability of choosing a \textit{good} prime $p$ to build the
$p$-adic expansion of $\mathcal G$.
","['\nEric Schost\n', '\nCatherine St-Pierre\n']","(ACM) Proceeding in International Symposium on Symbolic and Algebraic
  Computation 2023 (ISSAC 2023), July 24--27, 2023, Troms{\o}, Norway","ISSAC '23: Proceedings of the 2023 International Symposium on
  Symbolic and Algebraic ComputationJuly 2023",http://dx.doi.org/10.1145/3597066.3597086,math.AC,"['math.AC', 'cs.SC']",10.1145/3597066.3597086,,[]
An F5 Algorithm for Tropical Gröbner Bases in the Weyl Algebras,http://arxiv.org/abs/2312.14419v1,2023-12-22T03:54:15Z,2023-12-22T03:54:15Z,"  A Gr\""obner basis computation for the Weyl algebra with respect to a tropical
term order and by using a homogenization-dehomogenization technique is
sufficiently sluggish. A significant number of reductions to zero occur. To
improve the computation, a tropical F5 algorithm is developed for this context.
As a member of the family of signature-based algorithms, this algorithm keeps
track of where Weyl algebra elements come from to anticipate reductions to
zero. The total order for ordering module monomials or signatures in this paper
is designed as close as possible to the definition of the tropical term order.
As in Vaccon et al. (2021), this total order is not compatible with the
tropical term order.
","['\nAri Dwi Hartanto\n', '\nKatsuyoshi Ohara\n']",,,http://arxiv.org/abs/2312.14419v1,cs.SC,"['cs.SC', 'math.RA']",,,[]
"Iterated Resultants and Rational Functions in Real Quantifier
  Elimination",http://arxiv.org/abs/2312.16210v1,2023-12-23T17:32:45Z,2023-12-23T17:32:45Z,"  This paper builds and extends on the authors previous work related to the
algorithmic tool, Cylindrical Algebraic Decomposition (CAD), and one of its
core applications, Real Quantifier Elimination (QE). These topics are at the
heart of symbolic computation and were first implemented in computer algebra
systems decades ago, but have recently received renewed interest as part of the
ongoing development of SMT solvers for non-linear real arithmetic.
  First, we consider the use of iterated univariate resultants in traditional
CAD, and how this leads to inefficiencies, especially in the case of an input
with multiple equational constraints. We reproduce the workshop paper
[Davenport \& England, 2023], adding important clarifications to our
suggestions first made there to make use of multivariate resultants in the
projection phase of CAD. We then consider an alternative approach to this
problem first documented in [McCallum \& Brown, 2009] which redefines the
actual object under construction, albeit only in the case of two equational
constraints. We correct an important typo and provide a missing proof in that
paper.
  We finish by revising the topic of how to deal with SMT or Real QE problems
expressed using rational functions (as opposed to the usual polynomial ones)
noting that these are often found in industrial applications. We revisit a
proposal made in [Uncu, Davenport and England, 2023] for doing this in the case
of satisfiability, explaining why such an approach does not trivially extend to
more complicated quantification structure and giving a suitable alternative.
","['\nJames H. Davenport\n', '\nMatthew England\n', '\nScott McCallum\n', '\nAli K. Uncu\n']",To be submitted to Mathematics in Computer Science,,http://arxiv.org/abs/2312.16210v1,cs.SC,"['cs.SC', 'math.AG', '14W30 (primary) 68W30 (secondary)', 'I.1.2']",,,[]
Adaptive Flip Graph Algorithm for Matrix Multiplication,http://arxiv.org/abs/2312.16960v1,2023-12-28T11:05:55Z,2023-12-28T11:05:55Z,"  This study proposes the ""adaptive flip graph algorithm"", which combines
adaptive searches with the flip graph algorithm for finding fast and efficient
methods for matrix multiplication. The adaptive flip graph algorithm addresses
the inherent limitations of exploration and inefficient search encountered in
the original flip graph algorithm, particularly when dealing with large matrix
multiplication. For the limitation of exploration, the proposed algorithm
adaptively transitions over the flip graph, introducing a flexibility that does
not strictly reduce the number of multiplications. Concerning the issue of
inefficient search in large instances, the proposed algorithm adaptively
constraints the search range instead of relying on a completely random search,
facilitating more effective exploration. Numerical experimental results
demonstrate the effectiveness of the adaptive flip graph algorithm, showing a
reduction in the number of multiplications for a $4\times 5$ matrix multiplied
by a $5\times 5$ matrix from $76$ to $73$, and that from $95$ to $94$ for a $5
\times 5$ matrix multiplied by another $5\times 5$ matrix. These results are
obtained in characteristic two.
","['\nYamato Arai\n', '\nYuma Ichikawa\n', '\nKoji Hukushima\n']","26 pages, 2 figures",,http://arxiv.org/abs/2312.16960v1,cs.SC,"['cs.SC', 'cs.DS']",,,[]
Factoring sparse polynomials fast,http://arxiv.org/abs/2312.17380v1,2023-12-28T22:06:11Z,2023-12-28T22:06:11Z,"  Consider a sparse polynomial in several variables given explicitly as a sum
of non-zero terms with coefficients in an effective field. In this paper, we
present several algorithms for factoring such polynomials and related tasks
(such as gcd computation, square-free factorization, content-free
factorization, and root extraction). Our methods are all based on sparse
interpolation, but follow two main lines of attack: iteration on the number of
variables and more direct reductions to the univariate or bivariate case. We
present detailed probabilistic complexity bounds in terms of the complexity of
sparse interpolation and evaluation.
","['\nAlexander Demin\n', '\nJoris van der Hoeven\n']",,,http://arxiv.org/abs/2312.17380v1,cs.SC,"['cs.SC', 'math.AC']",,,[]
Fast interpolation of sparse multivariate polynomials,http://arxiv.org/abs/2312.17664v1,2023-12-29T16:06:31Z,2023-12-29T16:06:31Z,"  Consider a sparse multivariate polynomial f with integer coefficients. Assume
that f is represented as a ""modular black box polynomial"", e.g. via an
algorithm to evaluate f at arbitrary integer points, modulo arbitrary positive
integers. The problem of sparse interpolation is to recover f in its usual
sparse representation, as a sum of coefficients times monomials. For the first
time we present a quasi-optimal algorithm for this task.
","['\nJoris van der Hoeven\n', '\nGrégoire Lecerf\n']",,,http://arxiv.org/abs/2312.17664v1,cs.SC,"['cs.SC', 'math.AC']",,,[]
"Conditions for eigenvalue configurations of two real symmetric matrices:
  a symmetric function approach",http://arxiv.org/abs/2401.00089v1,2023-12-29T22:18:37Z,2023-12-29T22:18:37Z,"  For two real symmetric matrices, their eigenvalue configuration is the
arrangement of their eigenvalues on the real line. We study the problem of
determining a quantifier-free necessary and sufficient condition for two real
symmetric matrices to realize a given eigenvalue configuration as a
generalization of Descartes' rule of signs. We exploit the combinatorial
properties of our definition for eigenvalue configuration to reduce a
two-polynomial root counting problem into several single-polynomial root
counting problems of symmetric polynomials. We then leverage the fundamental
theorem of symmetric polynomials to derive a final quantifier-free necessary
and sufficient condition for two real symmetric matrices to realize a given
eigenvalue configuration.
","['\nHoon Hong\n', '\nDaniel Profili\n', '\nJ. Rafael Sendra\n']",,,http://arxiv.org/abs/2401.00089v1,math.AG,"['math.AG', 'cs.SC']",,,[]
"Conditions for eigenvalue configurations of two real symmetric matrices:
  a signature approach",http://arxiv.org/abs/2401.00866v1,2023-12-29T22:24:29Z,2023-12-29T22:24:29Z,"  For two real symmetric matrices, their eigenvalue configuration is the
arrangement of their eigenvalues on the real line. In this paper, we provide
quantifier-free necessary and sufficient conditions for two symmetric matrices
to realize a given eigenvalue configuration. The basic idea is to generate a
set of polynomials in the entries of the two matrices whose roots can be
counted to uniquely determine the eigenvalue configuration. This result can be
seen as ageneralization of Descartes' rule of signs to the case of two real
univariate polynomials.
","['\nHoon Hong\n', '\nDaniel Profili\n', '\nJ. Rafael Sendra\n']",arXiv admin note: substantial text overlap with arXiv:2401.00089,,http://arxiv.org/abs/2401.00866v1,math.AG,"['math.AG', 'cs.SC']",,,[]
"Computing superspecial hyperelliptic curves of genus 4 with automorphism
  group properly containing the Klein 4-group",http://arxiv.org/abs/2312.16858v1,2023-12-28T07:02:07Z,2023-12-28T07:02:07Z,"  In algebraic geometry, enumerating or finding superspecial curves in positive
characteristic $p$ is important both in theory and in computation. In this
paper, we propose feasible algorithms to enumerate or find superspecial
hyperelliptic curves of genus $4$ with automorphism group properly containing
the Klein $4$-group. Executing the algorithms on Magma, we succeeded in
enumerating such superspecial curves for every $p$ with $19 \leq p < 500$, and
in finding a single one for every $p$ with $19 \leq p < 7000$.
","['\nRyo Ohashi\n', '\nMomonari Kudo\n']","26 pages, and comments are welcome!",,http://arxiv.org/abs/2312.16858v1,math.AG,"['math.AG', 'cs.SC', 'math.NT']",,,[]
Learning for CasADi: Data-driven Models in Numerical Optimization,http://arxiv.org/abs/2312.05873v1,2023-12-10T13:03:58Z,2023-12-10T13:03:58Z,"  While real-world problems are often challenging to analyze analytically, deep
learning excels in modeling complex processes from data. Existing optimization
frameworks like CasADi facilitate seamless usage of solvers but face challenges
when integrating learned process models into numerical optimizations. To
address this gap, we present the Learning for CasADi (L4CasADi) framework,
enabling the seamless integration of PyTorch-learned models with CasADi for
efficient and potentially hardware-accelerated numerical optimization. The
applicability of L4CasADi is demonstrated with two tutorial examples: First, we
optimize a fish's trajectory in a turbulent river for energy efficiency where
the turbulent flow is represented by a PyTorch model. Second, we demonstrate
how an implicit Neural Radiance Field environment representation can be easily
leveraged for optimal control with L4CasADi. L4CasADi, along with examples and
documentation, is available under MIT license at
https://github.com/Tim-Salzmann/l4casadi
","['\nTim Salzmann\n', '\nJon Arrizabalaga\n', '\nJoel Andersson\n', '\nMarco Pavone\n', '\nMarkus Ryll\n']",,,http://arxiv.org/abs/2312.05873v1,eess.SY,"['eess.SY', 'cs.AI', 'cs.LG', 'cs.RO', 'cs.SC', 'cs.SY']",,,[]
Hybrid Intervals and Symbolic Block Matrices,http://arxiv.org/abs/2311.16571v1,2023-11-28T07:28:44Z,2023-11-28T07:28:44Z,"  Structured matrices with symbolic sizes appear frequently in the literature,
especially in the description of algorithms for linear algebra. Recent work has
treated these symbolic structured matrices themselves as computational objects,
showing how to add matrices with blocks of different symbolic sizes in a
general way while avoiding a combinatorial explosion of cases. The present
article introduces the concept of hybrid intervals, in which points may have
negative multiplicity. Various operations on hybrid intervals have compact and
elegant formulations that do not require cases to handle different orders of
the end points. This makes them useful to represent symbolic block matrix
structures and to express arithmetic on symbolic block matrices compactly. We
use these ideas to formulate symbolic block matrix addition and multiplication
in a compact and uniform way.
","['\nMike Ghesquiere\n', '\nStephen M. Watt\n']",,,http://arxiv.org/abs/2311.16571v1,cs.SC,['cs.SC'],,,[]
"${L}^{\infty}$-norm computation for linear time-invariant systems
  depending on parameters",http://arxiv.org/abs/2312.00760v2,2023-12-01T18:27:40Z,2023-12-04T06:45:42Z,"  This paper focuses on representing the $L^{\infty}$-norm of
finite-dimensional linear time-invariant systems with parameter-dependent
coefficients. Previous studies tackled the problem in a non-parametric scenario
by simplifying it to finding the maximum $y$-projection of real solutions $(x,
y)$ of a system of the form $\Sigma=\{P=0, \, \partial P/\partial x=0\}$, where
$P \in \Z[x, y]$. To solve this problem, standard computer algebra methods were
employed and analyzed \cite{bouzidi2021computation}.
  In this paper, we extend our approach to address the parametric case. We aim
to represent the ""maximal"" $y$-projection of real solutions of $\Sigma$ as a
function of the given parameters. %a set of parameters $\alpha$. To accomplish
this, we utilize cylindrical algebraic decomposition. This method allows us to
determine the desired value as a function of the parameters within specific
regions of parameter space.
","['\nAlban Quadrat\n', '\nFabrice Rouillier\n', '\nGrace Younes\n']",,,http://arxiv.org/abs/2312.00760v2,cs.SC,['cs.SC'],,,[]
The Inverse of the Complex Gamma Function,http://arxiv.org/abs/2311.16583v1,2023-11-28T08:00:38Z,2023-11-28T08:00:38Z,"  We consider the functional inverse of the Gamma function in the complex
plane, where it is multi-valued, and define a set of suitable branches by
proposing a natural extension from the real case.
","['\nDavid J. Jeffrey\n', '\nStephen M. Watt\n']",,,http://arxiv.org/abs/2311.16583v1,math.CV,"['math.CV', 'cs.SC']",,,[]
Nested Integrals and Rationalizing Transformations,http://arxiv.org/abs/2311.16992v1,2023-11-28T17:46:57Z,2023-11-28T17:46:57Z,"  A brief overview of some computer algebra methods for computations with
nested integrals is given. The focus is on nested integrals over integrands
involving square roots. Rewrite rules for conversion to and from associated
nested sums are discussed. We also include a short discussion comparing the
holonomic systems approach and the differential field approach. For
simplification to rational integrands, we give a comprehensive list of
univariate rationalizing transformations, including transformations tuned to
map the interval $[0,1]$ bijectively to itself.
",['\nClemens G. Raab\n'],"manuscript of 25 February 2021, in ""Anti-Differentiation and the
  Calculation of Feynman Amplitudes"", Springer",,http://arxiv.org/abs/2311.16992v1,cs.SC,"['cs.SC', 'math-ph', 'math.MP']",,,[]
"Here Is Not There: Measuring Entailment-Based Trajectory Similarity for
  Location-Privacy Protection and Beyond",http://arxiv.org/abs/2312.01151v1,2023-12-02T14:41:01Z,2023-12-02T14:41:01Z,"  While the paths humans take play out in social as well as physical space,
measures to describe and compare their trajectories are carried out in
abstract, typically Euclidean, space. When these measures are applied to
trajectories of actual individuals in an application area, alterations that are
inconsequential in abstract space may suddenly become problematic once overlaid
with geographic reality. In this work, we present a different view on
trajectory similarity by introducing a measure that utilizes logical
entailment. This is an inferential perspective that considers facts as triple
statements deduced from the social and environmental context in which the
travel takes place, and their practical implications. We suggest a
formalization of entailment-based trajectory similarity, measured as the
overlapping proportion of facts, which are spatial relation statements in our
case study. With the proposed measure, we evaluate LSTM-TrajGAN, a
privacy-preserving trajectory-generation model. The entailment-based model
evaluation reveals potential consequences of disregarding the rich structure of
geographic space (e.g., miscalculated insurance risk due to regional shifts in
our toy example). Our work highlights the advantage of applying logical
entailment to trajectory-similarity reasoning for location-privacy protection
and beyond.
","['\nZilong Liu\n', '\nKrzysztof Janowicz\n', '\nKitty Currier\n', '\nMeilin Shi\n', '\nJinmeng Rao\n', '\nSong Gao\n', '\nLing Cai\n', '\nAnita Graser\n']",,,http://dx.doi.org/10.5281/zenodo.8286277,cs.CY,"['cs.CY', 'cs.CL', 'cs.SC']",10.5281/zenodo.8286277,,[]
Robust Clustering using Hyperdimensional Computing,http://arxiv.org/abs/2312.02407v1,2023-12-05T00:46:29Z,2023-12-05T00:46:29Z,"  This paper addresses the clustering of data in the hyperdimensional computing
(HDC) domain. In prior work, an HDC-based clustering framework, referred to as
HDCluster, has been proposed. However, the performance of the existing
HDCluster is not robust. The performance of HDCluster is degraded as the
hypervectors for the clusters are chosen at random during the initialization
step. To overcome this bottleneck, we assign the initial cluster hypervectors
by exploring the similarity of the encoded data, referred to as \textit{query}
hypervectors. Intra-cluster hypervectors have a higher similarity than
inter-cluster hypervectors. Harnessing the similarity results among query
hypervectors, this paper proposes four HDC-based clustering algorithms:
similarity-based k-means, equal bin-width histogram, equal bin-height
histogram, and similarity-based affinity propagation. Experimental results
illustrate that: (i) Compared to the existing HDCluster, our proposed HDC-based
clustering algorithms can achieve better accuracy, more robust performance,
fewer iterations, and less execution time. Similarity-based affinity
propagation outperforms the other three HDC-based clustering algorithms on
eight datasets by 2~38% in clustering accuracy. (ii) Even for one-pass
clustering, i.e., without any iterative update of the cluster hypervectors, our
proposed algorithms can provide more robust clustering accuracy than HDCluster.
(iii) Over eight datasets, five out of eight can achieve higher or comparable
accuracy when projected onto the hyperdimensional space. Traditional clustering
is more desirable than HDC when the number of clusters, $k$, is large.
","['\nLulu Ge\n', '\nKeshab K. Parhi\n']",,,http://arxiv.org/abs/2312.02407v1,cs.LG,"['cs.LG', 'cs.DB', 'cs.SC']",,,[]
Physical Symbolic Optimization,http://arxiv.org/abs/2312.03612v1,2023-12-06T16:56:28Z,2023-12-06T16:56:28Z,"  We present a framework for constraining the automatic sequential generation
of equations to obey the rules of dimensional analysis by construction.
Combining this approach with reinforcement learning, we built $\Phi$-SO, a
Physical Symbolic Optimization method for recovering analytical functions from
physical data leveraging units constraints. Our symbolic regression algorithm
achieves state-of-the-art results in contexts in which variables and constants
have known physical units, outperforming all other methods on SRBench's Feynman
benchmark in the presence of noise (exceeding 0.1%) and showing resilience even
in the presence of significant (10%) levels of noise.
","['\nWassim Tenachi\n', '\nRodrigo Ibata\n', '\nFoivos I. Diakogiannis\n']","6 pages, 2 figures, 1 table. Accepted to NeurIPS 2023, Machine
  Learning for Physical Sciences workshop",,http://arxiv.org/abs/2312.03612v1,cs.LG,"['cs.LG', 'astro-ph.IM', 'cs.SC', 'physics.comp-ph', 'physics.data-an']",,,[]
Efficient Local Search for Nonlinear Real Arithmetic,http://arxiv.org/abs/2311.14249v1,2023-11-24T01:59:22Z,2023-11-24T01:59:22Z,"  Local search has recently been applied to SMT problems over various
arithmetic theories. Among these, nonlinear real arithmetic poses special
challenges due to its uncountable solution space and potential need to solve
higher-degree polynomials. As a consequence, existing work on local search only
considered fragments of the theory. In this work, we analyze the difficulties
and propose ways to address them, resulting in an efficient search algorithm
that covers the full theory of nonlinear real arithmetic. In particular, we
present two algorithmic improvements: incremental computation of variable
scores and temporary relaxation of equality constraints. We also discuss choice
of candidate moves and a look-ahead mechanism in case when no critical moves
are available. The resulting implementation is competitive on satisfiable
problem instances against complete methods such as MCSAT in existing SMT
solvers.
","['\nZhonghan Wang\n', '\nBohua Zhan\n', '\nBohan Li\n', '\nShaowei Cai\n']",Full version of VMCAI'2024 publication,,http://arxiv.org/abs/2311.14249v1,cs.SC,['cs.SC'],,,[]
"Geometric Fiber Classification of Morphisms and a Geometric Approach to
  Cylindrical Algebraic Decomposition",http://arxiv.org/abs/2311.10515v2,2023-11-17T13:36:57Z,2023-12-05T12:46:29Z,"  Cylindrical Algebraic Decomposition (CAD) is a classical construction in real
algebraic geometry. The original cylindrical algebraic decomposition was
proposed by Collins, using the classical elimination theory. In this paper, we
first study the geometric fibers cardinality classification problem of
morphisms of affine varieties (over a field of characteristic 0), using a
constructive version of Grothendieck's Generic Freeness Lemma and Parametric
Hermite Quadratic Forms, then we show how cylindrical algebraic decomposition
is related to this classification problem. This provides a new geometric view
of Cylindrical Algebraic Decomposition and a new theory of Cylindrical
Algebraic Decomposition is developed in this paper.
",['\nRizeng Chen\n'],"52 pages (32 pages on theory, 5 pages on application and 6 pages on
  analysis), many figures. Comments are welcome!",,http://arxiv.org/abs/2311.10515v2,math.AG,"['math.AG', 'cs.SC', 'math.AC']",,,[]
"Reduction-based Creative Telescoping for P-recursive Sequences via
  Integral Bases",http://arxiv.org/abs/2311.05246v1,2023-11-09T10:01:32Z,2023-11-09T10:01:32Z,"  We propose a way to split a given bivariate P-recursive sequence into a
summable part and a non-summable part in such a way that the non-summable part
is minimal in some sense. This decomposition gives rise to a new
reduction-based creative telescoping algorithm based on the concept of integral
bases.
","['\nShaoshi Chen\n', '\nLixin Du\n', '\nManuel Kauers\n', '\nRong-Hua Wang\n']",20 pages,,http://arxiv.org/abs/2311.05246v1,cs.SC,['cs.SC'],,,[]
On the Existence of Telescopers for P-recursive Sequences,http://arxiv.org/abs/2311.06065v1,2023-11-10T14:00:50Z,2023-11-10T14:00:50Z,"  We extend the criterion on the existence of telescopers for hypergeometric
terms to the case of P-recursive sequences. This criterion is based on the
concept of integral bases and the generalized Abramov-Petkovsek reduction for
P-recursive sequences.
",['\nLixin Du\n'],18 pages,,http://arxiv.org/abs/2311.06065v1,cs.SC,"['cs.SC', '68W30, 12H10, 33F10, 39A06, 68W40']",,,[]
ACL2 Proofs of Nonlinear Inequalities with Imandra,http://arxiv.org/abs/2311.08861v1,2023-11-15T10:48:30Z,2023-11-15T10:48:30Z,"  We present a proof-producing integration of ACL2 and Imandra for proving
nonlinear inequalities. This leverages a new Imandra interface exposing its
nonlinear decision procedures. The reasoning takes place over the reals, but
the proofs produced are valid over the rationals and may be run in both ACL2
and ACL2(r). The ACL2 proofs Imandra constructs are extracted from
Positivstellensatz refutations, a real algebraic analogue of the
Nullstellensatz, and are found using convex optimization.
",['\nGrant Passmore\nImandra Inc.\n'],"In Proceedings ACL2-2023, arXiv:2311.08373","EPTCS 393, 2023, pp. 151-160",http://dx.doi.org/10.4204/EPTCS.393.12,cs.LO,"['cs.LO', 'cs.SC']",10.4204/EPTCS.393.12,,['Imandra Inc.']
Scene-Driven Multimodal Knowledge Graph Construction for Embodied AI,http://arxiv.org/abs/2311.03783v1,2023-11-07T08:06:27Z,2023-11-07T08:06:27Z,"  Embodied AI is one of the most popular studies in artificial intelligence and
robotics, which can effectively improve the intelligence of real-world agents
(i.e. robots) serving human beings. Scene knowledge is important for an agent
to understand the surroundings and make correct decisions in the varied open
world. Currently, knowledge base for embodied tasks is missing and most
existing work use general knowledge base or pre-trained models to enhance the
intelligence of an agent. For conventional knowledge base, it is sparse,
insufficient in capacity and cost in data collection. For pre-trained models,
they face the uncertainty of knowledge and hard maintenance. To overcome the
challenges of scene knowledge, we propose a scene-driven multimodal knowledge
graph (Scene-MMKG) construction method combining conventional knowledge
engineering and large language models. A unified scene knowledge injection
framework is introduced for knowledge representation. To evaluate the
advantages of our proposed method, we instantiate Scene-MMKG considering
typical indoor robotic functionalities (Manipulation and Mobility), named
ManipMob-MMKG. Comparisons in characteristics indicate our instantiated
ManipMob-MMKG has broad superiority in data-collection efficiency and knowledge
quality. Experimental results on typical embodied tasks show that
knowledge-enhanced methods using our instantiated ManipMob-MMKG can improve the
performance obviously without re-designing model structures complexly. Our
project can be found at https://sites.google.com/view/manipmob-mmkg
","['\nSong Yaoxian\n', '\nSun Penglei\n', '\nLiu Haoyu\n', '\nLi Zhixu\n', '\nSong Wei\n', '\nXiao Yanghua\n', '\nZhou Xiaofang\n']","14 pages, 5 figures",,http://arxiv.org/abs/2311.03783v1,cs.AI,"['cs.AI', 'cs.RO', 'cs.SC']",,,[]
Stability Problems on D-finite Functions,http://arxiv.org/abs/2311.05897v1,2023-11-10T07:06:07Z,2023-11-10T07:06:07Z,"  This paper continues the studies of symbolic integration by focusing on the
stability problems on D-finite functions. We introduce the notion of stability
index in order to investigate the order growth of the differential operators
satisfied by iterated integrals of D-finite functions and determine bounds and
exact formula for stability indices of several special classes of differential
operators. With the basic properties of stability index, we completely solve
the stability problem on general hyperexponential functions.
","['\nShaoshi Chen\n', '\nRuyong Feng\n', '\nZewang Guo\n', '\nWei Lu\n']",9 pages,"Proceedings of ISSAC'23,2023",http://dx.doi.org/10.1145/3597066.3597085,cs.SC,"['cs.SC', 'math.CA', 'math.DS', '12H05, 37P15, 33F10', 'I.1.2']",10.1145/3597066.3597085,,[]
Formal Verification of Zero-Knowledge Circuits,http://arxiv.org/abs/2311.08858v1,2023-11-15T10:47:28Z,2023-11-15T10:47:28Z,"  Zero-knowledge circuits are sets of equality constraints over arithmetic
expressions interpreted in a prime field; they are used to encode computations
in cryptographic zero-knowledge proofs. We make the following contributions to
the problem of ensuring that a circuit correctly encodes a computation: a
formal framework for circuit correctness; an ACL2 library for prime fields; an
ACL2 model of the existing R1CS (Rank-1 Constraint Systems) formalism to
represent circuits, along with ACL2 and Axe tools to verify circuits of this
form; a novel PFCS (Prime Field Constraint Systems) formalism to represent
hierarchically structured circuits, along with an ACL2 model of it and ACL2
tools to verify circuits of this form in a compositional and scalable way;
verification of circuits, ranging from simple to complex; and discovery of bugs
and optimizations in existing zero-knowledge systems.
","['\nAlessandro Coglio\nKestrel Institute and Aleo Systems Inc.\n', '\nEric McCarthy\nKestrel Institute and Aleo Systems Inc.\n', '\nEric W. Smith\nKestrel Institute\n']","In Proceedings ACL2-2023, arXiv:2311.08373","EPTCS 393, 2023, pp. 94-112",http://dx.doi.org/10.4204/EPTCS.393.9,cs.LO,"['cs.LO', 'cs.CR', 'cs.SC']",10.4204/EPTCS.393.9,,"['Kestrel Institute and Aleo Systems Inc.', 'Kestrel Institute and Aleo Systems Inc.', 'Kestrel Institute']"
Computing Implicitizations of Multi-Graded Polynomial Maps,http://arxiv.org/abs/2311.07678v1,2023-11-13T19:01:48Z,2023-11-13T19:01:48Z,"  In this paper, we focus on computing the kernel of a map of polynomial rings
$\varphi$. This core problem in symbolic computation is known as
implicitization. While there are extremely effective Gr\""obner basis methods
used to solve this problem, these methods can become infeasible as the number
of variables increases. In the case when the map $\varphi$ is multigraded, we
consider an alternative approach. We demonstrate how to quickly compute a
matrix of maximal rank for which $\varphi$ has a positive multigrading. Then in
each graded component we compute the minimal generators of the kernel in that
multidegree with linear algebra. We have implemented our techniques in
Macaulay2 and show that our implementation can compute many generators of low
degree in examples where Gr\""obner techniques have failed. This includes
several examples coming from phylogenetics where even a complete list of
quadrics and cubics were unknown. When the multigrading refines total degree,
our algorithm is \emph{embarassingly parallel} and a fully parallelized version
of our algorithm will be forthcoming in OSCAR.
","['\nJoseph Cummings\n', '\nBenjamin Hollering\n']","16 pages, 2 figures. An implementation of our main algorithm can be
  found on our MathRepo page as well as our GitHub",,http://arxiv.org/abs/2311.07678v1,math.AG,"['math.AG', 'cs.SC', 'math.AC', 'math.ST', 'stat.TH', '68W3068W30 68W30 68W30 68W30, 13P25, 62R01']",,,[]
"A Novel Application of Polynomial Solvers in mmWave Analog Radio
  Beamforming",http://arxiv.org/abs/2310.18103v1,2023-10-27T12:41:41Z,2023-10-27T12:41:41Z,"  Beamforming is a signal processing technique where an array of antenna
elements can be steered to transmit and receive radio signals in a specific
direction. The usage of millimeter wave (mmWave) frequencies and multiple input
multiple output (MIMO) beamforming are considered as the key innovations of 5th
Generation (5G) and beyond communication systems. The technique initially
performs a beam alignment procedure, followed by data transfer in the aligned
directions between the transmitter and the receiver. Traditionally, beam
alignment involves periodical and exhaustive beam sweeping at both transmitter
and the receiver, which is a slow process causing extra communication overhead
with MIMO and massive MIMO radio units. In applications such as beam tracking,
angular velocity, beam steering etc., the beam alignment procedure is optimized
by estimating the beam directions using first order polynomial approximations.
Recent learning-based SOTA strategies for fast mmWave beam alignment also
require exploration over exhaustive beam pairs during the training procedure,
causing overhead to learning strategies for higher antenna configurations. In
this work, we first optimize the beam alignment cost functions e.g. the data
rate, to reduce the beam sweeping overhead by applying polynomial
approximations of its partial derivatives which can then be solved as a system
of polynomial equations using well-known tools from algebraic geometry. At this
point, a question arises: 'what is a good polynomial approximation?' In this
work, we attempt to obtain a 'good polynomial approximation'. Preliminary
experiments indicate that our estimated polynomial approximations attain a
so-called sweet-spot in terms of the solver speed and accuracy, when evaluated
on test beamforming problems.
","['\nSnehal Bhayani\n', '\nPraneeth Susarla\n', '\nS. S. Krishna Chaitanya Bulusu\n', '\nOlli Silven\n', '\nMarkku Juntti\n', '\nJanne Heikkila\n']","Accepted for publication in the SIGSAM's ACM Communications in
  Computer Algebra, as an extended abstract",,http://arxiv.org/abs/2310.18103v1,cs.SC,['cs.SC'],,,[]
"Linear difference operators with sequence coefficients having
  infinite-dimentional solution spaces",http://arxiv.org/abs/2311.02217v1,2023-11-03T20:08:02Z,2023-11-03T20:08:02Z,"  The notion of lacunary infinite numerical sequence is introduced. It is shown
that for an arbitrary linear difference operator L with coefficients belonging
to the set R of infinite numerical sequences, a criterion (i.e., a necessary
and sufficient condition) for the infinite dimensionality of its space $V_L$ of
solutions belonging to R is the presence of a lacunary sequence in $V_L$.
","['\nSergei Abramov\n', '\nGleb Pogudin\n']",In memory of Marko Petkov\v{s}ek,"ACM Communications in Computer Algebra, vol 57, issue 1, 2023",http://dx.doi.org/10.1145/3610377.3610378,cs.SC,['cs.SC'],10.1145/3610377.3610378,,[]
"On the dimension of the solution space of linear difference equations
  over the ring of infinite sequences",http://arxiv.org/abs/2311.02219v1,2023-11-03T20:13:55Z,2023-11-03T20:13:55Z,"  For a linear difference equation with the coefficients being computable
sequences, we establish algorithmic undecidability of the problem of
determining the dimension of the solution space including the case when some
additional prior information on the dimension is available.
","['\nSergei Abramov\n', '\nGleb Pogudin\n']",In memory of Marko Petkov\v{s}ek,,http://arxiv.org/abs/2311.02219v1,cs.SC,['cs.SC'],,,[]
"Dimensionally Homogeneous Jacobian using Extended Selection Matrix for
  Performance Evaluation and Optimization of Parallel Manipulators",http://arxiv.org/abs/2310.17863v1,2023-10-27T02:37:15Z,2023-10-27T02:37:15Z,"  This paper proposes a new methodology for deriving a point-based
dimensionally homogeneous Jacobian, intended for performance evaluation and
optimization of parallel manipulators with mixed degrees of freedom. Optimal
manipulator often rely on performance indices obtained from the Jacobian
matrix. However, when manipulators exhibit mixed translational and rotational
freedoms, the conventional Jacobian's inconsistency of units lead to unbalanced
optimal result. Addressing this issue, a point-based dimensionally homogeneous
Jacobian has appeared as a prominent solution. However, existing point-based
approaches for formulating dimensionally homogeneous Jacobian are applicable to
a limited variety of parallel manipulators. Moreover, they are complicated and
less intuitive. This paper introduces an extended selection matrix that
combines component velocities from different points to describe the entire
motion of moving plate. This proposed approach enables us to formulate an
intuitive point-based, dimensionally homogeneous Jacobian, which can be applied
to a wide variety of constrained parallel manipulators. To prove the validity
of proposed method, a numerical example is provided utilizing a
four-degree-of-freedom parallel manipulator.
","['\nHassen Nigatu\n', '\nDoik Kim\n']",,,http://arxiv.org/abs/2310.17863v1,cs.RO,"['cs.RO', 'cs.SC']",,,[]
"Optimizing Logical Execution Time Model for Both Determinism and Low
  Latency",http://arxiv.org/abs/2310.19699v2,2023-10-30T16:21:49Z,2024-01-21T21:25:43Z,"  The Logical Execution Time (LET) programming model has recently received
considerable attention, particularly because of its timing and dataflow
determinism. In LET, task computation appears always to take the same amount of
time (called the task's LET interval), and the task reads (resp. writes) at the
beginning (resp. end) of the interval. Compared to other communication
mechanisms, such as implicit communication and Dynamic Buffer Protocol (DBP),
LET performs worse on many metrics, such as end-to-end latency (including
reaction time and data age) and time disparity jitter. Compared with the
default LET setting, the flexible LET (fLET) model shrinks the LET interval
while still guaranteeing schedulability by introducing the virtual offset to
defer the read operation and using the virtual deadline to move up the write
operation. Therefore, fLET has the potential to significantly improve the
end-to-end timing performance while keeping the benefits of deterministic
behavior on timing and dataflow.
  To fully realize the potential of fLET, we consider the problem of optimizing
the assignments of its virtual offsets and deadlines. We propose new
abstractions to describe the task communication pattern and new optimization
algorithms to explore the solution space efficiently. The algorithms leverage
the linearizability of communication patterns and utilize symbolic operations
to achieve efficient optimization while providing a theoretical guarantee. The
framework supports optimizing multiple performance metrics and guarantees
bounded suboptimality when optimizing end-to-end latency. Experimental results
show that our optimization algorithms improve upon the default LET and its
existing extensions and significantly outperform implicit communication and DBP
in terms of various metrics, such as end-to-end latency, time disparity, and
its jitter.
","['\nSen Wang\n', '\nDong Li\n', '\nAshrarul H. Sifat\n', '\nShao-Yu Huang\n', '\nXuanliang Deng\n', '\nChanghee Jung\n', '\nRyan Williams\n', '\nHaibo Zeng\n']",Under Review,,http://arxiv.org/abs/2310.19699v2,eess.SY,"['eess.SY', 'cs.OS', 'cs.SC', 'cs.SY']",,,[]
Multi-Operational Mathematical Derivations in Latent Space,http://arxiv.org/abs/2311.01230v1,2023-11-02T13:33:07Z,2023-11-02T13:33:07Z,"  This paper investigates the possibility of approximating multiple
mathematical operations in latent space for expression derivation. To this end,
we introduce different multi-operational representation paradigms, modelling
mathematical operations as explicit geometric transformations. By leveraging a
symbolic engine, we construct a large-scale dataset comprising 1.7M derivation
steps stemming from 61K premises and 6 operators, analysing the properties of
each paradigm when instantiated with state-of-the-art neural encoders.
Specifically, we investigate how different encoding mechanisms can approximate
equational reasoning in latent space, exploring the trade-off between learning
different operators and specialising within single operations, as well as the
ability to support multi-step derivations and out-of-distribution
generalisation. Our empirical analysis reveals that the multi-operational
paradigm is crucial for disentangling different operators, while discriminating
the conclusions for a single operation is achievable in the original expression
encoder. Moreover, we show that architectural choices can heavily affect the
training dynamics, structural organisation, and generalisation of the latent
space, resulting in significant variations across paradigms and classes of
encoders.
","['\nMarco Valentino\n', '\nJordan Meadows\n', '\nLan Zhang\n', '\nAndré Freitas\n']",,,http://arxiv.org/abs/2311.01230v1,cs.LG,"['cs.LG', 'cs.AI', 'cs.SC']",,,[]
Dissipative quadratizations of polynomial ODE systems,http://arxiv.org/abs/2311.02508v2,2023-11-04T21:00:51Z,2024-01-24T22:59:59Z,"  Quadratization refers to a transformation of an arbitrary system of
polynomial ordinary differential equations to a system with at most quadratic
right-hand side. Such a transformation unveils new variables and model
structures that facilitate model analysis, simulation, and control and offers a
convenient parameterization for data-driven approaches. Quadratization
techniques have found applications in diverse fields, including systems theory,
fluid mechanics, chemical reaction modeling, and mathematical analysis.
  In this study, we focus on quadratizations that preserve the stability
properties of the original model, specifically dissipativity at given
equilibria. This preservation is desirable in many applications of
quadratization including reachability analysis and synthetic biology. We
establish the existence of dissipativity-preserving quadratizations, develop an
algorithm for their computation, and demonstrate it in several case studies.
","['\nYubo Cai\n', '\nGleb Pogudin\n']","Accepted by 30th International Conference on Tools and Algorithms for
  the Construction and Analysis of Systems (TACAS24)",,http://arxiv.org/abs/2311.02508v2,eess.SY,"['eess.SY', 'cs.NA', 'cs.SC', 'cs.SY', 'math.NA']",,,[]
A computational model of serial and parallel processing in visual search,http://arxiv.org/abs/2310.10061v1,2023-10-16T04:51:13Z,2023-10-16T04:51:13Z,"  The following is a dissertation aimed at understanding what the various
phenomena in visual search teach us about the nature of human visual
representations and processes. I first review some of the major empirical
findings in the study of visual search. I next present a theory of visual
search in terms of what I believe these findings suggest about the
representations and processes underlying ventral visual processing. These
principles are instantiated in a computational model called CASPER (Concurrent
Attention: Serial and Parallel Evaluation with Relations), originally developed
by Hummel, that I have adapted to account for a range of phenomena in visual
search. I then describe an extension of the CASPER model to account for our
ability to search for visual items defined not simply by the features composing
those items but by the spatial relations among those features. Seven
experiments (four main experiments and three replications) are described that
test CASPER's predictions about relational search. Finally, I evaluate the fit
between CASPER's predictions and the empirical findings and show with three
additional simulations that CASPER can account for negative acceleration in
search functions for relational stimuli if one postulates that the visual
system is leveraging an emergent feature that bypasses relational processing.
",['\nRachel F. Heaton\n'],116 pages,,http://arxiv.org/abs/2310.10061v1,cs.CV,"['cs.CV', 'cs.SC']",,,[]
"Accurate prediction of international trade flows: Leveraging knowledge
  graphs and their embeddings",http://arxiv.org/abs/2310.11161v1,2023-10-17T11:28:30Z,2023-10-17T11:28:30Z,"  Knowledge representation (KR) is vital in designing symbolic notations to
represent real-world facts and facilitate automated decision-making tasks.
Knowledge graphs (KGs) have emerged so far as a popular form of KR, offering a
contextual and human-like representation of knowledge. In international
economics, KGs have proven valuable in capturing complex interactions between
commodities, companies, and countries. By putting the gravity model, which is a
common economic framework, into the process of building KGs, important factors
that affect trade relationships can be taken into account, making it possible
to predict international trade patterns. This paper proposes an approach that
leverages Knowledge Graph embeddings for modeling international trade, focusing
on link prediction using embeddings. Thus, valuable insights are offered to
policymakers, businesses, and economists, enabling them to anticipate the
effects of changes in the international trade system. Moreover, the integration
of traditional machine learning methods with KG embeddings, such as decision
trees and graph neural networks are also explored. The research findings
demonstrate the potential for improving prediction accuracy and provide
insights into embedding explainability in knowledge representation. The paper
also presents a comprehensive analysis of the influence of embedding methods on
other intelligent algorithms.
","['\nDiego Rincon-Yanez\n', '\nChahinez Ounoughi\n', '\nBassem Sellami\n', '\nTarmo Kalvet\n', '\nMarek Tiits\n', '\nSabrina Senatore\n', '\nSadok Ben Yahia\n']","Accepted on Journal of King Saud University Computer and Information
  Sciences",,http://arxiv.org/abs/2310.11161v1,cs.AI,"['cs.AI', 'cs.SC']",,,[]
"Geometry of the signed support of a multivariate polynomial and
  Descartes' rule of signs",http://arxiv.org/abs/2310.05466v1,2023-10-09T07:22:21Z,2023-10-09T07:22:21Z,"  We describe conditions on the signed support, that is, on the set of the
exponent vectors and on the signs of the coefficients, of a multivariate
polynomial $f$ ensuring that the semi-algebraic set $\{ f < 0 \}$ defined in
the positive orthant has at most one connected component. These results
generalize Descartes' rule of signs in the sense that they provide a bound
which is independent of the values of the coefficients and the degree of the
polynomial. Based on how the exponent vectors lie on the faces of the Newton
polytope, we give a recursive algorithm that verifies a sufficient condition
for the set $\{ f < 0 \}$ to have one connected component. We apply the
algorithm to reaction networks in order to prove that the parameter region of
multistationarity of a ubiquitous network comprising phosphorylation cycles is
connected.
",['\nMáté L. Telek\n'],,,http://arxiv.org/abs/2310.05466v1,math.AG,"['math.AG', 'cs.SC']",,,[]
"Divide, Conquer and Verify: Improving Symbolic Execution Performance",http://arxiv.org/abs/2310.03598v2,2023-10-05T15:21:10Z,2023-11-07T09:51:41Z,"  Symbolic Execution is a formal method that can be used to verify the behavior
of computer programs and detect software vulnerabilities. Compared to other
testing methods such as fuzzing, Symbolic Execution has the advantage of
providing formal guarantees about the program. However, despite advances in
performance in recent years, Symbolic Execution is too slow to be applied to
real-world software. This is primarily caused by the \emph{path explosion
problem} as well as by the computational complexity of SMT solving. In this
paper, we present a divide-and-conquer approach for symbolic execution by
executing individual slices and later combining the side effects. This way, the
overall problem size is kept small, reducing the impact of computational
complexity on large problems.
","['\nChristopher Scherb\n', '\nLuc Bryan Heitz\n', '\nHermann Grieder\n', '\nOlivier Mattmann\n']",,,http://arxiv.org/abs/2310.03598v2,cs.CR,"['cs.CR', 'cs.SC', 'cs.SY', 'eess.SY']",,,[]
"What can knowledge graph alignment gain with Neuro-Symbolic learning
  approaches?",http://arxiv.org/abs/2310.07417v1,2023-10-11T12:03:19Z,2023-10-11T12:03:19Z,"  Knowledge Graphs (KG) are the backbone of many data-intensive applications
since they can represent data coupled with its meaning and context. Aligning
KGs across different domains and providers is necessary to afford a fuller and
integrated representation. A severe limitation of current KG alignment (KGA)
algorithms is that they fail to articulate logical thinking and reasoning with
lexical, structural, and semantic data learning. Deep learning models are
increasingly popular for KGA inspired by their good performance in other tasks,
but they suffer from limitations in explainability, reasoning, and data
efficiency. Hybrid neurosymbolic learning models hold the promise of
integrating logical and data perspectives to produce high-quality alignments
that are explainable and support validation through human-centric approaches.
This paper examines the current state of the art in KGA and explores the
potential for neurosymbolic integration, highlighting promising research
directions for combining these fields.
","['\nPedro Giesteira Cotovio\n', '\nErnesto Jimenez-Ruiz\n', '\nCatia Pesquita\n']",,,http://arxiv.org/abs/2310.07417v1,cs.AI,"['cs.AI', 'cs.LG', 'cs.SC']",,,[]
Three Paths to Rational Curves with Rational Arc Length,http://arxiv.org/abs/2310.08047v2,2023-10-12T05:39:56Z,2024-03-05T16:06:09Z,"  We solve the so far open problem of constructing all spatial rational curves
with rational arc length functions. More precisely, we present three different
methods for this construction. The first method adapts a recent approach of
(Kalkan et al. 2022) to rational PH curves and requires solving a modestly
sized system of linear equations. The second constructs the curve by imposing
zero-residue conditions, thus extending ideas of previous papers by (Farouki
and Sakkalis 2019) and the authors themselves (Schr\""ocker and \v{S}\'ir 2023).
The third method generalizes the dual approach of (Pottmann 1995) from planar
to spatial curves. The three methods share the same quaternion based
representation in which not only the PH curve but also its arc length function
are compactly expressed. We also present a new proof based on the quaternion
polynomial factorization theory of the well known characterization of the
Pythagorean quadruples.
","['\nHans-Peter Schröcker\n', '\nZbyněk Šìr\n']",,,http://arxiv.org/abs/2310.08047v2,cs.SC,"['cs.SC', 'cs.NA', 'math.DG', 'math.NA', '65D17']",,,[]
FMplex: A Novel Method for Solving Linear Real Arithmetic Problems,http://arxiv.org/abs/2310.00995v1,2023-10-02T08:58:04Z,2023-10-02T08:58:04Z,"  In this paper we introduce a novel quantifier elimination method for
conjunctions of linear real arithmetic constraints. Our algorithm is based on
the Fourier-Motzkin variable elimination procedure, but by case splitting we
are able to reduce the worst-case complexity from doubly to singly exponential.
The adaption of the procedure for SMT solving has strong correspondence to the
simplex algorithm, therefore we name it FMplex. Besides the theoretical
foundations, we provide an experimental evaluation in the context of SMT
solving.
","['\nJasper Nalbach\nRWTH Aachen University, Germany\n', '\nValentin Promies\nRWTH Aachen University, Germany\n', '\nErika Ábrahám\nRWTH Aachen University, Germany\n', '\nPaul Kobialka\nUniversity of Oslo, Norway\n']","In Proceedings GandALF 2023, arXiv:2309.17318. The extended version
  of this paper can be found at arXiv:2309.03138","EPTCS 390, 2023, pp. 16-32",http://dx.doi.org/10.4204/EPTCS.390.2,cs.SC,['cs.SC'],10.4204/EPTCS.390.2,,"['RWTH Aachen University, Germany', 'RWTH Aachen University, Germany', 'RWTH Aachen University, Germany', 'University of Oslo, Norway']"
Reducing Hyperexponential Functions over Monomial Extensions,http://arxiv.org/abs/2310.01194v1,2023-10-02T13:30:23Z,2023-10-02T13:30:23Z,"  We extend the shell and kernel reductions for hyperexponential functions over
the field of rational functions to a monomial extension. Both of the reductions
are incorporated into one algorithm. As an application, we present an additive
decomposition in rationally hyperexponential towers. The decomposition yields
an alternative algorithm for computing elementary integrals over such towers.
The alternative can find some elementary integrals that are unevaluated by the
integrators in the latest versions of Maple and Mathematica.
","['\nShaoshi Chen\n', '\nHao Du\n', '\nYiman Gao\n', '\nZiming Li\n']",,,http://arxiv.org/abs/2310.01194v1,cs.SC,['cs.SC'],,,[]
"LogicMP: A Neuro-symbolic Approach for Encoding First-order Logic
  Constraints",http://arxiv.org/abs/2309.15458v2,2023-09-27T07:52:30Z,2023-09-29T16:00:24Z,"  Integrating first-order logic constraints (FOLCs) with neural networks is a
crucial but challenging problem since it involves modeling intricate
correlations to satisfy the constraints. This paper proposes a novel neural
layer, LogicMP, whose layers perform mean-field variational inference over an
MLN. It can be plugged into any off-the-shelf neural network to encode FOLCs
while retaining modularity and efficiency. By exploiting the structure and
symmetries in MLNs, we theoretically demonstrate that our well-designed,
efficient mean-field iterations effectively mitigate the difficulty of MLN
inference, reducing the inference from sequential calculation to a series of
parallel tensor operations. Empirical results in three kinds of tasks over
graphs, images, and text show that LogicMP outperforms advanced competitors in
both performance and efficiency.
","['\nWeidi Xu\n', '\nJingwei Wang\n', '\nLele Xie\n', '\nJianshan He\n', '\nHongting Zhou\n', '\nTaifeng Wang\n', '\nXiaopei Wan\n', '\nJingdong Chen\n', '\nChao Qu\n', '\nWei Chu\n']","28 pages, 14 figures, 12 tables",,http://arxiv.org/abs/2309.15458v2,cs.AI,"['cs.AI', 'cs.SC']",,,[]
"Enhancing Zero-Shot Chain-of-Thought Reasoning in Large Language Models
  through Logic",http://arxiv.org/abs/2309.13339v2,2023-09-23T11:21:12Z,2024-02-29T07:26:00Z,"  Recent advancements in large language models have showcased their remarkable
generalizability across various domains. However, their reasoning abilities
still have significant room for improvement, especially when confronted with
scenarios requiring multi-step reasoning. Although large language models
possess extensive knowledge, their reasoning often fails to effectively utilize
this knowledge to establish a coherent thinking paradigm. These models
sometimes show hallucinations as their reasoning procedures are unconstrained
by logical principles. Aiming at improving the zero-shot chain-of-thought
reasoning ability of large language models, we propose LoT (Logical Thoughts)
prompting, a self-improvement framework that leverages principles rooted in
symbolic logic, particularly Reductio ad Absurdum, to systematically verify and
rectify the reasoning processes step by step. Experimental evaluations
conducted on language tasks in diverse domains, including arithmetic,
commonsense, symbolic, causal inference, and social problems, demonstrate the
efficacy of enhanced reasoning by logic.
","['\nXufeng Zhao\n', '\nMengdi Li\n', '\nWenhao Lu\n', '\nCornelius Weber\n', '\nJae Hee Lee\n', '\nKun Chu\n', '\nStefan Wermter\n']","Accepted in COLING 2024. Rename LogiCoT (previous version) to LoT
  (Logical Thoughts, current)",,http://arxiv.org/abs/2309.13339v2,cs.CL,"['cs.CL', 'cs.AI', 'cs.LG', 'cs.SC']",,,[]
Symmetric Functions over Finite Fields,http://arxiv.org/abs/2309.13804v1,2023-09-25T01:17:38Z,2023-09-25T01:17:38Z,"  The number of linear independent algebraic relations among elementary
symmetric polynomial functions over finite fields is computed. An algorithm
able to find all such relations is described. It is proved that the basis of
the ideal of algebraic relations found by the algorithm consists of polynomials
having coefficients in the prime field F_p.
",['\nMihai Prunescu\n'],"In Proceedings FROM 2023, arXiv:2309.12959","EPTCS 389, 2023, pp. 131-143",http://dx.doi.org/10.4204/EPTCS.389.11,cs.SC,"['cs.SC', 'cs.DM', 'math.CO', 'I.1.2']",10.4204/EPTCS.389.11,,[]
"Real-Time Emergency Vehicle Detection using Mel Spectrograms and Regular
  Expressions",http://arxiv.org/abs/2309.13920v2,2023-09-25T07:40:19Z,2024-01-27T02:09:17Z,"  In emergency situations, the high-speed movement of an ambulance through the
city streets can be hindered by vehicular traffic. This work presents a method
for detecting emergency vehicle sirens in real time. To obtain the audio
fingerprint of a Hi-Lo siren, DSP and signal symbolization techniques were
applied, which were contrasted against an audio classifier based on a deep
neural network, using the same 280 audios of ambient sounds and 52 Hi-Lo siren
audios dataset. In both methods, some classification accuracy metrics were
evaluated based on its confusion matrix, resulting in the DSP algorithm having
a slightly lower accuracy than the DNN model, however, it offers a
self-explanatory, adjustable, portable, high performance and lower energy and
consumption that makes it a more viable lower cost ADAS implementation to
identify Hi-Lo sirens in real time.
","['\nAlberto Pacheco-Gonzalez\n', '\nRaymundo Torres\n', '\nRaul Chacon\n', '\nIsidro Robledo\n']",in Spanish language,,http://arxiv.org/abs/2309.13920v2,cs.SD,"['cs.SD', 'cs.FL', 'cs.SC', 'eess.AS', 'I.5.5']",,,[]
On the Reduced Gröbner Bases of Blockwise Determinantal Ideals,http://arxiv.org/abs/2309.15035v1,2023-09-26T16:03:36Z,2023-09-26T16:03:36Z,"  Blockwise determinantal ideals are those generated by the union of all the
minors of specified sizes in certain blocks of a generic matrix, and they are
the natural generalization of many existing determinantal ideals like the
Schubert and ladder ones. In this paper we establish several criteria to verify
whether the Gr\""obner bases of blockwise determinantal ideals with respect to
(anti-)diagonal term orders are minimal or reduced. In particular, for Schubert
determinantal ideals, while all the elusive minors form the reduced Gr\""obner
bases when the defining permutations are vexillary, in the non-vexillary case
we derive an explicit formula for computing the reduced Gr\""obner basis from
elusive minors which avoids all algebraic operations. The fundamental
properties of being normal and strong for W-characteristic sets and
characteristic pairs, which are heavily connected to the reduced Gr\""obner
bases, of Schubert determinantal ideals are also proven.
","['\nChenqi Mou\n', '\nQiuye Song\n']",,,http://arxiv.org/abs/2309.15035v1,math.AC,"['math.AC', 'cs.SC', 'math.CO', '13P10 (Primary) 13C40, 05E14 (Secondary)']",,,[]
"HIVE: Scalable Hardware-Firmware Co-Verification using Scenario-based
  Decomposition and Automated Hint Extraction",http://arxiv.org/abs/2309.08002v1,2023-09-14T19:24:57Z,2023-09-14T19:24:57Z,"  Hardware-firmware co-verification is critical to design trustworthy systems.
While formal methods can provide verification guarantees, due to the complexity
of firmware and hardware, it can lead to state space explosion. There are
promising avenues to reduce the state space during firmware verification
through manual abstraction of hardware or manual generation of hints. Manual
development of abstraction or hints requires domain expertise and can be
time-consuming and error-prone, leading to incorrect proofs or inaccurate
results. In this paper, we effectively combine the scalability of
simulation-based validation and the completeness of formal verification. Our
proposed approach is applicable to actual firmware and hardware implementations
without requiring any manual intervention during formal model generation or
hint extraction. To reduce the state space complexity, we utilize both static
module-level analysis and dynamic execution of verification scenarios to
automatically generate system-level hints. These hints guide the underlying
solver to perform scalable equivalence checking using proofs. The extracted
hints are validated against the implementation before using them in the proofs.
Experimental evaluation on RISC-V based systems demonstrates that our proposed
framework is scalable due to scenario-based decomposition and automated hint
extraction. Moreover, our fully automated framework can identify complex bugs
in actual firmware-hardware implementations.
","['\nAruna Jayasena\n', '\nPrabhat Mishra\n']",,,http://arxiv.org/abs/2309.08002v1,cs.SC,"['cs.SC', 'cs.SE']",,,[]
"When to Trust AI: Advances and Challenges for Certification of Neural
  Networks",http://arxiv.org/abs/2309.11196v1,2023-09-20T10:31:09Z,2023-09-20T10:31:09Z,"  Artificial intelligence (AI) has been advancing at a fast pace and it is now
poised for deployment in a wide range of applications, such as autonomous
systems, medical diagnosis and natural language processing. Early adoption of
AI technology for real-world applications has not been without problems,
particularly for neural networks, which may be unstable and susceptible to
adversarial examples. In the longer term, appropriate safety assurance
techniques need to be developed to reduce potential harm due to avoidable
system failures and ensure trustworthiness. Focusing on certification and
explainability, this paper provides an overview of techniques that have been
developed to ensure safety of AI decisions and discusses future challenges.
","['\nMarta Kwiatkowska\n', '\nXiyue Zhang\n']",,,http://arxiv.org/abs/2309.11196v1,cs.LG,"['cs.LG', 'cs.AI', 'cs.CR', 'cs.SC']",,,[]
"FMplex: A Novel Method for Solving Linear Real Arithmetic Problems
  (Extended Version)",http://arxiv.org/abs/2309.03138v2,2023-09-06T16:22:01Z,2023-09-21T07:36:19Z,"  In this paper we introduce a novel quantifier elimination method for
conjunctions of linear real arithmetic constraints. Our algorithm is based on
the Fourier-Motzkin variable elimination procedure, but by case splitting we
are able to reduce the worst-case complexity from doubly to singly exponential.
The adaption of the procedure for SMT solving has strong correspondence to the
simplex algorithm, therefore we name it FMplex. Besides the theoretical
foundations, we provide an experimental evaluation in the context of SMT
solving.
","['\nJasper Nalbach\n', '\nValentin Promies\n', '\nErika Ábrahám\n', '\nPaul Kobialka\n']",,,http://arxiv.org/abs/2309.03138v2,cs.SC,['cs.SC'],,,[]
"Partial Proof of a Conjecture with Implications for Spectral
  Majorization",http://arxiv.org/abs/2309.01302v1,2023-09-04T01:02:19Z,2023-09-04T01:02:19Z,"  In this paper we report on new results relating to a conjecture regarding
properties of $n\times n$, $n\leq 6$, positive definite matrices. The
conjecture has been proven for $n\leq 4$ using computer-assisted sum of squares
(SoS) methods for proving polynomial nonnegativity. Based on these proven
cases, we report on the recent identification of a new family of matrices with
the property that their diagonals majorize their spectrum. We then present new
results showing that this family can extended via Kronecker composition to
$n>6$ while retaining the special majorization property. We conclude with
general considerations on the future of computer-assisted and AI-based proofs.
",['\nJeffrey Uhlmann\n'],,,http://arxiv.org/abs/2309.01302v1,cs.SC,"['cs.SC', 'cs.AI']",,,[]
Declarative Reasoning on Explanations Using Constraint Logic Programming,http://arxiv.org/abs/2309.00422v1,2023-09-01T12:31:39Z,2023-09-01T12:31:39Z,"  Explaining opaque Machine Learning (ML) models is an increasingly relevant
problem. Current explanation in AI (XAI) methods suffer several shortcomings,
among others an insufficient incorporation of background knowledge, and a lack
of abstraction and interactivity with the user. We propose REASONX, an
explanation method based on Constraint Logic Programming (CLP). REASONX can
provide declarative, interactive explanations for decision trees, which can be
the ML models under analysis or global/local surrogate models of any black-box
model. Users can express background or common sense knowledge using linear
constraints and MILP optimization over features of factual and contrastive
instances, and interact with the answer constraints at different levels of
abstraction through constraint projection. We present here the architecture of
REASONX, which consists of a Python layer, closer to the user, and a CLP layer.
REASONX's core execution engine is a Prolog meta-program with declarative
semantics in terms of logic theories.
","['\nLaura State\n', '\nSalvatore Ruggieri\n', '\nFranco Turini\n']",European Conference on Logics in Artificial Intelligence (JELIA 2023),,http://arxiv.org/abs/2309.00422v1,cs.AI,"['cs.AI', 'cs.CY', 'cs.LG', 'cs.SC']",,,[]
"Verifying the Unknown: Correct-by-Design Control Synthesis for Networks
  of Stochastic Uncertain Systems",http://arxiv.org/abs/2309.01276v1,2023-09-03T21:57:34Z,2023-09-03T21:57:34Z,"  In this paper, we present an approach for designing correct-by-design
controllers for cyber-physical systems composed of multiple dynamically
interconnected uncertain systems. We consider networked discrete-time uncertain
nonlinear systems with additive stochastic noise and model parametric
uncertainty. Such settings arise when multiple systems interact in an uncertain
environment and only observational data is available. We address two
limitations of existing approaches for formal synthesis of controllers for
networks of uncertain systems satisfying complex temporal specifications.
Firstly, whilst existing approaches rely on the stochasticity to be Gaussian,
the heterogeneous nature of composed systems typically yields a more complex
stochastic behavior. Secondly, exact models of the systems involved are
generally not available or difficult to acquire. To address these challenges,
we show how abstraction-based control synthesis for uncertain systems based on
sub-probability couplings can be extended to networked systems. We design
controllers based on parameter uncertainty sets identified from observational
data and approximate possibly arbitrary noise distributions using Gaussian
mixture models whilst quantifying the incurred stochastic coupling. Finally, we
demonstrate the effectiveness of our approach on a nonlinear package delivery
case study with a complex specification, and a platoon of cars.
","['\nOliver Schön\n', '\nBirgit van Huijgevoort\n', '\nSofie Haesaert\n', '\nSadegh Soudjani\n']","9 pages, 4 figures, accepted to CDC 2023",,http://arxiv.org/abs/2309.01276v1,eess.SY,"['eess.SY', 'cs.LO', 'cs.SC', 'cs.SY']",,,[]
Cognitive Architectures for Language Agents,http://arxiv.org/abs/2309.02427v2,2023-09-05T17:56:20Z,2023-09-27T15:27:25Z,"  Recent efforts have augmented large language models (LLMs) with external
resources (e.g., the Internet) or internal control flows (e.g., prompt
chaining) for tasks requiring grounding or reasoning, leading to a new class of
language agents. While these agents have achieved substantial empirical
success, we lack a systematic framework to organize existing agents and plan
future developments. In this paper, we draw on the rich history of cognitive
science and symbolic artificial intelligence to propose Cognitive Architectures
for Language Agents (CoALA). CoALA describes a language agent with modular
memory components, a structured action space to interact with internal memory
and external environments, and a generalized decision-making process to choose
actions. We use CoALA to retrospectively survey and organize a large body of
recent work, and prospectively identify actionable directions towards more
capable agents. Taken together, CoALA contextualizes today's language agents
within the broader history of AI and outlines a path towards language-based
general intelligence.
","['\nTheodore R. Sumers\n', '\nShunyu Yao\n', '\nKarthik Narasimhan\n', '\nThomas L. Griffiths\n']","v2 enriched actionable insights and discussions, and polished
  abstract and introduction. 18 pages of main content, 12 pages of references,
  5 figures. The first two authors contributed equally, order decided by coin
  flip. A CoALA-based repo of recent work on language agents:
  https://github.com/ysymyth/awesome-language-agents",,http://arxiv.org/abs/2309.02427v2,cs.AI,"['cs.AI', 'cs.CL', 'cs.LG', 'cs.SC']",,,[]
"Presenting the SWTC: A Symbolic Corpus of Themes from John Williams'
  Star Wars Episodes I-IX",http://arxiv.org/abs/2309.03298v1,2023-09-06T18:21:55Z,2023-09-06T18:21:55Z,"  This paper presents a new symbolic corpus of musical themes from the complete
Star Wars trilogies (Episodes I-IX) by John Williams. The corpus files are made
available in multiple formats (.krn, .sib, and .musicxml) and include melodic,
harmonic, and formal information. The Star Wars Thematic Corpus (SWTC) contains
a total of 64 distinctive, recurring, and symbolically meaningful themes and
motifs, commonly referred to as leitmotifs. Through this corpus we also
introduce a new humdrum standard for non-functional harmony encodings, **harte,
based on Harte (2005, 2010). This report details the motivation, describes the
transcription and encoding processes, and provides some brief summary
statistics. While relatively small in scale, the SWTC represents a unified
collection from one of the most prolific and influential composers of the 20th
century, and the under-studied subset of film and multimedia musical material
in general. We hope the SWTC will provide insights into John Williams'
compositional style, as well as prove useful in comparisons against other
thematic corpora from film and beyond.
","['\nClaire Arthur\n', '\nFrank Lehman\n', '\nJohn McNamara\n']",Corpus report (5000 words),,http://arxiv.org/abs/2309.03298v1,cs.SD,"['cs.SD', 'cs.SC', 'eess.AS']",,,[]
Incremental Property Directed Reachability,http://arxiv.org/abs/2308.12162v1,2023-08-23T14:23:48Z,2023-08-23T14:23:48Z,"  Property Directed Reachability (PDR) is a widely used technique for formal
verification of hardware and software systems. This paper presents an
incremental version of PDR (IPDR), which enables the automatic verification of
system instances of incremental complexity. The proposed algorithm leverages
the concept of incremental SAT solvers to reuse verification results from
previously verified system instances, thereby accelerating the verification
process. The new algorithm supports both incremental constraining and relaxing;
i.e., starting from an over-constrained instance that is gradually relaxed.
  To validate the effectiveness of the proposed algorithm, we implemented IPDR
and experimentally evaluate it on two different problem domains. First, we
consider a circuit pebbling problem, where the number of pebbles is both
constrained and relaxed. Second, we explore parallel program instances,
progressively increasing the allowed number of interleavings. The experimental
results demonstrate significant performance improvements compared to Z3's PDR
implementation SPACER. Experiments also show that the incremental approach
succeeds in reusing a substantial amount of clauses between instances, for both
the constraining and relaxing algorithm.
","['\nMax Blankestijn\n', '\nAlfons Laarman\n']",,,http://arxiv.org/abs/2308.12162v1,cs.SC,"['cs.SC', 'cs.LO', 'B.8.1; I.1.2']",,,[]
Inferring Compensatory Kinase Networks in Yeast using Prolog,http://arxiv.org/abs/2308.16309v1,2023-08-30T20:29:41Z,2023-08-30T20:29:41Z,"  Signalling pathways are conserved across different species, therefore making
yeast a model organism to study these via disruption of kinase activity. Yeast
has 159 genes that encode protein kinases and phosphatases, and 136 of these
have counterparts in humans. Therefore any insight in this model organism could
potentially offer indications of mechanisms of action in the human kinome. The
study utilises a Prolog-based approach, data from a yeast kinase deletions
strains study and publicly available kinase-protein associations. Prolog, a
programming language that is well-suited for symbolic reasoning is used to
reason over the data and infer compensatory kinase networks. This approach is
based on the idea that when a kinase is knocked out, other kinases may
compensate for this loss of activity. Background knowledge on kinases targeting
proteins is used to guide the analysis. This knowledge is used to infer the
potential compensatory interactions between kinases based on the changes in
phosphorylation observed in the phosphoproteomics data from the yeast study.
The results demonstrate the effectiveness of the Prolog-based approach in
analysing complex cell signalling mechanisms in yeast. The inferred
compensatory kinase networks provide new insights into the regulation of cell
signalling in yeast and may aid in the identification of potential therapeutic
targets for modulating signalling pathways in yeast and other organisms.
","['\nGeorge A. Elder\nQueen Mary University of London\n', '\nConrad Bessant\nQueen Mary University of London\n']","In Proceedings ICLP 2023, arXiv:2308.14898","EPTCS 385, 2023, pp. 260-273",http://dx.doi.org/10.4204/EPTCS.385.26,q-bio.MN,"['q-bio.MN', 'cs.SC']",10.4204/EPTCS.385.26,,"['Queen Mary University of London', 'Queen Mary University of London']"
Normative Conditional Reasoning as a Fragment of HOL,http://arxiv.org/abs/2308.10686v3,2023-08-21T12:47:30Z,2024-02-29T13:10:16Z,"  We report on the mechanization of (preference-based) conditional normative
reasoning. Our focus is on Aqvist's system E for conditional obligation, and
its extensions. Our mechanization is achieved via a shallow semantical
embedding in Isabelle/HOL. We consider two possible uses of the framework. The
first one is as a tool for meta-reasoning about the considered logic. We employ
it for the automated verification of deontic correspondences (broadly
conceived) and related matters, analogous to what has been previously achieved
for the modal logic cube. The equivalence is automatically verified in one
direction, leading from the property to the axiom. The second use is as a tool
for assessing ethical arguments. We provide a computer encoding of a well-known
paradox (or impossibility theorem) in population ethics, Parfit's repugnant
conclusion. While some have proposed overcoming the impossibility theorem by
abandoning the presupposed transitivity of ''better than'', our formalisation
unveils a less extreme approach, suggesting among other things the option of
weakening transitivity suitably rather than discarding it entirely. Whether the
presented encoding increases or decreases the attractiveness and persuasiveness
of the repugnant conclusion is a question we would like to pass on to
philosophy and ethics.
","['\nXavier Parent\n', '\nChristoph Benzmüller\n']","30 pages, 34 figures, 3 tables",,http://arxiv.org/abs/2308.10686v3,cs.LO,"['cs.LO', 'cs.AI', 'cs.SC', '03B60, 03B15, 68T27, 68T30, 68T15', 'I.2.3; I.2.4; I.2.0; F.4']",,,[]
Algebraic power series and their automatic complexity I: finite fields,http://arxiv.org/abs/2308.10977v1,2023-08-21T18:48:24Z,2023-08-21T18:48:24Z,"  Christol's theorem states that a power series with coefficients in a finite
field is algebraic if and only if its coefficient sequence is automatic. A
natural question is how the size of a polynomial describing such a sequence
relates to the size of an automaton describing the same sequence. Bridy used
tools from algebraic geometry to bound the size of the minimal automaton for a
sequence, given its minimal polynomial. We produce a new proof of Bridy's bound
by embedding algebraic sequences as diagonals of rational functions. Crucially
for our interests, our approach can be adapted to work not just over a finite
field but over the integers modulo $p^\alpha$.
","['\nEric Rowland\n', '\nManon Stipulanti\n', '\nReem Yassawi\n']","29 pages, 2 figures, 2 tables",,http://arxiv.org/abs/2308.10977v1,math.NT,"['math.NT', 'cs.FL', 'cs.SC', '11B85, 13F25']",,,[]
Proceedings 39th International Conference on Logic Programming,http://arxiv.org/abs/2308.14898v1,2023-08-28T20:46:59Z,2023-08-28T20:46:59Z,"  This volume contains the Technical Communications presented at the 39th
International Conference on Logic Programming (ICLP 2023), held at Imperial
College London, UK from July 9 to July 15, 2023. Technical Communications
included here concern the Main Track, the Doctoral Consortium, the Application
and Systems/Demo track, the Recently Published Research Track, the
Birds-of-a-Feather track, the Thematic Tracks on Logic Programming and Machine
Learning, and Logic Programming and Explainability, Ethics, and
Trustworthiness.
","['\nEnrico Pontelli\nNew Mexico State University, USA\n', ""\nStefania Costantini\nUniversity of L'Aquila, Italy\n"", '\nCarmine Dodaro\nUniversity of Calabria, Italy\n', '\nSarah Gaggl\nTU Dresden, Germany\n', '\nRoberta Calegari\nUniversity of Bologna, Italy\n', ""\nArtur D'Avila Garcez\nCity University of London, UK\n"", '\nFrancesco Fabiano\nUniversity of Udine, Italy\n', '\nAlessandra Mileo\nDCU, Ireland\n', '\nAlessandra Russo\nImperial College London, UK\n', '\nFrancesca Toni\nImperial College London, UK\n']",,"EPTCS 385, 2023",http://dx.doi.org/10.4204/EPTCS.385,cs.AI,"['cs.AI', 'cs.LO', 'cs.PL', 'cs.SC']",10.4204/EPTCS.385,,"['New Mexico State University, USA', ""University of L'Aquila, Italy"", 'University of Calabria, Italy', 'TU Dresden, Germany', 'University of Bologna, Italy', 'City University of London, UK', 'University of Udine, Italy', 'DCU, Ireland', 'Imperial College London, UK', 'Imperial College London, UK']"
Deep Inductive Logic Programming meets Reinforcement Learning,http://arxiv.org/abs/2308.16210v1,2023-08-30T09:08:46Z,2023-08-30T09:08:46Z,"  One approach to explaining the hierarchical levels of understanding within a
machine learning model is the symbolic method of inductive logic programming
(ILP), which is data efficient and capable of learning first-order logic rules
that can entail data behaviour. A differentiable extension to ILP, so-called
differentiable Neural Logic (dNL) networks, are able to learn Boolean functions
as their neural architecture includes symbolic reasoning. We propose an
application of dNL in the field of Relational Reinforcement Learning (RRL) to
address dynamic continuous environments. This represents an extension of
previous work in applying dNL-based ILP in RRL settings, as our proposed model
updates the architecture to enable it to solve problems in continuous RL
environments. The goal of this research is to improve upon current ILP methods
for use in RRL by incorporating non-linear continuous predicates, allowing RRL
agents to reason and make decisions in dynamic and continuous environments.
","['\nAndreas Bueff\nUniversity of Edinburgh\n', '\nVaishak Belle\nUniversity of Edinburgh\n']","In Proceedings ICLP 2023, arXiv:2308.14898","EPTCS 385, 2023, pp. 339-352",http://dx.doi.org/10.4204/EPTCS.385.37,cs.LG,"['cs.LG', 'cs.LO', 'cs.SC']",10.4204/EPTCS.385.37,,"['University of Edinburgh', 'University of Edinburgh']"
"Computing Mellin representations and asymptotics of nested binomial sums
  in a symbolic way: the RICA package",http://arxiv.org/abs/2308.06042v1,2023-08-11T09:43:05Z,2023-08-11T09:43:05Z,"  Nested binomial sums form a particular class of sums that arise in the
context of particle physics computations at higher orders in perturbation
theory within QCD and QED, but that are also mathematically relevant, e.g., in
combinatorics. We present the package RICA (Rule Induced Convolutions for
Asymptotics), which aims at calculating Mellin representations and asymptotic
expansions at infinity of those objects. These representations are of
particular interest to perform analytic continuations of such sums.
","['\nJohannes Bluemlein\n', '\nNikolai Fadeev\n', '\nCarsten Schneider\n']",,"ACM Communications in Computer Algebra, Vol. 57, No. 2, Issue 224,
  June 2023",http://dx.doi.org/10.1145/3614408.3614410,hep-ph,"['hep-ph', 'cs.SC']",10.1145/3614408.3614410,,[]
"Computational General Relativity in the Wolfram Language using Gravitas
  I: Symbolic and Analytic Computation",http://arxiv.org/abs/2308.07508v1,2023-08-15T00:24:06Z,2023-08-15T00:24:06Z,"  We introduce a new, open-source computational general relativity framework
for the Wolfram Language called Gravitas, which boasts a number of novel and
distinctive features as compared to the many pre-existing computational and
numerical relativity frameworks currently available within the open-source
community. These include, but are not limited to: seamless integration of its
powerful symbolic and numerical subsystems, and, by extension, seamless
transition between analytic/continuous representations and numerical/discrete
representations of arbitrary spacetime geometries; highly modular, general and
extensible representations of spacetime geometries, spacetime topologies, gauge
conditions, coordinate systems, matter fields, evolution equations and initial
data; ability to set up and run complex numerical relativity simulations, and
to perform 2D and 3D visualizations, symbolic computations and numerical
analysis (including the extraction of gravitational wave signals) on the
resulting data, all from within a single notebook environment; and a
totally-unstructured adaptive refinement scheme based on hypergraph rewriting,
allowing for exceedingly efficient discretization and numerical evolution of
Cauchy initial data for a wide range of challenging computational problems
involving strong relativistic field dynamics. In this first in a series of two
articles covering the framework, we focus on the design and capabilities of
Gravitas's symbolic subsystem, including its general and flexible handling of
arbitrary geometries parametrized by arbitrary curvilinear coordinate systems
(along with an in-built library of standard metrics and coordinate conditions),
as well as its various high-level tensor calculus and differential geometry
features. We proceed to show how this subsystem can be used to solve the
Einstein field equations both analytically and numerically.
",['\nJonathan Gorard\n'],"86 pages, 74 figures",,http://arxiv.org/abs/2308.07508v1,gr-qc,"['gr-qc', 'cs.SC']",,,[]
"AI Hilbert: A New Paradigm for Scientific Discovery by Unifying Data and
  Background Knowledge",http://arxiv.org/abs/2308.09474v2,2023-08-18T11:19:41Z,2023-09-23T11:50:07Z,"  The discovery of scientific formulae that parsimoniously explain natural
phenomena and align with existing background theory is a key goal in science.
Historically, scientists have derived natural laws by manipulating equations
based on existing knowledge, forming new equations, and verifying them
experimentally. In recent years, data-driven scientific discovery has emerged
as a viable competitor in settings with large amounts of experimental data.
Unfortunately, data-driven methods often fail to discover valid laws when data
is noisy or scarce. Accordingly, recent works combine regression and reasoning
to eliminate formulae inconsistent with background theory. However, the problem
of searching over the space of formulae consistent with background theory to
find one that fits the data best is not well-solved. We propose a solution to
this problem when all axioms and scientific laws are expressible via polynomial
equalities and inequalities and argue that our approach is widely applicable.
We further model notions of minimal complexity using binary variables and
logical constraints, solve polynomial optimization problems via mixed-integer
linear or semidefinite optimization, and prove the validity of our scientific
discoveries in a principled manner using Positivestellensatz certificates.
Remarkably, the optimization techniques leveraged in this paper allow our
approach to run in polynomial time with fully correct background theory, or
non-deterministic polynomial (NP) time with partially correct background
theory. We demonstrate that some famous scientific laws, including Kepler's
Third Law of Planetary Motion, the Hagen-Poiseuille Equation, and the Radiated
Gravitational Wave Power equation, can be derived in a principled manner from
background axioms and experimental data.
","['\nRyan Cory-Wright\n', '\nBachir El Khadir\n', '\nCristina Cornelio\n', '\nSanjeeb Dash\n', '\nLior Horesh\n']","Slightly revised from version 1, in particular polished the figures",,http://arxiv.org/abs/2308.09474v2,cs.AI,"['cs.AI', 'cs.SC', 'math.OC']",,,[]
Field theory with the Maxima computer algebra system,http://arxiv.org/abs/2308.09837v1,2023-08-18T22:12:18Z,2023-08-18T22:12:18Z,"  The Maxima computer algebra system, the open-source successor to MACSYMA, the
first general-purpose computer algebra system that was initially developed at
the Massachusetts Institute of Technology in the late 1960s and later
distributed by the United States Department of Energy, has some remarkable
capabilities, some of which are implemented in the form of add-on, ""share""
packages that are distributed along with the core Maxima system. One such share
package is itensor, for indicial tensor manipulation. One of the more
remarkable features of itensor is functional differentiation. Through this, it
is possible to use itensor to develop a Lagrangian field theory and derive the
corresponding field equations. In the present note, we demonstrate this
capability by deriving Maxwell's equations from the Maxwell Lagrangian, and
exploring the properties of the system, including current conservation.
",['\nViktor T. Toth\n'],6 pages,,http://arxiv.org/abs/2308.09837v1,cs.SC,"['cs.SC', 'gr-qc', 'physics.comp-ph']",,,[]
Iterated Resultants in CAD,http://arxiv.org/abs/2307.16750v1,2023-07-31T15:16:48Z,2023-07-31T15:16:48Z,"  Cylindrical Algebraic Decomposition (CAD) by projection and lifting requires
many iterated univariate resultants. It has been observed that these often
factor, but to date this has not been used to optimise implementations of CAD.
We continue the investigation into such factorisations, writing in the specific
context of SC-Square.
","['\nJames H. Davenport\n', '\nMatthew England\n']",Presented at the 2023 SC-Square Workshop,"Proceedings of the 8th Workshop on Satisfiability Checking and
  Symbolic Computation (SC2 '23), E. \'Abrah\'am and T. Sturm eds. CEUR
  Workshop Proceedings 3455, pp. 54-60, 2023",http://arxiv.org/abs/2307.16750v1,cs.SC,"['cs.SC', '68W30, 13P10', 'I.1.1']",,,[]
Algoritmos para Multiplicação Matricial,http://arxiv.org/abs/2309.00628v1,2023-08-03T23:20:20Z,2023-08-03T23:20:20Z,"  The goal of this article is to study algorithms that compute the product
between two matrixes, specifically using the ingenuous methods of Strassen and
Strassen-Winograd, which will be presented in Section 2. At present, the cited
methods are not the most optimal considering the arithmetic complexity of these
algorithms (see Table 1). However, changes to the Strassen and
Strassen-Winograd methods will be exposed which will result in a reduction in
their memory allocation and/or execution time. The algorithms in this study
were implemented using the Julia programming language, version 1.9.1, with the
aid of the packages Pluto (notebooks), Plots (graphic visualization of the
results) and BenchmarkTools (measurement of memory allocation and execution
time of the algorithms).
  --
  O objetivo deste artigo \'e estudar algoritmos que computam o produto entre
duas matrizes, mais especificamente utilizando os m\'etodos ing\^enuo, de
Strassen e de Strassen-Winograd, que ser\~ao apresentados na Se\c{c}\~ao 2.
Atualmente, os m\'etodos citados n\~ao s\~ao os mais otimizados considerando a
complexidade aritm\'etica de seus algoritmos (vide Tabela 1). No entanto,
ser\~ao expostas modifica\c{c}\~oes dos m\'etodos de Strassen e
Strassen-Winograd que conseguem reduzir sua aloca\c{c}\~ao de mem\'oria e/ou
tempo de execu\c{c}\~ao. Os algoritmos do problema em estudo foram
implementados utilizando a linguagem de programa\c{c}\~ao Julia, na vers\~ao
1.9.1, com o aux\'ilio dos pacotes Pluto (notebooks), Plots (visualiza\c{c}\~ao
gr\'afica dos resultados) e BenchmarkTools (medi\c{c}\~ao de aloca\c{c}\~ao de
mem\'oria e tempo de execu\c{c}\~ao dos algoritmos).
","['\nM. S. O. Poloi\n', '\nT. O. Quinelato\n']",in Portuguese language,,http://arxiv.org/abs/2309.00628v1,cs.SC,['cs.SC'],,,[]
SMT-Solving Induction Proofs of Inequalities,http://arxiv.org/abs/2307.16761v1,2023-07-31T15:32:16Z,2023-07-31T15:32:16Z,"  This paper accompanies a new dataset of non-linear real arithmetic problems
for the SMT-LIB benchmark collection. The problems come from an automated proof
procedure of Gerhold--Kauers, which is well suited for solution by SMT. The
problems of this type have not been tackled by SMT-solvers before. We describe
the proof technique and give one new such proof to illustrate it. We then
describe the dataset and the results of benchmarking. The benchmarks on the new
dataset are quite different to the existing ones. The benchmarking also brings
forward some interesting debate on the use/inclusion of rational functions and
algebraic numbers in the SMT-LIB.
","['\nAli K. Uncu\n', '\nJames H. Davenport\n', '\nMatthew England\n']",Presented at the 2022 SC-Square Workshop,"Proceedings of the 7th Workshop on Satisfiability Checking and
  Symbolic Computation (SC2 '22), A. Uncu and H. Barbosa eds. CEUR Workshop
  Proceedings 3458, pp. 10-24, 2023",http://arxiv.org/abs/2307.16761v1,cs.SC,"['cs.SC', 'cs.LO', '68W30', 'I.1.4; G.4']",,,[]
"Algorithmic computation of multivector inverses and characteristic
  polynomials in non-degenerate Clifford algebras",http://arxiv.org/abs/2308.02291v1,2023-08-04T12:51:55Z,2023-08-04T12:51:55Z,"  The power of Clifford or, geometric, algebra lies in its ability to represent
geometric operations in a concise and elegant manner. Clifford algebras provide
the natural generalizations of complex, dual numbers and quaternions into
non-commutative multivectors. The paper demonstrates an algorithm for the
computation of inverses of such numbers in a non-degenerate Clifford algebra of
an arbitrary dimension. The algorithm is a variation of the
Faddeev-LeVerrier-Souriau algorithm and is implemented in the open-source
Computer Algebra System Maxima. Symbolic and numerical examples in different
Clifford algebras are presented.
",['\nDimiter Prodanov\n'],11 pages. arXiv admin note: text overlap with arXiv:1904.00084,,http://arxiv.org/abs/2308.02291v1,math.NA,"['math.NA', 'cs.NA', 'cs.SC', '15A66']",,,[]
"New Bounds on Quotient Polynomials with Applications to Exact
  Divisibility and Divisibility Testing of Sparse Polynomials",http://arxiv.org/abs/2308.03885v1,2023-08-07T19:33:53Z,2023-08-07T19:33:53Z,"  A sparse polynomial (also called a lacunary polynomial) is a polynomial that
has relatively few terms compared to its degree. The sparse-representation of a
polynomial represents the polynomial as a list of its non-zero terms
(coefficient-degree pairs). In particular, the degree of a sparse polynomial
can be exponential in the sparse-representation size.
  We prove that for monic polynomials $f, g \in \mathbb{C}[x]$ such that $g$
divides $f$, the $\ell_2$-norm of the quotient polynomial $f/g$ is bounded by
$\lVert f \rVert_1 \cdot \tilde{O}(\lVert{g}\rVert_0^3\text{deg}^2{
f})^{\lVert{g}\rVert_0 - 1}$. This improves upon the exponential (in
$\text{deg}{ f}$) bounds for general polynomials and implies that the trivial
long division algorithm runs in time quasi-linear in the input size and number
of terms of the quotient polynomial $f/g$, thus solving a long-standing problem
on exact divisibility of sparse polynomials.
  We also study the problem of bounding the number of terms of $f/g$ in some
special cases. When $f, g \in \mathbb{Z}[x]$ and $g$ is a cyclotomic-free
(i.e., it has no cyclotomic factors) trinomial, we prove that
$\lVert{f/g}\rVert_0 \leq O(\lVert{f}\rVert_0 \text{size}({f})^2 \cdot
\log^6{\text{deg}{ g}})$. When $g$ is a binomial with $g(\pm 1) \neq 0$, we
prove that the sparsity is at most $O(\lVert{f}\rVert_0 (
\log{\lVert{f}\rVert_0} + \log{\lVert{f}\rVert_{\infty}}))$. Both upper bounds
are polynomial in the input-size. We leverage these results and give a
polynomial time algorithm for deciding whether a cyclotomic-free trinomial
divides a sparse polynomial over the integers.
  As our last result, we present a polynomial time algorithm for testing
divisibility by pentanomials over small finite fields when $\text{deg}{ f} =
\tilde{O}(\text{deg}{ g})$.
","['\nIdo Nahshon\n', '\nAmir Shpilka\n']",,,http://arxiv.org/abs/2308.03885v1,cs.SC,"['cs.SC', 'cs.CC', 'math.NT']",,,[]
Model of models -- Part 1,http://arxiv.org/abs/2308.04600v2,2023-08-08T21:56:52Z,2023-10-24T10:22:44Z,"  This paper proposes a new cognitive model, acting as the main component of an
AGI agent. The model is introduced in its mature intelligence state, and as an
extension of previous models, DENN, and especially AKREM, by including
operational models (frames/classes) and will. This model's core assumption is
that cognition is about operating on accumulated knowledge, with the guidance
of an appropriate will. Also, we assume that the actions, part of knowledge,
are learning to be aligned with will, during the evolution phase that precedes
the mature intelligence state. In addition, this model is mainly based on the
duality principle in every known intelligent aspect, such as exhibiting both
top-down and bottom-up model learning, generalization verse specialization, and
more. Furthermore, a holistic approach is advocated for AGI designing, and
cognition under constraints or efficiency is proposed, in the form of
reusability and simplicity. Finally, reaching this mature state is described
via a cognitive evolution from infancy to adulthood, utilizing a consolidation
principle. The final product of this cognitive model is a dynamic operational
memory of models and instances. Lastly, some examples and preliminary ideas for
the evolution phase to reach the mature state are presented.
",['\nShimon Komarovsky\n'],arXiv admin note: text overlap with arXiv:2301.13556,,http://arxiv.org/abs/2308.04600v2,cs.AI,"['cs.AI', 'cs.LO', 'cs.SC']",,,[]
In-place accumulation of fast multiplication formulae,http://arxiv.org/abs/2307.12712v2,2023-07-24T11:47:29Z,2024-01-16T09:19:30Z,"  This paper deals with simultaneously fast and in-place algorithms for
formulae where the result has to be linearly accumulated: some of the output
variables are also input variables, linked by a linear dependency. Fundamental
examples include the in-place accumulated multiplication of polynomials or
matrices, C+=AB. The difficulty is to combine in-place computations with fast
algorithms: those usually come at the expense of (potentially large) extra
temporary space, but with accumulation the output variables are not even
available to store intermediate values. We first propose a novel automatic
design of fast and in-place accumulating algorithms for any bilinear formulae
(and thus for polynomial and matrix multiplication) and then extend it to any
linear accumulation of a collection of functions. For this, we relax the
in-place model to any algorithm allowed to modify its inputs, provided that
those are restored to their initial state afterwards. This allows us, in fine,
to derive unprecedented in-place accumulating algorithms for fast polynomial
multiplications and for Strassen-like matrix multiplications.
","['\nJean-Guillaume Dumas\nCASC\n', '\nBruno Grenet\nCASC\n']",,,http://arxiv.org/abs/2307.12712v2,cs.SC,['cs.SC'],,,"['CASC', 'CASC']"
The free Abelian group in R: the frab package,http://arxiv.org/abs/2307.13184v1,2023-07-25T00:31:40Z,2023-07-25T00:31:40Z,"  In this short article I introduce the frab package which provides an
alternative interpretation of named vectors in the R programming language; it
is available on CRAN. The underlying mathematical object is the free Abelian
group.
",['\nRobin K. S. Hankin\n'],9 pages,,http://arxiv.org/abs/2307.13184v1,cs.SC,"['cs.SC', '08.04', 'G.4']",,,[]
"Rational Solutions of Parametric First-Order Algebraic Differential
  Equations",http://arxiv.org/abs/2307.05102v1,2023-07-11T08:24:25Z,2023-07-11T08:24:25Z,"  In this paper we give a procedure for finding rational solutions of a given
first-order ODE with functional and constant coefficients which occur in a
rational way. We derive an associated system with the same solvability, and
sufficient and necessary conditions for the existence of rational solutions are
given. In the case where all parametric coefficients are constant, we give an
algorithm to compute the rational solutions. In the case where one functional
coefficient appears, we algorithmically find rational general solutions which
rationally depend on the appearing transcendental constant. In the other cases,
the presented procedure is not completely algorithmic.
","['\nSebastian Falkensteiner\n', '\nRafael Sendra\n']",,,http://arxiv.org/abs/2307.05102v1,cs.SC,"['cs.SC', '34A05, 14H50, 34A34, 30C15, 35B30']",,,[]
A Program That Simplifies Regular Expressions (Tool paper),http://arxiv.org/abs/2307.06436v1,2023-07-12T20:03:22Z,2023-07-12T20:03:22Z,"  This paper presents the main features of a system that aims to transform
regular expressions into shorter equivalent expressions. The system is also
capable of computing other operations useful for simplification, such as
checking the inclusion of regular languages. The main novelty of this work is
that it combines known but distinct ways of representing regular languages into
a global unified data structure that makes the operations more efficient. In
addition, representations of regular languages are dynamically reduced as
operations are performed on them. Expressions are normalized and represented by
a unique identifier (an integer). Expressions found to be equivalent (i.e.
denoting the same regular language) are grouped into equivalence classes from
which a shortest representative is chosen. The article briefly describes the
main algorithms working on the global data structure. Some of them are direct
adaptations of well-known algorithms, but most of them incorporate new ideas,
which are really necessary to make the system efficient. Finally, to show its
usefulness, the system is applied to some examples from the literature.
Statistics on randomly generated sets of expressions are also provided.
",['\nBaudouin Le Charlier\n'],rejected at ATVA 2023,,http://arxiv.org/abs/2307.06436v1,cs.SC,['cs.SC'],,,[]
"Reduction-Based Creative Telescoping for Definite Summation of D-finite
  Functions",http://arxiv.org/abs/2307.07216v2,2023-07-14T08:20:15Z,2023-11-20T12:45:28Z,"  Creative telescoping is an algorithmic method initiated by Zeilberger to
compute definite sums by synthesizing summands that telescope, called
certificates. We describe a creative telescoping algorithm that computes
telescopers for definite sums of D-finite functions as well as the associated
certificates in a compact form. The algorithm relies on a discrete analogue of
the generalized Hermite reduction, or equivalently, a generalization of the
Abramov-Petkov\v{s}ek reduction. We provide a Maple implementation with good
timings on a variety of examples.
","['\nHadrien Brochet\n', '\nBruno Salvy\n']",15 pages,,http://arxiv.org/abs/2307.07216v2,cs.SC,['cs.SC'],,,[]
"An Efficient Canonical Narrowing Implementation with Irreducibility and
  SMT Constraints for Generic Symbolic Protocol Analysis",http://arxiv.org/abs/2307.06348v1,2023-07-12T17:52:00Z,2023-07-12T17:52:00Z,"  Narrowing and unification are very useful tools for symbolic analysis of
rewrite theories, and thus for any model that can be specified in that way. A
very clear example of their application is the field of formal cryptographic
protocol analysis, which is why narrowing and unification are used in tools
such as Maude-NPA, Tamarin and Akiss. In this work we present the
implementation of a canonical narrowing algorithm, which improves the standard
narrowing algorithm, extended to be able to process rewrite theories with
conditional rules. The conditions of the rules will contain SMT constraints,
which will be carried throughout the execution of the algorithm to determine if
the solutions have associated satisfiable or unsatisfiable constraints, and in
the latter case, discard them.
","['\nRaúl López-Rueda\n', '\nSantiago Escobar\n', '\nJulia Sapiña\n']","41 pages, 7 tables, 1 algorithm, 9 examples",,http://arxiv.org/abs/2307.06348v1,cs.SC,"['cs.SC', 'cs.LO']",,,[]
Data Augmentation for Mathematical Objects,http://arxiv.org/abs/2307.06984v1,2023-07-13T16:02:45Z,2023-07-13T16:02:45Z,"  This paper discusses and evaluates ideas of data balancing and data
augmentation in the context of mathematical objects: an important topic for
both the symbolic computation and satisfiability checking communities, when
they are making use of machine learning techniques to optimise their tools. We
consider a dataset of non-linear polynomial problems and the problem of
selecting a variable ordering for cylindrical algebraic decomposition to tackle
these with. By swapping the variable names in already labelled problems, we
generate new problem instances that do not require any further labelling when
viewing the selection as a classification problem. We find this augmentation
increases the accuracy of ML models by 63% on average. We study what part of
this improvement is due to the balancing of the dataset and what is achieved
thanks to further increasing the size of the dataset, concluding that both have
a very significant effect. We finish the paper by reflecting on how this idea
could be applied in other uses of machine learning in mathematics.
","['\nTereso del Rio\n', '\nMatthew England\n']",10 pages. To be presented at the 2023 SC-Square Workshop,"Proceedings of the 8th Workshop on Satisfiability Checking and
  Symbolic Computation (SC2 '23), E. \'Abrah\'am and T. Sturm eds. CEUR
  Workshop Proceedings 3455, pp. 29-38, 2023",http://arxiv.org/abs/2307.06984v1,cs.SC,"['cs.SC', 'cs.LG', '68W30, 68T05, 03C10', 'I.2.6; I.1.1']",,,[]
"Deciding One to One property of Boolean maps: Condition and algorithm in
  terms of implicants",http://arxiv.org/abs/2307.07788v3,2023-07-15T12:25:11Z,2024-02-02T15:14:24Z,"  This paper addresses the computational problem of deciding invertibility (or
one to one-ness) of a Boolean map $F$ in $n$-Boolean variables. This problem
has a special case of deciding invertibilty of a map
$F:\mathbb{F}_{2}^n\rightarrow\mathbb{F}_{2}^n$ over the binary field
$\mathbb{F}_2$. Further the problem can be extended and stated over a finite
field $\mathbb{F}$ instead of $\mathbb{F}_2$. Algebraic condition for
invertibility of $F$ in this special case over a finite field is well known to
be equivalent to invertibility of the Koopman operator of $F$ as shown in
\cite{RamSule}. In this paper a condition for invertibility is derived in the
special case of Boolean maps $F:B_0^n\rightarrow B_0^n$ where $B_0$ is the two
element Boolean algebra in terms of \emph{implicants} of Boolean equations.
This condition is then extended to the case of general maps in $n$ variables.
Hence this condition answers the special case of invertibility of the map $F$
defined over the binary field $\mathbb{F}_2$ alternatively, in terms of
implicants instead of the Koopman operator. The problem of deciding
invertibility of a map $F$ (or that of finding its $GOE$) over finite fields
appears to be distinct from the satisfiability problem (SAT) or the problem of
deciding consistency of polynomial equations over finite fields. Hence the well
known algorithms for deciding SAT or of solvability using Grobner basis for
checking membership in an ideal generated by polynomials is not known to answer
the question of invertibility of a map. Similarly it appears that algorithms
for satisfiability or polynomial solvability are not useful for computation of
$GOE(F)$ even for maps over the binary field $\mathbb{F}_2$.
",['\nVirendra Sule\n'],"I have fixed errors in proof of theorem 2 that I noticed. A new
  section and a theorem on one to one-ness is added. Paper is replaced as
  version 2",,http://arxiv.org/abs/2307.07788v3,cs.SC,"['cs.SC', 'cs.CC', 'F.2.1; I.1.1; I.1.2']",,,[]
First-Order Stable Model Semantics with Intensional Functions,http://arxiv.org/abs/2307.10225v1,2023-07-15T06:03:35Z,2023-07-15T06:03:35Z,"  In classical logic, nonBoolean fluents, such as the location of an object,
can be naturally described by functions. However, this is not the case in
answer set programs, where the values of functions are pre-defined, and
nonmonotonicity of the semantics is related to minimizing the extents of
predicates but has nothing to do with functions. We extend the first-order
stable model semantics by Ferraris, Lee, and Lifschitz to allow intensional
functions -- functions that are specified by a logic program just like
predicates are specified. We show that many known properties of the stable
model semantics are naturally extended to this formalism and compare it with
other related approaches to incorporating intensional functions. Furthermore,
we use this extension as a basis for defining Answer Set Programming Modulo
Theories (ASPMT), analogous to the way that Satisfiability Modulo Theories
(SMT) is defined, allowing for SMT-like effective first-order reasoning in the
context of ASP. Using SMT solving techniques involving functions, ASPMT can be
applied to domains containing real numbers and alleviates the grounding
problem. We show that other approaches to integrating ASP and CSP/SMT can be
related to special cases of ASPMT in which functions are limited to
non-intensional ones.
","['\nMichael Bartholomew\n', '\nJoohyung Lee\n']",69 pages,"Artificial Intelligence 273, 56-93, 2019",http://arxiv.org/abs/2307.10225v1,cs.AI,"['cs.AI', 'cs.SC']",,,[]
"Reasoning over the Behaviour of Objects in Video-Clips for Adverb-Type
  Recognition",http://arxiv.org/abs/2307.04132v2,2023-07-09T09:04:26Z,2023-07-12T10:57:00Z,"  In this work, following the intuition that adverbs describing scene-sequences
are best identified by reasoning over high-level concepts of object-behavior,
we propose the design of a new framework that reasons over object-behaviours
extracted from raw-video-clips to recognize the clip's corresponding
adverb-types. Importantly, while previous works for general scene
adverb-recognition assume knowledge of the clips underlying action-types, our
method is directly applicable in the more general problem setting where the
action-type of a video-clip is unknown. Specifically, we propose a novel
pipeline that extracts human-interpretable object-behaviour-facts from raw
video clips and propose novel symbolic and transformer based reasoning methods
that operate over these extracted facts to identify adverb-types. Experiment
results demonstrate that our proposed methods perform favourably against the
previous state-of-the-art. Additionally, to support efforts in symbolic
video-processing, we release two new datasets of object-behaviour-facts
extracted from raw video clips - the MSR-VTT-ASP and ActivityNet-ASP datasets.
","['\nAmrit Diggavi Seshadri\n', '\nAlessandra Russo\n']",,,http://arxiv.org/abs/2307.04132v2,cs.CV,"['cs.CV', 'cs.AI', 'cs.SC']",,,[]
Faster Rectangular Matrix Multiplication by Combination Loss Analysis,http://arxiv.org/abs/2307.06535v2,2023-07-13T02:47:27Z,2023-12-27T23:34:14Z,"  Duan, Wu and Zhou (FOCS 2023) recently obtained the improved upper bound on
the exponent of square matrix multiplication $\omega<2.3719$ by introducing a
new approach to quantify and compensate the ``combination loss"" in prior
analyses of powers of the Coppersmith-Winograd tensor. In this paper we show
how to use this new approach to improve the exponent of rectangular matrix
multiplication as well. Our main technical contribution is showing how to
combine this analysis of the combination loss and the analysis of the fourth
power of the Coppersmith-Winograd tensor in the context of rectangular matrix
multiplication developed by Le Gall and Urrutia (SODA 2018).
",['\nFrançois Le Gall\n'],35 pages; v2: minor corrections; accepted to SODA 2024,,http://arxiv.org/abs/2307.06535v2,cs.DS,"['cs.DS', 'cs.CC', 'cs.SC']",,,[]
"Coupling Large Language Models with Logic Programming for Robust and
  General Reasoning from Text",http://arxiv.org/abs/2307.07696v1,2023-07-15T03:29:59Z,2023-07-15T03:29:59Z,"  While large language models (LLMs), such as GPT-3, appear to be robust and
general, their reasoning ability is not at a level to compete with the best
models trained for specific natural language reasoning problems. In this study,
we observe that a large language model can serve as a highly effective few-shot
semantic parser. It can convert natural language sentences into a logical form
that serves as input for answer set programs, a logic-based declarative
knowledge representation formalism. The combination results in a robust and
general system that can handle multiple question-answering tasks without
requiring retraining for each new task. It only needs a few examples to guide
the LLM's adaptation to a specific task, along with reusable ASP knowledge
modules that can be applied to multiple tasks. We demonstrate that this method
achieves state-of-the-art performance on several NLP benchmarks, including
bAbI, StepGame, CLUTRR, and gSCAN. Additionally, it successfully tackles robot
planning tasks that an LLM alone fails to solve.
","['\nZhun Yang\n', '\nAdam Ishay\n', '\nJoohyung Lee\n']","32 pages, Findings of the Association for Computational Linguistics:
  ACL 2023, 5186-5219",,http://arxiv.org/abs/2307.07696v1,cs.CL,"['cs.CL', 'cs.AI', 'cs.SC']",,,[]
Leveraging Large Language Models to Generate Answer Set Programs,http://arxiv.org/abs/2307.07699v1,2023-07-15T03:40:55Z,2023-07-15T03:40:55Z,"  Large language models (LLMs), such as GPT-3 and GPT-4, have demonstrated
exceptional performance in various natural language processing tasks and have
shown the ability to solve certain reasoning problems. However, their reasoning
capabilities are limited and relatively shallow, despite the application of
various prompting techniques. In contrast, formal logic is adept at handling
complex reasoning, but translating natural language descriptions into formal
logic is a challenging task that non-experts struggle with. This paper proposes
a neuro-symbolic method that combines the strengths of large language models
and answer set programming. Specifically, we employ an LLM to transform natural
language descriptions of logic puzzles into answer set programs. We carefully
design prompts for an LLM to convert natural language descriptions into answer
set programs in a step by step manner. Surprisingly, with just a few in-context
learning examples, LLMs can generate reasonably complex answer set programs.
The majority of errors made are relatively simple and can be easily corrected
by humans, thus enabling LLMs to effectively assist in the creation of answer
set programs.
","['\nAdam Ishay\n', '\nZhun Yang\n', '\nJoohyung Lee\n']","17 pages, KR 2023",,http://arxiv.org/abs/2307.07699v1,cs.AI,"['cs.AI', 'cs.CL', 'cs.SC']",,,[]
NeurASP: Embracing Neural Networks into Answer Set Programming,http://arxiv.org/abs/2307.07700v1,2023-07-15T04:03:17Z,2023-07-15T04:03:17Z,"  We present NeurASP, a simple extension of answer set programs by embracing
neural networks. By treating the neural network output as the probability
distribution over atomic facts in answer set programs, NeurASP provides a
simple and effective way to integrate sub-symbolic and symbolic computation. We
demonstrate how NeurASP can make use of a pre-trained neural network in
symbolic computation and how it can improve the neural network's perception
result by applying symbolic reasoning in answer set programming. Also, NeurASP
can be used to train a neural network better by training with ASP rules so that
a neural network not only learns from implicit correlations from the data but
also from the explicit complex semantic constraints expressed by the rules.
","['\nZhun Yang\n', '\nAdam Ishay\n', '\nJoohyung Lee\n']","16 pages, 29th International Joint Conference on Artificial
  Intelligence (IJCAI 2020). arXiv admin note: substantial text overlap with
  arXiv:2009.10256",,http://arxiv.org/abs/2307.07700v1,cs.AI,"['cs.AI', 'cs.LG', 'cs.SC']",,,[]
Exploiting Strict Constraints in the Cylindrical Algebraic Covering,http://arxiv.org/abs/2306.16757v1,2023-06-29T07:54:58Z,2023-06-29T07:54:58Z,"  One of the few available complete methods for checking the satisfiability of
sets of polynomial constraints over the reals is the cylindrical algebraic
covering (CAlC) method. In this paper, we propose an extension for this method
to exploit the strictness of input constraints for reducing the computational
effort. We illustrate the concepts on a multidimensional example and provide
experimental results to evaluate the usefulness of our proposed extension.
","['\nPhilipp Bär\n', '\nJasper Nalbach\n', '\nErika Ábrahám\n', '\nChristopher W. Brown\n']",,,http://arxiv.org/abs/2306.16757v1,cs.SC,['cs.SC'],,,[]
Generating Elementary Integrable Expressions,http://arxiv.org/abs/2306.15572v1,2023-06-27T15:48:40Z,2023-06-27T15:48:40Z,"  There has been an increasing number of applications of machine learning to
the field of Computer Algebra in recent years, including to the prominent
sub-field of Symbolic Integration. However, machine learning models require an
abundance of data for them to be successful and there exist few benchmarks on
the scale required. While methods to generate new data already exist, they are
flawed in several ways which may lead to bias in machine learning models
trained upon them. In this paper, we describe how to use the Risch Algorithm
for symbolic integration to create a dataset of elementary integrable
expressions. Further, we show that data generated this way alleviates some of
the flaws found in earlier methods.
","['\nRashid Barket\n', '\nMatthew England\n', '\nJürgen Gerhard\n']","To appear in proceedings of CASC 2023. This version of the
  contribution has been accepted for publication, after peer review but is not
  the Version of Record and does not reflect post-acceptance improvements, or
  any corrections","In: F. Boulier, M. England, T.M. Sadykov, and E.V. Vorozhtsov,
  eds. Computer Algebra in Scientific Computing (Proc. CASC '23), pp. 21-38.
  (Lecture Notes in Computer Science, vol 14139). Springer International, 2023",http://dx.doi.org/10.1007/978-3-031-41724-5_2,cs.SC,"['cs.SC', 'cs.LG', '68W30, 68T05', 'I.2.6; I.1.1']",10.1007/978-3-031-41724-5_2,,[]
Discovering Asymptotic Expansions Using Symbolic Regression,http://arxiv.org/abs/2307.01876v1,2023-07-04T18:37:20Z,2023-07-04T18:37:20Z,"  Recently, symbolic regression (SR) has demonstrated its efficiency for
discovering basic governing relations in physical systems. A major impact can
be potentially achieved by coupling symbolic regression with asymptotic
methodology. The main advantage of asymptotic approach involves the robust
approximation to the sought for solution bringing a clear idea of the effect of
problem parameters. However, the analytic derivation of the asymptotic series
is often highly nontrivial especially, when the exact solution is not
available. In this paper, we adapt SR methodology to discover asymptotic
series. As an illustration we consider three problem in mechanics, including
two-mass collision, viscoelastic behavior of a Kelvin-Voigt solid and
propagation of Rayleigh-Lamb waves. The training data is generated from the
explicit exact solutions of these problems. The obtained SR results are
compared to the benchmark asymptotic expansions of the above mentioned exact
solutions. Both convergent and divergent asymptotic series are considered. A
good agreement between SR expansions and analytical results is observed. It is
demonstrated that the proposed approach can be used to identify material
parameters, e.g. Poisson's ratio, and has high prospects for utilizing
experimental and numerical data.
","['\nRasul Abdusalamov\n', '\nJulius Kaplunov\n', '\nMikhail Itskov\n']",,,http://arxiv.org/abs/2307.01876v1,cs.SC,"['cs.SC', 'physics.comp-ph']",,,[]
"Algorithms for computing norms and characteristic polynomials on general
  Drinfeld modules",http://arxiv.org/abs/2307.02879v3,2023-07-06T09:33:36Z,2024-01-30T10:10:41Z,"  We provide two families of algorithms to compute characteristic polynomials
of endomorphisms and norms of isogenies of Drinfeld modules. Our algorithms
work for Drinfeld modules of any rank, defined over any base curve. When the
base curve is $\mathbb P^1_{\mathbb F_q}$, we do a thorough study of the
complexity, demonstrating that our algorithms are, in many cases, the most
asymptotically performant. The first family of algorithms relies on the
correspondence between Drinfeld modules and Anderson motives, reducing the
computation to linear algebra over a polynomial ring. The second family,
available only for the Frobenius endomorphism, is based on a formula expressing
the characteristic polynomial of the Frobenius as a reduced norm in a central
simple algebra.
","['\nXavier Caruso\nLFANT, CANARI\n', '\nAntoine Leudière\nCARAMBA\n']",,,http://arxiv.org/abs/2307.02879v3,cs.SC,"['cs.SC', 'math.NT']",,,"['LFANT, CANARI', 'CARAMBA']"
Frex: dependently-typed algebraic simplification,http://arxiv.org/abs/2306.15375v1,2023-06-27T10:47:22Z,2023-06-27T10:47:22Z,"  We present an extensible, mathematically-structured algebraic simplification
library design. We structure the library using universal algebraic concepts: a
free algebra -- fral -- and a free extension -- frex -- of an algebra by a set
of variables. The library's dependently-typed API guarantees simplification
modules, even user-defined ones, are terminating, sound, and complete with
respect to a well-specified class of equations. Completeness offers intangible
benefits in practice -- our main contribution is the novel design. Cleanly
separating between the interface and implementation of simplification modules
provides two new modularity axes. First, simplification modules share thousands
of lines of infrastructure code dealing with term-representation,
pretty-printing, certification, and macros/reflection. Second, new
simplification modules can reuse existing ones. We demonstrate this design by
developing simplification modules for monoid varieties: ordinary, commutative,
and involutive. We implemented this design in the new Idris2 dependently-typed
programming language, and in Agda.
","['\nGuillaume Allais\n', '\nEdwin Brady\n', '\nNathan Corbyn\n', '\nOhad Kammar\n', '\nJeremy Yallop\n']",,,http://arxiv.org/abs/2306.15375v1,cs.PL,"['cs.PL', 'cs.LO', 'cs.SC']",,,[]
An ML approach to resolution of singularities,http://arxiv.org/abs/2307.00252v2,2023-07-01T07:17:33Z,2023-08-23T03:59:48Z,"  The solution set of a system of polynomial equations typically contains
ill-behaved, singular points. Resolution is a fundamental process in geometry
in which we replace singular points with smooth points, while keeping the rest
of the solution set unchanged. Resolutions are not unique: the usual way to
describe them involves repeatedly performing a fundamental operation known as
""blowing-up"", and the complexity of the resolution highly depends on certain
choices. The process can be translated into various versions of a 2-player
game, the so-called Hironaka game, and a winning strategy for the first player
provides a solution to the resolution problem. In this paper we introduce a new
approach to the Hironaka game that uses reinforcement learning agents to find
optimal resolutions of singularities. In certain domains, the trained model
outperforms state-of-the-art selection heuristics in total number of polynomial
additions performed, which provides a proof-of-concept that recent developments
in machine learning have the potential to improve performance of algorithms in
symbolic computation.
","['\nGergely Bérczi\n', '\nHonglu Fan\n', '\nMingcong Zeng\n']","To appear in Proceedings of the 40th International Conference on
  Machine Learning TAG Workshop (ICML-TAG 2023)",,http://arxiv.org/abs/2307.00252v2,cs.LG,"['cs.LG', 'cs.AI', 'cs.SC', 'math.AG']",,,[]
"Fuchs' theorem on linear differential equations in arbitrary
  characteristic",http://arxiv.org/abs/2307.01712v2,2023-07-04T13:31:20Z,2023-10-29T10:08:44Z,"  The paper generalizes Lazarus Fuchs' theorem on the solutions of complex
ordinary linear differential equations with regular singularities to the case
of ground fields of arbitrary characteristic, giving a precise description of
the shape of each solution. This completes partial investigations started by
Taira Honda and Bernard Dwork.
  The main features are the introduction of a differential ring $\mathcal{R}$
in infinitely many variables mimicking the role of the (complex) iterated
logarithms, and the proof that adding these ""logarithms"" already provides
sufficiently many primitives so as to solve any differential equation with
regular singularity in $\mathcal{R}$. A key step in the proof is the reduction
of the involved differential operator to an Euler operator, its normal form, to
solve Euler equations in $\mathcal{R}$ and to lift their (monomial) solutions
to solutions of the original equation.
  The first (and already very striking) example of this outset is the
exponential function $\exp_p$ in positive characteristic, solution of $y' = y$.
We prove that it necessarily involves all variables and we construct its
explicit (and quite mysterious) power series expansion. Additionally, relations
of our results to the Grothendieck-Katz $p$-curvature conjecture and related
conjectures will be discussed.
","['\nFlorian Fürnsinn\n', '\nHerwig Hauser\n']",40 pages,,http://arxiv.org/abs/2307.01712v2,math.CA,"['math.CA', 'cs.SC', 'math.AC', 'math.NT', '12H20 (Primary), 14G17, 34A05, 34M03, 47E05 (Secondary)']",,,[]
"Concept2Box: Joint Geometric Embeddings for Learning Two-View Knowledge
  Graphs",http://arxiv.org/abs/2307.01933v1,2023-07-04T21:37:39Z,2023-07-04T21:37:39Z,"  Knowledge graph embeddings (KGE) have been extensively studied to embed
large-scale relational data for many real-world applications. Existing methods
have long ignored the fact many KGs contain two fundamentally different views:
high-level ontology-view concepts and fine-grained instance-view entities. They
usually embed all nodes as vectors in one latent space. However, a single
geometric representation fails to capture the structural differences between
two views and lacks probabilistic semantics towards concepts' granularity. We
propose Concept2Box, a novel approach that jointly embeds the two views of a KG
using dual geometric representations. We model concepts with box embeddings,
which learn the hierarchy structure and complex relations such as overlap and
disjoint among them. Box volumes can be interpreted as concepts' granularity.
Different from concepts, we model entities as vectors. To bridge the gap
between concept box embeddings and entity vector embeddings, we propose a novel
vector-to-box distance metric and learn both embeddings jointly. Experiments on
both the public DBpedia KG and a newly-created industrial KG showed the
effectiveness of Concept2Box.
","['\nZijie Huang\n', '\nDaheng Wang\n', '\nBinxuan Huang\n', '\nChenwei Zhang\n', '\nJingbo Shang\n', '\nYan Liang\n', '\nZhengyang Wang\n', '\nXian Li\n', '\nChristos Faloutsos\n', '\nYizhou Sun\n', '\nWei Wang\n']",,ACL 2023,http://arxiv.org/abs/2307.01933v1,cs.AI,"['cs.AI', 'cs.CG', 'cs.CL', 'cs.SC']",,,[]
"Runtime Repeated Recursion Unfolding in CHR: A Just-In-Time Online
  Program Optimization Strategy That Can Achieve Super-Linear Speedup",http://arxiv.org/abs/2307.02180v2,2023-07-05T10:18:51Z,2024-01-29T14:56:47Z,"  We introduce a just-in-time runtime program transformation strategy based on
repeated recursion unfolding. Our online program optimization generates several
versions of a recursion differentiated by the minimal number of recursive steps
covered. The base case of the recursion is ignored in our technique.
  Our method is introduced here on the basis of single linear direct recursive
rules. When a recursive call is encountered at runtime, first an unfolder
creates specializations of the associated recursive rule on-the-fly and then an
interpreter applies these rules to the call. Our approach reduces the number of
recursive rule applications to its logarithm at the expense of introducing a
logarithmic number of generic unfolded rules.
  We prove correctness of our online optimization technique and determine its
time complexity. For recursions which have enough simplifyable unfoldings, a
super-linear is possible, i.e. speedup by more than a constant factor.The
necessary simplification is problem-specific and has to be provided at
compile-time. In our speedup analysis, we prove a sufficient condition as well
as a sufficient and necessary condition for super-linear speedup relating the
complexity of the recursive steps of the original rule and the unfolded rules.
  We have implemented an unfolder and meta-interpreter for runtime repeated
recursion unfolding with just five rules in Constraint Handling Rules (CHR)
embedded in Prolog. We illustrate the feasibility of our approach with
simplifications, time complexity results and benchmarks for some basic
tractable algorithms. The simplifications require some insight and were derived
manually. The runtime improvement quickly reaches several orders of magnitude,
consistent with the super-linear speedup predicted by our theorems.
",['\nThom Fruehwirth\n'],Major revision of submission to Journal Fundamenta Informaticae,,http://arxiv.org/abs/2307.02180v2,cs.PL,"['cs.PL', 'cs.CC', 'cs.PF', 'cs.SC']",,,[]
"RecallM: An Adaptable Memory Mechanism with Temporal Understanding for
  Large Language Models",http://arxiv.org/abs/2307.02738v3,2023-07-06T02:51:54Z,2023-10-03T01:16:33Z,"  Large Language Models (LLMs) have made extraordinary progress in the field of
Artificial Intelligence and have demonstrated remarkable capabilities across a
large variety of tasks and domains. However, as we venture closer to creating
Artificial General Intelligence (AGI) systems, we recognize the need to
supplement LLMs with long-term memory to overcome the context window limitation
and more importantly, to create a foundation for sustained reasoning,
cumulative learning and long-term user interaction. In this paper we propose
RecallM, a novel architecture for providing LLMs with an adaptable and
updatable long-term memory mechanism. Unlike previous methods, the RecallM
architecture is particularly effective at belief updating and maintaining a
temporal understanding of the knowledge provided to it. We demonstrate through
various experiments the effectiveness of this architecture. Furthermore,
through our own temporal understanding and belief updating experiments, we show
that RecallM is four times more effective than using a vector database for
updating knowledge previously stored in long-term memory. We also demonstrate
that RecallM shows competitive performance on general question-answering and
in-context learning tasks.
","['\nBrandon Kynoch\n', '\nHugo Latapie\n', '\nDwane van der Sluis\n']","8 pages, 7 figures, 1 table, Our code is publicly available online
  at: https://github.com/cisco-open/DeepVision/tree/main/recallm",,http://arxiv.org/abs/2307.02738v3,cs.AI,"['cs.AI', 'cs.CL', 'cs.SC']",,,[]
Existence and Construction of a Gröbner Basis for a Polynomial Ideal,http://arxiv.org/abs/2306.09602v1,2023-06-16T03:13:42Z,2023-06-16T03:13:42Z,"  This extended abstract gives a construction for lifting a Gr\""obner basis
algorithm for an ideal in a polynomial ring over a commutative ring R under the
condition that R also admits a Gr\""obner basis for every ideal in R.
","['\nDeepak Kapur\n', '\nPaliath Narendran\n']",,,http://arxiv.org/abs/2306.09602v1,math.AC,"['math.AC', 'cs.SC', 'math.AG']",,,[]
"A Finite Expression Method for Solving High-Dimensional Committor
  Problems",http://arxiv.org/abs/2306.12268v1,2023-06-21T13:43:59Z,2023-06-21T13:43:59Z,"  Transition path theory (TPT) is a mathematical framework for quantifying rare
transition events between a pair of selected metastable states $A$ and $B$.
Central to TPT is the committor function, which describes the probability to
hit the metastable state $B$ prior to $A$ from any given starting point of the
phase space. Once the committor is computed, the transition channels and the
transition rate can be readily found. The committor is the solution to the
backward Kolmogorov equation with appropriate boundary conditions. However,
solving it is a challenging task in high dimensions due to the need to mesh a
whole region of the ambient space. In this work, we explore the finite
expression method (FEX, Liang and Yang (2022)) as a tool for computing the
committor. FEX approximates the committor by an algebraic expression involving
a fixed finite number of nonlinear functions and binary arithmetic operations.
The optimal nonlinear functions, the binary operations, and the numerical
coefficients in the expression template are found via reinforcement learning.
The FEX-based committor solver is tested on several high-dimensional benchmark
problems. It gives comparable or better results than neural network-based
solvers. Most importantly, FEX is capable of correctly identifying the
algebraic structure of the solution which allows one to reduce the committor
problem to a low-dimensional one and find the committor with any desired
accuracy.
","['\nZezheng Song\n', '\nMaria K. Cameron\n', '\nHaizhao Yang\n']",,,http://arxiv.org/abs/2306.12268v1,math.NA,"['math.NA', 'cs.LG', 'cs.NA', 'cs.SC']",,,[]
"From Word Models to World Models: Translating from Natural Language to
  the Probabilistic Language of Thought",http://arxiv.org/abs/2306.12672v2,2023-06-22T05:14:00Z,2023-06-23T06:05:31Z,"  How does language inform our downstream thinking? In particular, how do
humans make meaning from language--and how can we leverage a theory of
linguistic meaning to build machines that think in more human-like ways? In
this paper, we propose rational meaning construction, a computational framework
for language-informed thinking that combines neural language models with
probabilistic models for rational inference. We frame linguistic meaning as a
context-sensitive mapping from natural language into a probabilistic language
of thought (PLoT)--a general-purpose symbolic substrate for generative world
modeling. Our architecture integrates two computational tools that have not
previously come together: we model thinking with probabilistic programs, an
expressive representation for commonsense reasoning; and we model meaning
construction with large language models (LLMs), which support broad-coverage
translation from natural language utterances to code expressions in a
probabilistic programming language. We illustrate our framework through
examples covering four core domains from cognitive science: probabilistic
reasoning, logical and relational reasoning, visual and physical reasoning, and
social reasoning. In each, we show that LLMs can generate context-sensitive
translations that capture pragmatically-appropriate linguistic meanings, while
Bayesian inference with the generated programs supports coherent and robust
commonsense reasoning. We extend our framework to integrate
cognitively-motivated symbolic modules (physics simulators, graphics engines,
and planning algorithms) to provide a unified commonsense thinking interface
from language. Finally, we explore how language can drive the construction of
world models themselves. We hope this work will provide a roadmap towards
cognitive models and AI systems that synthesize the insights of both modern and
classical computational perspectives.
","['\nLionel Wong\n', '\nGabriel Grand\n', '\nAlexander K. Lew\n', '\nNoah D. Goodman\n', '\nVikash K. Mansinghka\n', '\nJacob Andreas\n', '\nJoshua B. Tenenbaum\n']",,,http://arxiv.org/abs/2306.12672v2,cs.CL,"['cs.CL', 'cs.AI', 'cs.SC']",,,[]
On Isolating Roots in a Multiple Field Extension,http://arxiv.org/abs/2306.04271v1,2023-06-07T09:12:05Z,2023-06-07T09:12:05Z,"  We address univariate root isolation when the polynomial's coefficients are
in a multiple field extension. We consider a polynomial $F \in L[Y]$, where $L$
is a multiple algebraic extension of $\mathbb{Q}$. We provide aggregate bounds
for $F$ and algorithmic and bit-complexity results for the problem of isolating
its roots. For the latter problem we follow a common approach based on
univariate root isolation algorithms. For the particular case where $F$ does
not have multiple roots, we achieve a bit-complexity in
$\tilde{\mathcal{O}}_B(n d^{2n+2}(d+n\tau))$, where $d$ is the total degree and
$\tau$ is the bitsize of the involved polynomials.In the general case we need
to enhance our algorithm with a preprocessing step that determines the number
of distinct roots of $F$. We follow a numerical, yet certified, approach that
has bit-complexity $\tilde{\mathcal{O}}_B(n^2d^{3n+3}\tau + n^3 d^{2n+4}\tau)$.
","['\nChristina Katsamaki\nSU, OURAGAN\n', '\nFabrice Rouillier\nSU, OURAGAN\n']",,,http://arxiv.org/abs/2306.04271v1,cs.SC,['cs.SC'],,,"['SU, OURAGAN', 'SU, OURAGAN']"
"A sharper multivariate Christol's theorem with applications to diagonals
  and Hadamard products",http://arxiv.org/abs/2306.02640v1,2023-06-05T07:23:26Z,2023-06-05T07:23:26Z,"  We provide a new proof of the multivariate version of Christol's theorem
about algebraic power series with coefficients in finite fields, as well as of
its extension to perfect ground fields of positive characteristic obtained
independently by Denef and Lipshitz, Sharif and Woodcok, and Harase. Our proof
is elementary, effective, and allows for much sharper estimates. We discuss
various applications of such estimates, in particular to a problem raised by
Deligne concerning the algebraicity degree of reductions modulo $p$ of
diagonals of multivariate algebraic power series with integer coefficients.
","['\nBoris Adamczewski\n', '\nAlin Bostan\n', '\nXavier Caruso\n']",32 pages,,http://arxiv.org/abs/2306.02640v1,math.NT,"['math.NT', 'cs.SC']",,,[]
Faster real root decision algorithm for symmetric polynomials,http://arxiv.org/abs/2306.03855v1,2023-06-06T16:49:15Z,2023-06-06T16:49:15Z,"  In this paper, we consider the problem of deciding the existence of real
solutions to a system of polynomial equations having real coefficients, and
which are invariant under the action of the symmetric group. We construct and
analyze a Monte Carlo probabilistic algorithm which solves this problem, under
some regularity assumptions on the input, by taking advantage of the symmetry
invariance property. The complexity of our algorithm is polynomial in $d^s,
{{n+d} \choose d}$, and ${{n} \choose {s+1}}$, where $n$ is the number of
variables and $d$ is the maximal degree of $s$ input polynomials defining the
real algebraic set under study. In particular, this complexity is polynomial in
$n$ when $d$ and $s$ are fixed and is equal to $n^{O(1)}2^n$ when $d=n$.
","['\nGeorge Labahn\n', '\nCordian Riener\n', '\nMohab Safey El Din\n', '\nÉric Schost\n', '\nThi Xuan Vu\n']",,,http://dx.doi.org/10.1145/3597066.3597097,cs.SC,"['cs.SC', 'math.AG']",10.1145/3597066.3597097,,[]
Effective homology and periods of complex projective hypersurfaces,http://arxiv.org/abs/2306.05263v2,2023-06-08T15:08:46Z,2024-01-10T15:03:42Z,"  We introduce a new algorithm for computing the periods of a smooth complex
projective hypersurface. The algorithm intertwine with a new method for
computing an explicit basis of the singular homology of the hypersurface. It is
based on Picard-Lefschetz theory and relies on the computation of the monodromy
action induced by a one-parameter family of hyperplane sections on the homology
of a given section. We provide a SageMath implementation. For example, on a
laptop, it makes it possible to compute the periods of a smooth complex quartic
surface with hundreds of digits of precision in typically an hour.
","['\nPierre Lairez\n', '\nEric Pichon-Pharabod\n', '\nPierre Vanhove\n']",38 pages,,http://arxiv.org/abs/2306.05263v2,math.AG,"['math.AG', 'cs.SC', 'Primary 14Q15, Secondary 32G20, 14D05']",,,[]
Positivity certificates for linear recurrences,http://arxiv.org/abs/2306.05930v2,2023-06-09T14:44:38Z,2023-11-03T12:58:56Z,"  We consider linear recurrences with polynomial coefficients of Poincar\'e
type and with a unique simple dominant eigenvalue. We give an algorithm that
proves or disproves positivity of solutions provided the initial conditions
satisfy a precisely defined genericity condition. For positive sequences, the
algorithm produces a certificate of positivity that is a data-structure for a
proof by induction. This induction works by showing that an explicitly computed
cone is contracted by the iteration of the recurrence.
","['\nAlaa Ibrahim\n', '\nBruno Salvy\n']",18 pages. To appear in Proceedings SODA'24,"Proceedings of the 2024 Annual ACM-SIAM Symposium on Discrete
  Algorithms (SODA), 2024",http://dx.doi.org/10.1137/1.9781611977912.37,cs.SC,"['cs.SC', 'cs.DM', '05A20, 11B37, 39A06, 68V05', 'F.2.2; G.2.1; I.1.2']",10.1137/1.9781611977912.37,,[]
"Exact and Approximate Moment Derivation for Probabilistic Loops With
  Non-Polynomial Assignments",http://arxiv.org/abs/2306.07072v2,2023-06-12T12:38:01Z,2024-01-25T09:46:35Z,"  Many stochastic continuous-state dynamical systems can be modeled as
probabilistic programs with nonlinear non-polynomial updates in non-nested
loops. We present two methods, one approximate and one exact, to automatically
compute, without sampling, moment-based invariants for such probabilistic
programs as closed-form solutions parameterized by the loop iteration. The
exact method applies to probabilistic programs with trigonometric and
exponential updates and is embedded in the Polar tool. The approximate method
for moment computation applies to any nonlinear random function as it exploits
the theory of polynomial chaos expansion to approximate non-polynomial updates
as the sum of orthogonal polynomials. This translates the dynamical system to a
non-nested loop with polynomial updates, and thus renders it conformable with
the Polar tool that computes the moments of any order of the state variables.
We evaluate our methods on an extensive number of examples ranging from
modeling monetary policy to several physical motion systems in uncertain
environments. The experimental results demonstrate the advantages of our
approach with respect to the current state-of-the-art.
","['\nAndrey Kofnov\n', '\nMarcel Moosbrugger\n', '\nMiroslav Stankovič\n', '\nEzio Bartocci\n', '\nEfstathia Bura\n']","Published in ACM Transactions on Modeling and Computer Simulation
  (TOMACS). Extended version of the conference paper 'Moment-based Invariants
  for Probabilistic Loops with Non-polynomial Assignments' published at QEST
  2022 (Best paper award, see also the preprint arxiv.org/abs/2205.02577).
  arXiv admin note: substantial text overlap with arXiv:2205.02577",,http://dx.doi.org/10.1145/3641545,stat.AP,"['stat.AP', 'cs.NA', 'cs.SC', 'math.NA', 'math.ST', 'stat.TH', '62G05, 62P30', 'G.3']",10.1145/3641545,,[]
Efficient Quotients of Non-Commutative Polynomials,http://arxiv.org/abs/2305.17877v4,2023-05-29T04:15:57Z,2023-06-27T19:51:24Z,"  It is shown how to compute quotients efficiently in non-commutative
univariate polynomial rings. This extends earlier work where efficient generic
quotients were studied with a primary focus on commutative domains. Fast
algorithms are given for left and right quotients of polynomials where the
variable commutes with coefficients. These algorithms are based on the concept
of the ``whole shifted inverse'', which is a specialized quotient where the
dividend is a power of the polynomial variable. It is also shown that when the
variable does not commute with coefficients, that is for skew polynomials, left
and right whole shifted inverses are defined and may be used to compute right
and left quotients. In this case their computation is not asymptotically fast,
but once obtained, they may be used to compute multiple quotients, each with
one multiplication. Examples are shown of polynomials with matrix coefficients,
differential operators and difference operators. In addition, a
proof-of-concept generic Maple implementations is given.
",['\nStephen M. Watt\n'],,,http://arxiv.org/abs/2305.17877v4,cs.SC,['cs.SC'],,,[]
"Some New Non-Commutative Matrix Multiplication Algorithms of Size
  $(n,m,6)$",http://arxiv.org/abs/2306.00882v1,2023-06-01T16:41:55Z,2023-06-01T16:41:55Z,"  For various $2\leq n,m \leq 6$, we propose some new algorithms for
multiplying an $n\times m$ matrix with an $m \times 6$ matrix over a possibly
noncommutative coefficient ring.
","['\nManuel Kauers\n', '\nJakob Moosbauer\n']",9 pages,,http://arxiv.org/abs/2306.00882v1,cs.SC,['cs.SC'],,,[]
A family of Counterexamples on Inequality among Symmetric Functions,http://arxiv.org/abs/2305.19830v1,2023-05-31T13:14:06Z,2023-05-31T13:14:06Z,"  Inequalities among symmetric functions are fundamental questions in
mathematics and have various applications in science and engineering. In this
paper, we tackle a conjecture about inequalities among the complete homogeneous
symmetric function $H_{n,\lambda}$, that is, the inequality $H_{n,\lambda}\leq
H_{n,\mu}$ implies majorization order $\lambda\preceq\mu$. This conjecture was
proposed by Cuttler, Greene and Skandera in 2011. The conjecture is a close
analogy with other known results on Muirhead-type inequalities. In 2021, Heaton
and Shankar disproved the conjecture by showing a counterexample for degree
$d=8$ and number of variables $n=3$. They then asked whether the conjecture is
true when~ the number of variables, $n$, is large enough? In this paper, we
answer the question by proving that the conjecture does not hold when $d\geq8$
and $n\geq2$. A crucial step of the proof relies on variables reduction.
Inspired by this, we propose a new conjecture for $H_{n,\lambda}\leq
H_{n,\mu}$.
","['\nJia Xu\n', '\nYong Yao\n']",14 pages,,http://arxiv.org/abs/2305.19830v1,math.CO,"['math.CO', 'cs.SC', '05E05 14P99 90C22']",,,[]
Neural Machine Translation for Mathematical Formulae,http://arxiv.org/abs/2305.16433v1,2023-05-25T19:15:06Z,2023-05-25T19:15:06Z,"  We tackle the problem of neural machine translation of mathematical formulae
between ambiguous presentation languages and unambiguous content languages.
Compared to neural machine translation on natural language, mathematical
formulae have a much smaller vocabulary and much longer sequences of symbols,
while their translation requires extreme precision to satisfy mathematical
information needs. In this work, we perform the tasks of translating from LaTeX
to Mathematica as well as from LaTeX to semantic LaTeX. While recurrent,
recursive, and transformer networks struggle with preserving all contained
information, we find that convolutional sequence-to-sequence networks achieve
95.1% and 90.7% exact matches, respectively.
","['\nFelix Petersen\n', '\nMoritz Schubotz\n', '\nAndre Greiner-Petter\n', '\nBela Gipp\n']",Published at ACL 2023,,http://arxiv.org/abs/2305.16433v1,cs.CL,"['cs.CL', 'cs.SC', 'stat.AP']",,,[]
Representing Piecewise Linear Functions by Functions with Small Arity,http://arxiv.org/abs/2305.16933v1,2023-05-26T13:48:37Z,2023-05-26T13:48:37Z,"  A piecewise linear function can be described in different forms: as an
arbitrarily nested expression of $\min$- and $\max$-functions, as a difference
of two convex piecewise linear functions, or as a linear combination of maxima
of affine-linear functions. In this paper, we provide two main results: first,
we show that for every piecewise linear function there exists a linear
combination of $\max$-functions with at most $n+1$ arguments, and give an
algorithm for its computation. Moreover, these arguments are contained in the
finite set of affine-linear functions that coincide with the given function in
some open set. Second, we prove that the piecewise linear function $\max(0,
x_{1}, \ldots, x_{n})$ cannot be represented as a linear combination of maxima
of less than $n+1$ affine-linear arguments. This was conjectured by Wang and
Sun in 2005 in a paper on representations of piecewise linear functions as
linear combination of maxima.
","['\nChristoph Koutschan\n', '\nBernhard Moser\n', '\nAnton Ponomarchuk\n', '\nJosef Schicho\n']",,,http://arxiv.org/abs/2305.16933v1,cs.SC,"['cs.SC', 'cs.DM', 'cs.LG', 'math.CO']",,,[]
Reason to explain: Interactive contrastive explanations (REASONX),http://arxiv.org/abs/2305.18143v1,2023-05-29T15:13:46Z,2023-05-29T15:13:46Z,"  Many high-performing machine learning models are not interpretable. As they
are increasingly used in decision scenarios that can critically affect
individuals, it is necessary to develop tools to better understand their
outputs. Popular explanation methods include contrastive explanations. However,
they suffer several shortcomings, among others an insufficient incorporation of
background knowledge, and a lack of interactivity. While (dialogue-like)
interactivity is important to better communicate an explanation, background
knowledge has the potential to significantly improve their quality, e.g., by
adapting the explanation to the needs of the end-user. To close this gap, we
present REASONX, an explanation tool based on Constraint Logic Programming
(CLP). REASONX provides interactive contrastive explanations that can be
augmented by background knowledge, and allows to operate under a setting of
under-specified information, leading to increased flexibility in the provided
explanations. REASONX computes factual and constrative decision rules, as well
as closest constrative examples. It provides explanations for decision trees,
which can be the ML models under analysis, or global/local surrogate models of
any ML model. While the core part of REASONX is built on CLP, we also provide a
program layer that allows to compute the explanations via Python, making the
tool accessible to a wider audience. We illustrate the capability of REASONX on
a synthetic data set, and on a a well-developed example in the credit domain.
In both cases, we can show how REASONX can be flexibly used and tailored to the
needs of the user.
","['\nLaura State\n', '\nSalvatore Ruggieri\n', '\nFranco Turini\n']","The 1st World Conference on eXplainable Artificial Intelligence (xAI
  2023)",,http://arxiv.org/abs/2305.18143v1,cs.AI,"['cs.AI', 'cs.CY', 'cs.LG', 'cs.SC']",,,[]
"Information Fusion via Symbolic Regression: A Tutorial in the Context of
  Human Health",http://arxiv.org/abs/2306.00153v1,2023-05-31T19:52:17Z,2023-05-31T19:52:17Z,"  This tutorial paper provides a general overview of symbolic regression (SR)
with specific focus on standards of interpretability. We posit that
interpretable modeling, although its definition is still disputed in the
literature, is a practical way to support the evaluation of successful
information fusion. In order to convey the benefits of SR as a modeling
technique, we demonstrate an application within the field of health and
nutrition using publicly available National Health and Nutrition Examination
Survey (NHANES) data from the Centers for Disease Control and Prevention (CDC),
fusing together anthropometric markers into a simple mathematical expression to
estimate body fat percentage. We discuss the advantages and challenges
associated with SR modeling and provide qualitative and quantitative analyses
of the learned models.
","['\nJennifer J. Schnur\n', '\nNitesh V. Chawla\n']",,Information Fusion (2022),http://dx.doi.org/10.1016/j.inffus.2022.11.030,cs.LG,"['cs.LG', 'cs.AI', 'cs.SC']",10.1016/j.inffus.2022.11.030,,[]
"Interpretable and Explainable Logical Policies via Neurally Guided
  Symbolic Abstraction",http://arxiv.org/abs/2306.01439v2,2023-06-02T10:59:44Z,2023-10-25T16:40:27Z,"  The limited priors required by neural networks make them the dominating
choice to encode and learn policies using reinforcement learning (RL). However,
they are also black-boxes, making it hard to understand the agent's behaviour,
especially when working on the image level. Therefore, neuro-symbolic RL aims
at creating policies that are interpretable in the first place. Unfortunately,
interpretability is not explainability. To achieve both, we introduce Neurally
gUided Differentiable loGic policiEs (NUDGE). NUDGE exploits trained neural
network-based agents to guide the search of candidate-weighted logic rules,
then uses differentiable logic to train the logic agents. Our experimental
evaluation demonstrates that NUDGE agents can induce interpretable and
explainable policies while outperforming purely neural ones and showing good
flexibility to environments of different initial states and problem sizes.
","['\nQuentin Delfosse\n', '\nHikaru Shindo\n', '\nDevendra Dhami\n', '\nKristian Kersting\n']",9 main pages + appendix (19 in total),,http://arxiv.org/abs/2306.01439v2,cs.LG,"['cs.LG', 'cs.AI', 'cs.CL', 'cs.LO', 'cs.SC']",,,[]
"Log-concavity and log-convexity of series containing multiple Pochhammer
  symbols",http://arxiv.org/abs/2305.09029v3,2023-05-15T21:31:10Z,2023-12-09T10:06:08Z,"  In this paper, we study power series with coefficients equal to a product of
a generic sequence and an explicitly given function of a positive parameter
expressible in terms of the Pochhammer symbols. Four types of such series are
treated. We show that logarithmic concavity (convexity) of the generic sequence
leads to logarithmic concavity (convexity) of the sum of the series with
respect to the argument of the explicitly given function. The logarithmic
concavity (convexity) is derived from a stronger property, namely, positivity
(negativity) of the power series coefficients of the so-called generalized
Tur\'{a}nian. Applications to special functions such as the generalized
hypergeometric function and the Fox-Wright function are also discussed.
","['\nDmitrii Karp\n', '\nYi Zhang\n']",20 pages; no figures,,http://arxiv.org/abs/2305.09029v3,math.CA,"['math.CA', 'cs.SC', '26A51, 33C20, 33C60, 33F10']",,,[]
Bézout identities and control of the heat equation,http://arxiv.org/abs/2305.09340v1,2023-05-16T10:39:29Z,2023-05-16T10:39:29Z,"  Computing analytic B\'ezout identities remains a difficult task, which has
many applications in control theory. Flat PDE systems have cast a new light on
this problem. We consider here a simple case of special interest: a rod of
length $a+b$, insulated at both ends and heated at point $x=a$. The case $a=0$
is classical, the temperature of the other end $\theta(b,t)$ being then a flat
output, with parametrization $\theta(x,t)=\cosh((b-x)(\partial/\partial
t)^{1/2}\theta(b,t)$.
  When $a$ and $b$ are integers, with $a$ odd and $b$ even, the system is flat
and the flat output is obtained from the B\'ezout identity
$f(x)\cosh(ax)+g(x)\cosh(bx)=1$, the omputation of which boils down to a
B\'ezout identity of Chebyshev polynomials. But this form is not the most
efficient and a smaller expression $f(x)=\sum_{k=1}^{n} c_{k}\cosh(kx)$ may be
computed in linear time.
  These results are compared with an approximations by a finite system, using a
classical discretization.
  We provide experimental computations, approximating a non rational value $r$
by a sequence of fractions $b/a$, showing that the power series for the
B\'ezout relation seems to converge.
",['\nFrançois Ollivier\n'],"24 pages, 5 figures",,http://arxiv.org/abs/2305.09340v1,cs.SC,"['cs.SC', 'math.OC', '68W30, 35Q97, 30D20', 'G.1.8']",,,[]
"How to automatise proofs of operator statements: Moore-Penrose inverse
  -- a case study",http://arxiv.org/abs/2305.09448v2,2023-05-16T14:11:13Z,2023-06-27T08:53:17Z,"  We describe a recently developed algebraic framework for proving first-order
statements about linear operators by computations with noncommutative
polynomials. Furthermore, we present our new SageMath package operator_gb,
which offers functionality for automatising such computations. We aim to
provide a practical understanding of our approach and the software through
examples, while also explaining the completeness of the method in the sense
that it allows to find algebraic proofs for every true first-order operator
statement. We illustrate the capability of the framework in combination with
our software by a case study on statements about the Moore-Penrose inverse,
including classical facts and recent results, presented in an online notebook.
","['\nKlara Bernauer\n', '\nClemens Hofstadler\n', '\nGeorg Regensburger\n']","22 pages, plus 8 additional pages appendix",,http://arxiv.org/abs/2305.09448v2,cs.SC,"['cs.SC', 'cs.LO']",,,[]
"Using Symbolic Computation to Analyze Zero-Hopf Bifurcations of
  Polynomial Differential Systems",http://arxiv.org/abs/2305.11109v1,2023-05-18T16:52:56Z,2023-05-18T16:52:56Z,"  This paper is devoted to the study of infinitesimal limit cycles that can
bifurcate from zero-Hopf equilibria of differential systems based on the
averaging method. We develop an efficient symbolic program using Maple for
computing the averaged functions of any order for continuous differential
systems in arbitrary dimension. The program allows us to systematically analyze
zero-Hopf bifurcations of polynomial differential systems using symbolic
computation methods. We show that for the first-order averaging,
$\ell\in\{0,1,\ldots,2^{n-3}\}$ limit cycles can bifurcate from the zero-Hopf
equilibrium for the general class of perturbed differential systems and up to
the second-order averaging, the maximum number of limit cycles can be
determined by computing the mixed volume of a polynomial system obtained from
the averaged functions. A number of examples are presented to demonstrate the
effectiveness of the proposed algorithmic approach.
",['\nBo Huang\n'],"The 48th International Symposium on Symbolic and Algebraic
  Computation (ISSAC 2023). arXiv admin note: text overlap with
  arXiv:2205.14450",,http://arxiv.org/abs/2305.11109v1,cs.SC,"['cs.SC', 'math.DS']",,,[]
"Two-step Newton's method for deflation-one singular zeros of analytic
  systems",http://arxiv.org/abs/2305.10803v2,2023-05-18T08:31:45Z,2024-01-25T04:48:52Z,"  We propose a two-step Newton's method for refining an approximation of a
singular zero whose deflation process terminates after one step, also known as
a deflation-one singularity. Given an isolated singular zero of a square
analytic system, our algorithm exploits an invertible linear operator obtained
by combining the Jacobian and a projection of the Hessian in the direction of
the kernel of the Jacobian. We prove the quadratic convergence of the two-step
Newton method when it is applied to an approximation of a deflation-one
singular zero. Also, the algorithm requires a smaller size of matrices than the
existing methods, making it more efficient. We demonstrate examples and
experiments to show the efficiency of the method.
","['\nKisun Lee\n', '\nNan Li\n', '\nLihong Zhi\n']","23 pages, 1 figure, 4 tables, Version to appear in Journal of
  Symbolic Computation",,http://arxiv.org/abs/2305.10803v2,math.NA,"['math.NA', 'cs.NA', 'cs.SC', 'math.AG', '65D18, 14Q99']",,,[]
"Inverse kinematics and path planning of manipulator using real
  quantifier elimination based on Comprehensive Gröbner Systems",http://arxiv.org/abs/2305.12451v2,2023-05-21T13:09:44Z,2023-07-11T13:12:15Z,"  Methods for inverse kinematics computation and path planning of a three
degree-of-freedom (DOF) manipulator using the algorithm for quantifier
elimination based on Comprehensive Gr\""obner Systems (CGS), called CGS-QE
method, are proposed. The first method for solving the inverse kinematics
problem employs counting the real roots of a system of polynomial equations to
verify the solution's existence. In the second method for trajectory planning
of the manipulator, the use of CGS guarantees the existence of an inverse
kinematics solution. Moreover, it makes the algorithm more efficient by
preventing repeated computation of Gr\""obner basis. In the third method for
path planning of the manipulator, for a path of the motion given as a function
of a parameter, the CGS-QE method verifies the whole path's feasibility.
Computational examples and an experiment are provided to illustrate the
effectiveness of the proposed methods.
","['\nMizuki Yoshizawa\n', '\nAkira Terui\n', '\nMasahiko Mikawa\n']",26 pages. arXiv admin note: text overlap with arXiv:2111.00384,,http://arxiv.org/abs/2305.12451v2,cs.RO,"['cs.RO', 'cs.SC', 'math.AC', '13P15, 68W30']",,,[]
SCL(FOL) Can Simulate Non-Redundant Superposition Clause Learning,http://arxiv.org/abs/2305.12926v1,2023-05-22T11:12:39Z,2023-05-22T11:12:39Z,"  We show that SCL(FOL) can simulate the derivation of non-redundant clauses by
superposition for first-order logic without equality. Superposition-based
reasoning is performed with respect to a fixed reduction ordering. The
completeness proof of superposition relies on the grounding of the clause set.
It builds a ground partial model according to the fixed ordering, where minimal
false ground instances of clauses then trigger non-redundant superposition
inferences. We define a respective strategy for the SCL calculus such that
clauses learned by SCL and superposition inferences coincide. From this
perspective the SCL calculus can be viewed as a generalization of the
superposition calculus.
","['\nMartin Bromberger\n', '\nChaahat Jain\n', '\nChristoph Weidenbach\n']",,,http://arxiv.org/abs/2305.12926v1,cs.LO,"['cs.LO', 'cs.AI', 'cs.SC', 'I.2.3']",,,[]
The Complexity of Diagonalization,http://arxiv.org/abs/2305.10575v1,2023-05-17T21:21:11Z,2023-05-17T21:21:11Z,"  We survey recent progress on efficient algorithms for approximately
diagonalizing a square complex matrix in the models of rational (variable
precision) and finite (floating point) arithmetic. This question has been
studied across several research communities for decades, but many mysteries
remain. We present several open problems which we hope will be of broad
interest.
",['\nNikhil Srivastava\n'],11pp. Invited survey for ISSAC 2023. Comments welcome,,http://arxiv.org/abs/2305.10575v1,cs.SC,"['cs.SC', 'cs.CC', 'cs.DS', 'cs.NA', 'math.NA']",,,[]
Bézout Subresultants for Univariate Polynomials in General Basis,http://arxiv.org/abs/2305.03906v1,2023-05-06T02:51:06Z,2023-05-06T02:51:06Z,"  Subresultant is a powerful tool for developing various algorithms in computer
algebra. Subresultants for polynomials in standard basis (i.e., power basis)
have been well studied so far. With the popularity of basis-preserving
algorithms, resultants and subresultants in non-standard basis are drawing more
and more attention. In this paper, we develop a formula for B\'ezout
subresultants of univariate polynomials in general basis, which covers a broad
range of non-standard bases. More explicitly, the input polynomials are
provided in a given general basis and the resulting subresultants are
B\'ezout-type expressions in the same basis. It is shown that the subresultants
share the essential properties as the subresultants in standard basis.
","['\nJing Yang\n', '\nWei Yang\n']",,,http://arxiv.org/abs/2305.03906v1,cs.SC,['cs.SC'],,,[]
"Factorization and root-finding for polynomials over division quaternion
  algebras",http://arxiv.org/abs/2305.02072v1,2023-05-03T12:18:51Z,2023-05-03T12:18:51Z,"  Polynomial factorization and root finding are among the most standard themes
of computational mathematics. Yet still, little has been done for polynomials
over quaternion algebras, with the single exception of Hamiltonian quaternions
for which there are known numerical methods for polynomial root approximation.
The sole purpose of the present paper is to present a polynomial factorization
algorithm for division quaternion algebras over number fields, together with
its adaptation for root finding.
",['\nPrzemysław Koprowski\n'],,,http://arxiv.org/abs/2305.02072v1,cs.SC,"['cs.SC', 'math.RA', '11R52, 11Y40, 11Y05, 68W30', 'I.1.2']",,,[]
Score: A Rule Engine for the Scone Knowledge Base System,http://arxiv.org/abs/2305.04154v1,2023-05-07T00:50:05Z,2023-05-07T00:50:05Z,"  We present Score, a rule engine designed and implemented for the Scone
knowledge base system. Scone is a knowledge base system designed for storing
and manipulating rich representations of general knowledge in symbolic form. It
represents knowledge in the form of nodes and links in a network structure, and
it can perform basic inference about the relationships between different
elements efficiently. On its own, Scone acts as a sort of ""smart memory"" that
can interface with other software systems. One area of improvement for Scone is
how useful it can be in supplying knowledge to an intelligent agent that can
use the knowledge to perform actions and update the knowledge base with its
observations.
  We augment the Scone system with a production rule engine that automatically
performs simple inference based on existing and newly-added structures in
Scone's knowledge base, potentially improving the capabilities of any planning
systems built on top of Scone. Production rule systems consist of ""if-then""
production rules that try to match their predicates to existing knowledge and
fire their actions when their predicates are satisfied. We propose two kinds of
production rules, if-added and if-needed rules, that differ in how they are
checked and fired to cover multiple use cases. We then implement methods to
efficiently check and fire these rules in a large knowledge base. The new rule
engine is not meant to be a complex stand-alone planner, so we discuss how it
fits into the context of Scone and future work on planning systems.
","['\nJeffrey Chen\n', '\nScott E. Fahlman\n']","32 pages, 9 figures",,http://arxiv.org/abs/2305.04154v1,cs.AI,"['cs.AI', 'cs.SC']",,,[]
"Dimension Results for Extremal-Generic Polynomial Systems over Complete
  Toric Varieties",http://arxiv.org/abs/2305.07439v3,2023-05-12T13:01:36Z,2024-02-20T08:58:06Z,"  We study polynomial systems with prescribed monomial supports in the Cox
rings of toric varieties built from complete polyhedral fans. We present
combinatorial formulas for the dimensions of their associated subvarieties
under genericity assumptions on the coefficients of the polynomials. Using
these formulas, we identify at which degrees generic systems in polytopal
algebras form regular sequences. Our motivation comes from sparse elimination
theory, where knowing the expected dimension of these subvarieties leads to
specialized algorithms and to large speed-ups for solving sparse polynomial
systems. As a special case, we classify the degrees at which regular sequences
defined by weighted homogeneous polynomials can be found, answering an open
question in the Gr\""obner bases literature. We also show that deciding whether
a sparse system is generically a regular sequence in a polytopal algebra is
hard from the point of view of theoretical computational complexity.
","['\nMatías Bender\n', '\nPierre-Jean Spaenlehauer\n']",Accepted for publication in Journal of Algebra,,http://arxiv.org/abs/2305.07439v3,cs.SC,"['cs.SC', 'math.AG']",,,[]
Infinite matroids in tropical differential algebra,http://arxiv.org/abs/2305.04784v2,2023-05-08T15:35:20Z,2023-05-29T06:00:13Z,"  We consider a finite-dimensional vector space $W\subset K^E$ over an
arbitrary field $K$ and an arbitrary set $E$. We show that the set $C(W)\subset
2^E$ consisting of the minimal supports of $W$ are the circuits of a matroid on
$E$. In particular, we show that this matroid is cofinitary (hence, tame). When
the cardinality of $K$ is large enough (with respect to the cardinality of
$E$), then the set $trop(W)\subset 2^E$ consisting of all the supports of $W$
is a matroid itself.
  Afterwards we apply these results to tropical differential algebraic geometry
and study the set of supports $trop(Sol(\Sigma))\subset (2^{\mathbb{N}^{m}})^n$
of spaces of formal power series solutions $\text{Sol}(\Sigma)$ of systems of
linear differential equations $\Sigma$ in differential variables
$x_1,\ldots,x_n$ having coefficients in the ring ${K}[\![t_1,\ldots,t_m]\!]$.
If $\Sigma$ is of differential type zero, then the set $C(Sol(\Sigma))\subset
(2^{\mathbb{N}^{m}})^n$ of minimal supports defines a matroid on
$E=\mathbb{N}^{mn}$, and if the cardinality of $K$ is large enough, then the
set of supports $trop(Sol(\Sigma))$ itself is a matroid on $E$ as well. By
applying the fundamental theorem of tropical differential algebraic geometry
(fttdag), we give a necessary condition under which the set of solutions
$Sol(U)$ of a system $U$ of tropical linear differential equations to be a
matroid.
  We also give a counterexample to the fttdag for systems $\Sigma$ of linear
differential equations over countable fields. In this case, the set
$trop(Sol(\Sigma))$ may not form a matroid.
","['\nF. Aroca\n', '\nL. Bossinger\n', '\nS. Falkensteiner\n', '\nC. Garay Lopez\n', '\nL. R. Gonzalez-Ramirez\n', '\nC. V. Valencia Negrete\n']",,,http://arxiv.org/abs/2305.04784v2,math.AG,"['math.AG', 'cs.SC', 'math.CO', '14T99, 34A30, 13N99, 05B35']",,,[]
"G-MATT: Single-step Retrosynthesis Prediction using Molecular Grammar
  Tree Transformer",http://arxiv.org/abs/2305.03153v2,2023-05-04T21:04:19Z,2023-08-14T17:38:23Z,"  Various template-based and template-free approaches have been proposed for
single-step retrosynthesis prediction in recent years. While these approaches
demonstrate strong performance from a data-driven metrics standpoint, many
model architectures do not incorporate underlying chemistry principles. Here,
we propose a novel chemistry-aware retrosynthesis prediction framework that
combines powerful data-driven models with prior domain knowledge. We present a
tree-to-sequence transformer architecture that utilizes hierarchical SMILES
grammar-based trees, incorporating crucial chemistry information that is often
overlooked by SMILES text-based representations, such as local structures and
functional groups. The proposed framework, grammar-based molecular attention
tree transformer (G-MATT), achieves significant performance improvements
compared to baseline retrosynthesis models. G-MATT achieves a promising top-1
accuracy of 51% (top-10 accuracy of 79.1%), invalid rate of 1.5%, and bioactive
similarity rate of 74.8% on the USPTO- 50K dataset. Additional analyses of
G-MATT attention maps demonstrate the ability to retain chemistry knowledge
without relying on excessively complex model architectures.
","['\nKevin Zhang\n', '\nVipul Mann\n', '\nVenkat Venkatasubramanian\n']",,,http://arxiv.org/abs/2305.03153v2,cs.LG,"['cs.LG', 'cs.AI', 'cs.FL', 'cs.SC', 'q-bio.QM']",,,[]
On Rueppel's Linear Complexity Conjecture,http://arxiv.org/abs/2305.00405v1,2023-04-30T06:39:29Z,2023-04-30T06:39:29Z,"  Rueppel's conjecture on the linear complexity of the first $n$ terms of the
sequence $(1,1,0,1,0^3,1,0^7,1,0^{15},\ldots)$ was first proved by Dai using
the Euclidean algorithm. We have previously shown that we can attach a
homogeneous (annihilator) ideal of $F[x,z]$ to the first $n$ terms of a
sequence over a field $F$ and construct a pair of generating forms for it. This
approach gives another proof of Rueppel's conjecture. We also prove additional
properties of these forms and deduce the outputs of the LFSR synthesis
algorithm applied to the first $n$ terms. Further, dehomogenising the leading
generators yields the minimal polynomials of Dai.
",['\nGraham H. Norton\n'],,,http://arxiv.org/abs/2305.00405v1,cs.SC,['cs.SC'],,,[]
"Explainable AI Insights for Symbolic Computation: A case study on
  selecting the variable ordering for cylindrical algebraic decomposition",http://arxiv.org/abs/2304.12154v2,2023-04-24T15:05:04Z,2023-08-29T11:19:40Z,"  In recent years there has been increased use of machine learning (ML)
techniques within mathematics, including symbolic computation where it may be
applied safely to optimise or select algorithms. This paper explores whether
using explainable AI (XAI) techniques on such ML models can offer new insight
for symbolic computation, inspiring new implementations within computer algebra
systems that do not directly call upon AI tools. We present a case study on the
use of ML to select the variable ordering for cylindrical algebraic
decomposition. It has already been demonstrated that ML can make the choice
well, but here we show how the SHAP tool for explainability can be used to
inform new heuristics of a size and complexity similar to those human-designed
heuristics currently commonly used in symbolic computation.
","['\nLynn Pickering\n', '\nTereso Del Rio Almajano\n', '\nMatthew England\n', '\nKelly Cohen\n']",40 pages,"Journal of Symbolic Computation, Volume 123, Article Number
  102276. Elsevier, 2024",http://dx.doi.org/10.1016/j.jsc.2023.102276,cs.SC,"['cs.SC', 'cs.LG', '68W30, 68T05, 03C10', 'I.2.6; I.1.0']",10.1016/j.jsc.2023.102276,,[]
"Towards a generalizable simulation framework to study collisions between
  spacecraft and debris",http://arxiv.org/abs/2304.12799v1,2023-04-25T13:18:17Z,2023-04-25T13:18:17Z,"  In recent years, computer simulators of rigid-body systems have been
successfully used to improve and expand the field of developing new space
robots, becoming a leading tool for the preliminary investigation and
evaluation of space robotic missions. However, the impressive progress in
performance has not been matched yet by an improvement in modelling
capabilities, which remain limited to very basic representations of real
systems. We present a new approach to modelling and simulation of
collision-inclusive multibody dynamics by leveraging symbolic models generated
by a computer algebra system (CAS). While similar investigations into contact
dynamics on other domains exploit pre-existing models of common multibody
systems (e.g., industrial robot arms, humanoids, and wheeled robots), our focus
is on allowing researchers to develop models of novel designs of systems that
are not as common or yet to be fabricated: e.g., small spacecraft manipulators.
In this paper, we demonstrate the usefulness of our approach to investigate
spacecraft-debris collision dynamics.
","['\nSimone Asci\n', '\nAngadh Nanjangud\n']",,,http://arxiv.org/abs/2304.12799v1,cs.RO,"['cs.RO', 'cs.SC']",,,[]
On the Order of Power Series and the Sum of Square Roots Problem,http://arxiv.org/abs/2304.13605v1,2023-04-26T14:59:42Z,2023-04-26T14:59:42Z,"  This paper focuses on the study of the order of power series that are linear
combinations of a given finite set of power series. The order of a formal power
series, known as $\textrm{ord}(f)$, is defined as the minimum exponent of $x$
that has a non-zero coefficient in $f(x)$. Our first result is that the order
of the Wronskian of these power series is equivalent up to a polynomial factor,
to the maximum order which occurs in the linear combination of these power
series. This implies that the Wronskian approach used in (Kayal and Saha,
TOCT'2012) to upper bound the order of sum of square roots is optimal up to a
polynomial blowup. We also demonstrate similar upper bounds, similar to those
of (Kayal and Saha, TOCT'2012), for the order of power series in a variety of
other scenarios. We also solve a special case of the inequality testing problem
outlined in (Etessami et al., TOCT'2014).
  In the second part of the paper, we study the equality variant of the sum of
square roots problem, which is decidable in polynomial time due to (Bl\""omer,
FOCS'1991). We investigate a natural generalization of this problem when the
input integers are given as straight line programs. Under the assumption of the
Generalized Riemann Hypothesis (GRH), we show that this problem can be reduced
to the so-called ``one dimensional'' variant. We identify the key mathematical
challenges for solving this ``one dimensional'' variant.
","['\nLouis Gaillard\n', '\nGorav Jindal\n']",,,http://arxiv.org/abs/2304.13605v1,cs.CC,"['cs.CC', 'cs.SC']",,,[]
Drinfeld modules in SageMath,http://arxiv.org/abs/2305.00422v1,2023-04-30T08:03:19Z,2023-04-30T08:03:19Z,"  We present the first implementation of Drinfeld modules fully integrated in
the SageMath ecosystem. First features will be released with SageMath 10.0.
","['\nDavid Ayotte\n', '\nXavier Caruso\n', '\nAntoine Leudière\n', '\nJoseph Musleh\n']",,,http://arxiv.org/abs/2305.00422v1,cs.SC,"['cs.SC', 'math.NT']",,,[]
Arithmetic of D-Algebraic Functions,http://arxiv.org/abs/2305.00702v2,2023-05-01T08:01:34Z,2023-11-05T16:33:02Z,"  We are concerned with the arithmetic of solutions to ordinary or partial
nonlinear differential equations which are algebraic in the indeterminates and
their derivatives. We call these solutions D-algebraic functions, and their
equations are algebraic (ordinary or partial) differential equations (ADEs).
The general purpose is to find ADEs whose solutions contain specified rational
expressions of solutions to given ADEs. For univariate D-algebraic functions,
we show how to derive an ADE of smallest possible order. In the multivariate
case, we introduce a general algorithm for these computations and derive
conclusions on the order bound of the resulting algebraic PDE. Using our
accompanying Maple software, we discuss applications in physics, statistics,
and symbolic integration.
",['\nBertrand Teguia Tabuguia\n'],28 pages. 46 references,,http://arxiv.org/abs/2305.00702v2,cs.SC,"['cs.SC', 'math.AC', '12H05, 68W30, 13P10 (Primary), 34-04, 35-04 (Secondary)']",,,[]
"Deterministic identity testing paradigms for bounded top-fanin depth-4
  circuits",http://arxiv.org/abs/2304.11325v1,2023-04-22T05:36:12Z,2023-04-22T05:36:12Z,"  Polynomial Identity Testing (PIT) is a fundamental computational problem. The
famous depth-$4$ reduction result by Agrawal and Vinay (FOCS 2008) has made PIT
for depth-$4$ circuits an enticing pursuit. A restricted depth-4 circuit
computing a $n$-variate degree-$d$ polynomial of the form $\sum_{i = 1}^{k}
\prod_{j} g_{ij}$, where $\deg g_{ij} \leq \delta$ is called
$\Sigma^{[k]}\Pi\Sigma\Pi^{[\delta]}$ circuit. On further restricting $g_{ij}$
to be sum of univariates we obtain $\Sigma^{[k]}\Pi\Sigma\wedge$ circuits. The
largely open, special-cases of $\Sigma^{[k]}\Pi\Sigma\Pi^{[\delta]}$ for
constant $k$ and $\delta$, and $\Sigma^{[k]}\Pi\Sigma\wedge$ have been a source
of many great ideas in the last two decades. For eg. depth-$3$ ideas of Dvir
and Shpilka (STOC 2005), Kayal and Saxena (CCC 2006), and Saxena and Seshadhri
(FOCS 2010 and STOC 2011). Further, depth-$4$ ideas of Beecken, Mittmann and
Saxena (ICALP 2011), Saha, Saxena and Saptharishi (Comput.Compl. 2013), Forbes
(FOCS 2015), and Kumar and Saraf (CCC 2016). Additionally, geometric
Sylvester-Gallai ideas of Kayal and Saraf (FOCS 2009), Shpilka (STOC 2019), and
Peleg and Shpilka (CCC 2020, STOC 2021). Very recently, a subexponential-time
blackbox PIT algorithm for constant-depth circuits was obtained via lower bound
breakthrough of Limaye, Srinivasan, Tavenas (FOCS 2021). We solve two of the
basic underlying open problems in this work.
  We give the first polynomial-time PIT for $\Sigma^{[k]}\Pi\Sigma\wedge$. We
also give the first quasipolynomial time blackbox PIT for both
$\Sigma^{[k]}\Pi\Sigma\wedge$ and $\Sigma^{[k]}\Pi\Sigma\Pi^{[\delta]}$. A key
technical ingredient in all the three algorithms is how the logarithmic
derivative, and its power-series, modify the top $\Pi$-gate to $\wedge$.
","['\nPranjal Dutta\n', '\nPrateek Dwivedi\n', '\nNitin Saxena\n']","A preliminary version appeared in 36th Computational Complexity
  Conference (CCC), 2021",,http://dx.doi.org/10.4230/LIPIcs.CCC.2021.11,cs.CC,"['cs.CC', 'cs.SC', 'math.AC', 'F.2.1']",10.4230/LIPIcs.CCC.2021.11,,[]
Computing Circuit Polynomials in the Algebraic Rigidity Matroid,http://arxiv.org/abs/2304.12435v1,2023-04-24T20:20:03Z,2023-04-24T20:20:03Z,"  We present an algorithm for computing circuit polynomials in the algebraic
rigidity matroid $\mathcal{A}(\text{CM}_n)$ associated to the Cayley-Menger
ideal CM$_n$ for $n$ points in 2D. It relies on combinatorial resultants, a new
operation on graphs that captures properties of the Sylvester resultant of two
polynomials in this ideal. We show that every rigidity circuit has a
construction tree from K4 graphs based on this operation. Our algorithm
performs an algebraic elimination guided by such a construction tree, and uses
classical resultants, factorization and ideal membership. To highlight its
effectiveness, we implemented the algorithm in Mathematica: it took less than
15 seconds on an example where a Gr\""obner Basis calculation took 5 days and 6
hrs. Additional speed-ups are obtained using non-$K_4$ generators of the
Cayley-Menger ideal and simple variations on our main algorithm.
","['\nGoran Malic\n', '\nIleana Streinu\n']","To appear in SIAGA. arXiv admin note: substantial text overlap with
  arXiv:2103.08432",,http://arxiv.org/abs/2304.12435v1,math.CO,"['math.CO', 'cs.CG', 'cs.DM', 'cs.SC', 'math.AG']",,,[]
Operations for D-Algebraic Functions,http://arxiv.org/abs/2304.09675v2,2023-04-19T14:06:19Z,2023-07-10T13:35:27Z,"  A function is differentially algebraic (or simply D-algebraic) if there is a
polynomial relationship between some of its derivatives and the indeterminate
variable. Many functions in the sciences, such as Mathieu functions, the
Weierstrass elliptic functions, and holonomic or D-finite functions are
D-algebraic. These functions form a field, and are closed under composition,
taking functional inverse, and derivation. We present implementation for each
underlying operation. We also give a systematic way for computing an algebraic
differential equation from a linear differential equation with D-finite
function coefficients. Each command is a feature of our Maple package $NLDE$
available at https://mathrepo.mis.mpg.de/OperationsForDAlgebraicFunctions.
",['\nBertrand Teguia Tabuguia\n'],"4.5 pages + 14 references. ISSAC'23 software demonstration. To appear
  in ACM communications in Computer Algebra",,http://arxiv.org/abs/2304.09675v2,cs.SC,"['cs.SC', '68W30, 12H05 (Primary), 34-04 (Secondary)']",,,[]
"Computer-assisted proofs of ""Kariya's theorem"" with computer algebra",http://arxiv.org/abs/2304.07491v1,2023-04-15T06:56:55Z,2023-04-15T06:56:55Z,"  We demonstrate computer-assisted proofs of ""Kariya's theorem,"" a theorem in
elementary geometry, with computer algebra. In the proof of geometry theorem
with computer algebra, vertices of geometric figures that are subjects for the
proof are expressed as variables. The variables are classified into two
classes: arbitrarily given points and the points defined from the former points
by constraints. We show proofs of Kariya's theorem with two formulations
according to two ways for giving the arbitrary points: one is called ""vertex
formulation,"" and the other is called ""incenter formulation,"" with two methods:
one is Gr\""obner basis computation, and the other is Wu's method. Furthermore,
we show computer-assisted proofs of the property that the point so-called
""Kariya point"" is located on the hyperbola so-called ""Feuerbach's hyperbola"",
with two formulations and two methods.
","['\nAyane Ito\n', '\nTakefumi Kasai\n', '\nAkira Terui\n']",,,http://arxiv.org/abs/2304.07491v1,cs.SC,"['cs.SC', 'math.AC', '13P10, 68W30']",,,[]
"Algebraic solutions of linear differential equations: an arithmetic
  approach",http://arxiv.org/abs/2304.05061v1,2023-04-11T08:45:17Z,2023-04-11T08:45:17Z,"  Given a linear differential equation with coefficients in $\mathbb{Q}(x)$, an
important question is to know whether its full space of solutions consists of
algebraic functions, or at least if one of its specific solutions is algebraic.
After presenting motivating examples coming from various branches of
mathematics, we advertise in an elementary way a beautiful local-global
arithmetic approach to these questions, initiated by Grothendieck in the late
sixties. This approach has deep ramifications and leads to the still unsolved
Grothendieck-Katz $p$-curvature conjecture.
","['\nAlin Bostan\n', '\nXavier Caruso\n', '\nJulien Roques\n']",47 pages,,http://arxiv.org/abs/2304.05061v1,math.NT,"['math.NT', 'cs.SC', 'math.CA', 'math.CO']",,,[]
"Emergence of Symbols in Neural Networks for Semantic Understanding and
  Communication",http://arxiv.org/abs/2304.06377v3,2023-04-13T10:13:00Z,2023-06-25T05:53:06Z,"  The capacity to generate meaningful symbols and effectively employ them for
advanced cognitive processes, such as communication, reasoning, and planning,
constitutes a fundamental and distinctive aspect of human intelligence.
Existing deep neural networks still notably lag human capabilities in terms of
generating symbols for higher cognitive functions. Here, we propose a solution
(symbol emergence artificial network (SEA-net)) to endow neural networks with
the ability to create symbols, understand semantics, and achieve communication.
SEA-net generates symbols that dynamically configure the network to perform
specific tasks. These symbols capture compositional semantic information that
allows the system to acquire new functions purely by symbolic manipulation or
communication. In addition, these self-generated symbols exhibit an intrinsic
structure resembling that of natural language, suggesting a common framework
underlying the generation and understanding of symbols in both human brains and
artificial neural networks. We believe that the proposed framework will be
instrumental in producing more capable systems that can synergize the strengths
of connectionist and symbolic approaches for artificial intelligence (AI).
","['\nYang Chen\n', '\nLiangxuan Guo\n', '\nShan Yu\n']",,,http://arxiv.org/abs/2304.06377v3,cs.AI,"['cs.AI', 'cs.CL', 'cs.SC', 'q-bio.NC']",,,[]
Groebner.jl: A package for Gröbner bases computations in Julia,http://arxiv.org/abs/2304.06935v3,2023-04-14T05:47:34Z,2024-02-12T16:25:18Z,"  We present Groebner.jl, a Julia package for computing Groebner bases with the
F4 algorithm. Groebner.jl is an efficient, portable, and open-source software.
Groebner.jl works over integers modulo a prime and over the rationals, supports
basic multi-threading, and specializes in computation in the degree reverse
lexicographical monomial ordering. The implementation incorporates various
symbolic computation techniques and leverages the Julia type system and
tooling, which allows Groebner.jl to compete with the existing state of the
art, in many instances outperform it, and exceed them in extensibility.
Groebner.jl is freely available at https://github.com/sumiya11/Groebner.jl.
","['\nAlexander Demin\n', '\nShashi Gowda\n']",10 pages,,http://arxiv.org/abs/2304.06935v3,cs.MS,"['cs.MS', 'cs.SC', 'math.AC']",,,[]
Faster List Decoding of AG Codes,http://arxiv.org/abs/2304.07083v1,2023-04-14T12:18:35Z,2023-04-14T12:18:35Z,"  In this article, we present a fast algorithm performing an instance of the
Guruswami-Sudan list decoder for algebraic geometry codes. We show that any
such code can be decoded in $\tilde{O}(s^2\ell^{\omega-1}\mu^{\omega-1}(n+g) +
\ell^\omega \mu^\omega)$ operations in the underlying finite field, where $n$
is the code length, $g$ is the genus of the function field used to construct
the code, $s$ is the multiplicity parameter, $\ell$ is the designed list size
and $\mu$ is the smallest positive element in the Weierstrass semigroup of some
chosen place.
","['\nPeter Beelen\n', '\nVincent Neiger\n']",,,http://arxiv.org/abs/2304.07083v1,cs.IT,"['cs.IT', 'cs.SC', 'math.IT']",,,[]
"A multistep strategy for polynomial system solving over finite fields
  and a new algebraic attack on the stream cipher Trivium",http://arxiv.org/abs/2304.07820v1,2023-04-16T16:09:14Z,2023-04-16T16:09:14Z,"  In this paper we introduce a multistep generalization of the
guess-and-determine or hybrid strategy for solving a system of multivariate
polynomial equations over a finite field. In particular, we propose performing
the exhaustive evaluation of a subset of variables stepwise, that is, by
incrementing the size of such subset each time that an evaluation leads to a
polynomial system which is possibly unfeasible to solve. The decision about
which evaluation to extend is based on a preprocessing consisting in computing
an incomplete Grobner basis after the current evaluation, which possibly
generates linear polynomials that are used to eliminate further variables. If
the number of remaining variables in the system is deemed still too high, the
evaluation is extended and the preprocessing is iterated. Otherwise, we solve
the system by a Grobner basis computation.
  Having in mind cryptanalytic applications, we present an implementation of
this strategy in an algorithm called MultiSolve which is designed for
polynomial systems having at most one solution. We prove explicit formulas for
its complexity which are based on probability distributions that can be easily
estimated by performing the proposed preprocessing on a testset of evaluations
for different subsets of variables. We prove that an optimal complexity of
MultiSolve is achieved by using a full multistep strategy with a maximum number
of steps and in turn the classical guess-and-determine strategy, which
essentially is a strategy consisting of a single step, is the worst choice.
Finally, we extensively study the behaviour of MultiSolve when performing an
algebraic attack on the well-known stream cipher Trivium.
","['\nRoberto La Scala\n', '\nFederico Pintore\n', '\nSharwan K. Tiwari\n', '\nAndrea Visconti\n']",27 pages,,http://arxiv.org/abs/2304.07820v1,cs.SC,"['cs.SC', 'cs.CR', 'math.AC']",,,[]
Density Elicitation with applications in Probabilistic Loops,http://arxiv.org/abs/2304.09094v1,2023-04-17T14:46:38Z,2023-04-17T14:46:38Z,"  Probabilistic loops can be employed to implement and to model different
processes ranging from software to cyber-physical systems. One main challenge
is how to automatically estimate the distribution of the underlying continuous
random variables symbolically and without sampling. We develop an approach,
which we call K-series estimation, to approximate statically the joint and
marginal distributions of a vector of random variables updated in a
probabilistic non-nested loop with polynomial and non-polynomial assignments.
Our approach is a general estimation method for an unknown probability density
function with bounded support. It naturally complements algorithms for
automatic derivation of moments in probabilistic loops such
as~\cite{BartocciKS19,Moosbruggeretal2022}. Its only requirement is a finite
number of moments of the unknown density. We show that Gram-Charlier (GC)
series, a widely used estimation method, is a special case of K-series when the
normal probability density function is used as reference distribution. We
provide also a formulation suitable for estimating both univariate and
multivariate distributions. We demonstrate the feasibility of our approach
using multiple examples from the literature.
","['\nAndrey Kofnov\n', '\nEzio Bartocci\n', '\nEfstathia Bura\n']",34 pages,,http://arxiv.org/abs/2304.09094v1,stat.ME,"['stat.ME', 'cs.NA', 'cs.SC', 'cs.SY', 'eess.SY', 'math.NA', 'stat.AP', '62G07, 60E05 (Primary) 60B10 (Secondary)', 'G.3; I.1.1']",,,[]
Two Variants of Bezout Subresultants for Several Univariate Polynomials,http://arxiv.org/abs/2304.00262v2,2023-04-01T08:38:06Z,2023-05-09T00:58:06Z,"  In this paper, we develop two variants of Bezout subresultant formulas for
several polynomials, i.e., hybrid Bezout subresultant polynomial and
non-homogeneous Bezout subresultant polynomial. Rather than simply extending
the variants of Bezout subresultant formulas developed by Diaz-Toca and
Gonzalez-Vega in 2004 for two polynomials to arbitrary number of polynomials,
we propose a new approach to formulating two variants of the Bezout-type
subresultant polynomials for a set of univariate polynomials. Experimental
results show that the Bezout-type subresultant formulas behave better than
other known formulas when used to compute multi-polynomial subresultants, among
which the non-homogeneous Bezout-type formula shows the best performance.
","['\nWeidong Wang\n', '\nJing Yang\n']",,,http://arxiv.org/abs/2304.00262v2,cs.SC,['cs.SC'],,,[]
Efficient Generic Quotients Using Exact Arithmetic,http://arxiv.org/abs/2304.01753v5,2023-04-04T12:43:37Z,2023-06-27T23:13:27Z,"  The usual formulation of efficient division uses Newton iteration to compute
an inverse in a related domain where multiplicative inverses exist. On one
hand, Newton iteration allows quotients to be calculated using an efficient
multiplication method. On the other hand, working in another domain is not
always desirable and can lead to a library structure where arithmetic domains
are interdependent. This paper uses the concept of a whole shifted inverse and
modified Newton iteration to compute quotients efficiently without leaving the
original domain. The iteration is generic to domains having a suitable shift
operation, such as integers or polynomials with coefficients that do not
necessarily commute.
",['\nStephen M. Watt\n'],Reformat of v3,,http://arxiv.org/abs/2304.01753v5,cs.SC,['cs.SC'],,,[]
"Classifying sequences by combining context-free grammars and OWL
  ontologies",http://arxiv.org/abs/2304.03089v1,2023-04-06T14:16:46Z,2023-04-06T14:16:46Z,"  This paper describes a pattern to formalise context-free grammars in OWL and
its use for sequence classification. The proposed approach is compared to
existing methods in terms of computational complexity as well as pragmatic
applicability, with examples in the music domain.
","['\nNicolas Lazzari\n', '\nAndrea Poltronieri\n', '\nValentina Presutti\n']",Preprint - ESWC 2023,,http://dx.doi.org/10.1007/978-3-031-33455-9_10,cs.SC,"['cs.SC', 'cs.FL']",10.1007/978-3-031-33455-9_10,,[]
Taylor Polynomials of Rational Functions,http://arxiv.org/abs/2304.00712v1,2023-04-03T04:07:58Z,2023-04-03T04:07:58Z,"  A Taylor variety consists of all fixed order Taylor polynomials of rational
functions, where the number of variables and degrees of numerators and
denominators are fixed. In one variable, Taylor varieties are given by rank
constraints on Hankel matrices. Inversion of the natural parametrization is
known as Pad\'e approximation. We study the dimension and defining ideals of
Taylor varieties. Taylor hypersurfaces are interesting for projective geometry,
since their Hessians tend to vanish. In three and more variables, there exist
defective Taylor varieties whose dimension is smaller than the number of
parameters. We explain this with Fr\""oberg's Conjecture in commutative algebra.
","['\nAldo Conca\n', '\nSimone Naldi\n', '\nGiorgio Ottaviani\n', '\nBernd Sturmfels\n']",20 pages,,http://arxiv.org/abs/2304.00712v1,math.AG,"['math.AG', 'cs.SC', 'math.AC']",,,[]
"Stability and chaos of the duopoly model of Kopel: A study based on
  symbolic computations",http://arxiv.org/abs/2304.02136v2,2023-04-04T21:35:38Z,2023-05-29T03:59:09Z,"  Since Kopel's duopoly model was proposed about three decades ago, there are
almost no analytical results on the equilibria and their stability in the
asymmetric case. The first objective of our study is to fill this gap. This
paper analyzes the asymmetric duopoly model of Kopel analytically by using
several tools based on symbolic computations. We discuss the possibility of the
existence of multiple positive equilibria and establish necessary and
sufficient conditions for a given number of positive equilibria to exist. The
possible positions of the equilibria in Kopel's model are also explored.
Furthermore, in the asymmetric model of Kopel, if the duopolists adopt the best
response reactions or homogeneous adaptive expectations, we establish rigorous
conditions for the local stability of equilibria for the first time. The
occurrence of chaos in Kopel's model seems to be supported by observations
through numerical simulations, which, however, is challenging to prove
rigorously. The second objective is to prove the existence of snapback
repellers in Kopel's map, which implies the existence of chaos in the sense of
Li-Yorke according to Marotto's theorem.
","['\nXiaoliang Li\n', '\nKongyan Chen\n', '\nWei Niu\n', '\nBo Huang\n']",arXiv admin note: substantial text overlap with arXiv:2301.12628,,http://arxiv.org/abs/2304.02136v2,math.DS,"['math.DS', 'cs.SC', 'econ.TH']",,,[]
Supercomputer Environment for Recursive Matrix Algorithms,http://arxiv.org/abs/2303.11017v1,2023-03-20T10:52:02Z,2023-03-20T10:52:02Z,"  A new runtime environment for the execution of recursive matrix algorithms on
a supercomputer with distributed memory is proposed. It is designed both for
dense and sparse matrices. The environment ensures decentralized control of the
computation process. As an example of a block recursive algorithm, the Cholesky
factorization of a symmetric positive definite matrix in the form of a block
dichotomous algorithm is described. The results of experiments with different
numbers of cores are presented, demonstrating good scalability of the proposed
solution.
","['\nGennadi Malaschonok\n', '\nAlla Sidko\n']","24 pages, 9 figures","Program Comput Soft. 2022, 48, 90-101",http://dx.doi.org/10.1134/S0361768822020086,cs.SC,"['cs.SC', '15Axx, 15Bxx', 'G.4; I.1; J.7; C.3.4']",10.1134/S0361768822020086,,[]
"$D$-Module Techniques for Solving Differential Equations in the Context
  of Feynman Integrals",http://arxiv.org/abs/2303.11105v2,2023-03-20T13:41:20Z,2023-04-08T08:20:20Z,"  Feynman integrals are solutions to linear partial differential equations with
polynomial coefficients. Using a triangle integral with general exponents as a
case in point, we compare $D$-module methods to dedicated methods developed for
solving differential equations appearing in the context of Feynman integrals,
and provide a dictionary of the relevant concepts. In particular, we implement
an algorithm due to Saito, Sturmfels, and Takayama to derive canonical series
solutions of regular holonomic $D$-ideals, and compare them to asymptotic
series derived by the respective Fuchsian systems.
","['\nJohannes Henn\n', '\nElizabeth Pratt\n', '\nAnna-Laura Sattelberger\n', '\nSimone Zoia\n']","35 pages, 2 figures, 2 appendices; comments welcome",,http://arxiv.org/abs/2303.11105v2,hep-th,"['hep-th', 'cs.SC', 'math.AG', 'math.CA']",,,[]
"Abstract Visual Reasoning: An Algebraic Approach for Solving Raven's
  Progressive Matrices",http://arxiv.org/abs/2303.11730v1,2023-03-21T10:34:39Z,2023-03-21T10:34:39Z,"  We introduce algebraic machine reasoning, a new reasoning framework that is
well-suited for abstract reasoning. Effectively, algebraic machine reasoning
reduces the difficult process of novel problem-solving to routine algebraic
computation. The fundamental algebraic objects of interest are the ideals of
some suitably initialized polynomial ring. We shall explain how solving Raven's
Progressive Matrices (RPMs) can be realized as computational problems in
algebra, which combine various well-known algebraic subroutines that include:
Computing the Gr\""obner basis of an ideal, checking for ideal containment, etc.
Crucially, the additional algebraic structure satisfied by ideals allows for
more operations on ideals beyond set-theoretic operations.
  Our algebraic machine reasoning framework is not only able to select the
correct answer from a given answer set, but also able to generate the correct
answer with only the question matrix given. Experiments on the I-RAVEN dataset
yield an overall $93.2\%$ accuracy, which significantly outperforms the current
state-of-the-art accuracy of $77.0\%$ and exceeds human performance at $84.4\%$
accuracy.
","['\nJingyi Xu\n', '\nTushar Vaidya\n', '\nYufei Wu\n', '\nSaket Chandra\n', '\nZhangsheng Lai\n', '\nKai Fong Ernest Chong\n']","Accepted at IEEE/CVF Conference on Computer Vision and Pattern
  Recognition (CVPR) 2023. 30 pages, 7 figures (including supplementary
  material). First three authors contributed equally. Code is available at:
  https://github.com/Xu-Jingyi/AlgebraicMR",,http://arxiv.org/abs/2303.11730v1,cs.CV,"['cs.CV', 'cs.AI', 'cs.SC', 'math.AC', '13P25, 68W30', 'I.1; I.2.4; I.2.6; I.5.1']",,,[]
Efficient Symbolic Reasoning for Neural-Network Verification,http://arxiv.org/abs/2303.13588v1,2023-03-23T18:08:11Z,2023-03-23T18:08:11Z,"  The neural network has become an integral part of modern software systems.
However, they still suffer from various problems, in particular, vulnerability
to adversarial attacks. In this work, we present a novel program reasoning
framework for neural-network verification, which we refer to as symbolic
reasoning. The key components of our framework are the use of the symbolic
domain and the quadratic relation. The symbolic domain has very flexible
semantics, and the quadratic relation is quite expressive. They allow us to
encode many verification problems for neural networks as quadratic programs.
Our scheme then relaxes the quadratic programs to semidefinite programs, which
can be efficiently solved. This framework allows us to verify various
neural-network properties under different scenarios, especially those that
appear challenging for non-symbolic domains. Moreover, it introduces new
representations and perspectives for the verification tasks. We believe that
our framework can bring new theoretical insights and practical tools to
verification problems for neural networks.
","['\nZi Wang\nDj\n', '\nSomesh Jha\nDj\n', '\n Krishnamurthy\nDj\n', '\n Dvijotham\n']",,,http://arxiv.org/abs/2303.13588v1,cs.AI,"['cs.AI', 'cs.LG', 'cs.SC']",,,"['Dj', 'Dj', 'Dj']"
Learning Reward Machines in Cooperative Multi-Agent Tasks,http://arxiv.org/abs/2303.14061v4,2023-03-24T15:12:28Z,2023-05-24T07:20:20Z,"  This paper presents a novel approach to Multi-Agent Reinforcement Learning
(MARL) that combines cooperative task decomposition with the learning of reward
machines (RMs) encoding the structure of the sub-tasks. The proposed method
helps deal with the non-Markovian nature of the rewards in partially observable
environments and improves the interpretability of the learnt policies required
to complete the cooperative task. The RMs associated with each sub-task are
learnt in a decentralised manner and then used to guide the behaviour of each
agent. By doing so, the complexity of a cooperative multi-agent problem is
reduced, allowing for more effective learning. The results suggest that our
approach is a promising direction for future research in MARL, especially in
complex environments with large state spaces and multiple agents.
","['\nLeo Ardon\n', '\nDaniel Furelos-Blanco\n', '\nAlessandra Russo\n']","Neuro-symbolic AI for Agent and Multi-Agent Systems Workshop at
  AAMAS'23",,http://arxiv.org/abs/2303.14061v4,cs.AI,"['cs.AI', 'cs.MA', 'cs.SC']",,,[]
Learning to Operate in Open Worlds by Adapting Planning Models,http://arxiv.org/abs/2303.14272v1,2023-03-24T21:04:16Z,2023-03-24T21:04:16Z,"  Planning agents are ill-equipped to act in novel situations in which their
domain model no longer accurately represents the world. We introduce an
approach for such agents operating in open worlds that detects the presence of
novelties and effectively adapts their domain models and consequent action
selection. It uses observations of action execution and measures their
divergence from what is expected, according to the environment model, to infer
existence of a novelty. Then, it revises the model through a heuristics-guided
search over model changes. We report empirical evaluations on the CartPole
problem, a standard Reinforcement Learning (RL) benchmark. The results show
that our approach can deal with a class of novelties very quickly and in an
interpretable fashion.
","['\nWiktor Piotrowski\n', '\nRoni Stern\n', '\nYoni Sher\n', '\nJacob Le\n', '\nMatthew Klenk\n', '\nJohan deKleer\n', '\nShiwali Mohan\n']","To appears in the Proceedings of the 22nd International Conference on
  Autonomous Agents and Multiagent Systems (AAMAS 2023)",,http://arxiv.org/abs/2303.14272v1,cs.AI,"['cs.AI', 'cs.LG', 'cs.SC', 'I.2.6; I.2.8']",,,[]
Knowledge Enhanced Graph Neural Networks for Graph Completion,http://arxiv.org/abs/2303.15487v3,2023-03-27T07:53:43Z,2023-08-31T08:58:17Z,"  Graph data is omnipresent and has a wide variety of applications, such as in
natural science, social networks, or the semantic web. However, while being
rich in information, graphs are often noisy and incomplete. As a result, graph
completion tasks, such as node classification or link prediction, have gained
attention. On one hand, neural methods, such as graph neural networks, have
proven to be robust tools for learning rich representations of noisy graphs. On
the other hand, symbolic methods enable exact reasoning on graphs.We propose
Knowledge Enhanced Graph Neural Networks (KeGNN), a neuro-symbolic framework
for graph completion that combines both paradigms as it allows for the
integration of prior knowledge into a graph neural network model.Essentially,
KeGNN consists of a graph neural network as a base upon which knowledge
enhancement layers are stacked with the goal of refining predictions with
respect to prior knowledge.We instantiate KeGNN in conjunction with two
state-of-the-art graph neural networks, Graph Convolutional Networks and Graph
Attention Networks, and evaluate KeGNN on multiple benchmark datasets for node
classification.
","['\nLuisa Werner\nTYREX, UGA\n', '\nNabil Layaïda\nTYREX\n', '\nPierre Genevès\nCNRS, TYREX\n', '\nSarah Chlyah\nTYREX\n']",,,http://arxiv.org/abs/2303.15487v3,cs.AI,"['cs.AI', 'cs.LG', 'cs.LO', 'cs.SC']",,,"['TYREX, UGA', 'TYREX', 'CNRS, TYREX', 'TYREX']"
On real and observable realizations of input-output equations,http://arxiv.org/abs/2303.16799v1,2023-03-29T15:42:01Z,2023-03-29T15:42:01Z,"  Given a single algebraic input-output equation, we present a method for
finding different representations of the associated system in the form of
rational realizations; these are dynamical systems with rational right-hand
sides. It has been shown that in the case where the input-output equation is of
order one, rational realizations can be computed, if they exist. In this work,
we focus first on the existence and actual computation of the so-called
observable rational realizations, and secondly on rational realizations with
real coefficients. The study of observable realizations allows to find every
rational realization of a given first order input-output equation, and the
necessary field extensions in this process. We show that for first order
input-output equations the existence of a rational realization is equivalent to
the existence of an observable rational realization. Moreover, we give a
criterion to decide the existence of real rational realizations. The
computation of observable and real realizations of first order input-output
equations is fully algorithmic. We also present partial results for the case of
higher order input-output equations.
","['\nSebastian Falkensteiner\n', '\nDmitrii Pavlov\n', '\nRafael Sendra\n']",,,http://arxiv.org/abs/2303.16799v1,cs.SC,"['cs.SC', 'math.AG', 'math.DS', 'math.OC', '93B15, 93B07, 14H50, 34H05']",,,[]
Distance Evaluation to the Set of Defective Matrices,http://arxiv.org/abs/2303.07235v1,2023-03-13T16:07:41Z,2023-03-13T16:07:41Z,"  We treat the problem of the Frobenius distance evaluation from a given matrix
$ A \in \mathbb R^{n\times n} $ with distinct eigenvalues to the manifold of
matrices with multiple eigenvalues. On restricting considerations to the rank $
1 $ real perturbation matrices, we prove that the distance in question equals $
\sqrt{z_{\ast}} $ where $ z_{\ast} $ is a positive (generically, the least
positive) zero of the algebraic equation $$ \mathcal F(z) = 0, \ \mbox{where} \
\mathcal F(z):= \mathcal D_{\lambda} \left( \det \left[ (\lambda I - A)(\lambda
I - A^{\top})-z I_n \right] \right)/z^n $$ and $ \mathcal D_{\lambda} $ stands
for the discriminant of the polynomial treated with respect to $\lambda $. In
the framework of this approach we also provide the procedure for finding the
nearest to $ A $ matrix with multiple eigenvalue. Generalization of the problem
to the case of complex perturbations is also discussed. Several examples are
presented clarifying the computational aspects of the approach.
","['\nAlexei Yu. Uteshev\n', '\nElizaveta A. Kalinina\n', '\nMarina V. Goncharova\n']","28 pages, 1 figure",,http://arxiv.org/abs/2303.07235v1,cs.SC,"['cs.SC', '68W30, 15A18, 12D10, 58C40', 'I.1.4; G.1.3; G.1.5']",,,[]
Local Search for Solving Satisfiability of Polynomial Formulas,http://arxiv.org/abs/2303.09072v2,2023-03-16T04:08:31Z,2023-03-21T04:06:23Z,"  Satisfiability Modulo the Theory of Nonlinear Real Arithmetic, SMT(NRA) for
short, concerns the satisfiability of polynomial formulas, which are
quantifier-free Boolean combinations of polynomial equations and inequalities
with integer coefficients and real variables. In this paper, we propose a local
search algorithm for a special subclass of SMT(NRA), where all constraints are
strict inequalities. An important fact is that, given a polynomial formula with
$n$ variables, the zero level set of the polynomials in the formula decomposes
the $n$-dimensional real space into finitely many components (cells) and every
polynomial has constant sign in each cell. The key point of our algorithm is a
new operation based on real root isolation, called cell-jump, which updates the
current assignment along a given direction such that the assignment can `jump'
from one cell to another. One cell-jump may adjust the values of several
variables while traditional local search operations, such as flip for SAT and
critical move for SMT(LIA), only change that of one variable. We also design a
two-level operation selection to balance the success rate and efficiency.
Furthermore, our algorithm can be easily generalized to a wider subclass of
SMT(NRA) where polynomial equations linear with respect to some variable are
allowed. Experiments show the algorithm is competitive with state-of-the-art
SMT solvers, and performs particularly well on those formulas with high-degree
polynomials.
","['\nHaokun Li\n', '\nBican Xia\n', '\nTianqi Zhao\n']",,,http://arxiv.org/abs/2303.09072v2,cs.LO,"['cs.LO', 'cs.SC']",,,[]
Rigorous Analytic Combinatorics in Several Variables in SageMath,http://arxiv.org/abs/2303.09603v2,2023-03-16T19:08:17Z,2023-08-31T23:41:44Z,"  We introduce the new sage_acsv package for the SageMath computer algebra
system, allowing users to rigorously compute asymptotics for a large variety of
multivariate sequences with rational generating functions. Using Sage's support
for exact computations over the algebraic number field, this package provides
the first rigorous implementation of algorithms from the theory of analytic
combinatorics in several variables.
","['\nBenjamin Hackl\n', '\nAndrew Luo\n', '\nStephen Melczer\n', '\nJesse Selover\n', '\nElaine Wong\n']",8 pages; Package: https://pypi.org/project/sage-acsv/,"S\'eminaire Lotharingiende Combinatoire 89B (2023): Proceedings of
  the 35th FPSAC Conference, Article #90,12pp",http://arxiv.org/abs/2303.09603v2,math.CO,"['math.CO', 'cs.SC']",,,[]
Automated Grading of Automata with ACL2s,http://arxiv.org/abs/2303.05867v1,2023-03-10T11:37:27Z,2023-03-10T11:37:27Z,"  Almost all Computer Science programs require students to take a course on the
Theory of Computation (ToC) which covers various models of computation such as
finite automata, push-down automata and Turing machines. ToC courses tend to
give assignments that require paper-and-pencil solutions. Grading such
assignments takes time, so students typically receive feedback for their
solutions more than a week after they complete them. We present the Automatic
Automata Checker (A2C), an open source library that enables one to construct
executable automata using definitions that mimic those found in standard
textbooks. Such constructions are easy to reason about using semantic
equivalence checks, properties and test cases. Instructors can conveniently
specify solutions in the form of their own constructions. A2C can check for
semantic equivalence between student and instructor solutions and can
immediately generate actionable feedback, which helps students better
understand the material. A2C can be downloaded and used locally by students as
well as integrated into Learning Management Systems (LMS) like Gradescope to
automatically grade student submissions and generate feedback. A2C is based on
the ACL2s interactive theorem prover, which provides advanced methods for
stating, proving and disproving properties. Since feedback is automatic, A2C
can be deployed at scale and integrated into massively open online courses.
","['\nAnkit Kumar\nNortheastern University\n', '\nAndrew Walter\nNortheastern University\n', '\nPanagiotis Manolios\nNortheastern University\n']","In Proceedings ThEdu'22, arXiv:2303.05360","EPTCS 375, 2023, pp. 77-91",http://dx.doi.org/10.4204/EPTCS.375.7,cs.LO,"['cs.LO', 'cs.FL', 'cs.SC']",10.4204/EPTCS.375.7,,"['Northeastern University', 'Northeastern University', 'Northeastern University']"
MizAR 60 for Mizar 50,http://arxiv.org/abs/2303.06686v1,2023-03-12T15:13:05Z,2023-03-12T15:13:05Z,"  As a present to Mizar on its 50th anniversary, we develop an AI/TP system
that automatically proves about 60\% of the Mizar theorems in the hammer
setting. We also automatically prove 75\% of the Mizar theorems when the
automated provers are helped by using only the premises used in the
human-written Mizar proofs. We describe the methods and large-scale experiments
leading to these results. This includes in particular the E and Vampire
provers, their ENIGMA and Deepire learning modifications, a number of
learning-based premise selection methods, and the incremental loop that
interleaves growing a corpus of millions of ATP proofs with training
increasingly strong AI/TP systems on them. We also present a selection of Mizar
problems that were proved automatically.
","['\nJan Jakubův\n', '\nKarel Chvalovský\n', '\nZarathustra Goertzel\n', '\nCezary Kaliszyk\n', '\nMirek Olšák\n', '\nBartosz Piotrowski\n', '\nStephan Schulz\n', '\nMartin Suda\n', '\nJosef Urban\n']",,,http://arxiv.org/abs/2303.06686v1,cs.AI,"['cs.AI', 'cs.LG', 'cs.LO', 'cs.SC']",,,[]
"Exact and optimal quadratization of nonlinear finite-dimensional
  non-autonomous dynamical systems",http://arxiv.org/abs/2303.10285v4,2023-03-17T23:52:35Z,2023-12-05T21:18:27Z,"  Quadratization of polynomial and nonpolynomial systems of ordinary
differential equations is advantageous in a variety of disciplines, such as
systems theory, fluid mechanics, chemical reaction modeling and mathematical
analysis. A quadratization reveals new variables and structures of a model,
which may be easier to analyze, simulate, control, and provides a convenient
parametrization for learning. This paper presents novel theory, algorithms and
software capabilities for quadratization of non-autonomous ODEs. We provide
existence results, depending on the regularity of the input function, for cases
when a quadratic-bilinear system can be obtained through quadratization. We
further develop existence results and an algorithm that generalizes the process
of quadratization for systems with arbitrary dimension that retain the
nonlinear structure when the dimension grows. For such systems, we provide
dimension-agnostic quadratization. An example is semi-discretized PDEs, where
the nonlinear terms remain symbolically identical when the discretization size
increases. As an important aspect for practical adoption of this research, we
extended the capabilities of the QBee software towards both non-autonomous
systems of ODEs and ODEs with arbitrary dimension. We present several examples
of ODEs that were previously reported in the literature, and where our new
algorithms find quadratized ODE systems with lower dimension than the
previously reported lifting transformations. We further highlight an important
area of quadratization: reduced-order model learning. This area can benefit
significantly from working in the optimal lifting variables, where quadratic
models provide a direct parametrization of the model that also avoids
additional hyperreduction for the nonlinear terms. A solar wind example
highlights these advantages.
","['\nAndrey Bychkov\n', '\nOpal Issan\n', '\nGleb Pogudin\n', '\nBoris Kramer\n']",,,http://arxiv.org/abs/2303.10285v4,cs.SC,"['cs.SC', 'cs.NA', 'math.DS', 'math.NA']",,,[]
Machine-Learned Premise Selection for Lean,http://arxiv.org/abs/2304.00994v2,2023-03-17T10:37:34Z,2023-06-14T10:06:52Z,"  We introduce a machine-learning-based tool for the Lean proof assistant that
suggests relevant premises for theorems being proved by a user. The design
principles for the tool are (1) tight integration with the proof assistant, (2)
ease of use and installation, (3) a lightweight and fast approach. For this
purpose, we designed a custom version of the random forest model, trained in an
online fashion. It is implemented directly in Lean, which was possible thanks
to the rich and efficient metaprogramming features of Lean 4. The random forest
is trained on data extracted from mathlib -- Lean's mathematics library. We
experiment with various options for producing training features and labels. The
advice from a trained model is accessible to the user via the suggest_premises
tactic which can be called in an editor while constructing a proof
interactively.
","['\nBartosz Piotrowski\n', '\nRamon Fernández Mir\n', '\nEdward Ayers\n']",,,http://arxiv.org/abs/2304.00994v2,cs.AI,"['cs.AI', 'cs.LG', 'cs.LO', 'cs.SC']",,,[]
"Transformer Models for Type Inference in the Simply Typed Lambda
  Calculus: A Case Study in Deep Learning for Code",http://arxiv.org/abs/2304.10500v1,2023-03-15T17:44:39Z,2023-03-15T17:44:39Z,"  Despite a growing body of work at the intersection of deep learning and
formal languages, there has been relatively little systematic exploration of
transformer models for reasoning about typed lambda calculi. This is an
interesting area of inquiry for two reasons. First, typed lambda calculi are
the lingua franc of programming languages. A set of heuristics that relate
various typed lambda calculi to effective neural architectures would provide a
systematic method for mapping language features (e.g., polymorphism, subtyping,
inheritance, etc.) to architecture choices. Second, transformer models are
widely used in deep learning architectures applied to code, but the design and
hyperparameter space for them is large and relatively unexplored in programming
language applications. Therefore, we suggest a benchmark that allows us to
explore exactly this through perhaps the simplest and most fundamental property
of a programming language: the relationship between terms and types.
Consequently, we begin this inquiry of transformer architectures for typed
lambda calculi by exploring the effect of transformer warm-up and optimizer
selection in the task of type inference: i.e., predicting the types of lambda
calculus terms using only transformers. We find that the optimization landscape
is difficult even in this simple setting. One particular experimental finding
is that optimization by Adafactor converges much faster compared to the
optimization by Adam and RAdam. We conjecture that such different performance
of optimizers might be related to the difficulties of generalization over
formally generated dataset.
","['\nBrando Miranda\n', '\nAvi Shinnar\n', '\nVasily Pestun\n', '\nBarry Trager\n']",22 pages,,http://arxiv.org/abs/2304.10500v1,cs.PL,"['cs.PL', 'cs.AI', 'cs.LG', 'cs.LO', 'cs.SC']",,,[]
In-place fast polynomial modular remainder,http://arxiv.org/abs/2302.13600v5,2023-02-27T09:08:34Z,2024-01-16T09:23:50Z,"  We consider the simultaneously fast and in-place computation of the Euclidean
polynomial modular remainder $R(X) $\not\equiv$ A(X) \mod B(X)$ with $A$ and
$B$ of respective degrees $n$ and $m $\le$ n$. But fast algorithms for this
usually come at the expense of (potentially large) extra temporary space. To
remain in-place a further issue is to avoid the storage of the whole quotient
$Q(X)$ such that $A=BQ+R$. If the multiplication of two polynomials of degree
$k$ can be performed with $M(k)$ operations and $O(k)$ extra space, and if it
is allowed to use the input space of $A$ or $B$ for intermediate computations,
but putting $A$ and $B$ back to their initial states after the completion of
the remainder computation, we here propose an in-place algorithm (that is with
its extra required space reduced to $O(1)$ only) using at most $O(n/m
M(m)\log(m)$ arithmetic operations, if $\M(m)$ is quasi-linear, or $O(n/m
M(m)}$ otherwise. We also propose variants that compute -- still in-place and
with the same kind of complexity bounds -- the over-place remainder $A(X)
$\not\equiv$ A(X) \mod B(X)$, the accumulated remainder $R(X) += A(X) \mod
B(X)$ and the accumulated modular multiplication $R(X) += A(X)C(X) \mod B(X)$.
To achieve this, we develop techniques for Toeplitz matrix operations which
output is also part of the input. Fast and in-place accumulating versions are
obtained for the latter, and thus for convolutions, and then used for
polynomial remaindering. This is realized via further reductions to accumulated
polynomial multiplication, for which fast in-place algorithms have recently
been developed.
","['\nJean-Guillaume Dumas\nCASC\n', '\nBruno Grenet\nCASC\n']",,,http://arxiv.org/abs/2302.13600v5,cs.SC,['cs.SC'],,,"['CASC', 'CASC']"
"Sufficient conditions for the surjectivity of radical curve
  parametrizations",http://arxiv.org/abs/2303.00368v1,2023-03-01T09:50:34Z,2023-03-01T09:50:34Z,"  In this paper, we introduce the notion of surjective radical parametrization
and we prove sufficient conditions for a radical curve parametrization to be
surjective.
","['\nJorce Caravantes\n', '\nJ. Rafael Sendra\n', '\nDavid Sevilla\n', '\nCarlos Villarino\n']","17 pages, no figures",,http://arxiv.org/abs/2303.00368v1,math.AG,"['math.AG', 'cs.SC', '14Q05, 68W30']",,,[]
"Leveraging Symbolic Algebra Systems to Simulate Contact Dynamics in
  Rigid Body Systems",http://arxiv.org/abs/2303.01387v1,2023-03-02T16:15:14Z,2023-03-02T16:15:14Z,"  Collision detection plays a key role in the simulation of interacting rigid
bodies. However, owing to its computational complexity current methods
typically prioritize either maximizing processing speed or fidelity to
real-world behaviors. Fast real-time detection is achieved by simulating
collisions with simple geometric shapes whereas incorporating more realistic
geometries with multiple points of contact requires considerable computing
power which slows down collision detection. In this work, we present a new
approach to modeling and simulating collision-inclusive multibody dynamics by
leveraging computer algebra system (CAS). This approach offers flexibility in
modeling a diverse set of multibody systems applications ranging from human
biomechanics to space manipulators with docking interfaces, since the geometric
relationships between points and rigid bodies are handled in a generalizable
manner. We also analyze the performance of integrating this symbolic modeling
approach with collision detection formulated either as a traditional overlap
test or as a convex optimization problem. We compare these two collision
detection methods in different scenarios and collision resolution using a
penalty-based method to simulate dynamics. This work demonstrates an effective
simplification in solving collision dynamics problems using a symbolic
approach, especially for the algorithm based on convex optimization, which is
simpler to implement and, in complex collision scenarios, faster than the
overlap test.
","['\nSimone Asci\n', '\nAngadh Nanjangud\n']",,,http://arxiv.org/abs/2303.01387v1,cs.RO,"['cs.RO', 'cs.SC']",,,[]
Some D-finite and Some Possibly D-finite Sequences in the OEIS,http://arxiv.org/abs/2303.02793v2,2023-03-05T23:00:22Z,2023-04-24T20:28:25Z,"  In an automatic search, we found conjectural recurrences for some sequences
in the OEIS that were not previously recognized as being D-finite. In some
cases, we are able to prove the conjectured recurrence. In some cases, we are
not able to prove the conjectured recurrence, but we can prove that a
recurrence exists. In some remaining cases, we do not know where the recurrence
might come from.
","['\nManuel Kauers\n', '\nChristoph Koutschan\n']",,"Journal of Integer Sequences, vol. 26, article 23.4.5, 2023",http://arxiv.org/abs/2303.02793v2,cs.SC,"['cs.SC', 'math.CO', '05A15 (Primary) 68W30, 33F10 (Secondary)']",,,[]
"That's All Folks: a KG of Values as Commonsense Social Norms and
  Behaviors",http://arxiv.org/abs/2303.00632v1,2023-03-01T16:35:46Z,2023-03-01T16:35:46Z,"  Values, as intended in ethics, determine the shape and validity of moral and
social norms, grounding our everyday individual and community behavior on
commonsense knowledge. Formalising latent moral content in human interaction is
an appealing perspective that would enable a deeper understanding of both
social dynamics and individual cognitive and behavioral dimension. To tackle
this problem, several theoretical frameworks offer different values models, and
organize them into different taxonomies. The problem of the most used theories
is that they adopt a cultural-independent perspective while many entities that
are considered ""values"" are grounded in commonsense knowledge and expressed in
everyday life interaction. We propose here two ontological modules, FOLK, an
ontology for values intended in their broad sense, and That's All Folks, a
module for lexical and factual folk value triggers, whose purpose is to
complement the main theories, providing a method for identifying the values
that are not contemplated by the major value theories, but which nonetheless
play a key role in daily human interactions, and shape social structures,
cultural biases, and personal beliefs. The resource is tested via performing
automatic detection of values from text with a frame-based approach.
","['\nStefano De Giorgis\n', '\nAldo Gangemi\n']","16 pages, conference paper",,http://arxiv.org/abs/2303.00632v1,cs.CY,"['cs.CY', 'cs.AI', 'cs.SC']",,,[]
Robust Parameter Estimation for Rational Ordinary Differential Equations,http://arxiv.org/abs/2303.02159v3,2023-03-02T14:33:06Z,2023-12-17T12:01:55Z,"  We present a new approach for estimating parameters in rational ODE models
from given (measured) time series data.
  In typical existing approaches, an initial guess for the parameter values is
made from a given search interval. Then, in a loop, the corresponding outputs
are computed by solving the ODE numerically, followed by computing the error
from the given time series data. If the error is small, the loop terminates and
the parameter values are returned. Otherwise, heuristics/theories are used to
possibly improve the guess and continue the loop.
  These approaches tend to be non-robust in the sense that their accuracy
depend on the search interval and the true parameter values; furthermore, they
cannot handle the case where the parameters are locally identifiable.
  In this paper, we propose a new approach, which does not suffer from the
above non-robustness. In particular, it does not require making good initial
guesses for the parameter values or specifying search intervals. Instead, it
uses differential algebra, interpolation of the data using rational functions,
and multivariate polynomial system solving. We also compare the performance of
the resulting software with several other estimation software packages.
","['\nOren Bassik\n', '\nYosef Berman\n', '\nSoo Go\n', '\nHoon Hong\n', '\nIlia Ilmer\n', '\nAlexey Ovchinnikov\n', '\nChris Rackauckas\n', '\nPedro Soto\n', '\nChee Yap\n']",Updates regarding robustness,,http://arxiv.org/abs/2303.02159v3,cs.MS,"['cs.MS', 'cs.SC', 'math.DS', 'q-bio.QM']",,,[]
"Efficient Symbolic Approaches for Quantitative Reactive Synthesis with
  Finite Tasks",http://arxiv.org/abs/2303.03686v3,2023-03-07T07:08:20Z,2023-08-07T19:24:53Z,"  This work introduces efficient symbolic algorithms for quantitative reactive
synthesis. We consider resource-constrained robotic manipulators that need to
interact with a human to achieve a complex task expressed in linear temporal
logic. Our framework generates reactive strategies that not only guarantee task
completion but also seek cooperation with the human when possible. We model the
interaction as a two-player game and consider regret-minimizing strategies to
encourage cooperation. We use symbolic representation of the game to enable
scalability. For synthesis, we first introduce value iteration algorithms for
such games with min-max objectives. Then, we extend our method to the
regret-minimizing objectives. Our benchmarks reveal that our symbolic framework
not only significantly improves computation time (up to an order of magnitude)
but also can scale up to much larger instances of manipulation problems with up
to 2x number of objects and locations than the state of the art.
","['\nKaran Muvvala\n', '\nMorteza Lahijanian\n']",Accepted to IROS 2023,,http://arxiv.org/abs/2303.03686v3,cs.RO,"['cs.RO', 'cs.FL', 'cs.GT', 'cs.SC']",,,[]
"Computing the Characteristic Polynomial of Endomorphisms of a finite
  Drinfeld Module using Crystalline Cohomology",http://arxiv.org/abs/2302.08611v1,2023-02-16T22:33:12Z,2023-02-16T22:33:12Z,"  We present a new algorithm for computing the characteristic polynomial of an
arbitrary endomorphism of a finite Drinfeld module using its associated
crystalline cohomology. Our approach takes inspiration from Kedlaya's p-adic
algorithm for computing the characteristic polynomial of the Frobenius
endomorphism on a hyperelliptic curve using Monsky-Washnitzer cohomology. The
method is specialized using a baby-step giant-step algorithm for the particular
case of the Frobenius endomorphism, and in this case we include a complexity
analysis that demonstrates asymptotic gains over previously existing approaches
","['\nYossef Musleh\n', '\nÉric Schost\n']","15 pages, 1 figure",,http://arxiv.org/abs/2302.08611v1,cs.SC,['cs.SC'],,,[]
Elimination ideal and bivariate resultant over finite fields,http://arxiv.org/abs/2302.08891v1,2023-02-17T14:20:59Z,2023-02-17T14:20:59Z,"  A new algorithm is presented for computing the largest degree invariant
factor of the Sylvester matrix (with respect either to $x$ or $y$) associated
to two polynomials $a$ and $b$ in $\mathbb F_q[x,y]$ which have no non-trivial
common divisors. The algorithm is randomized of the Monte Carlo type and
requires $O((de)^{1+\epsilon}\log(q) ^{1+o(1)})$ bit operations, where $d$ an
$e$ respectively bound the input degrees in $x$ and in $y$. It follows that the
same complexity estimate is valid for computing: a generator of the elimination
ideal $\langle a,b \rangle \cap \mathbb F_q[x]$ (or $\mathbb F_q[y]$), as soon
as the polynomial system $a=b=0$ has not roots at infinity; the resultant of
$a$ and $b$ when they are sufficiently generic, especially so that the
Sylvester matrix has a unique non-trivial invariant factor. Our approach is to
use the reduction of the problem to a problem of minimal polynomial in the
quotient algebra $\mathbb F_q[x,y]/\langle a,b \rangle$. By proposing a new
method based on structured polynomial matrix division for computing with the
elements in the quotient, we manage to improve the best known complexity
bounds.
",['\nGilles Villard\n'],17 pages,,http://arxiv.org/abs/2302.08891v1,cs.SC,"['cs.SC', 'I.1.2']",,,[]
Jordan algebra in R,http://arxiv.org/abs/2303.06062v1,2023-02-21T01:02:40Z,2023-02-21T01:02:40Z,"  In this short article I introduce the ""jordan"" package which provides
functionality for working with different types of Jordan algebra. I give some
numerical verification of the Jordan identity for the five types of Jordan
algebras. The package is available on CRAN at
https://CRAN.R-project.org/package=stokes.
",['\nRobin K. S. Hankin\n'],12 pages,,http://arxiv.org/abs/2303.06062v1,cs.SC,['cs.SC'],,,[]
A Direttissimo Algorithm for Equidimensional Decomposition,http://arxiv.org/abs/2302.08174v2,2023-02-16T09:42:55Z,2023-06-09T08:45:15Z,"  We describe a recursive algorithm that decomposes an algebraic set into
locally closed equidimensional sets, i.e. sets which each have irreducible
components of the same dimension. At the core of this algorithm, we combine
ideas from the theory of triangular sets, a.k.a. regular chains, with Gr\""obner
bases to encode and work with locally closed algebraic sets. Equipped with
this, our algorithm avoids projections of the algebraic sets that are
decomposed and certain genericity assumptions frequently made when decomposing
polynomial systems, such as assumptions about Noether position. This makes it
produce fine decompositions on more structured systems where ensuring
genericity assumptions often destroys the structure of the system at hand.
Practical experiments demonstrate its efficiency compared to state-of-the-art
implementations.
","['\nChristian Eder\n', '\nPierre Lairez\n', '\nRafael Mohr\n', '\nMohab Safey El Din\n']","Some minor revisions, corrects a mistake in the proof of lemma 2.2",,http://arxiv.org/abs/2302.08174v2,cs.SC,"['cs.SC', 'math.AC']",,,[]
Algorithm for connectivity queries on real algebraic curves,http://arxiv.org/abs/2302.11347v3,2023-02-22T12:40:48Z,2023-07-11T10:27:00Z,"  We consider the problem of answering connectivity queries on a real algebraic
curve. The curve is given as the real trace of an algebraic curve, assumed to
be in generic position, and being defined by some rational parametrizations.
The query points are given by a zero-dimensional parametrization. We design an
algorithm which counts the number of connected components of the real curve
under study, and decides which query point lie in which connected component, in
time log-linear in $N^6$, where $N$ is the maximum of the degrees and
coefficient bit-sizes of the polynomials given as input. This matches the
currently best-known bound for computing the topology of real plane curves. The
main novelty of this algorithm is the avoidance of the computation of the
complete topology of the curve.
","['\nMd Nazrul Islam\nDiebold Nixdorf\n', '\nAdrien Poteaux\nCRIStAL\n', '\nRémi Prébet\nPolSys\n']","10 pages, 2 figures",,http://dx.doi.org/10.1145/3597066.3597081,cs.SC,"['cs.SC', 'math.AG']",10.1145/3597066.3597081,,"['Diebold Nixdorf', 'CRIStAL', 'PolSys']"
"Co-Driven Recognition of Semantic Consistency via the Fusion of
  Transformer and HowNet Sememes Knowledge",http://arxiv.org/abs/2302.10570v1,2023-02-21T09:53:19Z,2023-02-21T09:53:19Z,"  Semantic consistency recognition aims to detect and judge whether the
semantics of two text sentences are consistent with each other. However, the
existing methods usually encounter the challenges of synonyms, polysemy and
difficulty to understand long text. To solve the above problems, this paper
proposes a co-driven semantic consistency recognition method based on the
fusion of Transformer and HowNet sememes knowledge. Multi-level encoding of
internal sentence structures via data-driven is carried out firstly by
Transformer, sememes knowledge base HowNet is introduced for knowledge-driven
to model the semantic knowledge association among sentence pairs. Then,
interactive attention calculation is carried out utilizing soft-attention and
fusion the knowledge with sememes matrix. Finally, bidirectional long
short-term memory network (BiLSTM) is exploited to encode the conceptual
semantic information and infer the semantic consistency. Experiments are
conducted on two financial text matching datasets (BQ, AFQMC) and a
cross-lingual adversarial dataset (PAWSX) for paraphrase identification.
Compared with lightweight models including DSSM, MwAN, DRCN, and pre-training
models such as ERNIE etc., the proposed model can not only improve the accuracy
of semantic consistency recognition effectively (by 2.19%, 5.57% and 6.51%
compared with the DSSM, MWAN and DRCN models on the BQ dataset), but also
reduce the number of model parameters (to about 16M). In addition, driven by
the HowNet sememes knowledge, the proposed method is promising to adapt to
scenarios with long text.
","['\nFan Chen\n', '\nYan Huang\n', '\nXinfang Zhang\n', '\nKang Luo\n', '\nJinxuan Zhu\n', '\nRuixian He\n']","17 pages, 5 figures",,http://arxiv.org/abs/2302.10570v1,cs.CL,"['cs.CL', 'cs.AI', 'cs.SC']",,,[]
"Pourchet's theorem in action: decomposing univariate nonnegative
  polynomials as sums of five squares",http://arxiv.org/abs/2302.02202v1,2023-02-04T17:02:17Z,2023-02-04T17:02:17Z,"  Pourchet proved in 1971 that every nonnegative univariate polynomial with
rational coefficients is a sum of five or fewer squares. Nonetheless, there are
no known algorithms for constructing such a decomposition. The sole purpose of
the present paper is to present a set of algorithms that decompose a given
nonnegative polynomial into a sum of six (five under some unproven conjecture
or when allowing weights) squares of polynomials. Moreover, we prove that the
binary complexity can be expressed polynomially in terms of classical
operations of computer algebra and algorithmic number theory.
","['\nVictor Magron\n', '\nPrzemysław Koprowski\n', '\nTristan Vaccon\n']","10 pages, 9 algorithms, submitted at the ISSAC 2023 conference",,http://arxiv.org/abs/2302.02202v1,cs.SC,['cs.SC'],,,[]
Short proofs of ideal membership,http://arxiv.org/abs/2302.02832v2,2023-02-06T14:50:49Z,2023-07-12T13:00:48Z,"  A cofactor representation of an ideal element, that is, a representation in
terms of the generators, can be considered as a certificate for ideal
membership. Such a representation is typically not unique, and some can be a
lot more complicated than others. In this work, we consider the problem of
computing sparsest cofactor representations, i.e., representations with a
minimal number of terms, of a given element in a polynomial ideal. While we
focus on the more general case of noncommutative polynomials, all results also
apply to the commutative setting. We show that the problem of computing
cofactor representations with a bounded number of terms is decidable and
NP-complete. Moreover, we provide a practical algorithm for computing sparse
(not necessarily optimal) representations by translating the problem into a
linear optimization problem and by exploiting properties of signature-based
Gr\""obner basis algorithms. We show that for a certain class of ideals,
representations computed by this method are actually optimal, and we present
experimental data illustrating that it can lead to noticeably sparser cofactor
representations.
","['\nClemens Hofstadler\n', '\nThibaut Verron\n']",21 pages,,http://arxiv.org/abs/2302.02832v2,cs.SC,['cs.SC'],,,[]
"Refined telescoping algorithms in $RΠΣ$-extensions to reduce the
  degrees of the denominators",http://arxiv.org/abs/2302.03563v1,2023-02-07T16:19:28Z,2023-02-07T16:19:28Z,"  We present a general framework in the setting of difference ring extensions
that enables one to find improved representations of indefinite nested sums
such that the arising denominators within the summands have reduced degrees.
The underlying (parameterized) telescoping algorithms can be executed in
$R\Pi\Sigma$-ring extensions that are built over general $\Pi\Sigma$-fields. An
important application of this toolbox is the simplification of d'Alembertian
and Liouvillian solutions coming from recurrence relations where the
denominators of the arising sums do not factor nicely.
",['\nCarsten Schneider\n'],,,http://arxiv.org/abs/2302.03563v1,cs.SC,['cs.SC'],,,[]
Beating binary powering for polynomial matrices,http://arxiv.org/abs/2302.04299v2,2023-02-08T19:28:20Z,2023-05-26T06:42:04Z,"  The $N$th power of a polynomial matrix of fixed size and degree can be
computed by binary powering as fast as multiplying two polynomials of linear
degree in~$N$. When Fast Fourier Transform (FFT) is available, the resulting
complexity is \emph{softly linear} in~$N$, i.e.~linear in~$N$ with extra
logarithmic factors. We show that it is possible to beat binary powering, by an
algorithm whose complexity is \emph{purely linear} in~$N$, even in absence of
FFT. The key result making this improvement possible is that the entries of the
$N$th power of a polynomial matrix satisfy linear differential equations with
polynomial coefficients whose orders and degrees are independent of~$N$.
Similar algorithms are proposed for two related problems: computing the $N$th
term of a C-finite sequence of polynomials, and modular exponentiation to the
power $N$ for bivariate polynomials.
","['\nAlin Bostan\n', '\nVincent Neiger\n', '\nSergey Yurkevich\n']","10 pages, 3 figures, 2 tables, 5 algorithms",,http://arxiv.org/abs/2302.04299v2,cs.SC,['cs.SC'],,,[]
Exact computations with quasiseparable matrices,http://arxiv.org/abs/2302.04515v1,2023-02-09T09:17:18Z,2023-02-09T09:17:18Z,"  Quasi-separable matrices are a class of rank-structured matriceswidely used
in numerical linear algebra and of growing interestin computer algebra, with
applications in e.g. the linearization ofpolynomial matrices. Various
representation formats exist for thesematrices that have rarely been
compared.We show how the most central formats SSS and HSS can beadapted to
symbolic computation, where the exact rank replacesthreshold based numerical
ranks. We clarify their links and comparethem with the Bruhat format. To this
end, we state their space andtime cost estimates based on fast matrix
multiplication, and comparethem, with their leading constants. The comparison
is supportedby software experiments.We make further progresses for the Bruhat
format, for which wegive a generation algorithm, following a Crout elimination
scheme,which specializes into fast algorithms for the construction from asparse
matrix or from the sum of Bruhat representations.
","['\nClément Pernet\nCASC\n', '\nHippolyte Signargout\nCASC, ARIC\n', '\nGilles Villard\nARIC\n']",,,http://arxiv.org/abs/2302.04515v1,cs.SC,['cs.SC'],,,"['CASC', 'CASC, ARIC', 'ARIC']"
Hermite Reduction for D-finite Functions via Integral Bases,http://arxiv.org/abs/2302.04652v1,2023-02-09T14:07:26Z,2023-02-09T14:07:26Z,"  Trager's Hermite reduction solves the integration problem for algebraic
functions via integral bases. A generalization of this algorithm to D-finite
functions has so far been limited to the Fuchsian case. In the present paper,
we remove this restriction and propose a reduction algorithm based on integral
bases that is applicable to arbitrary D-finite functions.
","['\nShaoshi Chen\n', '\nLixin Du\n', '\nManuel Kauers\n']",21 pages,,http://arxiv.org/abs/2302.04652v1,cs.SC,['cs.SC'],,,[]
Lazard-style CAD and Equational Constraints,http://arxiv.org/abs/2302.05813v2,2023-02-11T23:24:07Z,2023-12-07T17:22:34Z,"  McCallum-style Cylindrical Algebra Decomposition (CAD) is a major improvement
on the original Collins version, and has had many subsequent advances, notably
for total or partial equational constraints. But it suffers from a problem with
nullification. The recently-justified Lazard-style CAD does not have this
problem. However, transporting the equational constraints work to Lazard-style
does reintroduce nullification issues. This paper explains the problem, and the
solutions to it, based on the second author's Ph.D. thesis and the
Brown--McCallum improvement to Lazard.
  With a single equational constraint, we can gain the same improvements in
Lazard-style as in McCallum-style CAD . Moreover, our approach does not fail
where McCallum would due to nullification. Unsurprisingly, it does not achieve
the same level of improvement as it does in the non-nullified cases. We also
consider the case of multiple equational constraints.
","['\nJames H. Davenport\n', '\nAkshar S. Nair\n', '\nGregory K. Sankaran\n', '\nAli K. Uncu\n']",9 pages,"Proceedings of ISSAC'23, 2023",http://dx.doi.org/10.1145/3597066.3597090,cs.SC,"['cs.SC', '68W30', 'I.1.2']",10.1145/3597066.3597090,,[]
"Fast evaluation and root finding for polynomials with floating-point
  coefficients",http://arxiv.org/abs/2302.06244v1,2023-02-13T10:29:31Z,2023-02-13T10:29:31Z,"  Evaluating or finding the roots of a polynomial $f(z) = f_0 + \cdots + f_d
z^d$ with floating-point number coefficients is a ubiquitous problem. By using
a piecewise approximation of $f$ obtained with a careful use of the Newton
polygon of $f$, we improve state-of-the-art upper bounds on the number of
operations to evaluate and find the roots of a polynomial. In particular, if
the coefficients of $f$ are given with $m$ significant bits, we provide for the
first time an algorithm that finds all the roots of $f$ with a relative
condition number lower than $2^m$, using a number of bit operations
quasi-linear in the bit-size of the floating-point representation of $f$.
Notably, our new approach handles efficiently polynomials with coefficients
ranging from $2^{-d}$ to $2^d$, both in theory and in practice.
","['\nRémi Imbach\nGAMBLE\n', '\nGuillaume Moroz\nGAMBLE\n']",,,http://arxiv.org/abs/2302.06244v1,cs.SC,['cs.SC'],,,"['GAMBLE', 'GAMBLE']"
Transcendence Certificates for D-finite Functions,http://arxiv.org/abs/2302.06396v2,2023-02-13T14:36:20Z,2023-09-19T12:14:17Z,"  Although in theory we can decide whether a given D-finite function is
transcendental, transcendence proofs remain a challenge in practice. Typically,
transcendence is certified by checking certain incomplete sufficient
conditions. In this paper we propose an additional such condition which catches
some cases on which other tests fail.
","['\nManuel Kauers\n', '\nChristoph Koutschan\n', '\nThibaut Verron\n']","9 pages, 1 figure","Proceedings of International Symposium on Symbolic and Algebraic
  Computation 2023",http://dx.doi.org/10.1145/3597066.3597091,cs.SC,['cs.SC'],10.1145/3597066.3597091,,[]
Techniques to Improve Neural Math Word Problem Solvers,http://arxiv.org/abs/2302.03145v1,2023-02-06T22:41:51Z,2023-02-06T22:41:51Z,"  Developing automatic Math Word Problem (MWP) solvers is a challenging task
that demands the ability of understanding and mathematical reasoning over the
natural language. Recent neural-based approaches mainly encode the problem text
using a language model and decode a mathematical expression over quantities and
operators iteratively. Note the problem text of a MWP consists of a context
part and a question part, a recent work finds these neural solvers may only
perform shallow pattern matching between the context text and the golden
expression, where question text is not well used. Meanwhile, existing decoding
processes fail to enforce the mathematical laws into the design, where the
representations for mathematical equivalent expressions are different. To
address these two issues, we propose a new encoder-decoder architecture that
fully leverages the question text and preserves step-wise commutative law.
Besides generating quantity embeddings, our encoder further encodes the
question text and uses it to guide the decoding process. At each step, our
decoder uses Deep Sets to compute expression representations so that these
embeddings are invariant under any permutation of quantities. Experiments on
four established benchmarks demonstrate that our framework outperforms
state-of-the-art neural MWP solvers, showing the effectiveness of our
techniques. We also conduct a detailed analysis of the results to show the
limitations of our approach and further discuss the potential future work. Code
is available at https://github.com/sophistz/Question-Aware-Deductive-MWP.
",['\nYouyuan Zhang\n'],,,http://arxiv.org/abs/2302.03145v1,cs.CL,"['cs.CL', 'cs.SC']",,,[]
A Unified Approach to Unimodality of Gaussian Polynomials,http://arxiv.org/abs/2302.04067v2,2023-02-08T14:08:56Z,2023-08-31T23:31:22Z,"  In 2013, Pak and Panova proved the strict unimodality property of
$q$-binomial coefficients $\binom{\ell+m}{m}_q$ (as polynomials in $q$) based
on the combinatorics of Young tableaux and the semigroup property of Kronecker
coefficients. They showed it to be true for all $\ell,m\geq 8$ and a few other
cases. We propose a different approach to this problem based on computer
algebra, where we establish a closed form for the coefficients of these
polynomials and then use cylindrical algebraic decomposition to identify
exactly the range of coefficients where strict unimodality holds. This strategy
allows us to tackle generalizations of the problem, e.g., to show unimodality
with larger gaps or unimodality of related sequences. In particular, we present
proofs of two additional cases of a conjecture by Stanley and Zanello.
","['\nChristoph Koutschan\n', '\nAli K. Uncu\n', '\nElaine Wong\n']",Supplementary material at https://wongey.github.io/unimodality,"ISSAC 2023: Proceedings of the 2023 International Symposium on
  Symbolic and Algebraic Computation, July 2023, Pages 434-442",http://dx.doi.org/10.1145/3597066.3597113,cs.SC,"['cs.SC', 'math.CO']",10.1145/3597066.3597113,,[]
Order bounds for $C^2$-finite sequences,http://arxiv.org/abs/2302.04070v1,2023-02-08T14:17:44Z,2023-02-08T14:17:44Z,"  A sequence is called $C$-finite if it satisfies a linear recurrence with
constant coefficients. We study sequences which satisfy a linear recurrence
with $C$-finite coefficients. Recently, it was shown that such $C^2$-finite
sequences satisfy similar closure properties as $C$-finite sequences. In
particular, they form a difference ring.
  In this paper we present new techniques for performing these closure
properties of $C^2$-finite sequences. These methods also allow us to derive
order bounds which were not known before. Additionally, they provide more
insight in the effectiveness of these computations.
  The results are based on the exponent lattice of algebraic numbers. We
present an iterative algorithm which can be used to compute bases of such
lattices.
","['\nManuel Kauers\n', '\nPhilipp Nuspl\n', '\nVeronika Pillwein\n']",,,http://arxiv.org/abs/2302.04070v1,math.RA,"['math.RA', 'cs.SC']",,,[]
"Isolating clusters of zeros of analytic systems using arbitrary-degree
  inflation",http://arxiv.org/abs/2302.04776v1,2023-02-09T17:10:12Z,2023-02-09T17:10:12Z,"  Given a system of analytic functions and an approximation to a cluster of
zeros, we wish to construct two regions containing the cluster and no other
zeros of the system. The smaller region tightly contains the cluster while the
larger region separates it from the other zeros of the system. We achieve this
using the method of inflation which, counterintuitively, relates it to another
system that is more amenable to our task but whose associated cluster of zeros
is larger.
","['\nMichael Burr\n', '\nKisun Lee\n', '\nAnton Leykin\n']","12 pages, 1 figure",,http://arxiv.org/abs/2302.04776v1,math.AG,"['math.AG', 'cs.SC']",,,[]
Refined $F_5$ Algorithms for Ideals of Minors of Square Matrices,http://arxiv.org/abs/2302.05375v2,2023-02-10T16:53:50Z,2023-06-14T14:18:48Z,"  We consider the problem of computing a grevlex Gr\""obner basis for the set
$F_r(M)$ of minors of size $r$ of an $n\times n$ matrix $M$ of generic linear
forms over a field of characteristic zero or large enough. Such sets are not
regular sequences; in fact, the ideal $\langle F_r(M) \rangle$ cannot be
generated by a regular sequence. As such, when using the general-purpose
algorithm $F_5$ to find the sought Gr\""obner basis, some computing time is
wasted on reductions to zero. We use known results about the first syzygy
module of $F_r(M)$ to refine the $F_5$ algorithm in order to detect more
reductions to zero. In practice, our approach avoids a significant number of
reductions to zero. In particular, in the case $r=n-2$, we prove that our new
algorithm avoids all reductions to zero, and we provide a corresponding
complexity analysis which improves upon the previously known estimates.
","['\nSriram Gopalakrishnan\n', '\nVincent Neiger\n', '\nMohab Safey El Din\n']","21 pages, 3 algorithms",,http://arxiv.org/abs/2302.05375v2,cs.SC,"['cs.SC', 'math.AC']",,,[]
SCL(FOL) Revisited,http://arxiv.org/abs/2302.05954v1,2023-02-12T16:52:56Z,2023-02-12T16:52:56Z,"  This paper presents an up-to-date and refined version of the SCL calculus for
first-order logic without equality. The refinement mainly consists of the
following two parts: First, we incorporate a stronger notion of regularity into
SCL(FOL). Our regularity definition is adapted from the SCL(T) calculus. This
adapted definition guarantees non-redundant clause learning during a run of
SCL. However, in contrast to the original presentation, it does not require
exhaustive propagation. Second, we introduce trail and model bounding to
achieve termination guarantees. In previous versions, no termination guarantees
about SCL were achieved. Last, we give rigorous proofs for soundness,
completeness and clause learning guarantees of SCL(FOL) and put SCL(FOL) into
context of existing first-order calculi.
","['\nMartin Bromberger\n', '\nSimon Schwarz\n', '\nChristoph Weidenbach\n']",,,http://arxiv.org/abs/2302.05954v1,cs.LO,"['cs.LO', 'cs.SC']",,,[]
Invariants for neural automata,http://arxiv.org/abs/2302.02149v1,2023-02-04T11:40:40Z,2023-02-04T11:40:40Z,"  Computational modeling of neurodynamical systems often deploys neural
networks and symbolic dynamics. A particular way for combining these approaches
within a framework called vector symbolic architectures leads to neural
automata. An interesting research direction we have pursued under this
framework has been to consider mapping symbolic dynamics onto neurodynamics,
represented as neural automata. This representation theory, enables us to ask
questions, such as, how does the brain implement Turing computations.
Specifically, in this representation theory, neural automata result from the
assignment of symbols and symbol strings to numbers, known as G\""odel encoding.
Under this assignment symbolic computation becomes represented by trajectories
of state vectors in a real phase space, that allows for statistical correlation
analyses with real-world measurements and experimental data. However, these
assignments are usually completely arbitrary. Hence, it makes sense to address
the problem question of, which aspects of the dynamics observed under such a
representation is intrinsic to the dynamics and which are not. In this study,
we develop a formally rigorous mathematical framework for the investigation of
symmetries and invariants of neural automata under different encodings. As a
central concept we define patterns of equality for such systems. We consider
different macroscopic observables, such as the mean activation level of the
neural network, and ask for their invariance properties. Our main result shows
that only step functions that are defined over those patterns of equality are
invariant under recodings, while the mean activation is not. Our work could be
of substantial importance for related regression studies of real-world
measurements with neurosymbolic processors for avoiding confounding results
that are dependant on a particular encoding and not intrinsic to the dynamics.
","['\nJone Uria-Albizuri\n', '\nGiovanni Sirio Carmantini\n', '\nPeter beim Graben\n', '\nSerafim Rodrigues\n']","28 pages, 8 figures",,http://arxiv.org/abs/2302.02149v1,cs.NE,"['cs.NE', 'cs.CL', 'cs.FL', 'cs.SC']",,,[]
Newton iteration for lexicographic Gröbner bases in two variables,http://arxiv.org/abs/2302.03766v1,2023-02-07T21:54:59Z,2023-02-07T21:54:59Z,"  We present an $m$-adic Newton iteration with quadratic convergence for
lexicographic Gr\""obner basis of zero dimensional ideals in two variables. We
rely on a structural result about the syzygies in such a basis due to Conca and
Valla, that allowed them to explicitly describe these Gr\""obner bases by affine
parameters; our Newton iteration works directly with these parameters.
","['\nÉric Schost\n', '\nCatherine St-Pierre\n']",,,http://arxiv.org/abs/2302.03766v1,math.AC,"['math.AC', 'cs.SC', 'math.AG']",,,[]
Symbolic Quantum Simulation with Quasimodo,http://arxiv.org/abs/2302.04349v2,2023-02-08T21:45:10Z,2023-05-29T20:20:00Z,"  The simulation of quantum circuits on classical computers is an important
problem in quantum computing. Such simulation requires representations of
distributions over very large sets of basis vectors, and recent work has used
symbolic data-structures such as Binary Decision Diagrams (BDDs) for this
purpose. In this tool paper, we present Quasimodo, an extensible, open-source
Python library for symbolic simulation of quantum circuits. Quasimodo is
specifically designed for easy extensibility to other backends. Quasimodo
allows simulations of quantum circuits, checking properties of the outputs of
quantum circuits, and debugging quantum circuits. It also allows the user to
choose from among several symbolic data-structures -- both unweighted and
weighted BDDs, and a recent structure called Context-Free-Language Ordered
Binary Decision Diagrams (CFLOBDDs) -- and can be easily extended to support
other symbolic data-structures.
","['\nMeghana Sistla\n', '\nSwarat Chaudhuri\n', '\nThomas Reps\n']","15 pages; 35th International Conference on Computer Aided
  Verification (CAV 2023)",,http://arxiv.org/abs/2302.04349v2,cs.FL,"['cs.FL', 'cs.SC', 'quant-ph']",,,[]
"Certified simultaneous isotopic approximation of pairs of curves via
  subdivision",http://arxiv.org/abs/2302.04908v1,2023-02-09T19:30:05Z,2023-02-09T19:30:05Z,"  We present a certified algorithm based on subdivision for computing an
isotopic approximation to a pair of curves in the plane. Our algorithm is based
on the certified curve approximation algorithm of Plantinga and Vegter. The
main challenge in this computation is to correctly and efficiently compute the
intersections of the curves. To address this issue, we introduce a new, but
simple test that guarantees the global correctness of our output.
","['\nMichael Burr\n', '\nMichael Byrd\n']",,,http://arxiv.org/abs/2302.04908v1,cs.CG,"['cs.CG', 'cs.SC', 'math.AG', '68W30, 13P15, 14Q05, 14Q20, 14Q30, 14P25']",,,[]
Fast Algorithms for Discrete Differential Equations,http://arxiv.org/abs/2302.06203v2,2023-02-13T09:21:05Z,2023-04-28T07:54:29Z,"  Discrete Differential Equations (DDEs) are functional equations that relate
polynomially a power series $F(t,u)$ in $t$ with polynomial coefficients in a
""catalytic"" variable $u$ and the specializations, say at $u=1$, of $F(t,u)$ and
of some of its partial derivatives in $u$. DDEs occur frequently in
combinatorics, especially in map enumeration. If a DDE is of fixed-point type
then its solution $F(t,u)$ is unique, and a general result by Popescu (1986)
implies that $F(t,u)$ is an algebraic power series. Constructive proofs of
algebraicity for solutions of fixed-point type DDEs were proposed by
Bousquet-M\'elou and Jehanne (2006). Bostan et. al (2022) initiated a
systematic algorithmic study of such DDEs of order 1.
  We generalize this study to DDEs of arbitrary order. First, we propose
nontrivial extensions of algorithms based on polynomial elimination and on the
guess-and-prove paradigm. Second, we design two brand-new algorithms that
exploit the special structure of the underlying polynomial systems. Last, but
not least, we report on implementations that are able to solve highly
challenging DDEs with a combinatorial origin.
","['\nAlin Bostan\n', '\nHadrien Notarantonio\n', '\nMohab Safey El Din\n']","11 pages, revised version",,http://arxiv.org/abs/2302.06203v2,cs.SC,"['cs.SC', 'math.AG', 'math.CO']",,,[]
Signature Gröbner bases in free algebras over rings,http://arxiv.org/abs/2302.06483v2,2023-02-13T16:04:09Z,2023-07-18T10:41:50Z,"  We generalize signature Gr\""obner bases, previously studied in the free
algebra over a field or polynomial rings over a ring, to ideals in the mixed
algebra $R[x_1,...,x_k]\langle y_1,\dots,y_n \rangle$ where $R$ is a principal
ideal domain. We give an algorithm for computing them, combining elements from
the theory of commutative and noncommutative (signature) Gr\""obner bases, and
prove its correctness.
  Applications include extensions of the free algebra with commutative
variables, e.g., for homogenization purposes or for performing ideal theoretic
operations such as intersections, and computations over $\mathbb{Z}$ as
universal proofs over fields of arbitrary characteristic.
  By extending the signature cover criterion to our setting, our algorithm also
lifts some technical restrictions from previous noncommutative signature-based
algorithms, now allowing, e.g., elimination orderings. We provide a prototype
implementation for the case when $R$ is a field, and show that our algorithm
for the mixed algebra is more efficient than classical approaches using
existing algorithms.
","['\nClemens Hofstadler\n', '\nThibaut Verron\n']",10 pages,"Proceedings of International Symposium on Symbolic and Algebraic
  Computation 2023",http://dx.doi.org/10.1145/3597066.3597071,math.AC,"['math.AC', 'cs.SC', 'math.RA']",10.1145/3597066.3597071,,[]
Logarithmically Sparse Symmetric Matrices,http://arxiv.org/abs/2301.10042v1,2023-01-24T14:34:21Z,2023-01-24T14:34:21Z,"  A positive definite matrix is called logarithmically sparse if its matrix
logarithm has many zero entries. Such matrices play a significant role in
high-dimensional statistics and semidefinite optimization. In this paper,
logarithmically sparse matrices are studied from the point of view of
computational algebraic geometry: we present a formula for the dimension of the
Zariski closure of a set of matrices with a given logarithmic sparsity pattern,
give a degree bound for this variety and develop implicitization algorithms
that allow to find its defining equations. We illustrate our approach with
numerous examples.
",['\nDmitrii Pavlov\n'],15 pages,,http://arxiv.org/abs/2301.10042v1,math.AG,"['math.AG', 'cs.SC']",,,[]
Learning Modulo Theories,http://arxiv.org/abs/2301.11435v1,2023-01-26T21:46:23Z,2023-01-26T21:46:23Z,"  Recent techniques that integrate \emph{solver layers} into Deep Neural
Networks (DNNs) have shown promise in bridging a long-standing gap between
inductive learning and symbolic reasoning techniques. In this paper we present
a set of techniques for integrating \emph{Satisfiability Modulo Theories} (SMT)
solvers into the forward and backward passes of a deep network layer, called
SMTLayer. Using this approach, one can encode rich domain knowledge into the
network in the form of mathematical formulas. In the forward pass, the solver
uses symbols produced by prior layers, along with these formulas, to construct
inferences; in the backward pass, the solver informs updates to the network,
driving it towards representations that are compatible with the solver's
theory. Notably, the solver need not be differentiable. We implement \layername
as a Pytorch module, and our empirical results show that it leads to models
that \emph{1)} require fewer training samples than conventional models,
\emph{2)} that are robust to certain types of covariate shift, and \emph{3)}
that ultimately learn representations that are consistent with symbolic
knowledge, and thus naturally interpretable.
","['\nMatt Fredrikson\n', '\nKaiji Lu\n', '\nSaranya Vijayakumar\n', '\nSomesh Jha\n', '\nVijay Ganesh\n', '\nZifan Wang\n']",,,http://arxiv.org/abs/2301.11435v1,cs.LG,"['cs.LG', 'cs.SC']",,,[]
"Differential Elimination and Algebraic Invariants of Polynomial
  Dynamical Systems",http://arxiv.org/abs/2301.10935v1,2023-01-26T04:47:38Z,2023-01-26T04:47:38Z,"  Invariant sets are a key ingredient for verifying safety and other properties
of cyber-physical systems that mix discrete and continuous dynamics. We adapt
the elimination-theoretic Rosenfeld-Gr\""{o}bner algorithm to systematically
obtain algebraic invariants of polynomial dynamical systems without using
Gr\""{o}bner bases or quantifier elimination. We identify totally real varieties
as an important class for efficient invariance checking.
","['\nWilliam Simmons\n', '\nAndré Platzer\n']",,,http://arxiv.org/abs/2301.10935v1,cs.SC,"['cs.SC', 'cs.LO', 'math.AG', 'math.DS']",,,[]
"The Automated Discovery of Kinetic Rate Models -- Methodological
  Frameworks",http://arxiv.org/abs/2301.11356v2,2023-01-26T19:09:47Z,2023-11-02T13:29:01Z,"  The industrialization of catalytic processes requires reliable kinetic models
for their design, optimization and control. Mechanistic models require
significant domain knowledge, while data-driven and hybrid models lack
interpretability. Automated knowledge discovery methods, such as ALAMO
(Automated Learning of Algebraic Models for Optimization), SINDy (Sparse
Identification of Nonlinear Dynamics), and genetic programming, have gained
popularity but suffer from limitations such as needing model structure
assumptions, exhibiting poor scalability, and displaying sensitivity to noise.
To overcome these challenges, we propose two methodological frameworks, ADoK-S
and ADoK-W (Automated Discovery of Kinetic rate models using a Strong/Weak
formulation of symbolic regression), for the automated generation of catalytic
kinetic models using a robust criterion for model selection. We leverage
genetic programming for model generation and a sequential optimization routine
for model refinement. The frameworks are tested against three case studies of
increasing complexity, demonstrating their ability to retrieve the underlying
kinetic rate model with limited noisy data from the catalytic systems,
showcasing their potential for chemical reaction engineering applications.
","['\nMiguel Ángel de Carvalho Servia\nMimi\n', '\nIlya Orson Sandoval\nMimi\n', '\nKlaus Hellgardt\nMimi\n', '\nKing Kuok\nMimi\n', '\n Hii\n', '\nDongda Zhang\n', '\nEhecatl Antonio del Rio Chanona\n']",,,http://arxiv.org/abs/2301.11356v2,cs.SC,"['cs.SC', 'cs.CE', 'q-bio.QM']",,,"['Mimi', 'Mimi', 'Mimi', 'Mimi']"
"Incorporating Background Knowledge in Symbolic Regression using a
  Computer Algebra System",http://arxiv.org/abs/2301.11919v2,2023-01-27T18:59:25Z,2023-05-04T14:52:27Z,"  Symbolic Regression (SR) can generate interpretable, concise expressions that
fit a given dataset, allowing for more human understanding of the structure
than black-box approaches. The addition of background knowledge (in the form of
symbolic mathematical constraints) allows for the generation of expressions
that are meaningful with respect to theory while also being consistent with
data. We specifically examine the addition of constraints to traditional
genetic algorithm (GA) based SR (PySR) as well as a Markov-chain Monte Carlo
(MCMC) based Bayesian SR architecture (Bayesian Machine Scientist), and apply
these to rediscovering adsorption equations from experimental, historical
datasets. We find that, while hard constraints prevent GA and MCMC SR from
searching, soft constraints can lead to improved performance both in terms of
search effectiveness and model meaningfulness, with computational costs
increasing by about an order-of-magnitude. If the constraints do not correlate
well with the dataset or expected models, they can hinder the search of
expressions. We find Bayesian SR is better these constraints (as the Bayesian
prior) than by modifying the fitness function in the GA
","['\nCharles Fox\n', '\nNeil Tran\n', '\nNikki Nacion\n', '\nSamiha Sharlin\n', '\nTyler R. Josephson\n']",,,http://arxiv.org/abs/2301.11919v2,cs.LG,"['cs.LG', 'cs.SC', 'physics.chem-ph']",,,[]
"The Optimal Choice of Hypothesis Is the Weakest, Not the Shortest",http://arxiv.org/abs/2301.12987v3,2023-01-30T15:29:40Z,2023-04-25T07:23:31Z,"  If $A$ and $B$ are sets such that $A \subset B$, generalisation may be
understood as the inference from $A$ of a hypothesis sufficient to construct
$B$. One might infer any number of hypotheses from $A$, yet only some of those
may generalise to $B$. How can one know which are likely to generalise? One
strategy is to choose the shortest, equating the ability to compress
information with the ability to generalise (a proxy for intelligence). We
examine this in the context of a mathematical formalism of enactive cognition.
We show that compression is neither necessary nor sufficient to maximise
performance (measured in terms of the probability of a hypothesis
generalising). We formulate a proxy unrelated to length or simplicity, called
weakness. We show that if tasks are uniformly distributed, then there is no
choice of proxy that performs at least as well as weakness maximisation in all
tasks while performing strictly better in at least one. In experiments
comparing maximum weakness and minimum description length in the context of
binary arithmetic, the former generalised at between $1.1$ and $5$ times the
rate of the latter. We argue this demonstrates that weakness is a far better
proxy, and explains why Deepmind's Apperception Engine is able to generalise
effectively.
",['\nMichael Timothy Bennett\n'],"Accepted for poster presentation at the 16th Conference on Artificial
  General Intelligence, taking place in Stockholm, 2023",,http://arxiv.org/abs/2301.12987v3,cs.AI,"['cs.AI', 'cs.LG', 'cs.SC', 'math.LO']",,,[]
A Survey on Compositional Generalization in Applications,http://arxiv.org/abs/2302.01067v1,2023-02-02T12:56:26Z,2023-02-02T12:56:26Z,"  The field of compositional generalization is currently experiencing a
renaissance in AI, as novel problem settings and algorithms motivated by
various practical applications are being introduced, building on top of the
classical compositional generalization problem. This article aims to provide a
comprehensive review of top recent developments in multiple real-life
applications of the compositional generalization. Specifically, we introduce a
taxonomy of common applications and summarize the state-of-the-art for each of
those domains. Furthermore, we identify important current trends and provide
new perspectives pertaining to the future of this burgeoning field.
","['\nBaihan Lin\n', '\nDjallel Bouneffouf\n', '\nIrina Rish\n']",,,http://arxiv.org/abs/2302.01067v1,cs.AI,"['cs.AI', 'cs.LG', 'cs.SC']",,,[]
"Exact hierarchical reductions of dynamical models via linear
  transformations",http://arxiv.org/abs/2301.11653v2,2023-01-27T11:08:55Z,2024-01-03T19:53:51Z,"  Dynamical models described by ordinary differential equations (ODEs) are a
fundamental tool in the sciences and engineering. Exact reduction aims at
producing a lower-dimensional model in which each macro-variable can be
directly related to the original variables, and it is thus a natural step
towards the model's formal analysis and mechanistic understanding. We present
an algorithm which, given a polynomial ODE model, computes a longest possible
chain of exact linear reductions of the model such that each reduction refines
the previous one, thus giving a user control of the level of detail preserved
by the reduction. This significantly generalizes over the existing approaches
which compute only the reduction of the lowest dimension subject to an
approach-specific constraint. The algorithm reduces finding exact linear
reductions to a question about representations of finite-dimensional algebras.
We provide an implementation of the algorithm, demonstrate its performance on a
set of benchmarks, and illustrate the applicability via case studies. Our
implementation is freely available at
https://github.com/x3042/ExactODEReduction.jl
","['\nAlexander Demin\n', '\nElizaveta Demitraki\n', '\nGleb Pogudin\n']",,,http://arxiv.org/abs/2301.11653v2,eess.SY,"['eess.SY', 'cs.MS', 'cs.SC', 'cs.SY', 'math.DS', '34C20, 34-04, 16G10']",,,[]
"Isolating Bounded and Unbounded Real Roots of a Mixed
  Trigonometric-Polynomial",http://arxiv.org/abs/2301.05847v1,2023-01-14T07:48:25Z,2023-01-14T07:48:25Z,"  Mixed trigonometric-polynomials (MTPs) are functions of the form
$f(x,\sin{x}, \cos{x})$ with $f\in\mathbb{Q}[x_1,x_2,x_3]$. In this paper, an
algorithm ``isolating"" all the real roots of an MTP is provided and
implemented. It automatically divides the real roots into two parts: one
consists of finitely many ``bounded"" roots in an interval $[\mu_-,\mu_+]$ while
the other consists of probably countably many ``periodic"" roots in
$\mathbb{R}\backslash[\mu_-,\mu_+]$. For bounded roots, the algorithm returns
isolating intervals and corresponding multiplicities while for periodic roots,
it returns finitely many mutually disjoint small intervals
$I_i\subset[-\pi,\pi]$, integers $c_i>0$ and multisets of root multiplicity
$\{m_{j,i}\}_{j=1}^{c_i}$ such that any periodic root $t>\mu_+$ is in the set
$(\sqcup_i\cup_{k\in\mathbb{N}}(I_i+2k\pi))$ and any interval
$I_i+2k\pi\subset(\mu_+,\infty)$ contains exactly $c_i$ periodic roots with
multiplicities $m_{1,i},...,m_{c_i,i}$, respectively. The effectiveness and
efficiency of the algorithm are shown by experiments. %In particular, our
results indicate that the ``distributions"" of the roots of an MTP in the
``periods"" $(-\pi,\pi]+2k\pi$ sufficiently far from $0$ share a same pattern.
Besides, the method used to isolate the roots in $[\mu_-,\mu_+]$ is applicable
to any other bounded interval as well. The algorithm takes advantages of the
weak Fourier sequence technique and deals with the intervals period-by-period
without scaling the coordinate so to keep the length of the sequence short. The
new approaches can easily be modified to decide whether there is any root, or
whether there are infinitely many roots in unbounded intervals of the form
$(-\infty,a)$ or $(a,\infty)$ with $a\in\mathbb{Q}$.
","['\nRizeng Chen\n', '\nHaokun Li\n', '\nBican Xia\n', '\nTianqi Zhao\n', '\nTao Zheng\n']",,,http://arxiv.org/abs/2301.05847v1,cs.SC,['cs.SC'],,,[]
"Towards Rigorous Understanding of Neural Networks via
  Semantics-preserving Transformations",http://arxiv.org/abs/2301.08013v2,2023-01-19T11:35:07Z,2023-04-28T15:00:01Z,"  In this paper we present an algebraic approach to the precise and global
verification and explanation of Rectifier Neural Networks, a subclass of
Piece-wise Linear Neural Networks (PLNNs), i.e., networks that semantically
represent piece-wise affine functions. Key to our approach is the symbolic
execution of these networks that allows the construction of semantically
equivalent Typed Affine Decision Structures (TADS). Due to their deterministic
and sequential nature, TADS can, similarly to decision trees, be considered as
white-box models and therefore as precise solutions to the model and outcome
explanation problem. TADS are linear algebras which allows one to elegantly
compare Rectifier Networks for equivalence or similarity, both with precise
diagnostic information in case of failure, and to characterize their
classification potential by precisely characterizing the set of inputs that are
specifically classified or the set of inputs where two network-based
classifiers differ. All phenomena are illustrated along a detailed discussion
of a minimal, illustrative example: the continuous XOR function.
","['\nMaximilian Schlüter\n', '\nGerrit Nolte\n', '\nAlnis Murtovi\n', '\nBernhard Steffen\n']",To appear in Software Tools for Technology Transfer,,http://arxiv.org/abs/2301.08013v2,cs.NE,"['cs.NE', 'cs.SC']",,,[]
Symbolic expression generation via Variational Auto-Encoder,http://arxiv.org/abs/2301.06064v1,2023-01-15T10:23:53Z,2023-01-15T10:23:53Z,"  There are many problems in physics, biology, and other natural sciences in
which symbolic regression can provide valuable insights and discover new laws
of nature. A widespread Deep Neural Networks do not provide interpretable
solutions. Meanwhile, symbolic expressions give us a clear relation between
observations and the target variable. However, at the moment, there is no
dominant solution for the symbolic regression task, and we aim to reduce this
gap with our algorithm. In this work, we propose a novel deep learning
framework for symbolic expression generation via variational autoencoder (VAE).
In a nutshell, we suggest using a VAE to generate mathematical expressions, and
our training strategy forces generated formulas to fit a given dataset. Our
framework allows encoding apriori knowledge of the formulas into fast-check
predicates that speed up the optimization process. We compare our method to
modern symbolic regression benchmarks and show that our method outperforms the
competitors under noisy conditions. The recovery rate of SEGVAE is 65% on the
Ngyuen dataset with a noise level of 10%, which is better than the previously
reported SOTA by 20%. We demonstrate that this value depends on the dataset and
can be even higher.
","['\nSergei Popov\n', '\nMikhail Lazarev\n', '\nVladislav Belavin\n', '\nDenis Derkach\n', '\nAndrey Ustyuzhanin\n']",,,http://arxiv.org/abs/2301.06064v1,cs.LG,"['cs.LG', 'cs.AI', 'cs.SC']",,,[]
Neuro-Symbolic World Models for Adapting to Open World Novelty,http://arxiv.org/abs/2301.06294v1,2023-01-16T07:49:12Z,2023-01-16T07:49:12Z,"  Open-world novelty--a sudden change in the mechanics or properties of an
environment--is a common occurrence in the real world. Novelty adaptation is an
agent's ability to improve its policy performance post-novelty. Most
reinforcement learning (RL) methods assume that the world is a closed, fixed
process. Consequentially, RL policies adapt inefficiently to novelties. To
address this, we introduce WorldCloner, an end-to-end trainable neuro-symbolic
world model for rapid novelty adaptation. WorldCloner learns an efficient
symbolic representation of the pre-novelty environment transitions, and uses
this transition model to detect novelty and efficiently adapt to novelty in a
single-shot fashion. Additionally, WorldCloner augments the policy learning
process using imagination-based adaptation, where the world model simulates
transitions of the post-novelty environment to help the policy adapt. By
blending ''imagined'' transitions with interactions in the post-novelty
environment, performance can be recovered with fewer total environment
interactions. Using environments designed for studying novelty in sequential
decision-making problems, we show that the symbolic world model helps its
neural policy adapt more efficiently than model-based and model-based
neural-only reinforcement learning methods.
","['\nJonathan Balloch\n', '\nZhiyu Lin\n', '\nRobert Wright\n', '\nXiangyu Peng\n', '\nMustafa Hussain\n', '\nAarun Srinivas\n', '\nJulia Kim\n', '\nMark O. Riedl\n']","9 pages, 8 figures, Extended Abstract accepted for presentation at
  AAMAS 2023",,http://arxiv.org/abs/2301.06294v1,cs.AI,"['cs.AI', 'cs.LG', 'cs.SC']",,,[]
Federated Automatic Differentiation,http://arxiv.org/abs/2301.07806v1,2023-01-18T22:28:49Z,2023-01-18T22:28:49Z,"  Federated learning (FL) is a general framework for learning across
heterogeneous clients while preserving data privacy, under the orchestration of
a central server. FL methods often compute gradients of loss functions purely
locally (ie. entirely at each client, or entirely at the server), typically
using automatic differentiation (AD) techniques. We propose a federated
automatic differentiation (FAD) framework that 1) enables computing derivatives
of functions involving client and server computation as well as communication
between them and 2) operates in a manner compatible with existing federated
technology. In other words, FAD computes derivatives across communication
boundaries. We show, in analogy with traditional AD, that FAD may be
implemented using various accumulation modes, which introduce distinct
computation-communication trade-offs and systems requirements. Further, we show
that a broad class of federated computations is closed under these various
modes of FAD, implying in particular that if the original computation can be
implemented using privacy-preserving primitives, its derivative may be computed
using only these same primitives. We then show how FAD can be used to create
algorithms that dynamically learn components of the algorithm itself. In
particular, we show that FedAvg-style algorithms can exhibit significantly
improved performance by using FAD to adjust the server optimization step
automatically, or by using FAD to learn weighting schemes for computing
weighted averages across clients.
","['\nKeith Rush\n', '\nZachary Charles\n', '\nZachary Garrett\n']","36 pages, 13 figures",,http://arxiv.org/abs/2301.07806v1,cs.LG,"['cs.LG', 'cs.DC', 'cs.SC']",,,[]
Discover governing differential equations from evolving systems,http://arxiv.org/abs/2301.07863v3,2023-01-19T03:18:54Z,2023-07-16T04:36:56Z,"  Discovering the governing equations of evolving systems from available
observations is essential and challenging. In this paper, we consider a new
scenario: discovering governing equations from streaming data. Current methods
struggle to discover governing differential equations with considering
measurements as a whole, leading to failure to handle this task. We propose an
online modeling method capable of handling samples one by one sequentially by
modeling streaming data instead of processing the entire dataset. The proposed
method performs well in discovering ordinary differential equations (ODEs) and
partial differential equations (PDEs) from streaming data. Evolving systems are
changing over time, which invariably changes with system status. Thus, finding
the exact change points is critical. The measurement generated from a changed
system is distributed dissimilarly to before; hence, the difference can be
identified by the proposed method. Our proposal is competitive in identifying
the change points and discovering governing differential equations in three
hybrid systems and two switching linear systems.
","['\nYuanyuan Li\n', '\nKai Wu\n', '\nJing Liu\n']","13 pages, 5 figures. Accepted by Physical Review Research",,http://arxiv.org/abs/2301.07863v3,physics.comp-ph,"['physics.comp-ph', 'cs.LG', 'cs.SC', 'math.DS', 'nlin.CD']",,,[]
Computing square roots in quaternion algebras,http://arxiv.org/abs/2301.00743v4,2023-01-02T16:34:52Z,2023-10-12T07:27:45Z,"  We present an explicit algorithmic method for computing square roots in
quaternion algebras over global fields of characteristic different from 2.
",['\nPrzemysław Koprowski\n'],"Final version of the paper formatted by the editorial board to appear
  in Fundamenta Informaticae","Fundamenta Informaticae, Volume 190, Issue 1 (October 14, 2023)
  fi:10772",http://arxiv.org/abs/2301.00743v4,cs.SC,"['cs.SC', 'math.RA', '68W30, 11Y40, 11R52', 'I.1.2']",,,[]
"An Automatic Method for Generating Symbolic Expressions of Zernike
  Circular Polynomials",http://arxiv.org/abs/2301.01859v3,2023-01-05T00:33:55Z,2023-06-19T14:55:29Z,"  Zernike circular polynomials (ZCP) play a significant role in optics
engineering. The symbolic expressions for ZCP are valuable for theoretic
analysis and engineering designs. However, there are still two problems which
remain open: firstly, there is a lack of sufficient mathematical formulas of
the ZCP for optics designers; secondly the formulas for inter-conversion of
Noll's single index and Born-Wolf's double indices of ZCP are neither uniquely
determinate nor satisfactory. An automatic method for generating symbolic
expressions for ZCP is proposed based on five essential factors: the new
theorems for converting the single/double indices of the ZCP, the robust and
effective numeric algorithms for computing key parameters of ZCP, the symbolic
algorithms for generating mathematical expressions of ZCP, and meta-programming
\& \LaTeX{} programming for generating the table of ZCP. The theorems, method,
algorithms and system architecture proposed are beneficial to both optics
design process, optics software, computer-output typesetting in publishing
industry as well as STEM education.
","['\nHong-Yan Zhang\n', '\nYu Zhou\n', '\nFu-Yun Li\n']","This paper has proposed a method for creating mathematical table as
  demonstrated by generating Zenike circular polynomials. The code is available
  on GitHub site, please see:
  https://github.com/GrAbsRD/ZernikeSymbolicExpression","Hong-Yan zhang, Yu Zhou and Fu-Yun Li. ""An Automatic Method for
  Generating Symbolic Expressions of Zernike Circular Polynomials"", IEEE
  Access, 2023, Vol. 11, No. 6, pp: 56481--56493",http://dx.doi.org/10.1109/ACCESS.2023.3283028,cs.SC,"['cs.SC', 'cs.MS']",10.1109/ACCESS.2023.3283028,,[]
"Proofs of Modulo 11 and 13 Cylindric Kanade-Russell Conjectures for
  $A_2$ Rogers-Ramanujan Type Identities",http://arxiv.org/abs/2301.01359v1,2023-01-03T21:25:23Z,2023-01-03T21:25:23Z,"  We present proofs of two new families of sum-product identities arising from
the cylindric partitions paradigm. Most of the presented expressions, the
related sum-product identities, and the ingredients for the proofs were first
conjectured by Kanade-Russell in the spirit of Andrews-Schilling-Warnaar
identities of the $A_2$ Rogers-Ramanujan type. We follow the footsteps of
Kanade-Russell while we alter the computations heavily to accomplish our goals.
",['\nAli Kemal Uncu\n'],19 pages,,http://arxiv.org/abs/2301.01359v1,math.NT,"['math.NT', 'cs.SC', 'math.CO', 'math.RT', '05A15, 05A17, 05A19, 11B65, 11P84, 17B65, 68R05']",,,[]
D-Algebraic Functions,http://arxiv.org/abs/2301.02512v2,2023-01-06T13:55:45Z,2023-05-10T12:11:15Z,"  Differentially-algebraic (D-algebraic) functions are solutions of polynomial
equations in the function, its derivatives, and the independent variables. We
revisit closure properties of these functions by providing constructive proofs.
We present algorithms to compute algebraic differential equations for
compositions and arithmetic manipulations of univariate D-algebraic functions
and derive bounds for the order of the resulting differential equations. We
apply our methods to examples in the sciences.
","['\nRida Ait El Manssour\n', '\nAnna-Laura Sattelberger\n', '\nBertrand Teguia Tabuguia\n']","32 pages, comments welcome",,http://arxiv.org/abs/2301.02512v2,math.AG,"['math.AG', 'cs.SC', 'math.CA', '12H05, 68W30 (primary), 34-04 (secondary)']",,,[]
"Computing error bounds for asymptotic expansions of regular P-recursive
  sequences",http://arxiv.org/abs/2212.11742v2,2022-12-22T14:34:15Z,2023-06-26T12:21:17Z,"  Over the last several decades, improvements in the fields of analytic
combinatorics and computer algebra have made determining the asymptotic
behaviour of sequences satisfying linear recurrence relations with polynomial
coefficients largely a matter of routine, under assumptions that hold often in
practice. The algorithms involved typically take a sequence, encoded by a
recurrence relation and initial terms, and return the leading terms in an
asymptotic expansion up to a big-O error term. Less studied, however, are
effective techniques giving an explicit bound on asymptotic error terms. Among
other things, such explicit bounds typically allow the user to automatically
prove sequence positivity (an active area of enumerative and algebraic
combinatorics) by exhibiting an index when positive leading asymptotic
behaviour dominates any error terms. In this article, we present a practical
algorithm for computing such asymptotic approximations with rigorous error
bounds, under the assumption that the generating series of the sequence is a
solution of a differential equation with regular (Fuchsian) dominant
singularities. Our algorithm approximately follows the singularity analysis
method of Flajolet and Odlyzko, except that all big-O terms involved in the
derivation of the asymptotic expansion are replaced by explicit error terms.
The computation of the error terms combines analytic bounds from the literature
with effective techniques from rigorous numerics and computer algebra. We
implement our algorithm in the SageMath computer algebra system and exhibit its
use on a variety of applications (including our original motivating example,
solution uniqueness in the Canham model for the shape of genus one
biomembranes).
","['\nRuiwen Dong\nLIX\n', '\nStephen Melczer\nLIX\n', '\nMarc Mezzarobba\nLIX\n']",,"Mathematics of Computation, In press",http://arxiv.org/abs/2212.11742v2,cs.SC,"['cs.SC', 'math.CO']",,,"['LIX', 'LIX', 'LIX']"
"Anticipation of Method Execution in Mixed Consistency Systems --
  Technical Report",http://arxiv.org/abs/2212.14651v1,2022-12-30T12:24:43Z,2022-12-30T12:24:43Z,"  Distributed applications often deal with data with different consistency
requirements: while a post in a social network only requires weak consistency,
the user balance in turn has strong correctness requirements, demanding
mutations to be synchronised. To deal efficiently with sequences of operations
on different replicas of the distributed application, it is useful to know
which operations commute with others and thus, when can an operation not
requiring synchronisation be anticipated wrt other requiring it, thus avoiding
unnecessary waits. Herein we present a language-based static analysis to
extract at compile-time from code information on which operations can commute
with which other operations and thus get information that can be used by the
run-time support to decide on call anticipations of operations in replicas
without compromising consistency. We illustrate the formal analysis on several
paradigmatic examples and briefly present a proof-of-concept implementation in
Java.
","['\nMarco Giunti\n', '\nHervé Paulino\n', '\nAntónio Ravara\n']","12 pages, 1 figure, companion technical report of paper ""Anticipation
  of Method Execution in Mixed Consistency Systems"" to appear in SAC
  (ACM/SIGAPP Symposium on Applied Computing) 2023",,http://arxiv.org/abs/2212.14651v1,cs.SC,"['cs.SC', 'cs.DC', 'cs.PL']",,,[]
Matrix Multiplication: Verifying Strong Uniquely Solvable Puzzles,http://arxiv.org/abs/2301.00074v1,2022-12-30T23:53:51Z,2022-12-30T23:53:51Z,"  Cohn and Umans proposed a framework for developing fast matrix multiplication
algorithms based on the embedding computation in certain groups algebras. In
subsequent work with Kleinberg and Szegedy, they connected this to the search
for combinatorial objects called strong uniquely solvable puzzles (strong
USPs). We begin a systematic computer-aided search for these objects. We
develop and implement constraint-based algorithms build on reductions to
$\mathrm{SAT}$ and $\mathrm{IP}$ to verify that puzzles are strong USPs, and to
search for large strong USPs. We produce tight bounds on the maximum size of a
strong USP for width $k \le 5$, construct puzzles of small width that are
larger than previous work, and improve the upper bounds on strong USP size for
$k \le 12$. Although our work only deals with puzzles of small-constant width,
the strong USPs we find imply matrix multiplication algorithms that run in
$O(n^\omega)$ time with exponent $\omega \le 2.66$. While our algorithms do not
beat the fastest algorithms, our work provides evidence and, perhaps, a path to
finding families of strong USPs that imply matrix multiplication algorithms
that are more efficient than those currently known.
","['\nMatthew Anderson\n', '\nZongliang Ji\n', '\nAnthony Yang Xu\n']","35 pages, 7 figures, full version of SAT 2020 extended abstract",,http://arxiv.org/abs/2301.00074v1,cs.CC,"['cs.CC', 'cs.AI', 'cs.DS', 'cs.SC', 'F.2.1; I.2.8; G.4; I.3.2']",,,[]
Fast Approximation of Polynomial Zeros and Matrix Eigenvalues,http://arxiv.org/abs/2301.11268v4,2022-12-31T16:54:07Z,2023-06-12T03:06:23Z,"  We approximate the d complex zeros of a univariate polynomial p(x) of a
degree d or those zeros that lie in a fixed region of interest on the complex
plane such as a disc or a square. Our divide and conquer algorithm of STOC 1995
supports solution of this problem in optimal Boolean time (up to a
poly-logarithmic factor), that is, runs nearly as fast as one can access the
coefficients of p with the precision necessary to support required accuracy of
the output. That record complexity has not been matched by any other algorithm
yet, but our root-finder of 1995 is quite involved and has never been
implemented. We present alternative nearly optimal root-finders based on our
novel variants of the classical subdivision iterations. Unlike our predecessor
of 1995, we require randomization of Las Vegas type, allowing us to detect any
output error at a dominated computational cost, but our new root-finders are
much simpler to implement than their predecessor of 1995. According to the
results of extensive test with standard test polynomials for their preliminary
version, which incorporates only a part of our novel techniques, the new
root-finders compete and for a large class of inputs significantly supersedes
the package of root-finding subroutines MPSolve, which for decades has been
user's choice package. Unlike our predecessor of 1995 and all known fast
algorithms for the cited tasks of polynomial root-finding, our new algorithms
can be also applied to a polynomial given by a black box oracle for its
evaluation rather than by its coefficients. This makes our root-finders
particularly efficient for polynomials p(x) that can be evaluated fast such as
the Mandelbrot polynomials or those given by the sum of a small number of
shifted monomials. Our algorithm can be readily extended to fast approximation
of the eigenvalues of a matrix or a matrix polynomial.
","['\nVictor Y. Pan\n', '\nSoo Go\n', '\nQi Luan\n', '\nLiang Zhao\n']","Wrong file intended for the revision of arXiv:1805.12042 was
  erroneously uploaded",,http://arxiv.org/abs/2301.11268v4,cs.SC,"['cs.SC', 'cs.NA', 'math.NA']",,,[]
Quantum algebra in R: the weyl package,http://arxiv.org/abs/2212.09230v1,2022-12-19T03:39:16Z,2022-12-19T03:39:16Z,"  Weyl algebra is a simple noncommutative system used in quantum mechanics.
Here I introduce the weyl package, written in the R computing language, which
furnishes functionality for working with univariate and multivariate Weyl
algebras. The package is available on CRAN at
https://CRAN.R-project.org/package=weyl.
",['\nRobin K. S. Hankin\n'],8 pages,,http://arxiv.org/abs/2212.09230v1,cs.SC,['cs.SC'],,,[]
Levelwise construction of a single cylindrical algebraic cell,http://arxiv.org/abs/2212.09309v2,2022-12-19T09:11:55Z,2023-07-20T13:50:47Z,"  Satisfiability Modulo Theories (SMT) solvers check the satisfiability of
quantifier-free first-order logic formulas. We consider the theory of
non-linear real arithmetic where the formulae are logical combinations of
polynomial constraints. Here a commonly used tool is the Cylindrical Algebraic
Decomposition (CAD) to decompose real space into cells where the constraints
are truth-invariant through the use of projection polynomials.
  An improved approach is to repackage the CAD theory into a search-based
algorithm: one that guesses sample points to satisfy the formula, and
generalizes guesses that conflict constraints to cylindrical cells around
samples which are avoided in the continuing search. Such an approach can lead
to a satisfying assignment more quickly, or conclude unsatisfiability with
fewer cells. A notable example of this approach is Jovanovi\'c and de Moura's
NLSAT algorithm. Since these cells are produced locally to a sample we might
need fewer projection polynomials than the traditional CAD projection. The
original NLSAT algorithm reduced the set a little; while Brown's single cell
construction reduced it much further still. However, the shape and size of the
cell produced depends on the order in which the polynomials are considered.
  This paper proposes a method to construct such cells levelwise, i.e. built
level-by-level according to a variable ordering. We still use a reduced number
of projection polynomials, but can now consider a variety of different
reductions and use heuristics to select the projection polynomials in order to
optimise the shape of the cell under construction. We formulate all the
necessary theory as a proof system: while not a common presentation for work in
this field, it allows an elegant decoupling of heuristics from the algorithm
and its proof of correctness.
","['\nJasper Nalbach\n', '\nErika Ábrahám\n', '\nPhilippe Specht\n', '\nChristopher W. Brown\n', '\nJames H. Davenport\n', '\nMatthew England\n']",,"Journal of Symbolic Computation, Volume 123, Article Number
  102288. Elsevier, 2024",http://dx.doi.org/10.1016/j.jsc.2023.102288,cs.SC,['cs.SC'],10.1016/j.jsc.2023.102288,,[]
"Influence of rationality levels on dynamics of heterogeneous Cournot
  duopolists with quadratic costs",http://arxiv.org/abs/2212.07128v1,2022-12-14T09:27:06Z,2022-12-14T09:27:06Z,"  This paper is intended to investigate the dynamics of heterogeneous Cournot
duopoly games, where the first players adopt identical gradient adjustment
mechanisms but the second players are endowed with distinct rationality levels.
Based on tools of symbolic computations, we introduce a new approach and use it
to establish rigorous conditions of the local stability for these models. We
analytically investigate the bifurcations and prove that the period-doubling
bifurcation is the only possible bifurcation that may occur for all the
considered models. The most important finding of our study is regarding the
influence of players' rational levels on the stability of heterogeneous
duopolistic competition. It is derived that the stability region of the model
where the second firm is rational is the smallest, while that of the one where
the second firm is boundedly rational is the largest. This fact is
counterintuitive and contrasts with relative conclusions in the existing
literature. Furthermore, we also provide numerical simulations to demonstrate
the emergence of complex dynamics such as periodic solutions with different
orders and strange attractors.
","['\nXiaoliang Li\n', '\nYihuo Jiang\n']",,,http://arxiv.org/abs/2212.07128v1,econ.TH,"['econ.TH', 'cs.SC', 'math.DS']",,,[]
BNSynth: Bounded Boolean Functional Synthesis,http://arxiv.org/abs/2212.08170v1,2022-12-15T22:10:11Z,2022-12-15T22:10:11Z,"  The automated synthesis of correct-by-construction Boolean functions from
logical specifications is known as the Boolean Functional Synthesis (BFS)
problem. BFS has many application areas that range from software engineering to
circuit design. In this paper, we introduce a tool BNSynth, that is the first
to solve the BFS problem under a given bound on the solution space. Bounding
the solution space induces the synthesis of smaller functions that benefit
resource constrained areas such as circuit design. BNSynth uses a
counter-example guided, neural approach to solve the bounded BFS problem.
Initial results show promise in synthesizing smaller solutions; we observe at
least \textbf{3.2X} (and up to \textbf{24X}) improvement in the reduction of
solution size on average, as compared to state of the art tools on our
benchmarks. BNSynth is available on GitHub under an open source license.
","['\nRavi Raja\nIndian Institute of Science, Bangalore\n', '\nStanly Samuel\nIndian Institute of Science, Bangalore\n', '\nChiranjib Bhattacharyya\nIndian Institute of Science, Bangalore\n', ""\nDeepak D'Souza\nIndian Institute of Science, Bangalore\n"", '\nAditya Kanade\nMicrosoft Research, Bangalore\n']",,,http://arxiv.org/abs/2212.08170v1,cs.AI,"['cs.AI', 'cs.LG', 'cs.LO', 'cs.SC', 'I.2.2; I.2.6; B.6.0']",,,"['Indian Institute of Science, Bangalore', 'Indian Institute of Science, Bangalore', 'Indian Institute of Science, Bangalore', 'Indian Institute of Science, Bangalore', 'Microsoft Research, Bangalore']"
Implementation of general formal translators,http://arxiv.org/abs/2212.08482v2,2022-12-16T13:55:22Z,2022-12-22T09:04:07Z,"  The general translator formalism and computing specific implementations are
proposed. The implementation of specific elements necessary to process the
source and destination information within the translators are presented. Some
common directives or instructions, such as classes and procedures, were unified
and generalized in order to allow general translations implementations. In
order to cover general cases, two levels of processing are required, related to
the source and destination information appropriate transformations, with the
related control and processing instructions. The proposed general translator
elements are useful for processing natural or artificial information described
through any types of languages or systems.
",['\nIosif Iulian Petrila\n'],,,http://arxiv.org/abs/2212.08482v2,cs.CL,"['cs.CL', 'cs.FL', 'cs.PL', 'cs.SC']",,,[]
On Eigenvalue Gaps of Integer Matrices,http://arxiv.org/abs/2212.07032v2,2022-12-14T04:55:55Z,2023-06-13T15:45:22Z,"  Given an $n\times n$ matrix with integer entries in the range $[-h,h]$, how
close can two of its distinct eigenvalues be?
  The best previously known examples have a minimum gap of $h^{-O(n)}$. Here we
give an explicit construction of matrices with entries in $[0,h]$ with two
eigenvalues separated by at most $h^{-n^2/16+o(n^2)}$. Up to a constant in the
exponent, this agrees with the known lower bound of
$\Omega((2\sqrt{n})^{-n^2}h^{-n^2})$ \cite{mahler1964inequality}. Bounds on the
minimum gap are relevant to the worst case analysis of algorithms for
diagonalization and computing canonical forms of integer matrices.
  In addition to our explicit construction, we show there are many matrices
with a slightly larger gap of roughly $h^{-n^2/32}$. We also construct 0-1
matrices which have two eigenvalues separated by at most $2^{-n^2/64+o(n^2)}$.
","['\nAaron Abrams\n', '\nZeph Landau\n', '\nJamie Pommersheim\n', '\nNikhil Srivastava\n']",9pp,,http://arxiv.org/abs/2212.07032v2,math.CO,"['math.CO', 'cs.NA', 'cs.SC', 'math.NA', 'math.NT', '15A18, 15B36']",,,[]
Flip Graphs for Matrix Multiplication,http://arxiv.org/abs/2212.01175v1,2022-12-02T13:58:57Z,2022-12-02T13:58:57Z,"  We introduce a new method for discovering matrix multiplication schemes based
on random walks in a certain graph, which we call the flip graph. Using this
method, we were able to reduce the number of multiplications for the matrix
formats (4, 4, 5) and (5, 5, 5), both in characteristic two and for arbitrary
ground fields.
","['\nManuel Kauers\n', '\nJakob Moosbauer\n']",,,http://arxiv.org/abs/2212.01175v1,cs.SC,['cs.SC'],,,[]
"Generalized Companion Subresultants of Several Univariate Polynomials in
  Newton Basis",http://arxiv.org/abs/2212.03422v1,2022-12-07T03:09:23Z,2022-12-07T03:09:23Z,"  In this paper, the concept of companion subresultant for polynomials in power
basis is extended to that for polynomials in Newton basis and an explicit
formula in the form of determinental polynomial is developed for the
generalized companion subresultant. It is noted that the resulting subresultant
is expressed with the same Newton basis as the input polynomials and it shares
the essential property with the subresultant in power basis.
","['\nWeidong Wang\n', '\nJing Yang\n']",,,http://arxiv.org/abs/2212.03422v1,cs.SC,"['cs.SC', 'math.AC']",,,[]
CHC-COMP 2022: Competition Report,http://arxiv.org/abs/2211.12231v1,2022-11-22T12:35:56Z,2022-11-22T12:35:56Z,"  CHC-COMP 2022 is the fifth edition of the competition of solvers for
Constrained Horn Clauses. The competition was run in March 2022; the results
were presented at the 9th Workshop on Horn Clauses for Verification and
Synthesis held in Munich, Germany, on April 3, 2022. This edition featured six
solvers, and eight tracks consisting of sets of linear and nonlinear clauses
with constraints over linear integer arithmetic, linear real arithmetic,
arrays, and algebraic data types. This report provides an overview of the
organization behind the competition runs: it includes the technical details of
the competition setup as well as presenting the results of the 2022 edition.
","['\nEmanuele De Angelis\nIASI-CNR, Italy\n', '\nHari Govind V K\nUniversity of Waterloo, Canada\n']","In Proceedings HCVS/VPT 2022, arXiv:2211.10675. arXiv admin note:
  text overlap with arXiv:2109.04635, arXiv:2008.02939 by other authors","EPTCS 373, 2022, pp. 44-62",http://dx.doi.org/10.4204/EPTCS.373.5,cs.LO,"['cs.LO', 'cs.SC', 'F.3.1']",10.4204/EPTCS.373.5,,"['IASI-CNR, Italy', 'University of Waterloo, Canada']"
"Proceedings 9th Workshop on Horn Clauses for Verification and Synthesis
  and 10th International Workshop on Verification and Program Transformation",http://arxiv.org/abs/2211.10675v1,2022-11-19T11:59:45Z,2022-11-19T11:59:45Z,"  These proceedings include selected papers presented at the 9th Workshop on
Horn Clauses for Verification and Synthesis and the Tenth International
Workshop on Verification and Program Transformation, both affiliated with ETAPS
2022.
  Many Program Verification and Synthesis problems of interest can be modeled
directly using Horn clauses and many recent advances in the CLP and CAV
communities have centered around efficiently solving problems presented as Horn
clauses.
  The HCVS series of workshops aims to bring together researchers working in
the communities of Constraint/Logic Programming (e.g., ICLP and CP), Program
Verification (e.g., CAV, TACAS, and VMCAI), and Automated Deduction (e.g.,
CADE, IJCAR), on the topic of Horn clause based analysis, verification, and
synthesis.
  Horn clauses for verification and synthesis have been advocated by these
communities in different times and from different perspectives and HCVS is
organized to stimulate interaction and a fruitful exchange and integration of
experiences.
  The aim of the VPT workshop is to bring together researchers working in the
fields of Program Verification and Program Transformation.
  There is a great potential for beneficial interactions between these two
fields because:
  1) On one hand, methods and tools developed in the field of Program
Transformation such as partial evaluation, fold/unfold transformations, and
supercompilation, have all been applied with success for the verification of
infinite state and parameterized systems.
  2) On the other hand, model checking, abstract interpretation, SAT and SMT
solving and automated theorem proving have been used to enhance program
transformation techniques. Moreover, the formal certification of program
transformation tools, such as automated refactoring tools and compilers, has
recently attracted considerable interest, posed major challenges.
","['\nGeoffrey W. Hamilton\nDublin City University, Ireland\n', '\nTemesghen Kahsai\nAmazon, USA\n', '\nMaurizio Proietti\nIASI-CNR, Italy\n']",,"EPTCS 373, 2022",http://dx.doi.org/10.4204/EPTCS.373,cs.PL,"['cs.PL', 'cs.LO', 'cs.SC', 'cs.SE']",10.4204/EPTCS.373,,"['Dublin City University, Ireland', 'Amazon, USA', 'IASI-CNR, Italy']"
"Specognitor: Identifying Spectre Vulnerabilities via Prediction-Aware
  Symbolic Execution",http://arxiv.org/abs/2211.13526v1,2022-11-24T10:46:23Z,2022-11-24T10:46:23Z,"  Spectre attacks exploit speculative execution to leak sensitive information.
In the last few years, a number of static side-channel detectors have been
proposed to detect cache leakage in the presence of speculative execution.
However, these techniques either ignore branch prediction mechanism, detect
static pre-defined patterns which is not suitable for detecting new patterns,
or lead to false negatives.
  In this paper, we illustrate the weakness of prediction-agnostic
state-of-the-art approaches. We propose Specognitor, a novel prediction-aware
symbolic execution engine to soundly explore program paths and detect subtle
spectre variant 1 and variant 2 vulnerabilities. We propose a dynamic pattern
detection mechanism to account for both existing and future vulnerabilities.
Our experimental results show the effectiveness and efficiency of Specognitor
in analyzing real-world cryptographic programs w.r.t. different processor
families.
",['\nAli Sahraee\n'],,,http://arxiv.org/abs/2211.13526v1,cs.CR,"['cs.CR', 'cs.AR', 'cs.SC', 'cs.SE']",,,[]
P4Testgen: An Extensible Test Oracle For P4,http://arxiv.org/abs/2211.15300v3,2022-11-28T13:31:42Z,2023-08-06T11:15:37Z,"  We present P4Testgen, a test oracle for the P4$_{16}$ language. P4Testgen
supports automatic test generation for any P4 target and is designed to be
extensible to many P4 targets. It models the complete semantics of the target's
packet-processing pipeline including the P4 language, architectures and
externs, and target-specific extensions. To handle non-deterministic behaviors
and complex externs (e.g., checksums and hash functions), P4Testgen uses taint
tracking and concolic execution. It also provides path selection strategies
that reduce the number of tests required to achieve full coverage.
  We have instantiated P4Testgen for the V1model, eBPF, PNA, and Tofino P4
architectures. Each extension required effort commensurate with the complexity
of the target. We validated the tests generated by P4Testgen by running them
across the entire P4C test suite as well as the programs supplied with the
Tofino P4 Studio. Using the tool, we have also confirmed 25 bugs in mature,
production toolchains for BMv2 and Tofino.
","['\nFabian Ruffy\n', '\nJed Liu\n', '\nPrathima Kotikalapudi\n', '\nVojtěch Havel\n', '\nHanneli Tavante\n', '\nRob Sherwood\n', '\nVladyslav Dubina\n', '\nVolodymyr Peschanenko\n', '\nAnirudh Sivaraman\n', '\nNate Foster\n']",,ACM SIGCOMM 2023 Conference (ACM SIGCOMM '23),http://dx.doi.org/10.1145/3603269.3604834,cs.NI,"['cs.NI', 'cs.SC', 'cs.SE']",10.1145/3603269.3604834,,[]
The free algebra in R,http://arxiv.org/abs/2211.04002v1,2022-11-08T04:54:39Z,2022-11-08T04:54:39Z,"  The free algebra is an interesting and useful algebraic object. Here I
introduce ""freealg"", an R package which furnishes computational support for
free algebras. The package uses the standard template library's ""map"" class for
efficiency, which uses the fact that the order of the terms is algebraically
immaterial. The package follows ""disordR"" discipline. I demonstrate some
properties of free algebra using the package, and showcase package idiom. The
package is available on CRAN at https://CRAN.R-project.org/package=freealg.
",['\nRobin K. S. Hankin\n'],5 pages,,http://arxiv.org/abs/2211.04002v1,cs.SC,['cs.SC'],,,[]
CFLOBDDs: Context-Free-Language Ordered Binary Decision Diagrams,http://arxiv.org/abs/2211.06818v3,2022-11-13T04:57:29Z,2023-04-17T00:31:05Z,"  This paper presents a new compressed representation of Boolean functions,
called CFLOBDDs (for Context-Free-Language Ordered Binary Decision Diagrams).
They are essentially a plug-compatible alternative to BDDs (Binary Decision
Diagrams), and hence useful for representing certain classes of functions,
matrices, graphs, relations, etc. in a highly compressed fashion. CFLOBDDs
share many of the good properties of BDDs, but--in the best case--the CFLOBDD
for a Boolean function can be exponentially smaller than any BDD for that
function. Compared with the size of the decision tree for a function, a
CFLOBDD--again, in the best case--can give a double-exponential reduction in
size. They have the potential to permit applications to (i) execute much
faster, and (ii) handle much larger problem instances than has been possible
heretofore.
  CFLOBDDs are a new kind of decision diagram that go beyond BDDs (and their
many relatives). The key insight is a new way to reuse sub-decision-diagrams:
components of CFLOBDDs are structured hierarchically, so that
sub-decision-diagrams can be treated as standalone ''procedures'' and reused.
  We applied CFLOBDDs to the problem of simulating quantum circuits, and found
that for several standard problems the improvement in scalability--compared to
simulation using BDDs--is quite dramatic. In particular, the number of qubits
that could be handled using CFLOBDDs was larger, compared to BDDs, by a factor
of 128x for GHZ; 1,024x for BV; 8,192x for DJ; and 128x for Grover's algorithm.
(With a 15-minute timeout, the number of qubits that CFLOBDDs can handle are
65,536 for GHZ, 524,288 for BV; 4,194,304 for DJ; and 4,096 for Grover's
Algorithm.)
","['\nMeghana Sistla\n', '\nSwarat Chaudhuri\n', '\nThomas Reps\n']",130 pages,,http://arxiv.org/abs/2211.06818v3,cs.SC,['cs.SC'],,,[]
The free group in R,http://arxiv.org/abs/2212.05883v1,2022-11-15T01:26:40Z,2022-11-15T01:26:40Z,"  Here I present the freegroup package for working with the free group on a
finite set of symbols. The package is vectorised; internally it uses an
efficient matrix-based representation for free group objects but uses a
configurable print method. A range of R-centric functionality is provided. It
is available on CRAN at https://CRAN.R-project.org/package=freegroup.
",['\nRobin K. S. Hankin\n'],5 pages,,http://arxiv.org/abs/2212.05883v1,cs.SC,['cs.SC'],,,[]
Low-depth arithmetic circuit lower bounds via shifted partials,http://arxiv.org/abs/2211.07691v1,2022-11-14T19:08:22Z,2022-11-14T19:08:22Z,"  We prove super-polynomial lower bounds for low-depth arithmetic circuits
using the shifted partials measure [Gupta-Kamath-Kayal-Saptharishi, CCC 2013],
[Kayal, ECCC 2012] and the affine projections of partials measure
[Garg-Kayal-Saha, FOCS 2020], [Kayal-Nair-Saha, STACS 2016]. The recent
breakthrough work of Limaye, Srinivasan and Tavenas [FOCS 2021] proved these
lower bounds by proving lower bounds for low-depth set-multilinear circuits. An
interesting aspect of our proof is that it does not require conversion of a
circuit to a set-multilinear circuit, nor does it involve a random restriction.
We are able to upper bound the measures for homogeneous formulas directly,
without going via set-multilinearity. Our lower bounds hold for the iterated
matrix multiplication as well as the Nisan-Wigderson design polynomials. We
also define a subclass of homogeneous formulas which we call unique parse tree
(UPT) formulas, and prove superpolynomial lower bounds for these. This
generalizes the superpolynomial lower bounds for regular formulas in
[Kayal-Saha-Saptharishi, STOC 2014], [Fournier-Limaye-Malod-Srinivasan, STOC
2014].
","['\nPrashanth Amireddy\n', '\nAnkit Garg\n', '\nNeeraj Kayal\n', '\nChandan Saha\n', '\nBhargav Thankey\n']",,,http://arxiv.org/abs/2211.07691v1,cs.CC,"['cs.CC', 'cs.SC']",,,[]
"Abstraction-Based Verification of Approximate Pre-Opacity for Control
  Systems",http://arxiv.org/abs/2211.04098v1,2022-11-08T08:57:16Z,2022-11-08T08:57:16Z,"  In this paper, we consider the problem of verifying pre-opacity for
discrete-time control systems. Pre-opacity is an important information-flow
security property that secures the intention of a system to execute some secret
behaviors in the future. Existing works on pre-opacity only consider non-metric
discrete systems, where it is assumed that intruders can distinguish different
output behaviors precisely. However, for continuous-space control systems whose
output sets are equipped with metrics (which is the case for most real-world
applications), it is too restrictive to assume precise measurements from
outside observers. In this paper, we first introduce a concept of approximate
pre-opacity by capturing the security level of control systems with respect to
the measurement precision of the intruder. Based on this new notion of
pre-opacity, we propose a verification approach for continuous-space control
systems by leveraging abstraction-based techniques. In particular, a new
concept of approximate pre-opacity preserving simulation relation is introduced
to characterize the distance between two systems in terms of preserving
pre-opacity. This new system relation allows us to verify pre-opacity of
complex continuous-space control systems using their finite abstractions. We
also present a method to construct pre-opacity preserving finite abstractions
for a class of discrete-time control systems under certain stability
assumptions.
","['\nJunyao Hou\n', '\nSiyuan Liu\n', '\nXiang Yin\n', '\nMajid Zamani\n']","Discrete Event Systems, Opacity, Formal Abstractions",,http://arxiv.org/abs/2211.04098v1,eess.SY,"['eess.SY', 'cs.SC', 'cs.SY']",,,[]
"Is the Machine Smarter than the Theorist: Deriving Formulas for Particle
  Kinematics with Symbolic Regression",http://arxiv.org/abs/2211.08420v1,2022-11-15T18:57:59Z,2022-11-15T18:57:59Z,"  We demonstrate the use of symbolic regression in deriving analytical
formulas, which are needed at various stages of a typical experimental analysis
in collider phenomenology. As a first application, we consider kinematic
variables like the stransverse mass, $M_{T2}$, which are defined
algorithmically through an optimization procedure and not in terms of an
analytical formula. We then train a symbolic regression and obtain the correct
analytical expressions for all known special cases of $M_{T2}$ in the
literature. As a second application, we reproduce the correct analytical
expression for a next-to-leading order (NLO) kinematic distribution from data,
which is simulated with a NLO event generator. Finally, we derive analytical
approximations for the NLO kinematic distributions after detector simulation,
for which no known analytical formulas currently exist.
","['\nZhongtian Dong\n', '\nKyoungchul Kong\n', '\nKonstantin T. Matchev\n', '\nKatia Matcheva\n']","15 pages, 13 figures, 8 tables",,http://dx.doi.org/10.1103/PhysRevD.107.055018,hep-ph,"['hep-ph', 'cs.AI', 'cs.LG', 'cs.SC', 'hep-ex']",10.1103/PhysRevD.107.055018,,[]
Fast multivariate polynomials in R: the mvp package,http://arxiv.org/abs/2210.15991v1,2022-10-28T08:44:58Z,2022-10-28T08:44:58Z,"  In this short article I introduce the mvp package, which provides some
functionality for handling multivariate polynomials. The package uses the C++
Standard Template Library's map class to store and retrieve elements; it
conforms to disordR discipline for coefficients. The package is available on
CRAN at https://CRAN.R-project.org/package=mvp.
",['\nRobin K. S. Hankin\n'],11 pages,,http://arxiv.org/abs/2210.15991v1,cs.SC,['cs.SC'],,,[]
Stokes's theorem in R,http://arxiv.org/abs/2210.17008v1,2022-10-31T01:51:36Z,2022-10-31T01:51:36Z,"  In this short article I introduce the stokes package which provides
functionality for working with tensors, alternating forms, wedge products, and
related concepts from the exterior calculus. Notation and spirit follow Spivak.
Stokes's generalized integral theorem, viz $\int_{\partial X}\phi=\int_Xd\phi$,
is demonstrated here using the package; it is available on CRAN
athttps://CRAN.R-project.org/package=stokes.
",['\nRobin K. S. Hankin\n'],18 pages,,http://arxiv.org/abs/2210.17008v1,cs.SC,['cs.SC'],,,[]
Analysis and object oriented implementation of the Kovacic algorithm,http://arxiv.org/abs/2211.00804v1,2022-11-02T00:48:26Z,2022-11-02T00:48:26Z,"  This paper gives a detailed overview and a number of worked out examples
illustrating the Kovacic \cite{Kovacic86} algorithm for solving second order
linear differential equation ${A(x) y""+ B(x) y' + C(x) y=0}$ where $A,B,C$ are
rational functions with complex coefficients in the independent variable $x$.
All three cases of the algorithm were implemented in a software package based
on an object oriented design and complete source code listing given in the
appendix with usage examples. Implementation used the Maple computer algebra
language. The complete Kovacic package in one mpl file accompany the arXiv
version of this paper. This package was then used to analyze the distribution
of Kovacic algorithm cases on $3000$ differential equations
",['\nNasser M. Abbasi\n'],"74 pages, 5 figures. Software package",,http://arxiv.org/abs/2211.00804v1,cs.SC,"['cs.SC', 'math.CA', '34-04', 'I.1']",,,[]
A Note on the Ramanujan Machine,http://arxiv.org/abs/2211.01058v2,2022-11-02T11:48:41Z,2022-11-03T15:40:49Z,"  The Ramanujan Machine project detects new expressions related to constants of
interest, such as $\zeta$ function values, $\gamma$ and algebraic numbers (to
name a few). In particular the project lists a number of conjectures involving
even and odd $\zeta$ function values, logarithms etc. We show that many
relations detected by the Ramanujan Machine Project stem from a specific
algebraic observation and show how to generate infinitely many. This provides
an automated proof and/or an explanation of many of the relations listed as
conjectures by the project (although not all of them).
","['\nEric Brier\n', '\nDavid Naccache\n', '\nOfer Yifrach-Stav\n']",,,http://arxiv.org/abs/2211.01058v2,math.NT,"['math.NT', 'cs.SC']",,,[]
On Catalan Constant Continued Fractions,http://arxiv.org/abs/2210.15669v5,2022-10-30T10:24:23Z,2022-11-18T18:59:25Z,"  The Ramanujan Machine project detects new expressions related to constants of
interest, such as $\zeta$ function values, $\gamma$ and algebraic numbers (to
name a few). In particular the project lists a number of conjectures concerning
the Catalan constant $G= 0.91596559\ldots$ We show how to generate infinitely
many. We used an ad hoc software toolchain and rather tedious mathematical
developments. Because we do not provide a proper peer-reviewed proof of the
relations given here we do not claim them to be theorems.
","['\nDavid Naccache\n', '\nOfer Yifrach-Stav\n']",,,http://arxiv.org/abs/2210.15669v5,cs.SC,"['cs.SC', 'cs.DM', 'math.NT']",,,[]
Neural Combinatorial Logic Circuit Synthesis from Input-Output Examples,http://arxiv.org/abs/2210.16606v1,2022-10-29T14:06:42Z,2022-10-29T14:06:42Z,"  We propose a novel, fully explainable neural approach to synthesis of
combinatorial logic circuits from input-output examples. The carrying advantage
of our method is that it readily extends to inductive scenarios, where the set
of examples is incomplete but still indicative of the desired behaviour. Our
method can be employed for a virtually arbitrary choice of atoms - from logic
gates to FPGA blocks - as long as they can be formulated in a differentiable
fashion, and consistently yields good results for synthesis of practical
circuits of increasing size. In particular, we succeed in learning a number of
arithmetic, bitwise, and signal-routing operations, and even generalise towards
the correct behaviour in inductive scenarios. Our method, attacking a discrete
logical synthesis problem with an explainable neural approach, hints at a wider
promise for synthesis and reasoning-related tasks.
","['\nPeter Belcak\n', '\nRoger Wattenhofer\n']","Accepted to the 2nd Workshop on Math-AI (MATH-AI@NeurIPS'22). 10
  pages, 1 figure",,http://arxiv.org/abs/2210.16606v1,cs.LG,"['cs.LG', 'cs.AI', 'cs.SC']",,,[]
"Index Reduction for Degenerated Differential-Algebraic Equations by
  Embedding",http://arxiv.org/abs/2210.16707v1,2022-10-29T23:04:52Z,2022-10-29T23:04:52Z,"  To find consistent initial data points for a system of differential-algebraic
equations, requires the identification of its missing constraints. An efficient
class of structural methods exploiting a dependency graph for this task was
initiated by Pantiledes. More complete methods rely on differential-algebraic
geometry but suffer from other issues (e.g. high complexity). In this paper we
give a new class of efficient structural methods combined with new tools from
numerical real algebraic geometry that has much improved completeness
properties. Existing structural methods may fail for a system of
differential-algebraic equations if its Jacobian matrix after differentiation
is still singular due to symbolic cancellation or numerical degeneration.
Existing structural methods can only handle degenerated cases caused by
symbolic cancellation. However, if a system has parameters, then its parametric
Jacobian matrix may be still singular after application of the structural
method for certain values of the parameters. This case is called numerical
degeneration.
  For polynomially nonlinear systems of differential-algebraic equations,
numerical methods are given to solve both degenerated cases using numerical
real algebraic geometry. First, we introduce a witness point method, which
produces at least one witness point on every constraint component. This can
help to ensure constant rank and detection of degeneration on all components of
such systems. Secondly, we present a Constant Rank Embedding Lemma, and based
on it propose an Index Reduction by Embedding (IRE) method which can construct
an equivalent system with a full rank Jacobian matrix. Thirdly, IRE leads to a
global structural differentiation method, to solve degenerated
differential-algebraic equations on all components numerically. Application
examples from circuits, mechanics, are used to demonstrate our method.
","['\nWenqiang Yang\n', '\nWenyuan Wu\n', '\nGreg Reid\n']",arXiv admin note: substantial text overlap with arXiv:2111.08160,,http://arxiv.org/abs/2210.16707v1,math.NA,"['math.NA', 'cs.NA', 'cs.SC']",,,[]
"Comparision Of Adversarial And Non-Adversarial LSTM Music Generative
  Models",http://arxiv.org/abs/2211.00731v1,2022-11-01T20:23:49Z,2022-11-01T20:23:49Z,"  Algorithmic music composition is a way of composing musical pieces with
minimal to no human intervention. While recurrent neural networks are
traditionally applied to many sequence-to-sequence prediction tasks, including
successful implementations of music composition, their standard supervised
learning approach based on input-to-output mapping leads to a lack of note
variety. These models can therefore be seen as potentially unsuitable for tasks
such as music generation. Generative adversarial networks learn the generative
distribution of data and lead to varied samples. This work implements and
compares adversarial and non-adversarial training of recurrent neural network
music composers on MIDI data. The resulting music samples are evaluated by
human listeners, their preferences recorded. The evaluation indicates that
adversarial training produces more aesthetically pleasing music.
","[""\nMoseli Mots'oehli\n"", '\nAnna Sergeevna Bosman\n', '\nJohan Pieter De Villiers\n']","Submitted to a 2023 conference, 20 pages, 13 figures",,http://arxiv.org/abs/2211.00731v1,cs.LG,"['cs.LG', 'cs.SC', 'cs.SD', 'eess.AS']",,,[]
An analog of the Edwards model for Jacobians of genus 2 curves,http://arxiv.org/abs/2211.01450v3,2022-11-02T19:40:31Z,2023-10-24T10:47:35Z,"  We give the explicit equations for a P^3 x P^3 embedding of the Jacobian of a
curve of genus 2, which gives a natural analog for abelian surfaces of the
Edwards curve model of elliptic curves. This gives a much more succinct
description of the Jacobian variety than the standard version in P^{15}. We
also give a condition under which, as for the Edwards curve, the abelian
surfaces have a universal group law.
","['\nE. Victor Flynn\n', '\nKamal Khuri-Makdisi\n']","42 pages, with two supplemental ancillary files of maple code. v3:
  extensive revisions, mainly to sections 2 and 5. In particular, section 2 was
  completely rewritten to use the language of algebraic theta functions; this
  resulted in a longer exposition",,http://arxiv.org/abs/2211.01450v3,math.NT,"['math.NT', 'cs.SC', 'math.AG', '11G30, 11G10, 14H40']",,,[]
Sparse arrays in R: the spray package,http://arxiv.org/abs/2210.10848v1,2022-10-19T19:22:53Z,2022-10-19T19:22:53Z,"  In this short article I introduce the spray package, which provides some
functionality for handling sparse arrays. The package uses the C++ Standard
Template Library's map class to store and retrieve elements. One natural
application for sparse arrays is multivariate polynomials and I give two
examples of the package in use, one drawn from the fields of random walks on
lattices and one from the field of recreational combinatorics. The package is
available on CRAN at https://CRAN.R-project.org/package=spray.
",['\nRobin K. S. Hankin\n'],11 pages,,http://arxiv.org/abs/2210.10848v1,cs.SC,['cs.SC'],,,[]
"Equivalence Checking of Parameterized Quantum Circuits: Verifying the
  Compilation of Variational Quantum Algorithms",http://arxiv.org/abs/2210.12166v1,2022-10-21T18:00:04Z,2022-10-21T18:00:04Z,"  Variational quantum algorithms have been introduced as a promising class of
quantum-classical hybrid algorithms that can already be used with the noisy
quantum computing hardware available today by employing parameterized quantum
circuits. Considering the non-trivial nature of quantum circuit compilation and
the subtleties of quantum computing, it is essential to verify that these
parameterized circuits have been compiled correctly. Established equivalence
checking procedures that handle parameter-free circuits already exist. However,
no methodology capable of handling circuits with parameters has been proposed
yet. This work fills this gap by showing that verifying the equivalence of
parameterized circuits can be achieved in a purely symbolic fashion using an
equivalence checking approach based on the ZX-calculus. At the same time,
proofs of inequality can be efficiently obtained with conventional methods by
taking advantage of the degrees of freedom inherent to parameterized circuits.
We implemented the corresponding methods and proved that the resulting
methodology is complete. Experimental evaluations (using the entire parametric
ansatz circuit library provided by Qiskit as benchmarks) demonstrate the
efficacy of the proposed approach. The implementation is open source and
publicly available as part of the equivalence checking tool QCEC
(https://github.com/cda-tum/qcec) which is part of the Munich Quantum Toolkit
(MQT).
","['\nTom Peham\n', '\nLukas Burgholzer\n', '\nRobert Wille\n']","7 pages, 3 figures, 2 tables, 28th Asia and South Pacific Design
  Automation Conference (ASPDAC '23)",,http://dx.doi.org/10.1145/3566097.3567932,quant-ph,"['quant-ph', 'cs.SC']",10.1145/3566097.3567932,,[]
Axioms for a theory of signature bases,http://arxiv.org/abs/2210.13788v3,2022-10-25T06:19:26Z,2024-01-08T16:35:27Z,"  Twenty years after the discovery of the F5 algorithm, Gr\""obner bases with
signatures are still challenging to understand and to adapt to different
settings. This contrasts with Buchberger's algorithm, which we can bend in many
directions keeping correctness and termination obvious. I propose an axiomatic
approach to Gr\""obner bases with signatures with the purpose of uncoupling the
theory and the algorithms, and giving general results applicable in many
different settings (e.g. Gr\""obner for submodules, F4-style reduction,
noncommutative rings, non-Noetherian settings, etc.).
",['\nPierre Lairez\n'],,"Journal of Symbolic Computation, Volume 123, 2024, 102275",http://dx.doi.org/10.1016/j.jsc.2023.102275,cs.SC,"['cs.SC', 'math.AC']",10.1016/j.jsc.2023.102275,,[]
Convexity Certificates from Hessians,http://arxiv.org/abs/2210.10430v1,2022-10-19T09:52:03Z,2022-10-19T09:52:03Z,"  The Hessian of a differentiable convex function is positive semidefinite.
Therefore, checking the Hessian of a given function is a natural approach to
certify convexity. However, implementing this approach is not straightforward
since it requires a representation of the Hessian that allows its analysis.
Here, we implement this approach for a class of functions that is rich enough
to support classical machine learning. For this class of functions, it was
recently shown how to compute computational graphs of their Hessians. We show
how to check these graphs for positive semidefiniteness. We compare our
implementation of the Hessian approach with the well-established disciplined
convex programming (DCP) approach and prove that the Hessian approach is at
least as powerful as the DCP approach for differentiable functions.
Furthermore, we show for a state-of-the-art implementation of the DCP approach
that, for differentiable functions, the Hessian approach is actually more
powerful. That is, it can certify the convexity of a larger class of
differentiable functions.
","['\nJulien Klaus\n', '\nNiklas Merk\n', '\nKonstantin Wiedom\n', '\nSören Laue\n', '\nJoachim Giesen\n']",Accepted for publication at NeurIPS 2022,,http://arxiv.org/abs/2210.10430v1,math.OC,"['math.OC', 'cs.LG', 'cs.SC']",,,[]
A partial order view of message-passing communication models,http://arxiv.org/abs/2210.13062v2,2022-10-24T09:31:25Z,2023-01-13T14:36:45Z,"  There is a wide variety of message-passing communication models, ranging from
synchronous ''rendez-vous'' communications to fully asynchronous/out-of-order
communications. For large-scale distributed systems, the communication model is
determined by the transport layer of the network, and a few classes of orders
of message delivery (FIFO, causally ordered) have been identified in the early
days of distributed computing. For local-scale message-passing applications,
e.g., running on a single machine, the communication model may be determined by
the actual implementation of message buffers and by how FIFO queues are used.
While large-scale communication models, such as causal ordering, are defined by
logical axioms, local-scale models are often defined by an operational
semantics. In this work, we connect these two approaches, and we present a
unified hierarchy of communication models encompassing both large-scale and
local-scale models, based on their concurrent behaviors. We also show that all
the communication models we consider can be axiomatized in the monadic second
order logic, and may therefore benefit from several bounded verification
techniques based on bounded special treewidth.
","['\nCinzia Di Giusto\nC&A\n', '\nDavide Ferré\nC&A\n', '\nLaetitia Laversa\nC&A\n', '\nEtienne Lozes\nC&A\n']",,,http://arxiv.org/abs/2210.13062v2,cs.CL,"['cs.CL', 'cs.FL', 'cs.SC']",,,"['C&A', 'C&A', 'C&A', 'C&A']"
Gosper's algorithm and Bell numbers,http://arxiv.org/abs/2210.13520v1,2022-10-24T18:20:07Z,2022-10-24T18:20:07Z,"  Computers are good at evaluating finite sums in closed form, but there are
finite sums which do not have closed forms. Summands which do not produce a
closed form can often be ``fixed'' by multiplying them by a suitable
polynomial. We provide an explicit description of a class of such polynomials
for simple hypergeometric summands in terms of the Bell numbers.
",['\nRobert Dougherty-Bliss\n'],13 pages,,http://arxiv.org/abs/2210.13520v1,cs.SC,"['cs.SC', 'math.CO', 'math.NT', '68R05']",,,[]
Galois Groups of Linear Difference-Differential Equations,http://arxiv.org/abs/2211.01977v2,2022-10-18T08:57:29Z,2022-11-04T01:19:58Z,"  We study the relation between the Galois group $G$ of a linear
difference-differential system and two classes $\mathcal{C}_1$ and
$\mathcal{C}_2$ of groups that are the Galois groups of the specializations of
the linear difference equation and the linear differential equation in this
system respectively. We show that almost all groups in $\mathcal{C}_1\cup
\mathcal{C}_2$ are algebraic subgroups of $G$, and there is a nonempty subset
of $\mathcal{C}_1$ and a nonempty subset of $\mathcal{C}_2$ such that $G$ is
the product of any pair of groups from these two subsets. These results have
potential application to the computation of the Galois group of a linear
difference-differential system. We also give a criterion for testing linear
dependence of elements in a simple difference-differential ring, which
generalizes Kolchin's criterion for partial differential fields.
","['\nRuyong Feng\n', '\nWei Lu\n']",32 pages,,http://arxiv.org/abs/2211.01977v2,math.RA,"['math.RA', 'cs.SC', 'math.NT', '12H05 12H10 39A06 34A30']",,,[]
Context-driven Visual Object Recognition based on Knowledge Graphs,http://arxiv.org/abs/2210.11233v1,2022-10-20T13:09:00Z,2022-10-20T13:09:00Z,"  Current deep learning methods for object recognition are purely data-driven
and require a large number of training samples to achieve good results. Due to
their sole dependence on image data, these methods tend to fail when confronted
with new environments where even small deviations occur. Human perception,
however, has proven to be significantly more robust to such distribution
shifts. It is assumed that their ability to deal with unknown scenarios is
based on extensive incorporation of contextual knowledge. Context can be based
either on object co-occurrences in a scene or on memory of experience. In
accordance with the human visual cortex which uses context to form different
object representations for a seen image, we propose an approach that enhances
deep learning methods by using external contextual knowledge encoded in a
knowledge graph. Therefore, we extract different contextual views from a
generic knowledge graph, transform the views into vector space and infuse it
into a DNN. We conduct a series of experiments to investigate the impact of
different contextual views on the learned object representations for the same
image dataset. The experimental results provide evidence that the contextual
views influence the image representations in the DNN differently and therefore
lead to different predictions for the same images. We also show that context
helps to strengthen the robustness of object recognition models for
out-of-distribution images, usually occurring in transfer learning tasks or
real-world scenarios.
","['\nSebastian Monka\n', '\nLavdim Halilaj\n', '\nAchim Rettinger\n']",ISWC 2022,,http://arxiv.org/abs/2210.11233v1,cs.AI,"['cs.AI', 'cs.CL', 'cs.CV', 'cs.LG', 'cs.SC']",,,[]
Disordered vectors in R: introducing the disordR package,http://arxiv.org/abs/2210.03856v2,2022-10-08T00:14:02Z,2022-10-17T00:21:31Z,"  Objects in the {\tt stl map} class of {\tt C++} associate a value to each of
a set of keys. Accessing values or keys of such an object is problematic in the
R programming language because the value-key pairs are not stored in a
well-defined order. This document motivates and discusses the concept of
""disordered vector"" as implemented by the {\tt disordR} package which
facilitates the handling of {\tt map} objects. Values and keys of a map are
stored in an implementation-specific way so certain extraction and replacement
operations should be forbidden. For example, if values are real, then the
""first"" value is implementation specific\ldots but the maximum value has a
well-defined result. The {\tt disordR} package makes forbidden operations
impossible while allowing transparent R idiom for permitted operations. An
illustrative R session is given in which the package is used abstractly,
without reference to any particular application, and then shows how it can be
used to manipulate multivariate polynomials. The {\tt disordR} package is a
dependency of {\tt clifford}, {\tt freealg}, {\tt hyper2}, {\tt mvp}, {\tt
spray}, {\tt stokes}, and {\tt weyl}. The {\tt disordR} package is available on
CRAN at \url{https://CRAN.R-project.org/package=disordR}.
",['\nRobin K. S. Hankin\n'],8 pages,,http://arxiv.org/abs/2210.03856v2,cs.SC,"['cs.SC', '68V99', 'I.1.m']",,,[]
Computing groups of Hecke characters,http://arxiv.org/abs/2210.02716v1,2022-10-06T06:57:33Z,2022-10-06T06:57:33Z,"  We describe algorithms to represent and compute groups of Hecke characters.
We make use of an id{\`e}lic point of view and obtain the whole family of such
characters, including transcendental ones. We also show how to isolate the
algebraic characters, which are of particular interest in number theory. This
work has been implemented in Pari/GP, and we illustrate our work with a variety
of explicit examples using our implementation.
","['\nPascal Molin\n', '\nAurel Page\n']",,"ANTS XV, Aug 2022, Bristol, United Kingdom",http://arxiv.org/abs/2210.02716v1,cs.SC,"['cs.SC', 'math.NT']",,,[]
"The FBHHRBNRSSSHK-Algorithm for Multiplication in
  $\mathbb{Z}_2^{5\times5}$ is still not the end of the story",http://arxiv.org/abs/2210.04045v3,2022-10-08T15:04:25Z,2022-10-13T15:16:19Z,"  In response to a recent Nature article which announced an algorithm for
multiplying $5\times5$-matrices over $\mathbb{Z}_2$ with only 96
multiplications, two fewer than the previous record, we present an algorithm
that does the job with only 95 multiplications.
","['\nManuel Kauers\n', '\nJakob Moosbauer\n']",,,http://arxiv.org/abs/2210.04045v3,cs.SC,"['cs.SC', 'cs.CC']",,,[]
On the Partial Differential Lüroth's Theorem,http://arxiv.org/abs/2210.05469v1,2022-10-11T14:13:30Z,2022-10-11T14:13:30Z,"  We study the L\""{u}roth problem for partial differential fields. The main
result is the following partial differential analog of generalized L\""{u}roth's
theorem: Let $\mathcal{F}$ be a differential field of characteristic 0 with $m$
derivation operators, $\textbf{u}=u_1,\ldots,u_n$ a set of differential
indeterminates over $\mathcal{F}$. We prove that an intermediate differential
field $\mathcal{G}$ between $\mathcal{F}$ and $\mathcal{F}\langle
\textbf{u}\rangle$ is a simple differential extension of $\mathcal{F}$ if and
only if the differential dimension polynomial of $\textbf{u}$ over
$\mathcal{G}$ is of the form $\omega_{\textbf{u}/\mathcal{G}}(t)=n{t+m\choose
m}-{t+m-s\choose m}$ for some $s\in\mathbb N$. This result generalizes the
classical differential L\""uroth's theorem proved by Ritt and Kolchin in the
case $m=n=1$. We then present an algorithm to decide whether a given finitely
generated differential extension field of $\mathcal{F}$ contained in
$\mathcal{F}\langle \textbf{u}\rangle$ is a simple extension, and in the
affirmative case, to compute a L\""{u}roth generator. As an application, we
solve the proper re-parameterization problem for unirational differential
curves.
","['\nWei Li\n', '\nChen-Rui Wei\n']",19 pages,,http://arxiv.org/abs/2210.05469v1,math.AG,"['math.AG', 'cs.SC', '12H05']",,,[]
Critical Points at Infinity for Hyperplanes of Directions,http://arxiv.org/abs/2210.05748v1,2022-10-11T19:27:38Z,2022-10-11T19:27:38Z,"  Analytic combinatorics in several variables (ACSV) analyzes the asymptotic
growth of the coefficients of a meromorphic generating function $F = G/H$ in a
direction $\mathbf{r}$. It uses Morse theory on the pole variety $V := \{ H = 0
\} \subseteq (\mathbb{C}^*)^d$ of $F$ to deform the torus $T$ in the
multivariate Cauchy Integral Formula via the downward gradient flow for the
\textit{height} function $h = h_{\mathbf{r}} = -\sum_{j=1}^d r_j \log |z_j|$,
giving a homology decomposition of $T$ into cycles around \textit{critical
points} of $h$ on $V$. The deformation can flow to infinity at finite height
when the height function is not a proper map. This happens only in the presence
of a critical point at infinity (CPAI): a sequence of points on $V$ approaching
a point at infinity, and such that log-normals to $V$ converge projectively to
$\mathbf{r}$. The CPAI is called \textit{heighted} if the height function also
converges to a finite value. This paper studies whether all CPAI are heighted,
and in which directions CPAI can occur. We study these questions by examining
sequences converging to faces of a toric compactification defined by a multiple
of the Newton polytope $\mathcal{P}$ of the polynomial $H$. Under generically
satisfied conditions, any projective limit of log-normals of a sequence
converging to a face $F$ must be parallel to $F$; this implies that CPAI must
always be heighted and can only occur in directions parallel to some face of
$\mathcal{P}$. When this generic condition fails, we show under a smoothness
condition, that a point in a codimension-1 face $F$ can still only be a CPAI
for directions parallel to $F$, and that the directions for a codimension-2
face can be a larger set, which can be computed explicitly and still has
positive codimension.
",['\nStephen Gillen\n'],,,http://arxiv.org/abs/2210.05748v1,math.CO,"['math.CO', 'cs.SC']",,,[]
Clifford algebra in R,http://arxiv.org/abs/2209.13659v2,2022-09-27T19:57:07Z,2022-11-05T22:39:19Z,"  Here I present the 'clifford' package for working with Clifford algebras in
the R programming language. The algebra is described and package idiom is
given.
",['\nRobin K. S. Hankin\n'],8 pages,,http://arxiv.org/abs/2209.13659v2,cs.SC,['cs.SC'],,,[]
Solving homogeneous linear equations over polynomial semirings,http://arxiv.org/abs/2209.13347v2,2022-09-27T12:55:52Z,2022-10-26T10:04:05Z,"  For a subset $B$ of $\mathbb{R}$, denote by $\operatorname{U}(B)$ be the
semiring of (univariate) polynomials in $\mathbb{R}[X]$ that are strictly
positive on $B$. Let $\mathbb{N}[X]$ be the semiring of (univariate)
polynomials with non-negative integer coefficients. We study solutions of
homogeneous linear equations over the polynomial semirings
$\operatorname{U}(B)$ and $\mathbb{N}[X]$. In particular, we prove local-global
principles for solving single homogeneous linear equations over these
semirings. We then show PTIME decidability of determining the existence of
non-zero solutions over $\mathbb{N}[X]$ of single homogeneous linear equations.
  Our study of these polynomial semirings is largely motivated by several
semigroup algorithmic problems in the wreath product $\mathbb{Z} \wr
\mathbb{Z}$. As an application of our results, we show that the Identity
Problem (whether a given semigroup contains the neutral element?) and the Group
Problem (whether a given semigroup is a group?) for finitely generated
sub-semigroups of the wreath product $\mathbb{Z} \wr \mathbb{Z}$ is decidable
when elements of the semigroup generator have the form $(y, \pm 1)$.
",['\nRuiwen Dong\n'],"21 pages including appendix, 1 figure",,http://arxiv.org/abs/2209.13347v2,math.RA,"['math.RA', 'cs.SC', 'math.GR']",,,[]
"Topological descriptors of the parameter region of multistationarity:
  deciding upon connectivity",http://arxiv.org/abs/2209.13936v2,2022-09-28T09:12:15Z,2023-03-18T08:20:55Z,"  Switch-like responses arising from bistability have been linked to cell
signaling processes and memory. Revealing the shape and properties of the set
of parameters that lead to bistability is necessary to understand the
underlying biological mechanisms, but is a complex mathematical problem. We
present an efficient approach to determine a basic topological property of the
parameter region of multistationary, namely whether it is connected or not. The
connectivity of this region can be interpreted in terms of the biological
mechanisms underlying bistability and the switch-like patterns that the system
can create.
  We provide an algorithm to assert that the parameter region of
multistationarity is connected, targeting reaction networks with mass-action
kinetics. We show that this is the case for numerous relevant cell signaling
motifs, previously described to exhibit bistability. However, we show that for
a motif displaying a phosphorylation cycle with allosteric enzyme regulation,
the region of multistationarity has two distinct connected components,
corresponding to two different, but symmetric, biological mechanisms. The
method relies on linear programming and bypasses the expensive computational
cost of direct and generic approaches to study parametric polynomial systems.
This characteristic makes it suitable for mass-screening of reaction networks.
","['\nMáté L. Telek\n', '\nElisenda Feliu\n']",Accepted in Plos Computational Biology,,http://dx.doi.org/10.1371/journal.pcbi.1010970,q-bio.MN,"['q-bio.MN', 'cs.SC', 'q-bio.QM']",10.1371/journal.pcbi.1010970,,[]
"AI-Assisted Discovery of Quantitative and Formal Models in Social
  Science",http://arxiv.org/abs/2210.00563v3,2022-10-02T16:25:47Z,2023-08-16T17:45:13Z,"  In social science, formal and quantitative models, such as ones describing
economic growth and collective action, are used to formulate mechanistic
explanations, provide predictions, and uncover questions about observed
phenomena. Here, we demonstrate the use of a machine learning system to aid the
discovery of symbolic models that capture nonlinear and dynamical relationships
in social science datasets. By extending neuro-symbolic methods to find compact
functions and differential equations in noisy and longitudinal data, we show
that our system can be used to discover interpretable models from real-world
data in economics and sociology. Augmenting existing workflows with symbolic
regression can help uncover novel relationships and explore counterfactual
models during the scientific process. We propose that this AI-assisted
framework can bridge parametric and non-parametric models commonly employed in
social science research by systematically exploring the space of nonlinear
models and enabling fine-grained control over expressivity and
interpretability.
","['\nJulia Balla\n', '\nSihao Huang\n', '\nOwen Dugan\n', '\nRumen Dangovski\n', '\nMarin Soljacic\n']","19 pages, 4 figures",,http://arxiv.org/abs/2210.00563v3,cs.SC,"['cs.SC', 'cs.LG', 'econ.EM']",,,[]
"A cubic algorithm for computing the Hermite normal form of a nonsingular
  integer matrix",http://arxiv.org/abs/2209.10685v2,2022-09-21T22:19:56Z,2023-08-28T14:31:11Z,"  A Las Vegas randomized algorithm is given to compute the Hermite normal form
of a nonsingular integer matrix $A$ of dimension $n$. The algorithm uses
quadratic integer multiplication and cubic matrix multiplication and has
running time bounded by $O(n^3 (\log n + \log ||A||)^2(\log n)^2)$ bit
operations, where $||A||= \max_{ij} |A_{ij}|$ denotes the largest entry of $A$
in absolute value. A variant of the algorithm that uses pseudo-linear integer
multiplication is given that has running time $(n^3 \log ||A||)^{1+o(1)}$ bit
operations, where the exponent $""+o(1)""$ captures additional factors $c_1 (\log
n)^{c_2} (\log \log ||A||)^{c_3}$ for positive real constants $c_1,c_2,c_3$.
","['\nStavros Birmpilis\n', '\nGeorge Labahn\n', '\nArne Storjohann\n']",36 pages,,http://arxiv.org/abs/2209.10685v2,cs.DS,"['cs.DS', 'cs.SC']",,,[]
Error bounds for the asymptotic expansion of the partition function,http://arxiv.org/abs/2209.07887v1,2022-09-16T12:27:20Z,2022-09-16T12:27:20Z,"  Asymptotic study on the partition function $p(n)$ began with the work of
Hardy and Ramanujan. Later Rademacher obtained a convergent series for $p(n)$
and an error bound was given by Lehmer. Despite having this, a full asymptotic
expansion for $p(n)$ with an explicit error bound is not known. Recently
O'Sullivan studied the asymptotic expansion of $p^{k}(n)$-partitions into $k$th
powers, initiated by Wright, and consequently obtained an asymptotic expansion
for $p(n)$ along with a concise description of the coefficients involved in the
expansion but without any estimation of the error term. Here we consider a
detailed and comprehensive analysis on an estimation of the error term obtained
by truncating the asymptotic expansion for $p(n)$ at any positive integer $n$.
This gives rise to an infinite family of inequalities for $p(n)$ which finally
answers to a question proposed by Chen. Our error term estimation predominantly
relies on applications of algorithmic methods from symbolic summation.
","['\nKoustav Banerje\n', '\nPeter Paule\n', '\nCristian-Silviu Radu\n', '\nCarsten Schneider\n']",,,http://arxiv.org/abs/2209.07887v1,math.NT,"['math.NT', 'cs.SC', 'math.CO', '05A16, 11P82, 68W30']",,,[]
FACT: Learning Governing Abstractions Behind Integer Sequences,http://arxiv.org/abs/2209.09543v1,2022-09-20T08:20:03Z,2022-09-20T08:20:03Z,"  Integer sequences are of central importance to the modeling of concepts
admitting complete finitary descriptions. We introduce a novel view on the
learning of such concepts and lay down a set of benchmarking tasks aimed at
conceptual understanding by machine learning models. These tasks indirectly
assess model ability to abstract, and challenge them to reason both
interpolatively and extrapolatively from the knowledge gained by observing
representative examples. To further aid research in knowledge representation
and reasoning, we present FACT, the Finitary Abstraction Comprehension Toolkit.
The toolkit surrounds a large dataset of integer sequences comprising both
organic and synthetic entries, a library for data pre-processing and
generation, a set of model performance evaluation tools, and a collection of
baseline model implementations, enabling the making of the future advancements
with ease.
","['\nPeter Belcák\n', '\nArd Kastrati\n', '\nFlavio Schenker\n', '\nRoger Wattenhofer\n']","Accepted to the 36th Conference on Neural Information Processing
  Systems (NeurIPS 2022) Track on Datasets and Benchmarks. 37 pages",,http://arxiv.org/abs/2209.09543v1,cs.LG,"['cs.LG', 'cs.AI', 'cs.SC']",,,[]
"CryptoSolve: Towards a Tool for the Symbolic Analysis of Cryptographic
  Algorithms",http://arxiv.org/abs/2209.10321v1,2022-09-21T12:45:57Z,2022-09-21T12:45:57Z,"  Recently, interest has been emerging in the application of symbolic
techniques to the specification and analysis of cryptosystems. These
techniques, when accompanied by suitable proofs of soundness/completeness, can
be used both to identify insecure cryptosystems and prove sound ones secure.
But although a number of such symbolic algorithms have been developed and
implemented, they remain scattered throughout the literature. In this paper, we
present a tool, CryptoSolve, which provides a common basis for specification
and implementation of these algorithms, CryptoSolve includes libraries that
provide the term algebras used to express symbolic cryptographic systems, as
well as implementations of useful algorithms, such as unification and variant
generation. In its current initial iteration, it features several algorithms
for the generation and analysis of cryptographic modes of operation, which
allow one to use block ciphers to encrypt messages more than one block long.
The goal of our work is to continue expanding the tool in order to consider
additional cryptosystems and security questions, as well as extend the symbolic
libraries to increase their applicability.
","['\nDalton Chichester\nUniversity of Mary Washington\n', '\nWei Du\nUniversity at Albany-SUNY\n', '\nRaymond Kauffman\nUniversity of Mary Washington\n', '\nHai Lin\nClarkson University\n', '\nChristopher Lynch\nClarkson University\n', '\nAndrew M. Marshall\nUniversity of Mary Washington\n', '\nCatherine A. Meadows\nNaval Research Laboratory\n', '\nPaliath Narendran\nUniversity at Albany-SUNY\n', '\nVeena Ravishankar\nUniversity of Mary Washington\n', '\nLuis Rovira\nUniversity of Mary Washington\n', '\nBrandon Rozek\nRensselaer Polytechnic Institute\n']","In Proceedings GandALF 2022, arXiv:2209.09333","EPTCS 370, 2022, pp. 147-161",http://dx.doi.org/10.4204/EPTCS.370.10,cs.LO,"['cs.LO', 'cs.CR', 'cs.SC']",10.4204/EPTCS.370.10,,"['University of Mary Washington', 'University at Albany-SUNY', 'University of Mary Washington', 'Clarkson University', 'Clarkson University', 'University of Mary Washington', 'Naval Research Laboratory', 'University at Albany-SUNY', 'University of Mary Washington', 'University of Mary Washington', 'Rensselaer Polytechnic Institute']"
SC-Square: Overview to 2021,http://arxiv.org/abs/2209.04359v1,2022-09-09T15:44:23Z,2022-09-09T15:44:23Z,"  This extended abstract was written to accompany an invited talk at the 2021
SC-Square Workshop, where the author was asked to give an overview of SC-Square
progress to date. The author first reminds the reader of the definition of
SC-Square, then briefly outlines some of the history, before picking out some
(personal) scientific highlights.
",['\nMatthew England\n'],6 pages. Survey paper. Accepted into the workshop proceedings,"Proceedings of the 6th Workshop on Satisfiability Checking and
  Symbolic Computation (SC2 '21), C. Bright and J.H. Davenport eds. CEUR
  Workshop Proceedings 3273, pp. 1-6, 2022",http://arxiv.org/abs/2209.04359v1,cs.SC,['cs.SC'],,,[]
"Minimization of differential equations and algebraic values of
  $E$-functions",http://arxiv.org/abs/2209.01827v3,2022-09-05T08:21:14Z,2023-07-18T09:09:56Z,"  A power series being given as the solution of a linear differential equation
with appropriate initial conditions, minimization consists in finding a
non-trivial linear differential equation of minimal order having this power
series as a solution. This problem exists in both homogeneous and inhomogeneous
variants; it is distinct from, but related to, the classical problem of
factorization of differential operators. Recently, minimization has found
applications in Transcendental Number Theory, more specifically in the
computation of non-zero algebraic points where Siegel's $E$-functions take
algebraic values. We present algorithms and implementations for these
questions, and discuss examples and experiments.
","['\nAlin Bostan\n', '\nTanguy Rivoal\n', '\nBruno Salvy\n']",48 pages,,http://arxiv.org/abs/2209.01827v3,cs.SC,"['cs.SC', 'math.NT', '68W30, 11J81, 16S32, 34M15, 33F10']",,,[]
The art of algorithmic guessing in $\texttt{gfun}$,http://arxiv.org/abs/2209.03059v1,2022-09-07T10:49:59Z,2022-09-07T10:49:59Z,"  The technique of guessing can be very fruitful when dealing with sequences
which arise in practice. This holds true especially when guessing is performed
algorithmically and efficiently. One highly useful tool for this purpose is the
package named $\texttt{gfun}$ in the software Maple. In this text we explore
and explain some of $\texttt{gfun}$'s possibilities and illustrate them on two
examples from recent mathematical research by the author and his collaborators.
",['\nSergey Yurkevich\n'],,"Proceedings of the Maple Conference 2021, Vol. 2 No. 1 (2022), pp.
  14421:1 -- 14421:19",http://dx.doi.org/10.5206/mt.v2i1.14421,math.CO,"['math.CO', 'cs.SC']",10.5206/mt.v2i1.14421,,[]
SC-Square: Future Progress with Machine Learning?,http://arxiv.org/abs/2209.04361v1,2022-09-09T15:47:04Z,2022-09-09T15:47:04Z,"  The algorithms employed by our communities are often underspecified, and thus
have multiple implementation choices, which do not effect the correctness of
the output, but do impact the efficiency or even tractability of its
production. In this extended abstract, to accompany a keynote talk at the 2021
SC-Square Workshop, we survey recent work (both the author's and from the
literature) on the use of Machine Learning technology to improve algorithms of
interest to SC-Square.
",['\nMatthew England\n'],"10 pages. Survey Paper. Accepted into SC-Square 2021 Workshop
  Proceedings","Proceedings of the 6th Workshop on Satisfiability Checking and
  Symbolic Computation (SC2 '21), C. Bright and J.H. Davenport eds. CEUR
  Workshop Proceedings 3273, pp. 7-16, 2022",http://arxiv.org/abs/2209.04361v1,cs.SC,"['cs.SC', 'cs.LG']",,,[]
Computer algebra calculations in supersymmetric electrodynamics,http://arxiv.org/abs/2209.05295v1,2022-09-12T14:57:26Z,2022-09-12T14:57:26Z,"  We propose a new symbolic algorithm and a C++ program for generating and
calculating supersymmetric Feynman diagrams for ${\cal N}=1$ supersymmetric
electrodynamics regularized by higher derivatives in four dimensions. According
to standard rules, the program generates all diagrams that are necessary to
calculate a specific contribution to the two-point Green function of matter
superfields in the needed order, and then reduces the answer to the sum of
Euclidean momentum integrals. At the moment, the program was used to calculate
the anomalous dimension in ${\cal N}=1$ supersymmetric quantum electrodynamics,
regularized by higher derivatives, in the three-loop approximation.
",['\nIlya Shirokov\n'],"14 pages, 1 figure, accepted for publication in ""Programming and
  Computer Software""","Program. Comput. Software 49 (2023), 122-130",http://dx.doi.org/10.1134/S0361768823020147,hep-th,"['hep-th', 'cs.SC']",10.1134/S0361768823020147,,[]
Survey on Applications of Neurosymbolic Artificial Intelligence,http://arxiv.org/abs/2209.12618v1,2022-09-08T18:18:41Z,2022-09-08T18:18:41Z,"  In recent years, the Neurosymbolic framework has attracted a lot of attention
in various applications, from recommender systems and information retrieval to
healthcare and finance. This success is due to its stellar performance combined
with attractive properties, such as learning and reasoning. The new emerging
Neurosymbolic field is currently experiencing a renaissance, as novel
frameworks and algorithms motivated by various practical applications are being
introduced, building on top of the classical neural and reasoning problem
setting. This article aims to provide a comprehensive review of significant
recent developments in real-world applications of Neurosymbolic Artificial
Intelligence. Specifically, we introduce a taxonomy of common Neurosymbolic
applications and summarize the state-of-the-art for each of those domains.
Furthermore, we identify important current trends and provide new perspectives
pertaining to the future of this burgeoning field.
","['\nDjallel Bouneffouf\n', '\nCharu C. Aggarwal\n']",,,http://arxiv.org/abs/2209.12618v1,cs.AI,"['cs.AI', 'cs.SC']",,,[]
"Some explicit arithmetics on curves of genus three and their
  applications",http://arxiv.org/abs/2209.02926v2,2022-09-07T04:38:56Z,2023-03-01T16:47:29Z,"  A Richelot isogeny between Jacobian varieties is an isogeny whose kernel is
included in the $2$-torsion subgroup of the domain. In particular, a Richelot
isogeny whose codomain is the product of two or more principally porlalized
abelian varieties is called a decomposed Richelot isogeny. In this paper, we
develop some explicit arithmetics on curves of genus $3$, including algorithms
to compute the codomain of a decomposed Richelot isogeny. As solutions to
compute the domain of a decomposed Richelot isogeny, explicit formulae of
defining equations for Howe curves of genus $3$ are also given. Using the
formulae, we shall construct an algorithm with complexity $\tilde{O}(p^3)$
(resp.\ $\tilde{O}(p^4)$) to enumerate all hyperelliptic (resp.\
non-hyperelliptic) superspecial Howe curves of genus $3$.
","['\nTomoki Moriya\n', '\nMomonari Kudo\n']",Comments are welcome!,,http://arxiv.org/abs/2209.02926v2,math.AG,"['math.AG', 'cs.SC', 'math.NT']",,,[]
"Exact Algorithms for Computing Generalized Eigenspaces of Matrices via
  Annihilating Polynomials",http://arxiv.org/abs/2209.04807v3,2022-09-11T08:00:32Z,2023-02-15T04:41:47Z,"  An effective exact method is proposed for computing generalized eigenspaces
of a matrix of integers or rational numbers. Keys of our approach are the use
of minimal annihilating polynomials and the concept of the Jourdan-Krylov
basis. A new method, called Jordan-Krylov elimination, is introduced to design
an algorithm for computing Jordan-Krylov basis. The resulting algorithm outputs
generalized eigenspaces as a form of Jordan chains. Notably, in the output,
components of generalized eigenvectors are expressed as polynomials in the
associated eigenvalue as a variable.
","['\nShinichi Tajima\n', '\nKatsuyoshi Ohara\n', '\nAkira Terui\n']",,,http://arxiv.org/abs/2209.04807v3,math.RA,"['math.RA', 'cs.SC', 'math.AC', '15A18, 68W30']",,,[]
A Note on the Games-Chan Algorithm,http://arxiv.org/abs/2209.00148v2,2022-08-31T22:50:23Z,2022-09-14T22:25:20Z,"  The Games-Chan algorithm finds the minimal period of a periodic binary
sequence of period $2^n$, in $n$ iterations. We generalise this to periodic
$q$-ary sequences (where $q$ is a prime power) using generating functions and
polynomials and apply this to find the multiplicity of $x-1$ in a $q$-ary
polynomial $f$ in $\log_{\,q}\deg(f)$ iterations.
",['\nGraham H. Norton\n'],"Exposition and main theorem improved, typos corrected. Application to
  finding multiplicity of x-1 in any $q$-ary polynomial added",,http://arxiv.org/abs/2209.00148v2,cs.SC,['cs.SC'],,,[]
"Factoring differential operators over algebraic curves in positive
  characteristic",http://arxiv.org/abs/2208.11365v1,2022-08-24T08:24:27Z,2022-08-24T08:24:27Z,"  We present an algorithm for factoring linear differential operators with
coefficients in a finite separable extension of F p (x). Our methods rely on
specific tools arising in positive characteristic: p-curvature, structure of
simple central algebras and p-Riccati equations.
",['\nRaphaël Pagès\nUB\n'],,"ISSAC, Jul 2022, Lille, France",http://arxiv.org/abs/2208.11365v1,cs.SC,"['cs.SC', 'math.OA']",,,['UB']
A proof of the Brill-Noether method from scratch,http://arxiv.org/abs/2208.12725v1,2022-08-26T15:25:30Z,2022-08-26T15:25:30Z,"  In 1874 Brill and Noether designed a seminal geometric method for computing
bases of Riemann-Roch spaces. From then, their method has led to several
algorithms, some of them being implemented in computer algebra systems. The
usual proofs often rely on abstract concepts of algebraic geometry and
commutative algebra. In this paper we present a short self-contained and
elementary proof that mostly needs Newton polygons, Hensel lifting, bivariate
resultants, and Chinese remaindering.
","['\nElena Berardini\n', '\nAlain Couvreur\n', '\nGrégoire Lecerf\n']",,,http://arxiv.org/abs/2208.12725v1,math.AG,"['math.AG', 'cs.SC']",,,[]
"Learn Basic Skills and Reuse: Modularized Adaptive Neural Architecture
  Search (MANAS)",http://arxiv.org/abs/2208.11083v2,2022-08-23T17:05:46Z,2022-08-24T13:55:09Z,"  Human intelligence is able to first learn some basic skills for solving basic
problems and then assemble such basic skills into complex skills for solving
complex or new problems. For example, the basic skills ""dig hole,"" ""put tree,""
""backfill"" and ""watering"" compose a complex skill ""plant a tree"". Besides, some
basic skills can be reused for solving other problems. For example, the basic
skill ""dig hole"" not only can be used for planting a tree, but also can be used
for mining treasures, building a drain, or landfilling. The ability to learn
basic skills and reuse them for various tasks is very important for humans
because it helps to avoid learning too many skills for solving each individual
task, and makes it possible to solve a compositional number of tasks by
learning just a few number of basic skills, which saves a considerable amount
of memory and computation in the human brain. We believe that machine
intelligence should also capture the ability of learning basic skills and
reusing them by composing into complex skills. In computer science language,
each basic skill is a ""module"", which is a reusable network of a concrete
meaning and performs a specific basic operation. The modules are assembled into
a bigger ""model"" for doing a more complex task. The assembling procedure is
adaptive to the input or task, i.e., for a given task, the modules should be
assembled into the best model for solving the task. As a result, different
inputs or tasks could have different assembled models, which enables
Auto-Assembling AI (AAAI). In this work, we propose Modularized Adaptive Neural
Architecture Search (MANAS) to demonstrate the above idea. Experiments on
different datasets show that the adaptive architecture assembled by MANAS
outperforms static global architectures. Further experiments and empirical
analysis provide insights to the effectiveness of MANAS.
","['\nHanxiong Chen\n', '\nYunqi Li\n', '\nHe Zhu\n', '\nYongfeng Zhang\n']",In ACM CIKM 2022,,http://dx.doi.org/10.1145/3511808.3557385,cs.LG,"['cs.LG', 'cs.AI', 'cs.IR', 'cs.SC']",10.1145/3511808.3557385,,[]
Hiding canonicalisation in tensor computer algebra,http://arxiv.org/abs/2208.11946v1,2022-08-25T09:02:35Z,2022-08-25T09:02:35Z,"  Simplification of expressions in computer algebra systems often involves a
step known as ""canonicalisation"", which reduces equivalent expressions to the
same form. However, such forms may not be natural from the perspective of a
pen-and-paper computation, or may be unwieldy, or both. This is, for example,
the case for expressions involving tensor multi-term symmetries. We propose an
alternative strategy to handle such tensor expressions, which hides canonical
forms from the user entirely, and present an implementation of this idea in the
Cadabra computer algebra system.
","['\nDominic Price\n', '\nKasper Peeters\n', '\nMarija Zamaklar\n']","24 pages, 7 figures, software available at https://cadabra.science",,http://arxiv.org/abs/2208.11946v1,cs.SC,"['cs.SC', 'gr-qc', 'hep-th']",,,[]
"Constraining Gaussian Processes to Systems of Linear Ordinary
  Differential Equations",http://arxiv.org/abs/2208.12515v1,2022-08-26T09:16:53Z,2022-08-26T09:16:53Z,"  Data in many applications follows systems of Ordinary Differential Equations
(ODEs). This paper presents a novel algorithmic and symbolic construction for
covariance functions of Gaussian Processes (GPs) with realizations strictly
following a system of linear homogeneous ODEs with constant coefficients, which
we call LODE-GPs. Introducing this strong inductive bias into a GP improves
modelling of such data. Using smith normal form algorithms, a symbolic
technique, we overcome two current restrictions in the state of the art: (1)
the need for certain uniqueness conditions in the set of solutions, typically
assumed in classical ODE solvers and their probabilistic counterparts, and (2)
the restriction to controllable systems, typically assumed when encoding
differential equations in covariance functions. We show the effectiveness of
LODE-GPs in a number of experiments, for example learning physically
interpretable parameters by maximizing the likelihood.
","['\nAndreas Besginow\n', '\nMarkus Lange-Hegermann\n']",,,http://arxiv.org/abs/2208.12515v1,cs.LG,"['cs.LG', 'cs.SC', 'stat.ML', '60G15, 62G08, 12H05, 68W30, 13J30, 34-04', 'I.2.6; G.1.6; G.3; J.2; I.1.4']",,,[]
Four-Dimensional Lie Algebras Revisited,http://arxiv.org/abs/2208.14631v1,2022-08-31T04:52:24Z,2022-08-31T04:52:24Z,"  The projective variety of Lie algebra structures on a 4-dimensional vector
space has four irreducible components of dimension 11. We compute their prime
ideals in the polynomial ring in 24 variables. By listing their degrees and
Hilbert polynomials, we correct an earlier publication and we answer a 1987
question by Kirillov and Neretin.
","['\nLaurent Manivel\n', '\nBernd Sturmfels\n', '\nSvala Sverrisdóttir\n']",9 pages,,http://arxiv.org/abs/2208.14631v1,math.RA,"['math.RA', 'cs.SC', 'math.AC']",,,[]
"CASPER: Cognitive Architecture for Social Perception and Engagement in
  Robots",http://arxiv.org/abs/2209.01012v1,2022-09-01T10:15:03Z,2022-09-01T10:15:03Z,"  Our world is being increasingly pervaded by intelligent robots with varying
degrees of autonomy. To seamlessly integrate themselves in our society, these
machines should possess the ability to navigate the complexities of our daily
routines even in the absence of a human's direct input. In other words, we want
these robots to understand the intentions of their partners with the purpose of
predicting the best way to help them. In this paper, we present CASPER
(Cognitive Architecture for Social Perception and Engagement in Robots): a
symbolic cognitive architecture that uses qualitative spatial reasoning to
anticipate the pursued goal of another agent and to calculate the best
collaborative behavior. This is performed through an ensemble of parallel
processes that model a low-level action recognition and a high-level goal
understanding, both of which are formally verified. We have tested this
architecture in a simulated kitchen environment and the results we have
collected show that the robot is able to both recognize an ongoing goal and to
properly collaborate towards its achievement. This demonstrates a new use of
Qualitative Spatial Relations applied to the problem of intention reading in
the domain of human-robot interaction.
","['\nSamuele Vinanzi\n', '\nAngelo Cangelosi\n']","16 pages, 13 figures",,http://arxiv.org/abs/2209.01012v1,cs.RO,"['cs.RO', 'cs.AI', 'cs.SC']",,,[]
LogicRank: Logic Induced Reranking for Generative Text-to-Image Systems,http://arxiv.org/abs/2208.13518v1,2022-08-29T11:40:36Z,2022-08-29T11:40:36Z,"  Text-to-image models have recently achieved remarkable success with seemingly
accurate samples in photo-realistic quality. However as state-of-the-art
language models still struggle evaluating precise statements consistently, so
do language model based image generation processes. In this work we showcase
problems of state-of-the-art text-to-image models like DALL-E with generating
accurate samples from statements related to the draw bench benchmark.
Furthermore we show that CLIP is not able to rerank those generated samples
consistently. To this end we propose LogicRank, a neuro-symbolic reasoning
framework that can result in a more accurate ranking-system for such
precision-demanding settings. LogicRank integrates smoothly into the generation
process of text-to-image models and moreover can be used to further fine-tune
towards a more logical precise model.
","['\nBjörn Deiseroth\n', '\nPatrick Schramowski\n', '\nHikaru Shindo\n', '\nDevendra Singh Dhami\n', '\nKristian Kersting\n']",,,http://arxiv.org/abs/2208.13518v1,cs.AI,"['cs.AI', 'cs.CL', 'cs.CV', 'cs.LO', 'cs.SC']",,,[]
Sturm's Theorem with Endpoints,http://arxiv.org/abs/2208.07904v1,2022-08-16T18:47:09Z,2022-08-16T18:47:09Z,"  Sturm's Theorem is a fundamental 19th century result relating the number of
real roots of a polynomial $f$ in an interval to the number of sign
alternations in a sequence of polynomial division-like calculations. We provide
a short direct proof of Sturm's Theorem, including the numerically vexing case
(ignored in many published accounts) where an interval endpoint is a root of
$f$.
","['\nPhilippe Pébay\n', '\nJ. Maurice Rojas\n', '\nDavid C. Thompson\n']","4 pages. A software implementation can be found in algorithm
  vtkPolynomialSolversUnivariate , within the VTK (Visualization Toolkit)
  software package",,http://arxiv.org/abs/2208.07904v1,cs.SC,"['cs.SC', 'math.AC']",,,[]
"A Scalable, Interpretable, Verifiable & Differentiable Logic Gate
  Convolutional Neural Network Architecture From Truth Tables",http://arxiv.org/abs/2208.08609v3,2022-08-18T03:06:25Z,2023-02-02T10:24:13Z,"  We propose $\mathcal{T}$ruth $\mathcal{T}$able net ($\mathcal{TT}$net), a
novel Convolutional Neural Network (CNN) architecture that addresses, by
design, the open challenges of interpretability, formal verification, and logic
gate conversion. $\mathcal{TT}$net is built using CNNs' filters that are
equivalent to tractable truth tables and that we call Learning Truth Table
(LTT) blocks. The dual form of LTT blocks allows the truth tables to be easily
trained with gradient descent and makes these CNNs easy to interpret, verify
and infer. Specifically, $\mathcal{TT}$net is a deep CNN model that can be
automatically represented, after post-training transformation, as a sum of
Boolean decision trees, or as a sum of Disjunctive/Conjunctive Normal Form
(DNF/CNF) formulas, or as a compact Boolean logic circuit. We demonstrate the
effectiveness and scalability of $\mathcal{TT}$net on multiple datasets,
showing comparable interpretability to decision trees, fast complete/sound
formal verification, and scalable logic gate representation, all compared to
state-of-the-art methods. We believe this work represents a step towards making
CNNs more transparent and trustworthy for real-world critical applications.
","['\nAdrien Benamira\n', '\nTristan Guérand\n', '\nThomas Peyrin\n', '\nTrevor Yap\n', '\nBryan Hooi\n']",,,http://arxiv.org/abs/2208.08609v3,cs.AI,"['cs.AI', 'cs.FL', 'cs.LG', 'cs.SC']",,,[]
Basic Elements of Logical Graphs,http://arxiv.org/abs/2208.03194v1,2022-08-05T14:27:33Z,2022-08-05T14:27:33Z,"  We considers how a particular kind of graph corresponds to multiplicative
intuitionistic linear logic formula. The main feature of the graphical notation
is that it absorbs certain symmetries between conjunction and implication. We
look at the basic definitions and present details of an implementation in the
functional programming language Standard ML. This provides a functional
approach to graph traversal and demonstrates how graph isomorphism be
implemented in just a few lines of readable code. This works takes the initial
steps towards a graphical language and toolkit for working with logic formula
and derivations.
",['\nLucas Dixon\n'],"Published in the CAMCAD-09 workshop
  (https://www.irit.fr/~Ralph.Matthes/CAMCAD09/)",,http://arxiv.org/abs/2208.03194v1,cs.LO,"['cs.LO', 'cs.SC']",,,[]
"Convergent expansions and bounds for the incomplete elliptic integral of
  the second kind near the logarithmic singularity",http://arxiv.org/abs/2208.05242v2,2022-08-10T09:51:06Z,2023-05-30T13:26:51Z,"  We find two series expansions for Legendre's second incomplete elliptic
integral $E(\lambda, k)$ in terms of recursively computed elementary functions.
Both expansions converge at every point of the unit square in the $(\lambda,
k)$ plane. Partial sums of the proposed expansions form a sequence of
approximations to $E(\lambda,k)$ which are asymptotic when $\lambda$ and/or $k$
tend to unity, including when both approach the logarithmic singularity
$\lambda=k=1$ from any direction. Explicit two-sided error bounds are given at
each approximation order. These bounds yield a sequence of increasingly precise
asymptotically correct two-sided inequalities for $E(\lambda, k)$. For the
reader's convenience we further present explicit expressions for low-order
approximations and numerical examples to illustrate their accuracy. Our
derivations are based on series rearrangements, hypergeometric summation
algorithms and extensive use of the properties of the generalized
hypergeometric functions including some recent inequalities.
","['\nDmitrii Karp\n', '\nYi Zhang\n']","26 pages, 1 figures, 3 tables with numerical experiments",,http://arxiv.org/abs/2208.05242v2,math.CA,"['math.CA', 'cs.SC', '33E05, 33F10, 33C20, 33C60']",,,[]
"Improvement of algebraic attacks for solving superdetermined MinRank
  instances",http://arxiv.org/abs/2208.01442v1,2022-08-02T13:19:02Z,2022-08-02T13:19:02Z,"  The MinRank (MR) problem is a computational problem that arises in many
cryptographic applications. In Verbel et al. (PQCrypto 2019), the authors
introduced a new way to solve superdetermined instances of the MinRank problem,
starting from the bilinear Kipnis-Shamir (KS) modeling. They use linear algebra
on specific Macaulay matrices, considering only multiples of the initial
equations by one block of variables, the so called ''kernel'' variables. Later,
Bardet et al. (Asiacrypt 2020) introduced a new Support Minors modeling (SM),
that consider the Pl{\""u}cker coordinates associated to the kernel variables,
i.e. the maximal minors of the Kernel matrix in the KS modeling. In this paper,
we give a complete algebraic explanation of the link between the (KS) and (SM)
modelings (for any instance). We then show that superdetermined MinRank
instances can be seen as easy instances of the SM modeling. In particular, we
show that performing computation at the smallest possible degree (the ''first
degree fall'') and the smallest possible number of variables is not always the
best strategy. We give complexity estimates of the attack for generic random
instances.We apply those results to the DAGS cryptosystem, that was submitted
to the first round of the NIST standardization process. We show that the
algebraic attack from Barelli and Couvreur (Asiacrypt 2018), improved in Bardet
et al. (CBC 2019), is a particular superdetermined MinRank instance.Here, the
instances are not generic, but we show that it is possible to analyse the
particular instances from DAGS and provide a way toselect the optimal
parameters (number of shortened positions) to solve a particular instance.
","['\nMagali Bardet\nLITIS\n', '\nManon Bertin\nLITIS\n']",,"PQCrypto 2022, Sep 2022, virtual, France",http://arxiv.org/abs/2208.01442v1,cs.CR,"['cs.CR', 'cs.IT', 'cs.SC', 'math.IT']",,,"['LITIS', 'LITIS']"
Homotopy techniques for analytic combinatorics in several variables,http://arxiv.org/abs/2208.04490v2,2022-08-09T01:34:47Z,2022-09-05T04:27:54Z,"  We combine tools from homotopy continuation solvers with the methods of
analytic combinatorics in several variables to give the first practical
algorithm and implementation for the asymptotics of multivariate rational
generating functions not relying on a non-algorithmically checkable
`combinatorial' non-negativity assumption. Our homotopy implementation
terminates on examples from the literature in three variables, and we
additionally describe heuristic methods that terminate and correctly predict
asymptotic behaviour in reasonable time on examples in even higher dimension.
Our results are implemented in Julia, through the use of the
HomotopyContinuation.jl package, and we provide a selection of examples and
benchmarks.
","['\nKisun Lee\n', '\nStephen Melczer\n', '\nJosip Smolčić\n']",16 pages. Accepted for presentation at SYNASC 2022,,http://arxiv.org/abs/2208.04490v2,math.CO,"['math.CO', 'cs.SC', 'math.AG']",,,[]
"Unitary canonical forms over Clifford algebras, and an observed
  unification of some real-matrix decompositions",http://arxiv.org/abs/2208.04272v2,2022-08-08T17:08:53Z,2023-01-24T18:12:44Z,"  We show that the spectral theorem -- which we understand to be a statement
that every self-adjoint matrix admits a certain type of canonical form under
unitary similarity -- admits analogues over other $*$-algebras distinct from
the complex numbers. If these $*$-algebras contain nilpotents, then it is shown
that there is a consistent way in which many classic matrix decompositions --
such as the Singular Value Decomposition, the Takagi decomposition, the
skew-Takagi decomposition, and the Jordan decomposition, among others -- are
immediate consequences of these. If producing the relevant canonical form of a
self-adjoint matrix were a subroutine in some programming language, then the
corresponding classic matrix decomposition would be a 1-line invocation with no
additional steps. We also suggest that by employing operator overloading in a
programming language, a numerical algorithm for computing a unitary
diagonalisation of a complex self-adjoint matrix would generalise immediately
to solving problems like SVD or Takagi. While algebras without nilpotents (like
the quaternions) allow for similar unifying behaviour, the classic matrix
decompositions which they unify are never obtained as easily. In the process of
doing this, we develop some spectral theory over Clifford algebras of the form
$\cl_{p,q,0}(\mathbb R)$ and $\cl_{p,q,1}(\mathbb R)$ where the former is
admittedly quite easy. We propose a broad conjecture about spectral theorems.
",['\nRan Gutin\n'],,,http://arxiv.org/abs/2208.04272v2,math.RA,"['math.RA', 'cs.NA', 'cs.SC', 'math.NA', 'math.SP']",,,[]
Bit Complexity of Polynomial GCD on Sparse Representation,http://arxiv.org/abs/2207.13874v1,2022-07-28T03:58:08Z,2022-07-28T03:58:08Z,"  An input- and output-sensitive GCD algorithm for multi-variate polynomials
over finite fields is proposed by combining the modular method with the
Ben-Or/Tiwari sparse interpolation. The bit complexity of the algorithm is
given and is sensitive to the sparse representation, while for previous sparse
GCD algorithms, the complexities were given only in some special cases. It is
shown that the new algorithm is superior both in theory and in practice
comparing with existing GCD algorithms: the complexity in the degree is
decreased from quadratic to linear and the running times are decreased by 1-3
orders of magnitude in various benchmarks.
","['\nQiao-Long Huang\n', '\nXiao-Shan Gao\n']",,,http://arxiv.org/abs/2207.13874v1,cs.SC,['cs.SC'],,,[]
Approximate Real Symmetric Tensor Rank,http://arxiv.org/abs/2207.12529v4,2022-07-25T20:56:40Z,2023-08-17T14:21:53Z,"  We investigate the effect of an $\varepsilon$-room of perturbation tolerance
on symmetric tensor decomposition. To be more precise, suppose a real symmetric
$d$-tensor $f$, a norm $||.||$ on the space of symmetric $d$-tensors, and
$\varepsilon >0$ are given. What is the smallest symmetric tensor rank in the
$\varepsilon$-neighborhood of $f$? In other words, what is the symmetric tensor
rank of $f$ after a clever $\varepsilon$-perturbation? We prove two theorems
and develop three corresponding algorithms that give constructive upper bounds
for this question. With expository goals in mind; we present probabilistic and
convex geometric ideas behind our results, reproduce some known results, and
point out open problems.
","['\nAlperen A. Ergür\n', '\nJesus Rebollo Bueno\n', '\nPetros Valettas\n']","Fixed few typos and error in writing of Algorithm 1. To appear in
  Arnold Mathematical Journal",,http://arxiv.org/abs/2207.12529v4,math.NA,"['math.NA', 'cs.LG', 'cs.NA', 'cs.SC', 'math.AC', 'math.OC']",,,[]
"Proceedings The 7th International Workshop on Symbolic-Numeric Methods
  for Reasoning about CPS and IoT",http://arxiv.org/abs/2207.04391v1,2022-07-10T06:22:20Z,2022-07-10T06:22:20Z,"  The proceedings of the 7th International Workshop on Symbolic-Numeric Methods
for Reasoning about CPS and IoT (SNR 2021) feature five peer-reviewed
contributions and three invited talks.
  SNR focuses on the combination of symbolic and numeric methods for reasoning
about Cyber-Physical Systems and the Internet of Things to facilitate model
identification, specification, verification, and control synthesis for these
systems. The synergy between symbolic and numerical approaches is fruitful
thanks to their complementarity.
","['\nAnne Remke\nUniversity of Münster, Germany\n', '\nDung Hoang Tran\nUniversity of Nebraska Lincoln, USA\n']",,"EPTCS 361, 2022",http://dx.doi.org/10.4204/EPTCS.361,cs.SC,"['cs.SC', 'cs.FL']",10.4204/EPTCS.361,,"['University of Münster, Germany', 'University of Nebraska Lincoln, USA']"
Parallel Flowshop in YewPar,http://arxiv.org/abs/2207.06902v2,2022-07-14T13:29:48Z,2022-07-21T09:03:57Z,"  Parallelism may reduce the time to find exact solutions for many Operations
Research (OR) problems, but parallelising combinatorial search is extremely
challenging. YewPar is a new combinatorial search framework designed to allow
domain specialists to benefit from parallelism by reusing sophisticated
parallel search patterns. This paper shows (1) that it is low effort to encode
and parallelise a typical OR problem (Flowshop Scheduling FSP) in YewPar even
for scalable clusters; (2) that the YewPar library makes it extremely easy to
exploit three alternate FSP parallelisations; (3) that the YewPar FSP
implementations are valid, and have sequential performance comparable with a
published algorithm; and (4) provides a systematic performance evaluation of
the three parallel FSP versions on 10 standard FSP instances with up to 240
workers on a Beowulf cluster.
","['\nIgnas Knizikevičius\n', '\nPhil Trinder\n', '\nBlair Archibald\n', '\nJinghua Yan\n']","13 pages, 2 figures",,http://arxiv.org/abs/2207.06902v2,cs.DC,"['cs.DC', 'cs.SC']",,,[]
Multi: a Formal Playground for Multi-Smart Contract Interaction,http://arxiv.org/abs/2207.06681v1,2022-07-14T06:19:39Z,2022-07-14T06:19:39Z,"  Blockchains are maintained by a network of participants that run algorithms
designed to maintain collectively a distributed machine tolerant to Byzantine
attacks. From the point of view of users, blockchains provide the illusion of
centralized computers that perform trustable verifiable computations, where all
computations are deterministic and the results cannot be manipulated or undone.
Smart-contracts are written in a special-purpose programming language with
deterministic semantics. Each transaction begins with an invocation from an
external user to a smart contract. Contracts have local storage and can call
other contracts, and more importantly, they store, send and receive
cryptocurrency. It is very important to guarantee that contracts are correct
before deployment since their code cannot be modified afterward deployment.
However, the resulting ecosystem makes it very difficult to reason about
program correctness, since contracts can be executed by malicious users or
malicious contracts can be designed to exploit other contracts that call them.
Many attacks and bugs are caused by unexpected interactions between multiple
contracts, the attacked contract and unknown code that performs the exploit.
Moreover, there is a very aggressive competition between different blockchains
to expand their user base. Ideas are implemented fast and blockchains compete
to offer and adopt new features quickly. In this paper, we propose a formal
extensible playground that allows reasoning about multi-contract interactions
to ultimately prove properties before features are incorporated into the real
blockchain. We implemented a model of computation that models the execution
platform, abstracts the internal code of each individual contract and focuses
on contract interactions. Moreover, we show how many features, existing or
proposed, can be used to reason about multi-contract interactions.
","['\nMartán Ceresa\n', '\nCésar Sánchez\n']",,,http://arxiv.org/abs/2207.06681v1,cs.LO,"['cs.LO', 'cs.PL', 'cs.SC']",,,[]
Verification of Sigmoidal Artificial Neural Networks using iSAT,http://arxiv.org/abs/2207.06755v1,2022-07-14T09:08:38Z,2022-07-14T09:08:38Z,"  This paper presents an approach for verifying the behaviour of nonlinear
Artificial Neural Networks (ANNs) found in cyber-physical safety-critical
systems. We implement a dedicated interval constraint propagator for the
sigmoid function into the SMT solver iSAT and compare this approach with a
compositional approach encoding the sigmoid function by basic arithmetic
features available in iSAT and an approximating approach. Our experimental
results show that the dedicated and the compositional approach clearly
outperform the approximating approach. Throughout all our benchmarks, the
dedicated approach showed an equal or better performance compared to the
compositional approach.
","['\nDominik Grundt\nGerman Aerospace Center e.V.\n', '\nSorin Liviu Jurj\nGerman Aerospace Center e.V.\n', '\nWillem Hagemann\nGerman Aerospace Center e.V.\n', '\nPaul Kröger\nCarl von Ossietzky University Oldenburg\n', '\nMartin Fränzle\nCarl von Ossietzky University Oldenburg\n']","In Proceedings SNR 2021, arXiv:2207.04391","EPTCS 361, 2022, pp. 45-60",http://dx.doi.org/10.4204/EPTCS.361.6,cs.AI,"['cs.AI', 'cs.LG', 'cs.SC']",10.4204/EPTCS.361.6,,"['German Aerospace Center e.V.', 'German Aerospace Center e.V.', 'German Aerospace Center e.V.', 'Carl von Ossietzky University Oldenburg', 'Carl von Ossietzky University Oldenburg']"
Computer Algebra and Hypergeometric Structures for Feynman Integrals,http://arxiv.org/abs/2207.08524v1,2022-07-18T11:41:50Z,2022-07-18T11:41:50Z,"  We present recent computer algebra methods that support the calculations of
(multivariate) series solutions for (certain coupled systems of partial) linear
differential equations. The summand of the series solutions may be built by
hypergeometric products and more generally by indefinite nested sums defined
over such products. Special cases are hypergeometric structures such as
Appell-functions or generalizations of them that arise frequently when dealing
with parameter Feynman integrals.
","['\nJohannes Bluemlein\n', '\nMarco Saragnese\n', '\nCarsten Schneider\n']",,,http://arxiv.org/abs/2207.08524v1,math-ph,"['math-ph', 'cs.SC', 'hep-ph', 'math.MP']",,,[]
"Model Checking for Rectangular Hybrid Systems: A Quantified Encoding
  Approach",http://arxiv.org/abs/2207.08775v1,2022-07-14T09:09:34Z,2022-07-14T09:09:34Z,"  Satisfiability Modulo Theories (SMT) solvers have been successfully applied
to solve many problems in formal verification such as bounded model checking
(BMC) for many classes of systems from integrated circuits to cyber-physical
systems. Typically, BMC is performed by checking satisfiability of a possibly
long, but quantifier-free formula. However, BMC problems can naturally be
encoded as quantified formulas over the number of BMC steps. In this approach,
we then use decision procedures supporting quantifiers to check satisfiability
of these quantified formulas. This approach has previously been applied to
perform BMC using a Quantified Boolean Formula (QBF) encoding for purely
discrete systems, and then discharges the QBF checks using QBF solvers. In this
paper, we present a new quantified encoding of BMC for rectangular hybrid
automata (RHA), which requires using more general logics due to the real
(dense) time and real-valued state variables modeling continuous states. We
have implemented a preliminary experimental prototype of the method using the
HyST model transformation tool to generate the quantified BMC (QBMC) queries
for the Z3 SMT solver. We describe experimental results on several timed and
hybrid automata benchmarks, such as the Fischer and Lynch-Shavit mutual
exclusion algorithms. We compare our approach to quantifier-free BMC
approaches, such as those in the dReach tool that uses the dReal SMT solver,
and the HyComp tool built on top of nuXmv that uses the MathSAT SMT solver.
Based on our promising experimental results, QBMC may in the future be an
effective and scalable analysis approach for RHA and other classes of hybrid
automata as further improvements are made in quantifier handling in SMT solvers
such as Z3.
","['\nLuan V. Nguyen\nUniversity of Dayton\n', '\nWesam Haddad\nUniversity of Dayton\n', '\nTaylor T. Johnson\nVanderbilt University\n']","In Proceedings SNR 2021, arXiv:2207.04391","EPTCS 361, 2022, pp. 9-23",http://dx.doi.org/10.4204/EPTCS.361.4,cs.LO,"['cs.LO', 'cs.FL', 'cs.SC']",10.4204/EPTCS.361.4,,"['University of Dayton', 'University of Dayton', 'Vanderbilt University']"
Guessing With Quadratic Differential Equations,http://arxiv.org/abs/2207.01037v1,2022-07-03T13:26:50Z,2022-07-03T13:26:50Z,"  By holonomic guessing, we denote the process of finding a linear differential
equation with polynomial coefficients satisfied by the generating function of a
sequence, for which only a few first terms are known. Holonomic guessing has
been used in computer algebra for over three decades to demonstrate the value
of the guess-and-prove paradigm in intuition processes preceding proofs, as
propagated in The Art of Solving (Polya, 1978). Among the prominent packages
used to perform guessing, one can cite the Maple Gfun package of Salvy and
Zimmermann; the Mathematica GeneratingFunctions package of Mallinger; and the
Sage ore_algebra package of Kauers, Jaroschek, and Johansson.
  We propose an approach that extends holonomic guessing by allowing the
targeted differential equations to be of degree at most two. Consequently, it
enables us to capture more generating functions than just holonomic functions.
The corresponding recurrence equations are similar to known equations for the
Bernoulli, Euler, and Bell numbers. As a result, our software finds the correct
recurrence and differential equations for the generating functions of the
up/down numbers (https://oeis.org/A000111), the evaluations of the zeta
function at positive even integers, the Taylor coefficients of the Lambert W
function, and many more. Our Maple implementation ($delta2guess$) is part of
the FPS package which can be downloaded at
http://www.mathematik.uni-kassel.de/~bteguia/FPS_webpage/FPS.htm
",['\nBertrand Teguia Tabuguia\n'],"5 pages, 14 references, ISSAC'2022 software presentation",,http://arxiv.org/abs/2207.01037v1,cs.SC,['cs.SC'],,,[]
"Tuple Interpretations and Applications to Higher-Order Runtime
  Complexity",http://arxiv.org/abs/2206.15202v1,2022-06-30T11:36:53Z,2022-06-30T11:36:53Z,"  Tuple interpretations are a class of algebraic interpretation that subsumes
both polynomial and matrix interpretations as it does not impose simple
termination and allows non-linear interpretations. It was developed in the
context of higher-order rewriting to study derivational complexity of algebraic
functional systems. In this short paper, we continue our journey to study the
complexity of higher-order TRSs by tailoring tuple interpretations to deal with
innermost runtime complexity.
","['\nCynthia Kop\n', '\nDeivid Vale\n']",,,http://arxiv.org/abs/2206.15202v1,cs.LO,"['cs.LO', 'cs.SC', 'F.4.1']",,,[]
"Asymptotics of multivariate sequences IV: generating functions with
  poles on a hyperplane arrangement",http://arxiv.org/abs/2207.00717v2,2022-07-02T02:39:21Z,2022-07-23T21:12:53Z,"  Let F be the quotient of an analytic function with a product of linear
functions. Working in the framework of analytic combinatorics in several
variables, we compute asymptotic formulae for the Taylor coefficients of F
using multivariate residues and saddle-point approximations. Because the
singular set of F is the union of hyperplanes, we are able to make explicit the
topological decompositions which arise in the multivariate singularity
analysis. In addition to effective and explicit asymptotic results, we provide
the first results on transitions between different asymptotic regimes, and
provide the first software package to verify and compute asymptotics in
non-smooth cases of analytic combinatorics in several variables. It is also our
hope that this paper will serve as an entry to the more advanced corners of
analytic combinatorics in several variables for combinatorialists.
","['\nYuliy Baryshnikov\n', '\nStephen Melczer\n', '\nRobin Pemantle\n']",,,http://arxiv.org/abs/2207.00717v2,math.CO,"['math.CO', 'cs.SC']",,,[]
"FPS In Action: An Easy Way To Find Explicit Formulas For Interlaced
  Hypergeometric Sequences",http://arxiv.org/abs/2207.01031v1,2022-07-03T13:04:52Z,2022-07-03T13:04:52Z,"  Linear recurrence equations with constant coefficients define the power
series coefficients of rational functions. However, one usually prefers to have
an explicit formula for the sequence of coefficients, provided that such a
formula is ""simple"" enough. Simplicity is related to the compactness of the
formula due to the presence of algebraic numbers: ""the smaller, the simpler"".
This poster showcases the capacity of recent updates on the Formal Power Series
(FPS) algorithm, implemented in Maxima and Maple (convert/FormalPowerSeries),
to find simple formulas for sequences like those from https://oeis.org/A307717,
https://oeis.org/A226782, or https://oeis.org/A226784 by computing power series
representations of their correctly guessed generating functions. We designed
the algorithm for the more general context of univariate $P$-recursive
sequences. Our implementations are available at
http://www.mathematik.uni-kassel.de/~bteguia/FPS_webpage/FPS.htm
","['\nBertrand Teguia Tabuguia\n', '\nWolfram Koepf\n']","5 pages, 15 references, ISSAC'22 poster presentation",,http://arxiv.org/abs/2207.01031v1,cs.SC,"['cs.SC', 'math.CO']",,,[]
"Deep Learning and Symbolic Regression for Discovering Parametric
  Equations",http://arxiv.org/abs/2207.00529v2,2022-07-01T16:25:59Z,2023-05-28T15:10:44Z,"  Symbolic regression is a machine learning technique that can learn the
governing formulas of data and thus has the potential to transform scientific
discovery. However, symbolic regression is still limited in the complexity and
dimensionality of the systems that it can analyze. Deep learning on the other
hand has transformed machine learning in its ability to analyze extremely
complex and high-dimensional datasets. We propose a neural network architecture
to extend symbolic regression to parametric systems where some coefficient may
vary but the structure of the underlying governing equation remains constant.
We demonstrate our method on various analytic expressions, ODEs, and PDEs with
varying coefficients and show that it extrapolates well outside of the training
domain. The neural network-based architecture can also integrate with other
deep learning architectures so that it can analyze high-dimensional data while
being trained end-to-end. To this end we integrate our architecture with
convolutional neural networks to analyze 1D images of varying spring systems.
","['\nMichael Zhang\n', '\nSamuel Kim\n', '\nPeter Y. Lu\n', '\nMarin Soljačić\n']","Michael Zhang and Samuel Kim contributed equally to this work. 13
  pages, 7 figures",,http://arxiv.org/abs/2207.00529v2,cs.LG,"['cs.LG', 'cs.SC', 'physics.comp-ph', 'physics.data-an']",,,[]
Combinatory Adjoints and Differentiation,http://arxiv.org/abs/2207.00847v1,2022-07-02T14:34:54Z,2022-07-02T14:34:54Z,"  We develop a compositional approach for automatic and symbolic
differentiation based on categorical constructions in functional analysis where
derivatives are linear functions on abstract vectors rather than being limited
to scalars, vectors, matrices or tensors represented as multi-dimensional
arrays. We show that both symbolic and automatic differentiation can be
performed using a differential calculus for generating linear functions
representing Fr\'echet derivatives based on rules for primitive, constant,
linear and bilinear functions as well as their sequential and parallel
composition. Linear functions are represented in a combinatory domain-specific
language. Finally, we provide a calculus for symbolically computing the adjoint
of a derivative without using matrices, which are too inefficient to use on
high-dimensional spaces. The resulting symbolic representation of a derivative
retains the data-parallel operations from the input program. The combination of
combinatory differentiation and computing formal adjoints turns out to be
behaviorally equivalent to reverse-mode automatic differentiation. In
particular, it provides opportunities for optimizations where matrices are too
inefficient to represent linear functions.
","['\nMartin Elsman\nUniversity of Copenhagen\n', '\nFritz Henglein\nUniversity of Copenhagen\n', '\nRobin Kaarsgaard\nUniversity of Edinburgh\n', '\nMikkel Kragh Mathiesen\nUniversity of Copenhagen\n', '\nRobert Schenck\nUniversity of Copenhagen\n']","In Proceedings MSFP 2022, arXiv:2206.09534","EPTCS 360, 2022, pp. 1-26",http://dx.doi.org/10.4204/EPTCS.360.1,cs.PL,"['cs.PL', 'cs.DC', 'cs.LG', 'cs.SC']",10.4204/EPTCS.360.1,,"['University of Copenhagen', 'University of Copenhagen', 'University of Edinburgh', 'University of Copenhagen', 'University of Copenhagen']"
The Programming of Algebra,http://arxiv.org/abs/2207.00850v1,2022-07-02T14:35:52Z,2022-07-02T14:35:52Z,"  We present module theory and linear maps as a powerful generalised and
computationally efficient framework for the relational data model, which
underpins today's relational database systems. Based on universal constructions
of modules we obtain compact and computationally efficient data structures for
data collections corresponding to union and deletion, repeated union, Cartesian
product and key-indexed data. Free modules naturally give rise to polysets,
which generalise multisets and facilitate expressing database queries as
multilinear maps with asymptotically efficient evaluation on polyset
constructors. We introduce compact maps as a way of representing infinite
(poly)sets constructible from an infinite base set and its elements by addition
and subtraction. We show how natural joins generalise to algebraic joins, while
intersection is implemented by a novel algorithm on nested compact maps that
carefully avoids visiting parts of the input that do not contribute to the
eventual output. Our algebraic framework leads to a worst-case optimal
evaluation of cyclic relational queries, which is known to be impossible using
textbook query optimisers that operate on lists of records only.
","['\nFritz Henglein\nUniversity of Copenhagen\n', '\nRobin Kaarsgaard\nUniversity of Edinburgh\n', '\nMikkel Kragh Mathiesen\nUniversity of Copenhagen\n']","In Proceedings MSFP 2022, arXiv:2206.09534","EPTCS 360, 2022, pp. 71-92",http://dx.doi.org/10.4204/EPTCS.360.4,cs.PL,"['cs.PL', 'cs.DB', 'cs.SC']",10.4204/EPTCS.360.4,,"['University of Copenhagen', 'University of Edinburgh', 'University of Copenhagen']"
"New heuristic to choose a cylindrical algebraic decomposition variable
  ordering motivated by complexity analysis",http://arxiv.org/abs/2206.13480v1,2022-06-27T17:45:14Z,2022-06-27T17:45:14Z,"  It is well known that the variable ordering can be critical to the efficiency
or even tractability of the cylindrical algebraic decomposition (CAD)
algorithm. We propose new heuristics inspired by complexity analysis of CAD to
choose the variable ordering. These heuristics are evaluated against existing
heuristics with experiments on the SMT-LIB benchmarks using both existing
performance metrics and a new metric we propose for the problem at hand. The
best of these new heuristics chooses orderings that lead to timings on average
17% slower than the virtual-best: an improvement compared to the prior
state-of-the-art which achieved timings 25% slower.
","['\nTereso del Río\n', '\nMatthew England\n']",,"In: F. Boulier, M. England, T.M. Sadykov, and E.V. Vorozhtsov,
  eds. Computer Algebra in Scientific Computing (Proc. CASC '22), pp. 300-317.
  (Lecture Notes in Computer Science, 13366). Springer International, 2022",http://dx.doi.org/10.1007/978-3-031-14788-3_17,cs.SC,['cs.SC'],10.1007/978-3-031-14788-3_17,,[]
"Rethinking Symbolic Regression Datasets and Benchmarks for Scientific
  Discovery",http://arxiv.org/abs/2206.10540v5,2022-06-21T17:15:45Z,2024-03-05T07:36:09Z,"  This paper revisits datasets and evaluation criteria for Symbolic Regression
(SR), specifically focused on its potential for scientific discovery. Focused
on a set of formulas used in the existing datasets based on Feynman Lectures on
Physics, we recreate 120 datasets to discuss the performance of symbolic
regression for scientific discovery (SRSD). For each of the 120 SRSD datasets,
we carefully review the properties of the formula and its variables to design
reasonably realistic sampling ranges of values so that our new SRSD datasets
can be used for evaluating the potential of SRSD such as whether or not an SR
method can (re)discover physical laws from such datasets. We also create
another 120 datasets that contain dummy variables to examine whether SR methods
can choose necessary variables only. Besides, we propose to use normalized edit
distances (NED) between a predicted equation and the true equation trees for
addressing a critical issue that existing SR metrics are either binary or
errors between the target values and an SR model's predicted values for a given
input. We conduct benchmark experiments on our new SRSD datasets using various
representative SR methods. The experimental results show that we provide a more
realistic performance evaluation, and our user study shows that the NED
correlates with human judges significantly more than an existing SR metric. We
publish repositories of our code and 240 SRSD datasets.
","['\nYoshitomo Matsubara\n', '\nNaoya Chiba\n', '\nRyo Igarashi\n', '\nYoshitaka Ushiku\n']","Accepted at DMLR. Code and datasets are available at
  https://github.com/omron-sinicx/srsd-benchmark
  https://huggingface.co/datasets/yoshitomo-matsubara/srsd-feynman_easy
  https://huggingface.co/datasets/yoshitomo-matsubara/srsd-feynman_medium
  https://huggingface.co/datasets/yoshitomo-matsubara/srsd-feynman_hard and
  another three sets of SRSD datasets with dummy variables",,http://arxiv.org/abs/2206.10540v5,cs.LG,"['cs.LG', 'cs.AI', 'cs.NE', 'cs.SC']",,,[]
Synthesizing Mathematical Identities with E-Graphs,http://arxiv.org/abs/2206.07086v1,2022-06-14T18:21:01Z,2022-06-14T18:21:01Z,"  Identities compactly describe properties of a mathematical expression and can
be leveraged into faster and more accurate function implementations. However,
identities must currently be discovered manually, which requires a lot of
expertise. We propose a two-phase synthesis and deduplication pipeline that
discovers these identities automatically. In the synthesis step, a set of
rewrite rules is composed, using an e-graph, to discover candidate identities.
However, most of these candidates are duplicates, which a secondary
deduplication step discards using integer linear programming and another
e-graph. Applied to a set of 61 benchmarks, the synthesis phase generates 7215
candidate identities which the deduplication phase then reduces down to 125
core identities.
","['\nIan Briggs\n', '\nPavel Panchekha\n']",,,http://arxiv.org/abs/2206.07086v1,cs.SC,['cs.SC'],,,[]
GAMR: A Guided Attention Model for (visual) Reasoning,http://arxiv.org/abs/2206.04928v5,2022-06-10T07:52:06Z,2023-03-21T15:35:50Z,"  Humans continue to outperform modern AI systems in their ability to flexibly
parse and understand complex visual scenes. Here, we present a novel module for
visual reasoning, the Guided Attention Model for (visual) Reasoning (GAMR),
which instantiates an active vision theory -- positing that the brain solves
complex visual reasoning problems dynamically -- via sequences of attention
shifts to select and route task-relevant visual information into memory.
Experiments on an array of visual reasoning tasks and datasets demonstrate
GAMR's ability to learn visual routines in a robust and sample-efficient
manner. In addition, GAMR is shown to be capable of zero-shot generalization on
completely novel reasoning tasks. Overall, our work provides computational
support for cognitive theories that postulate the need for a critical interplay
between attention and memory to dynamically maintain and manipulate
task-relevant visual information to solve complex visual reasoning tasks.
","['\nMohit Vaishnav\n', '\nThomas Serre\n']",,"Eleventh International Conference on Learning Representations
  (ICLR) 2023",http://arxiv.org/abs/2206.04928v5,cs.AI,"['cs.AI', 'cs.LG', 'cs.NE', 'cs.SC']",,,[]
A computational framework for weighted simplicial homology,http://arxiv.org/abs/2206.04612v1,2022-06-09T16:59:43Z,2022-06-09T16:59:43Z,"  We provide a bottom up construction of torsion generators for weighted
homology of a weighted complex over a discrete valuation ring
$R=\mathbb{F}[[\pi]]$. This is achieved by starting from a basis for classical
homology of the $n$-th skeleton for the underlying complex with coefficients in
the residue field $\mathbb{F}$ and then lifting it to a basis for the weighted
homology with coefficients in the ring $R$. Using the latter, a bijection is
established between $n+1$ and $n$ dimensional simplices whose weight ratios
provide the exponents of the $\pi$-monomials that generate each torsion summand
in the structure theorem of the weighted homology modules over $R$. We present
algorithms that subsume the torsion computation by reducing it to normalization
over the residue field of $R$, and describe a Python package we implemented
that takes advantage of this reduction and performs the computation
efficiently.
","['\nAndrei C. Bura\n', '\nNeelav S. Dutta\n', '\nThomas J. X. Li\n', '\nChristian M. Reidys\n']","16 pages, 2 figures",,http://arxiv.org/abs/2206.04612v1,math.AT,"['math.AT', 'cs.SC', 'math.CO', 'math.GN', 'math.KT', '05E45, 55U10, 55N35, 13P20']",,,[]
Zero-Hopf Bifurcation of Limit Cycles in Certain Differential Systems,http://arxiv.org/abs/2205.14450v3,2022-05-28T15:01:08Z,2023-04-29T16:54:07Z,"  This paper studies the number of limit cycles that may bifurcate from an
equilibrium of an autonomous system of differential equations. The system in
question is assumed to be of dimension $n$, have a zero-Hopf equilibrium at the
origin, and consist only of homogeneous terms of order $m$. Denote by
$H_k(n,m)$ the maximum number of limit cycles of the system that can be
detected by using the averaging method of order $k$. We prove that
$H_1(n,m)\leq(m-1)\cdot m^{n-2}$ and $H_k(n,m)\leq(km)^{n-1}$ for generic
$n\geq3$, $m\geq2$ and $k>1$. The exact numbers of $H_k(n,m)$ or tight bounds
on the numbers are determined by computing the mixed volumes of some polynomial
systems obtained from the averaged functions. Based on symbolic and algebraic
computation, a general and algorithmic approach is proposed to derive
sufficient conditions for a given differential system to have a prescribed
number of limit cycles. The effectiveness of the proposed approach is
illustrated by a family of third-order differential equations and by a
four-dimensional hyperchaotic differential system.
","['\nBo Huang\n', '\nDongming Wang\n']",,,http://arxiv.org/abs/2205.14450v3,math.DS,"['math.DS', 'cs.SC']",,,[]
Elementary remarks about Pisano periods,http://arxiv.org/abs/2206.07095v2,2022-06-01T08:09:23Z,2022-06-20T07:59:44Z,"  In this short note, we reprove in a very elementary way some known facts
about Pisano periods as well as some considerations about the link between
Pisano periods and the order of roots of the characteristic equation. The
technics only requires a small background in ring theory (merely the definition
of a commutative ring). The tools set here can be reused for all linear
recurrences with quadratic non-constant characteristic equation.
","['\nGérard Henry Edmond Duchamp\nLIPN\n', '\nPierre Simonnet\nUDC\n']",,,http://arxiv.org/abs/2206.07095v2,cs.SC,"['cs.SC', 'math.CO']",,,"['LIPN', 'UDC']"
"Flat singularities of chained systems, illustrated with an aircraft
  model",http://arxiv.org/abs/2205.14608v5,2022-05-29T09:04:04Z,2023-12-06T20:39:59Z,"  We consider flat differential control systems for which there exist flat
outputs that are part of the state variables and study them using Jacobi bound.
We introduce a notion of saddle Jacobi bound for an ordinary differential
system for $n$ equations in $n+m$ variables. Systems with saddle Jacobi number
generalize various notions of chained and diagonal systems and form the widest
class of systems admitting subsets of state variables as flat output, for which
flat parametrization may be computed without differentiating the initial
equations. We investigate apparent and intrinsic flat singularities of such
systems. As an illustration, we consider the case of a simplified aircraft
model, providing new flat outputs and showing that it is flat at all points
except possibly in stalling conditions. Finally, we present numerical
simulations showing that a feedback using those flat outputs is robust to
perturbations and can also compensate model errors, when using a more realistic
aerodynamic model.
","['\nYirmeyahu J. Kaminski\n', '\nFrançois Ollivier\n']","41 pages, 8 figures",,http://arxiv.org/abs/2205.14608v5,math.OC,"['math.OC', 'cs.SC', 'cs.SY', 'eess.SY', '93-08 (primary), 68W30 (secondary)', 'I.6.3; I.1.0']",,,[]
Gluing Neural Networks Symbolically Through Hyperdimensional Computing,http://arxiv.org/abs/2205.15534v1,2022-05-31T04:44:02Z,2022-05-31T04:44:02Z,"  Hyperdimensional Computing affords simple, yet powerful operations to create
long Hyperdimensional Vectors (hypervectors) that can efficiently encode
information, be used for learning, and are dynamic enough to be modified on the
fly. In this paper, we explore the notion of using binary hypervectors to
directly encode the final, classifying output signals of neural networks in
order to fuse differing networks together at the symbolic level. This allows
multiple neural networks to work together to solve a problem, with little
additional overhead. Output signals just before classification are encoded as
hypervectors and bundled together through consensus summation to train a
classification hypervector. This process can be performed iteratively and even
on single neural networks by instead making a consensus of multiple
classification hypervectors. We find that this outperforms the state of the
art, or is on a par with it, while using very little overhead, as hypervector
operations are extremely fast and efficient in comparison to the neural
networks. This consensus process can learn online and even grow or lose models
in real time. Hypervectors act as memories that can be stored, and even further
bundled together over time, affording life long learning capabilities.
Additionally, this consensus structure inherits the benefits of
Hyperdimensional Computing, without sacrificing the performance of modern
Machine Learning. This technique can be extrapolated to virtually any neural
model, and requires little modification to employ - one simply requires
recording the output signals of networks when presented with a testing example.
","['\nPeter Sutor\n', '\nDehao Yuan\n', '\nDouglas Summers-Stay\n', '\nCornelia Fermuller\n', '\nYiannis Aloimonos\n']","10 pages, 3 figures, 6 tables, accepted to IJCNN 2022 / IEEE WCCI
  2022",,http://arxiv.org/abs/2205.15534v1,cs.SC,"['cs.SC', 'cs.AI', 'cs.CV', 'cs.LG']",,,[]
"Bayesian Learning to Discover Mathematical Operations in Governing
  Equations of Dynamic Systems",http://arxiv.org/abs/2206.00669v1,2022-06-01T10:31:14Z,2022-06-01T10:31:14Z,"  Discovering governing equations from data is critical for diverse scientific
disciplines as they can provide insights into the underlying phenomenon of
dynamic systems. This work presents a new representation for governing
equations by designing the Mathematical Operation Network (MathONet) with a
deep neural network-like hierarchical structure. Specifically, the MathONet is
stacked by several layers of unary operations (e.g., sin, cos, log) and binary
operations (e.g., +,-), respectively. An initialized MathONet is typically
regarded as a super-graph with a redundant structure, a sub-graph of which can
yield the governing equation. We develop a sparse group Bayesian learning
algorithm to extract the sub-graph by employing structurally constructed priors
over the redundant mathematical operations. By demonstrating the chaotic Lorenz
system, Lotka-Volterra system, and Kolmogorov-Petrovsky-Piskunov system, the
proposed method can discover the ordinary differential equations (ODEs) and
partial differential equations (PDEs) from the observations given limited
mathematical operations, without any prior knowledge on possible expressions of
the ODEs and PDEs.
","['\nHongpeng Zhou\n', '\nWei Pan\n']",,,http://arxiv.org/abs/2206.00669v1,cs.LG,"['cs.LG', 'cs.AI', 'cs.SC']",,,[]
"OntoMerger: An Ontology Integration Library for Deduplicating and
  Connecting Knowledge Graph Nodes",http://arxiv.org/abs/2206.02238v1,2022-06-05T18:52:26Z,2022-06-05T18:52:26Z,"  Duplication of nodes is a common problem encountered when building knowledge
graphs (KGs) from heterogeneous datasets, where it is crucial to be able to
merge nodes having the same meaning. OntoMerger is a Python ontology
integration library whose functionality is to deduplicate KG nodes. Our
approach takes a set of KG nodes, mappings and disconnected hierarchies and
generates a set of merged nodes together with a connected hierarchy. In
addition, the library provides analytic and data testing functionalities that
can be used to fine-tune the inputs, further reducing duplication, and to
increase connectivity of the output graph. OntoMerger can be applied to a wide
variety of ontologies and KGs. In this paper we introduce OntoMerger and
illustrate its functionality on a real-world biomedical KG.
","['\nDavid Geleta\n', '\nAndriy Nikolov\n', '\nMark ODonoghue\n', '\nBenedek Rozemberczki\n', '\nAnna Gogleva\n', '\nValentina Tamma\n', '\nTerry R. Payne\n']",Code available under: https://github.com/AstraZeneca/onto_merger,,http://arxiv.org/abs/2206.02238v1,cs.AI,"['cs.AI', 'cs.LG', 'cs.SC']",,,[]
Drawing out of Distribution with Neuro-Symbolic Generative Models,http://arxiv.org/abs/2206.01829v2,2022-06-03T21:40:22Z,2022-06-27T11:21:52Z,"  Learning general-purpose representations from perceptual inputs is a hallmark
of human intelligence. For example, people can write out numbers or characters,
or even draw doodles, by characterizing these tasks as different instantiations
of the same generic underlying process -- compositional arrangements of
different forms of pen strokes. Crucially, learning to do one task, say
writing, implies reasonable competence at another, say drawing, on account of
this shared process. We present Drawing out of Distribution (DooD), a
neuro-symbolic generative model of stroke-based drawing that can learn such
general-purpose representations. In contrast to prior work, DooD operates
directly on images, requires no supervision or expensive test-time inference,
and performs unsupervised amortised inference with a symbolic stroke model that
better enables both interpretability and generalization. We evaluate DooD on
its ability to generalise across both data and tasks. We first perform
zero-shot transfer from one dataset (e.g. MNIST) to another (e.g. Quickdraw),
across five different datasets, and show that DooD clearly outperforms
different baselines. An analysis of the learnt representations further
highlights the benefits of adopting a symbolic stroke model. We then adopt a
subset of the Omniglot challenge tasks, and evaluate its ability to generate
new exemplars (both unconditionally and conditionally), and perform one-shot
classification, showing that DooD matches the state of the art. Taken together,
we demonstrate that DooD does indeed capture general-purpose representations
across both data and task, and takes a further step towards building general
and robust concept-learning systems.
","['\nYichao Liang\n', '\nJoshua B. Tenenbaum\n', '\nTuan Anh Le\n', '\nN. Siddharth\n']",Preprint. Under review. 25 pages,,http://arxiv.org/abs/2206.01829v2,cs.LG,"['cs.LG', 'cs.AI', 'cs.CV', 'cs.NE', 'cs.SC']",,,[]
Symbolic-Numeric Factorization of Differential Operators,http://arxiv.org/abs/2205.08991v3,2022-05-18T15:23:05Z,2022-06-03T09:28:30Z,"  We present a symbolic-numeric Las Vegas algorithm for factoring Fuchsian
ordinary differential operators with rational function coefficients. The new
algorithm combines ideas of van Hoeij's ""local-to-global"" method and of the
''analytic'' approach proposed by van der Hoeven. It essentially reduces to the
former in ''easy'' cases where the local-to-global method succeeds, and to an
optimized variant of the latter in the ""hardest"" cases, while handling
intermediate cases more efficiently than both.
","['\nFrédéric Chyzak\nMATHEXP\n', '\nAlexandre Goyer\nMATHEXP\n', '\nMarc Mezzarobba\nLIX\n']",,,http://arxiv.org/abs/2205.08991v3,cs.SC,['cs.SC'],,,"['MATHEXP', 'MATHEXP', 'LIX']"
Solving sparse polynomial systems using Groebner bases and resultants,http://arxiv.org/abs/2205.09888v1,2022-05-19T22:38:23Z,2022-05-19T22:38:23Z,"  Solving systems of polynomial equations is a central problem in nonlinear and
computational algebra. Since Buchberger's algorithm for computing Gr\""obner
bases in the 60s, there has been a lot of progress in this domain. Moreover,
these equations have been employed to model and solve problems from diverse
disciplines such as biology, cryptography, and robotics. Currently, we have a
good understanding of how to solve generic systems from a theoretical and
algorithmic point of view. However, polynomial equations encountered in
practice are usually structured, and so many properties and results about
generic systems do not apply to them. For this reason, a common trend in the
last decades has been to develop mathematical and algorithmic frameworks to
exploit specific structures of systems of polynomials.
  Arguably, the most common structure is sparsity; that is, the polynomials of
the systems only involve a few monomials. Since Bernstein, Khovanskii, and
Kushnirenko's work on the expected number of solutions of sparse systems, toric
geometry has been the default mathematical framework to employ sparsity. In
particular, it is the crux of the matter behind the extension of classical
tools to systems, such as resultant computations, homotopy continuation
methods, and most recently, Gr\""obner bases. In this work, we will review these
classical tools, their extensions, and recent progress in exploiting sparsity
for solving polynomial systems.
  This manuscript complements its homonymous tutorial presented at the
conference ISSAC 2022.
",['\nMatías R. Bender\n'],,,http://dx.doi.org/10.1145/3476446.3535498,cs.SC,"['cs.SC', 'math.AG']",10.1145/3476446.3535498,,[]
Unsupervised Tokenization Learning,http://arxiv.org/abs/2205.11443v4,2022-05-23T16:33:41Z,2022-12-15T17:26:00Z,"  In the presented study, we discover that the so-called ""transition freedom""
metric appears superior for unsupervised tokenization purposes in comparison to
statistical metrics such as mutual information and conditional probability,
providing F-measure scores in range from 0.71 to 1.0 across explored
multilingual corpora. We find that different languages require different
offshoots of that metric (such as derivative, variance, and ""peak values"") for
successful tokenization. Larger training corpora do not necessarily result in
better tokenization quality, while compressing the models by eliminating
statistically weak evidence tends to improve performance. The proposed
unsupervised tokenization technique provides quality better than or comparable
to lexicon-based ones, depending on the language.
","['\nAnton Kolonin\n', '\nVignav Ramesh\n']","16 pages, 9 figures; Paper accepted to the EMNLP 2022 conference",,http://arxiv.org/abs/2205.11443v4,cs.CL,"['cs.CL', 'cs.AI', 'cs.SC']",,,[]
VWSIM: A Circuit Simulator,http://arxiv.org/abs/2205.11698v1,2022-05-24T01:16:21Z,2022-05-24T01:16:21Z,"  VWSIM is a circuit simulator for rapid, single-flux, quantum (RSFQ) circuits.
The simulator is designed to model and simulate primitive-circuit devices such
as capacitors, inductors, Josephson Junctions, and can be extended to simulate
other circuit families, such as CMOS. Circuit models can be provided in the
native VWSIM netlist format or as SPICE-compatible netlists, which are
flattened and transformed into symbolic equations that can be manipulated and
simulated. Written in the ACL2 logic, VWSIM provides logical guarantees about
each of the circuit models it simulates. Note, our matrix solving and
evaluation routines use Common Lisp floating-point numbers, and work is ongoing
to admit these models into ACL2. We currently use VWSIM to help us design
self-timed, RSFQ-based circuits. Our eventual goal is to prove properties of
RSFQ circuit models. The ACL2-based definition of the VWSIM simulator offers a
path for specifying and verifying RSFQ circuit models.
","['\nWarren A. Hunt Jr.\nThe University of Texas, ForrestHunt, Inc.\n', '\nVivek Ramanathan\nThe University of Texas, ForrestHunt, Inc.\n', '\nJ Strother Moore\nThe University of Texas, ForrestHunt, Inc.\n']","In Proceedings ACL2 2022, arXiv:2205.11103","EPTCS 359, 2022, pp. 61-75",http://dx.doi.org/10.4204/EPTCS.359.7,cs.LO,"['cs.LO', 'cs.MS', 'cs.SC', 'B.1.2; B.7.2; D.1.1; D.2.4; F.3.1; F.4.1; G.1.3; I.1.3; I.2.3;\n  I.6.4; J.2']",10.4204/EPTCS.359.7,,"['The University of Texas, ForrestHunt, Inc.', 'The University of Texas, ForrestHunt, Inc.', 'The University of Texas, ForrestHunt, Inc.']"
"FabKG: A Knowledge graph of Manufacturing Science domain utilizing
  structured and unconventional unstructured knowledge source",http://arxiv.org/abs/2206.10318v1,2022-05-24T02:32:04Z,2022-05-24T02:32:04Z,"  As the demands for large-scale information processing have grown, knowledge
graph-based approaches have gained prominence for representing general and
domain knowledge. The development of such general representations is essential,
particularly in domains such as manufacturing which intelligent processes and
adaptive education can enhance. Despite the continuous accumulation of text in
these domains, the lack of structured data has created information extraction
and knowledge transfer barriers. In this paper, we report on work towards
developing robust knowledge graphs based upon entity and relation data for both
commercial and educational uses. To create the FabKG (Manufacturing knowledge
graph), we have utilized textbook index words, research paper keywords, FabNER
(manufacturing NER), to extract a sub knowledge base contained within Wikidata.
Moreover, we propose a novel crowdsourcing method for KG creation by leveraging
student notes, which contain invaluable information but are not captured as
meaningful information, excluding their use in personal preparation for
learning and written exams. We have created a knowledge graph containing 65000+
triples using all data sources. We have also shown the use case of
domain-specific question answering and expression/formula-based question
answering for educational purposes.
","['\nAman Kumar\n', '\nAkshay G Bharadwaj\n', '\nBinil Starly\n', '\nCollin Lynch\n']","NAACL 2022 Workshop on Structured and Unstructured Knowledge
  Integration (SUKI)",,http://arxiv.org/abs/2206.10318v1,cs.CL,"['cs.CL', 'cs.AI', 'cs.SC']",,,[]
Order-Degree-Height Surfaces for Linear Operators,http://arxiv.org/abs/2205.06030v1,2022-05-12T11:28:07Z,2022-05-12T11:28:07Z,"  It is known for linear operators with polynomial coefficients annihilating a
given D-finite function that there is a trade-off between order and degree.
Raising the order may give room for lowering the degree. The relationship
between order and degree is typically described by a hyperbola known as the
order-degree curve. In this paper, we add the height into the picture, i.e., a
measure for the size of the coefficients in the polynomial coefficients. For
certain situations, we derive relationships between order, degree, and height
that can be viewed as order-degree-height surfaces.
","['\nHui Huang\n', '\nManuel Kauers\n', '\nGargi Mukherjee\n']",,,http://arxiv.org/abs/2205.06030v1,cs.SC,['cs.SC'],,,[]
"Moment-based Invariants for Probabilistic Loops with Non-polynomial
  Assignments",http://arxiv.org/abs/2205.02577v3,2022-05-05T11:19:37Z,2022-07-01T11:20:52Z,"  We present a method to automatically approximate moment-based invariants of
probabilistic programs with non-polynomial updates of continuous state
variables to accommodate more complex dynamics. Our approach leverages
polynomial chaos expansion to approximate non-linear functional updates as sums
of orthogonal polynomials. We exploit this result to automatically estimate
state-variable moments of all orders in Prob-solvable loops with non-polynomial
updates. We showcase the accuracy of our estimation approach in several
examples, such as the turning vehicle model and the Taylor rule in monetary
policy.
","['\nAndrey Kofnov\n', '\nMarcel Moosbrugger\n', '\nMiroslav Stankovič\n', '\nEzio Bartocci\n', '\nEfstathia Bura\n']",23 pages,,http://arxiv.org/abs/2205.02577v3,stat.AP,"['stat.AP', 'cs.SC', '62G05 (Primary) 62P30 (Secondary)', 'G.3']",,,[]
"Normalization, Square Roots, and the Exponential and Logarithmic Maps in
  Geometric Algebras of Less than 6D",http://arxiv.org/abs/2206.07496v2,2022-05-11T21:11:41Z,2022-08-23T14:23:54Z,"  Geometric algebras of dimension $n < 6$ are becoming increasingly popular for
the modeling of 3D and 3+1D geometry. With this increased popularity comes the
need for efficient algorithms for common operations such as normalization,
square roots, and exponential and logarithmic maps. The current work presents a
signature agnostic analysis of these common operations in all geometric
algebras of dimension $n < 6$, and gives efficient numerical implementations in
the most popular algebras $\mathbb{R}_{4}$, $\mathbb{R}_{3,1}$,
$\mathbb{R}_{3,0,1}$ and $\mathbb{R}_{4,1}$, in the hopes of lowering the
threshold for adoption of geometric algebra solutions by code maintainers.
","['\nSteven De Keninck\n', '\nMartin Roelfs\n']","16 pages, 4 figures",Math Meth Appl Sci. 2022; 1- 17,http://dx.doi.org/10.1002/mma.8639,cs.CG,"['cs.CG', 'cs.SC', '15A67']",10.1002/mma.8639,,[]
"The GPGCD Algorithm with the Bézout Matrix for Multiple Univariate
  Polynomials",http://arxiv.org/abs/2205.02984v1,2022-05-06T02:24:05Z,2022-05-06T02:24:05Z,"  We propose a modification of the GPGCD algorithm, which has been presented in
our previous research, for calculating approximate greatest common divisor
(GCD) of more than 2 univariate polynomials with real coefficients and a given
degree. In transferring the approximate GCD problem to a constrained
minimization problem, different from the original GPGCD algorithm for multiple
polynomials which uses the Sylvester subresultant matrix, the proposed
algorithm uses the B\'ezout matrix. Experiments show that the proposed
algorithm is more efficient than the original GPGCD algorithm for multiple
polynomials with maintaining almost the same accuracy for most of the cases.
","['\nBoming Chi\n', '\nAkira Terui\n']",,,http://arxiv.org/abs/2205.02984v1,math.AC,"['math.AC', 'cs.NA', 'cs.SC', 'math.NA', '13P99, 68W30', 'I.1.2; F.2.1; G.1.6']",,,[]
"Structured, flexible, and robust: benchmarking and improving large
  language models towards more human-like behavior in out-of-distribution
  reasoning tasks",http://arxiv.org/abs/2205.05718v1,2022-05-11T18:14:33Z,2022-05-11T18:14:33Z,"  Human language offers a powerful window into our thoughts -- we tell stories,
give explanations, and express our beliefs and goals through words. Abundant
evidence also suggests that language plays a developmental role in structuring
our learning. Here, we ask: how much of human-like thinking can be captured by
learning statistical patterns in language alone? We first contribute a new
challenge benchmark for comparing humans and distributional large language
models (LLMs). Our benchmark contains two problem-solving domains (planning and
explanation generation) and is designed to require generalization to new,
out-of-distribution problems expressed in language. We find that humans are far
more robust than LLMs on this benchmark. Next, we propose a hybrid
Parse-and-Solve model, which augments distributional LLMs with a structured
symbolic reasoning module. We find that this model shows more robust adaptation
to out-of-distribution planning problems, demonstrating the promise of hybrid
AI models for more human-like reasoning.
","['\nKatherine M. Collins\n', '\nCatherine Wong\n', '\nJiahai Feng\n', '\nMegan Wei\n', '\nJoshua B. Tenenbaum\n']",Originally accepted to the 2022 Cognitive Science (CogSci) conference,,http://arxiv.org/abs/2205.05718v1,cs.CL,"['cs.CL', 'cs.AI', 'cs.LG', 'cs.SC']",,,[]
Skew-sparse matrix multiplication,http://arxiv.org/abs/2205.06429v1,2022-05-13T02:44:03Z,2022-05-13T02:44:03Z,"  Based on the observation that $\mathbb{Q}^{(p-1) \times (p-1)}$ is isomorphic
to a quotient skew polynomial ring, we propose a new method for $(p-1)\times
(p-1)$ matrix multiplication over $\mathbb{Q}$, where $p$ is a prime number.
The main feature of our method is the acceleration for matrix multiplication if
the product is skew-sparse. Based on the new method, we design a deterministic
algorithm with complexity $O(T^{\omega-2} p^2)$, where $T\le p-1$ is a
parameter determined by the skew-sparsity of input matrices and $\omega$ is the
asymptotic exponent of matrix multiplication. Moreover, by introducing
randomness, we also propose a probabilistic algorithm with complexity
$O^\thicksim(t^{\omega-2}p^2+p^2\log\frac{1}{\nu})$, where $t\le p-1$ is the
skew-sparsity of the product and $\nu$ is the probability parameter.
","['\nQiao-Long Huang\n', '\nKe Ye\n', '\nXiao-Shan Gao\n']",,,http://arxiv.org/abs/2205.06429v1,cs.CC,"['cs.CC', 'cs.NA', 'cs.SC', 'math.NA']",,,[]
MathPartner Computer Algebra,http://arxiv.org/abs/2204.11549v1,2022-04-25T10:49:10Z,2022-04-25T10:49:10Z,"  In this paper, we describe general characteristics of the MathPartner
computer algebra system (CAS) and Mathpar programming language thereof.
MathPartner can be used for scientific and engineering calculations, as well as
in high schools and universities. It allows one to carry out both simple
calculations (acting as a scientific calculator) and complex calculations with
large-scale mathematical objects. Mathpar is a procedural language; it supports
a large number of elementary and special functions, as well as matrix and
polynomial operators. This service allows one to build function images and
animate them. MathPartner also makes it possible to solve some symbolic
computation problems on supercomputers with distributed memory. We highlight
main differences of MathPartner from other CASs and describe the Mathpar
language along with the user service provided.
",['\nGennadi Malaschonok\n'],9 pages,"Programming and Computer Software, 43, 2 (2017) 112-118",http://arxiv.org/abs/2204.11549v1,cs.SC,"['cs.SC', '68W30', 'I.1.4']",,,[]
About MathPartner web service,http://arxiv.org/abs/2206.07088v1,2022-04-25T15:39:05Z,2022-04-25T15:39:05Z,"  The report is devoted to the current state of the MathPartner computer
algebra web project. We discuss the main directions of development of the
project and give several examples of using it to solve selected problems.
","['\nGennadi Malaschonok\n', '\nIvan Borisov\n']",5 pages,"International conference Polynomial International conference
  Polynomial Computer Algebra. St.Petersburg, PDMI RAS, (2014) 50-54",http://arxiv.org/abs/2206.07088v1,cs.SC,"['cs.SC', '68W30', 'I.1.4']",,,[]
"Does a Program Yield the Right Distribution? Verifying Probabilistic
  Programs via Generating Functions",http://arxiv.org/abs/2205.01449v2,2022-05-03T12:21:59Z,2022-06-20T09:37:33Z,"  We study discrete probabilistic programs with potentially unbounded looping
behaviors over an infinite state space. We present, to the best of our
knowledge, the first decidability result for the problem of determining whether
such a program generates exactly a specified distribution over its outputs
(provided the program terminates almost surely). The class of distributions
that can be specified in our formalism consists of standard distributions
(geometric, uniform, etc.) and finite convolutions thereof. Our method relies
on representing these (possibly infinite-support) distributions as probability
generating functions which admit effective arithmetic operations. We have
automated our techniques in a tool called prodigy, which supports automatic
invariance checking, compositional reasoning of nested loops, and efficient
queries on various quantities of to the output distribution, as demonstrated by
experiments.
","['\nMingshuai Chen\n', '\nJoost-Pieter Katoen\n', '\nLutz Klinkenberg\n', '\nTobias Winkler\n']","Full version of CAV2022 paper including an appendix with proofs and
  further material",,http://arxiv.org/abs/2205.01449v2,cs.LO,"['cs.LO', 'cs.SC']",,,[]
LoopStack: a Lightweight Tensor Algebra Compiler Stack,http://arxiv.org/abs/2205.00618v1,2022-05-02T01:57:58Z,2022-05-02T01:57:58Z,"  We present LoopStack, a domain specific compiler stack for tensor operations,
composed of a frontend, LoopTool, and an efficient optimizing code generator,
LoopNest. This stack enables us to compile entire neural networks and generate
code targeting the AVX2, AVX512, NEON, and NEONfp16 instruction sets while
incorporating optimizations often missing from other machine learning compiler
backends. We evaluate our stack on a collection of full neural networks and
commonly used network blocks as well as individual operators, and show that
LoopStack generates machine code that matches and frequently exceeds the
performance of in state-of-the-art machine learning frameworks in both cases.
We also show that for a large collection of schedules LoopNest's compilation is
orders of magnitude faster than LLVM, while resulting in equal or improved run
time performance. Additionally, LoopStack has a very small memory footprint - a
binary size of 245KB, and under 30K lines of effective code makes it ideal for
use on mobile and embedded devices.
","['\nBram Wasti\n', '\nJosé Pablo Cambronero\n', '\nBenoit Steiner\n', '\nHugh Leather\n', '\nAleksandar Zlateski\n']",,,http://arxiv.org/abs/2205.00618v1,cs.LG,"['cs.LG', 'cs.PF', 'cs.SC']",,,[]
"Neurocompositional computing: From the Central Paradox of Cognition to a
  new generation of AI systems",http://arxiv.org/abs/2205.01128v1,2022-05-02T18:00:10Z,2022-05-02T18:00:10Z,"  What explains the dramatic progress from 20th-century to 21st-century AI, and
how can the remaining limitations of current AI be overcome? The widely
accepted narrative attributes this progress to massive increases in the
quantity of computational and data resources available to support statistical
learning in deep artificial neural networks. We show that an additional crucial
factor is the development of a new type of computation. Neurocompositional
computing adopts two principles that must be simultaneously respected to enable
human-level cognition: the principles of Compositionality and Continuity. These
have seemed irreconcilable until the recent mathematical discovery that
compositionality can be realized not only through discrete methods of symbolic
computing, but also through novel forms of continuous neural computing. The
revolutionary recent progress in AI has resulted from the use of limited forms
of neurocompositional computing. New, deeper forms of neurocompositional
computing create AI systems that are more robust, accurate, and comprehensible.
","['\nPaul Smolensky\n', '\nR. Thomas McCoy\n', '\nRoland Fernandez\n', '\nMatthew Goldrick\n', '\nJianfeng Gao\n']","21 pages, 6 figures. For a general AI audience: to appear in AI
  Magazine. A more extensive presentation of this work is ""Neurocompositional
  computing in human and machine intelligence: A tutorial"", Microsoft Technical
  Report MSR-TR-2022-5; see
  https://www.microsoft.com/en-us/research/publication/neurocompositional-computing-in-human-and-machine-intelligence-a-tutorial/",,http://arxiv.org/abs/2205.01128v1,cs.AI,"['cs.AI', 'cs.NE', 'cs.SC']",,,[]
"Shift Equivalence Testing of Polynomials and Symbolic Summation of
  Multivariate Rational Functions",http://arxiv.org/abs/2204.06968v2,2022-04-14T13:48:52Z,2023-08-29T01:59:12Z,"  The Shift Equivalence Testing (SET) of polynomials is deciding whether two
polynomials $p(x_1, \ldots, x_m)$ and $q(x_1, \ldots, x_m)$ satisfy the
relation $p(x_1 + a_1, \ldots, x_m + a_m) = q(x_1, \ldots, x_m)$ for some $a_1,
\ldots, a_m$ in the coefficient field. The SET problem is one of basic
computational problems in computer algebra and algebraic complexity theory,
which was reduced by Dvir, Oliveira and Shpilka in 2014 to the Polynomial
Identity Testing (PIT) problem. This paper presents a general scheme for
designing algorithms to solve the SET problem which includes
Dvir-Oliveira-Shpilka's algorithm as a special case. With the algorithms for
the SET problem over integers, we give complete solutions to two challenging
problems in symbolic summation of multivariate rational functions, namely the
rational summability problem and the existence problem of telescopers for
multivariate rational functions. Our approach is based on the structure of
isotropy groups of polynomials introduced by Sato in 1960s. Our results can be
used to detect the applicability of the Wilf-Zeilberger method to multivariate
rational functions.
","['\nShaoshi Chen\n', '\nLixin Du\n', '\nHanqian Fang\n']",54 pages,,http://arxiv.org/abs/2204.06968v2,cs.SC,"['cs.SC', 'cs.CC', 'math.RA', '68W30, 12H05, 12H10', 'I.1.2']",,,[]
SymForce: Symbolic Computation and Code Generation for Robotics,http://arxiv.org/abs/2204.07889v2,2022-04-17T00:15:10Z,2022-05-06T17:15:46Z,"  We present SymForce, a library for fast symbolic computation, code
generation, and nonlinear optimization for robotics applications like computer
vision, motion planning, and controls. SymForce combines the development speed
and flexibility of symbolic math with the performance of autogenerated, highly
optimized code in C++ or any target runtime language. SymForce provides
geometry and camera types, Lie group operations, and branchless singularity
handling for creating and analyzing complex symbolic expressions in Python,
built on top of SymPy. Generated functions can be integrated as factors into
our tangent-space nonlinear optimizer, which is highly optimized for real-time
production use. We introduce novel methods to automatically compute
tangent-space Jacobians, eliminating the need for bug-prone handwritten
derivatives. This workflow enables faster runtime code, faster development
time, and fewer lines of handwritten code versus the state-of-the-art. Our
experiments demonstrate that our approach can yield order of magnitude speedups
on computational tasks core to robotics. Code is available at
https://github.com/symforce-org/symforce.
","['\nHayk Martiros\n', '\nAaron Miller\n', '\nNathan Bucki\n', '\nBradley Solliday\n', '\nRyan Kennedy\n', '\nJack Zhu\n', '\nTung Dang\n', '\nDominic Pattison\n', '\nHarrison Zheng\n', '\nTeo Tomic\n', '\nPeter Henry\n', '\nGareth Cross\n', '\nJosiah VanderMey\n', '\nAlvin Sun\n', '\nSamuel Wang\n', '\nKristen Holtz\n']","10 pages, 5 figures. RSS 2022",,http://dx.doi.org/10.15607/RSS.2022.XVIII.041,cs.RO,"['cs.RO', 'cs.CV', 'cs.SC']",10.15607/RSS.2022.XVIII.041,,[]
"A Vergleichsstellensatz of Strassen's Type for a Noncommutative
  Preordered Semialgebra through the Semialgebra of its Fractions",http://arxiv.org/abs/2204.02577v3,2022-04-06T04:47:34Z,2023-05-19T12:59:42Z,"  Preordered semialgebras and semirings are two kinds of algebraic structures
occurring in real algebraic geometry frequently and usually play important
roles therein. They have many interesting and promising applications in the
fields of real algebraic geometry, probability theory, theoretical computer
science, quantum information theory, \emph{etc.}. In these applications,
Strassen's Vergleichsstellensatz and its generalized versions, which are
analogs of those Positivstellens\""atze in real algebraic geometry, play
important roles. While these Vergleichsstellens\""atze accept only a commutative
setting (for the semirings in question), we prove in this paper a
noncommutative version of one of the generalized Vergleichsstellens\""atze
proposed by Fritz [\emph{Comm. Algebra}, 49 (2) (2021), pp. 482-499]. The most
crucial step in our proof is to define the semialgebra of the fractions of a
noncommutative semialgebra, which generalizes the definitions in the
literature. Our new Vergleichsstellensatz characterizes the relaxed preorder on
a noncommutative semialgebra induced by all monotone homomorphisms to
$\mathbb{R}_+$ by three other equivalent conditions on the semialgebra of its
fractions equipped with the derived preorder, which may result in more
applications in the future.
","['\nTao Zheng\n', '\nLihong Zhi\n']",32 pages,,http://arxiv.org/abs/2204.02577v3,cs.SC,['cs.SC'],,,[]
"A note on the van der Waerden conjecture on random polynomials with
  symmetric Galois group for function fields",http://arxiv.org/abs/2204.02836v2,2022-04-06T14:00:28Z,2022-05-25T16:38:18Z,"  Let f(x) = x^n + (a[n-1] t + b[n-1]) x^(n-1) + ... + (a[0] t + b[0]) be of
constant degree n in x and degree <= 1 in t, where all a[i],b[i] are randomly
and uniformly selected from a finite field GF(q) of q elements. Then the
probability that the Galois group of f over the rational function field
GF(q)(t) is the symmetric group S(n) on n elements is 1 - O(1/q). Furthermore,
the probability that the Galois group of f(x) over GF(q)(t) is not S(n) is >=
1/q for n >= 3 and > 1/q - 1/(2q^2) for n = 2.
",['\nErich L. Kaltofen\n'],,,http://arxiv.org/abs/2204.02836v2,math.NT,"['math.NT', 'cs.SC', '12E25, 12F10']",,,[]
Finding Counterfactual Explanations through Constraint Relaxations,http://arxiv.org/abs/2204.03429v1,2022-04-07T13:18:54Z,2022-04-07T13:18:54Z,"  Interactive constraint systems often suffer from infeasibility (no solution)
due to conflicting user constraints. A common approach to recover infeasibility
is to eliminate the constraints that cause the conflicts in the system. This
approach allows the system to provide an explanation as: ""if the user is
willing to drop out some of their constraints, there exists a solution"".
However, one can criticise this form of explanation as not being very
informative. A counterfactual explanation is a type of explanation that can
provide a basis for the user to recover feasibility by helping them understand
which changes can be applied to their existing constraints rather than removing
them. This approach has been extensively studied in the machine learning field,
but requires a more thorough investigation in the context of constraint
satisfaction. We propose an iterative method based on conflict detection and
maximal relaxations in over-constrained constraint satisfaction problems to
help compute a counterfactual explanation.
","['\nSharmi Dev Gupta\n', '\nBegum Genc\n', ""\nBarry O'Sullivan\n""]","This work has appeared in Explainable Agency in Artificial
  Intelligence Workshop (EAAI'22) at AAAI'22",,http://arxiv.org/abs/2204.03429v1,cs.AI,"['cs.AI', 'cs.SC']",,,[]
"More Efficient Identifiability Verification in ODE Models by Reducing
  Non-Identifiability",http://arxiv.org/abs/2204.01623v1,2022-04-04T16:12:48Z,2022-04-04T16:12:48Z,"  Structural global parameter identifiability indicates whether one can
determine a parameter's value from given inputs and outputs in the absence of
noise. If a given model has parameters for which there may be infinitely many
values, such parameters are called non-identifiable. We present a procedure for
accelerating a global identifiability query by eliminating algebraically
independent non-identifiable parameters. Our proposed approach significantly
improves performance across different computer algebra frameworks.
","['\nIlia Ilmer\n', '\nAlexey Ovchinnikov\n', '\nGleb Pogudin\n', '\nPedro Soto\n']",,,http://arxiv.org/abs/2204.01623v1,cs.SC,"['cs.SC', 'cs.LG', 'math.AG']",,,[]
"Computing critical points for algebraic systems defined by
  hyperoctahedral invariant polynomials",http://arxiv.org/abs/2203.16094v2,2022-03-30T06:46:50Z,2022-06-10T12:54:17Z,"  Let $\mathbb{K}$ be a field of characteristic zero and $\mathbb{K}[x_1,
\dots, x_n]$ the corresponding multivariate polynomial ring. Given a sequence
of $s$ polynomials $\mathbf{f} = (f_1, \dots, f_s)$ and a polynomial $\phi$,
all in $\mathbb{K}[x_1, \dots, x_n]$ with $s<n$, we consider the problem of
computing the set $W(\phi, \mathbf{f})$ of points at which $\mathbf{f}$
vanishes and the Jacobian matrix of $\mathbf{f}, \phi$ with respect to $x_1,
\dots, x_n$ does not have full rank. This problem plays an essential role in
many application areas.
  In this paper we focus on a case where the polynomials are all invariant
under the action of the signed symmetric group $B_n$. We introduce a notion
called {\em hyperoctahedral representation} to describe $B_n$-invariant sets.
We study the invariance properties of the input polynomials to split $W(\phi,
\mathbf{f})$ according to the orbits of $B_n$ and then design an algorithm
whose output is a {hyperoctahedral representation} of $W(\phi, \mathbf{f})$.
The runtime of our algorithm is polynomial in the total number of points
described by the output.
",['\nThi Xuan Vu\n'],,,http://arxiv.org/abs/2203.16094v2,cs.SC,['cs.SC'],,,[]
Matrix Multiplication with Less Arithmetic Complexity and IO Complexity,http://arxiv.org/abs/2203.16053v1,2022-03-30T04:45:15Z,2022-03-30T04:45:15Z,"  After Strassen presented the first sub-cubic matrix multiplication algorithm,
many Strassen-like algorithms are presented. Most of them with low asymptotic
cost have large hidden leading coefficient which are thus impractical. To
reduce the leading coefficient, Cenk and Hasan give a general approach reducing
the leading coefficient of $<2,2,2;7>$-algorithm to $5$ but increasing IO
complexity. In 2017, Karstadt and Schwartz also reduce the leading coefficient
of $<2,2,2;7>$-algorithm to $5$ by the Alternative Basis Matrix Multiplication
method. Meanwhile, their method reduces the IO complexity and low-order
monomials in arithmetic complexity. In 2019, Beniamini and Schwartz generalize
Alternative Basis Matrix Multiplication method reducing leading coefficient in
arithmetic complexity but increasing IO complexity. In this paper, we propose a
new matrix multiplication algorithm which reduces leading coefficient both in
arithmetic complexity and IO complexity. We apply our method to Strassen-like
algorithms improving arithmetic complexity and IO complexity (the comparison
with previous results are shown in Tables 1 and 2). Surprisingly, our IO
complexity of $<3,3,3;23>$-algorithm is $14n^{\log_323}M^{-\frac{1}{2}} +
o(n^{\log_323})$ which breaks Ballard's IO complexity low bound
($\Omega(n^{\log_323}M^{1-\frac{\log_323}{2}})$) for recursive Strassen-like
algorithms.
","['\nPu Wu\n', '\nHuiqing Jiang\n', '\nZehui Shao\n', '\nJin Xu\n']",24 pages and one figure,,http://arxiv.org/abs/2203.16053v1,cs.SC,"['cs.SC', 'cs.DM', '68R01', 'F.2.1; I.1.2']",,,[]
"The SAGEX Review on Scattering Amplitudes, Chapter 4: Multi-loop Feynman
  Integrals",http://arxiv.org/abs/2203.13015v3,2022-03-24T12:00:19Z,2023-01-10T15:23:16Z,"  The analytic integration and simplification of multi-loop Feynman integrals
to special functions and constants plays an important role to perform higher
order perturbative calculations in the Standard Model of elementary particles.
In this survey article the most recent and relevant computer algebra and
special function algorithms are presented that are currently used or that may
play an important role to perform such challenging precision calculations in
the future. They are discussed in the context of analytic zero, single and
double scale calculations in the Quantum Field Theories of the Standard Model
and effective field theories, also with classical applications. These
calculations play a central role in the analysis of precision measurements at
present and future colliders to obtain ultimate information for fundamental
physics.
","['\nJohannes Blümlein\n', '\nCarsten Schneider\n']","42 pages, see also the overview article arXiv:2203.13011.v3: journal
  version",J. Phys. A: Math. Theor. 55 443005 (2022),http://dx.doi.org/10.1088/1751-8121/ac8086,hep-th,"['hep-th', 'cs.SC', 'hep-ph']",10.1088/1751-8121/ac8086,,[]
"Explainable Artificial Intelligence for Exhaust Gas Temperature of
  Turbofan Engines",http://arxiv.org/abs/2203.13108v2,2022-03-24T15:05:32Z,2022-03-25T08:38:36Z,"  Data-driven modeling is an imperative tool in various industrial
applications, including many applications in the sectors of aeronautics and
commercial aviation. These models are in charge of providing key insights, such
as which parameters are important on a specific measured outcome or which
parameter values we should expect to observe given a set of input parameters.
At the same time, however, these models rely heavily on assumptions (e.g.,
stationarity) or are ""black box"" (e.g., deep neural networks), meaning that
they lack interpretability of their internal working and can be viewed only in
terms of their inputs and outputs. An interpretable alternative to the ""black
box"" models and with considerably less assumptions is symbolic regression (SR).
SR searches for the optimal model structure while simultaneously optimizing the
model's parameters without relying on an a-priori model structure. In this
work, we apply SR on real-life exhaust gas temperature (EGT) data, collected at
high frequencies through the entire flight, in order to uncover meaningful
algebraic relationships between the EGT and other measurable engine parameters.
The experimental results exhibit promising model accuracy, as well as
explainability returning an absolute difference of 3{\deg}C compared to the
ground truth and demonstrating consistency from an engineering perspective.
","['\nMarios Kefalas\n', '\nJuan de Santiago Rojo Jr.\n', '\nAsteris Apostolidis\n', '\nDirk van den Herik\n', '\nBas van Stein\n', '\nThomas Bäck\n']","Main paper: 20 pages, 4 figures. Supplemental material: 18 pages, 30
  figures. Published; Removed footnote on page 11 of the main article regarding
  a typo on formula (8) on the Journal version of this work. The Journal has
  corrected this typo since, therefore, there is no need for the footnote",,http://dx.doi.org/10.2514/1.I011058,cs.LG,"['cs.LG', 'cs.AI', 'cs.NE', 'cs.SC']",10.2514/1.I011058,,[]
Insights From the NeurIPS 2021 NetHack Challenge,http://arxiv.org/abs/2203.11889v1,2022-03-22T17:01:07Z,2022-03-22T17:01:07Z,"  In this report, we summarize the takeaways from the first NeurIPS 2021
NetHack Challenge. Participants were tasked with developing a program or agent
that can win (i.e., 'ascend' in) the popular dungeon-crawler game of NetHack by
interacting with the NetHack Learning Environment (NLE), a scalable,
procedurally generated, and challenging Gym environment for reinforcement
learning (RL). The challenge showcased community-driven progress in AI with
many diverse approaches significantly beating the previously best results on
NetHack. Furthermore, it served as a direct comparison between neural (e.g.,
deep RL) and symbolic AI, as well as hybrid systems, demonstrating that on
NetHack symbolic bots currently outperform deep RL by a large margin. Lastly,
no agent got close to winning the game, illustrating NetHack's suitability as a
long-term benchmark for AI research.
","['\nEric Hambro\n', '\nSharada Mohanty\n', '\nDmitrii Babaev\n', '\nMinwoo Byeon\n', '\nDipam Chakraborty\n', '\nEdward Grefenstette\n', '\nMinqi Jiang\n', '\nDaejin Jo\n', '\nAnssi Kanervisto\n', '\nJongmin Kim\n', '\nSungwoong Kim\n', '\nRobert Kirk\n', '\nVitaly Kurin\n', '\nHeinrich Küttler\n', '\nTaehwon Kwon\n', '\nDonghoon Lee\n', '\nVegard Mella\n', '\nNantas Nardelli\n', '\nIvan Nazarov\n', '\nNikita Ovsov\n', '\nJack Parker-Holder\n', '\nRoberta Raileanu\n', '\nKarolis Ramanauskas\n', '\nTim Rocktäschel\n', '\nDanielle Rothermel\n', '\nMikayel Samvelyan\n', '\nDmitry Sorokin\n', '\nMaciej Sypetkowski\n', '\nMichał Sypetkowski\n']","Under review at PMLR for the NeuRIPS 2021 Competition Workshop Track,
  10 pages + 10 in appendices",,http://arxiv.org/abs/2203.11889v1,cs.LG,"['cs.LG', 'cs.AI', 'cs.NE', 'cs.SC', 'stat.ML']",,,[]
"Artificial Intelligence Software Structured to Simulate Human Working
  Memory, Mental Imagery, and Mental Continuity",http://arxiv.org/abs/2204.05138v1,2022-03-29T22:23:36Z,2022-03-29T22:23:36Z,"  This article presents an artificial intelligence (AI) architecture intended
to simulate the human working memory system as well as the manner in which it
is updated iteratively. It features several interconnected neural networks
designed to emulate the specialized modules of the cerebral cortex. These are
structured hierarchically and integrated into a global workspace. They are
capable of temporarily maintaining high-level patterns akin to the
psychological items maintained in working memory. This maintenance is made
possible by persistent neural activity in the form of two modalities: sustained
neural firing (resulting in a focus of attention) and synaptic potentiation
(resulting in a short-term store). This persistent activity is updated
iteratively resulting in incremental changes to the content of the working
memory system. As the content stored in working memory gradually evolves,
successive states overlap and are continuous with one another. The present
article will explore how this architecture can lead to gradual shift in the
distribution of coactive representations, ultimately leading to mental
continuity between processing states, and thus to human-like cognition.
",['\nJared Edward Reser\n'],,,http://arxiv.org/abs/2204.05138v1,q-bio.NC,"['q-bio.NC', 'cs.AI', 'cs.LG', 'cs.NE', 'cs.SC']",,,[]
"Gröbner bases and critical values: The asymptotic combinatorics of
  determinantal systems",http://arxiv.org/abs/2203.10021v1,2022-03-18T15:37:01Z,2022-03-18T15:37:01Z,"  We consider ideals involving the maximal minors of a polynomial matrix. For
example, those arising in the computation of the critical values of a
polynomial restricted to a variety for polynomial optimisation. Gr\""obner bases
are a classical tool for solving polynomial systems. For practical
computations, this consists of two stages. First, a Gr\""obner basis is computed
with respect to a DRL (degree reverse lexicographic) ordering. Then, a change
of ordering algorithm, such as \textsf{Sparse-FGLM}, designed by Faug\`ere and
Mou, is used to find a Gr\""obner basis of the same ideal but with respect to a
lexicographic ordering. The complexity of this latter step, in terms of
arithmetic operations, is $O(mD^2)$, where $D$ is the degree of the ideal and
$m$ is the number of non-trivial columns of a certain $D \times D$ matrix.
While asymptotic estimates are known for $m$ for generic polynomial systems,
thus far, the complexity of \textsf{Sparse-FGLM} was unknown for determinantal
systems.
  By assuming Fr\""oberg's conjecture we expand the work of Moreno-Soc\'ias by
detailing the structure of the DRL staircase in the determinantal setting. Then
we study the asymptotics of the quantity $m$ by relating it to the coefficients
of these Hilbert series. Consequently, we arrive at a new bound on the
complexity of the \textsf{Sparse-FGLM} algorithm for generic determinantal
systems and for generic critical point systems. We consider the ideal in the
polynomial ring $\mathbb{K}[x_1, \dots, x_n]$, where $\mathbb{K}$ is some
infinite field, generated by $p$ generic polynomials of degree $d$ and the
maximal minors of a $p \times (n-1)$ polynomial matrix with generic entries of
degree $d-1$. Then for the case $d=2$ and for $n \gg p$ we give an exact
formula for $m$ in terms of $n$ and $p$. Moreover, for $d \geq 3$, we give an
asymptotic formula, as $n \to \infty$, for $m$ in terms of $n,p$ and $d$.
","['\nAlin Bostan\n', '\nJérémy Berthomieu\n', '\nAndrew Ferguson\n', '\nMohab Safey El Din\n']",,,http://arxiv.org/abs/2203.10021v1,math.AC,"['math.AC', 'cs.SC']",,,[]
"Computing a Group Action from the Class Field Theory of Imaginary
  Hyperelliptic Function Fields",http://arxiv.org/abs/2203.06970v5,2022-03-14T10:11:35Z,2023-07-27T06:40:47Z,"  We explore algorithmic aspects of a simply transitive commutative group
action coming from the class field theory of imaginary hyperelliptic function
fields. Namely, the Jacobian of an imaginary hyperelliptic curve defined over
$\mathbb F_q$ acts on a subset of isomorphism classes of Drinfeld modules. We
describe an algorithm to compute the group action efficiently. This is a
function field analog of the Couveignes-Rostovtsev-Stolbunov group action. We
report on an explicit computation done with our proof-of-concept C++/NTL
implementation; it took a fraction of a second on a standard computer. We prove
that the problem of inverting the group action reduces to the problem of
finding isogenies of fixed $\tau$-degree between Drinfeld $\mathbb
F_q[X]$-modules, which is solvable in polynomial time thanks to an algorithm by
Wesolowski. We give asymptotic complexity bounds for all algorithms presented
in this paper.
","['\nAntoine Leudière\n', '\nPierre-Jean Spaenlehauer\n']","This paper is a rewrite of arXiv:2203.06970v2. It takes into account
  the recent attack of Wesolowski on the cryptographic applications
  (https://eprint.iacr.org/2022/438). We removed cryptographic applications,
  and the introduction and experimental results have been widely rewritten.
  Complexity results have been added",,http://arxiv.org/abs/2203.06970v5,cs.SC,"['cs.SC', 'cs.CR', 'math.NT']",,,[]
"Linear slices of hyperbolic polynomials and positivity of symmetric
  polynomial functions",http://arxiv.org/abs/2203.08727v2,2022-03-16T16:20:03Z,2023-03-08T17:54:56Z,"  A real univariate polynomial of degree $n$ is called hyperbolic if all of its
$n$ roots are on the real line. Such polynomials appear quite naturally in
different applications, for example, in combinatorics and optimization. The
focus of this article are families of hyperbolic polynomials which are
determined through $k$ linear conditions on the coefficients. The coefficients
corresponding to such a family of hyperbolic polynomials form a semi-algebraic
set which we call a \emph{hyperbolic slice}. We initiate here the study of the
geometry of these objects in more detail. The set of hyperbolic polynomials is
naturally stratified with respect to the multiplicities of the real zeros and
this stratification induces also a stratification on the hyperbolic slices. Our
main focus here is on the \emph{local extreme points} of hyperbolic slices,
i.e., the local extreme points of linear functionals, and we show that these
correspond precisely to those hyperbolic polynomials in the hyperbolic slice
which have at most $k$ distinct roots and we can show that generically the
convex hull of such a family is a polyhedron. Building on these results, we
give consequences of our results to the study of symmetric real varieties and
symmetric semi-algebraic sets. Here, we show that sets defined by symmetric
polynomials which can be expressed sparsely in terms of elementary symmetric
polynomials can be sampled on points with few distinct coordinates. This in
turn allows for algorithmic simplifications, for example, to verify that such
polynomials are non-negative or that a semi-algebraic set defined by such
polynomials is empty.
","['\nCordian Riener\n', '\nRobin Schabert\n']",18 pages; revision of previous version,,http://arxiv.org/abs/2203.08727v2,math.AG,"['math.AG', 'cs.SC', 'math.OC', '14P05, 20C30, 90C30']",,,[]
"AdaLoGN: Adaptive Logic Graph Network for Reasoning-Based Machine
  Reading Comprehension",http://arxiv.org/abs/2203.08992v1,2022-03-16T23:51:01Z,2022-03-16T23:51:01Z,"  Recent machine reading comprehension datasets such as ReClor and LogiQA
require performing logical reasoning over text. Conventional neural models are
insufficient for logical reasoning, while symbolic reasoners cannot directly
apply to text. To meet the challenge, we present a neural-symbolic approach
which, to predict an answer, passes messages over a graph representing logical
relations between text units. It incorporates an adaptive logic graph network
(AdaLoGN) which adaptively infers logical relations to extend the graph and,
essentially, realizes mutual and iterative reinforcement between neural and
symbolic reasoning. We also implement a novel subgraph-to-node message passing
mechanism to enhance context-option interaction for answering multiple-choice
questions. Our approach shows promising results on ReClor and LogiQA.
","['\nXiao Li\n', '\nGong Cheng\n', '\nZiheng Chen\n', '\nYawei Sun\n', '\nYuzhong Qu\n']","11 pages, accepted to the main conference of ACL 2022",,http://arxiv.org/abs/2203.08992v1,cs.CL,"['cs.CL', 'cs.AI', 'cs.NE', 'cs.SC']",,,[]
A $p$-adic Descartes solver: the Strassman solver,http://arxiv.org/abs/2203.07016v1,2022-03-14T11:55:56Z,2022-03-14T11:55:56Z,"  Solving polynomials is a fundamental computational problem in mathematics. In
the real setting, we can use Descartes' rule of signs to efficiently isolate
the real roots of a square-free real polynomial. In this paper, we translate
this method into the $p$-adic worlds. We show how the $p$-adic analog of
Descartes' rule of signs, Strassman's theorem, leads to an algorithm to isolate
the roots of a square-free $p$-adic polynomial. Moreover, we show that this
algorithm runs in $\mathcal{O}(d^2\log^3d)$-time for a random $p$-adic
polynomial of degree $d$. To perform this analysis, we introduce the
condition-based complexity framework from real/complex numerical algebraic
geometry into $p$-adic numerical algebraic geometry.
",['\nJosué Tonelli-Cueto\n'],"36 pages, 1 figure",,http://arxiv.org/abs/2203.07016v1,math.NT,"['math.NT', 'cs.CC', 'cs.NA', 'cs.SC', 'math.NA', 'math.PR', '11D88, 14Q20, 03D15']",,,[]
"A Signature-based Algorithm for Computing the Nondegenerate Locus of a
  Polynomial System",http://arxiv.org/abs/2202.13784v2,2022-02-28T13:26:01Z,2022-07-22T12:04:48Z,"  Polynomial system solving arises in many application areas to model
non-linear geometric properties. In such settings, polynomial systems may come
with degeneration which the end-user wants to exclude from the solution set.
The nondegenerate locus of a polynomial system is the set of points where the
codimension of the solution set matches the number of equations. Computing the
nondegenerate locus is classically done through ideal-theoretic operations in
commutative algebra such as saturation ideals or equidimensional decompositions
to extract the component of maximal codimension. By exploiting the algebraic
features of signature-based Gr\""obner basis algorithms we design an algorithm
which computes a Gr\""obner basis of the equations describing the closure of the
nondegenerate locus of a polynomial system, without computing first a Gr\""obner
basis for the whole polynomial system.
","['\nChristian Eder\n', '\nPierre Lairez\n', '\nRafael Mohr\n', '\nMohab Safey El Din\n']","22 pages, 2 figures. Substantial rewrite of content of the parts of
  the paper involving signature-based Gr\""obner basis algorithms, both the
  exposition and the description of the core algorithm of the paper changed","Journal of Symbolic Computation 119, 2023",http://dx.doi.org/10.1016/j.jsc.2023.02.001,cs.SC,"['cs.SC', '13P10, 13P05', 'I.1.2; G.4']",10.1016/j.jsc.2023.02.001,,[]
"On the complexity of invariant polynomials under the action of finite
  reflection groups",http://arxiv.org/abs/2203.04123v2,2022-03-08T14:45:36Z,2022-05-31T13:11:36Z,"  Let $\mathbb{K}[x_1, \dots, x_n]$ be a multivariate polynomial ring over a
field $\mathbb{K}$. Let $(u_1, \dots, u_n)$ be a sequence of $n$ algebraically
independent elements in $\mathbb{K}[x_1, \dots, x_n]$. Given a polynomial $f$
in $\mathbb{K}[u_1, \dots, u_n]$, a subring of $\mathbb{K}[x_1, \dots, x_n]$
generated by the $u_i$'s, we are interested infinding the unique polynomial
$f_{\rm new}$ in $\mathbb{K}[e_1,\dots, e_n]$, where $e_1, \dots, e_n$ are new
variables, such that $f_{\mathrm{new}}(u_1, \dots, u_n) = f(x_1, \dots, x_n)$.
We provide an algorithm and analyze its arithmetic complexity to compute
$f_{\mathrm{new}}$ knowing $f$ and $(u_1, \dots, u_n)$.
",['\nThi Xuan Vu\n'],,,http://arxiv.org/abs/2203.04123v2,cs.SC,['cs.SC'],,,[]
"Computing roadmaps in unbounded smooth real algebraic sets I:
  connectivity results",http://arxiv.org/abs/2203.03961v2,2022-03-08T09:35:01Z,2023-06-07T08:28:52Z,"  Answering connectivity queries in real algebraic sets is a fundamental
problem in effective real algebraic geometry that finds many applications in
e.g. robotics where motion planning issues are topical. This computational
problem is tackled through the computation of so-called roadmaps which are real
algebraic subsets of the set V under study, of dimension at most one, and which
have a connected intersection with all semi-algebraically connected components
of V. Algorithms for computing roadmaps rely on statements establishing
connectivity properties of some well-chosen subsets of V , assuming that V is
bounded.
  In this paper, we extend such connectivity statements by dropping the
boundedness assumption on V. This exploits properties of so-called generalized
polar varieties, which are critical loci of V for some well-chosen polynomial
maps.
","['\nRémi Prébet\nPolSys\n', '\nMohab Safey El Din\nPolSys\n', '\nÉric Schost\n']","26 pages, 23 figures",,http://arxiv.org/abs/2203.03961v2,cs.SC,"['cs.SC', 'math.AG', '14Q20, 14Q30, 14P05, 68W30']",,,"['PolSys', 'PolSys']"
Identification in Tree-shaped Linear Structural Causal Models,http://arxiv.org/abs/2203.01852v2,2022-03-03T16:59:49Z,2022-03-04T14:45:09Z,"  Linear structural equation models represent direct causal effects as directed
edges and confounding factors as bidirected edges. An open problem is to
identify the causal parameters from correlations between the nodes. We
investigate models, whose directed component forms a tree, and show that there,
besides classical instrumental variables, missing cycles of bidirected edges
can be used to identify the model. They can yield systems of quadratic
equations that we explicitly solve to obtain one or two solutions for the
causal parameters of adjacent directed edges. We show how multiple missing
cycles can be combined to obtain a unique solution. This results in an
algorithm that can identify instances that previously required approaches based
on Gr\""obner bases, which have doubly-exponential time complexity in the number
of structural parameters.
","['\nBenito van der Zander\n', '\nMarcel Wienöbst\n', '\nMarkus Bläser\n', '\nMaciej Liśkiewicz\n']","9 pages (excluding references and appendix), accepted by AISTATS 2022",,http://arxiv.org/abs/2203.01852v2,cs.AI,"['cs.AI', 'cs.LG', 'cs.SC', 'stat.ML']",,,[]
"Verification of Bitcoin Script in Agda using Weakest Preconditions for
  Access Control",http://arxiv.org/abs/2203.03054v3,2022-03-06T21:07:28Z,2022-06-12T00:54:29Z,"  This paper contributes to the verification of programs written in Bitcoin's
smart contract language SCRIPT in the interactive theorem prover Agda. It
focuses on the security property of access control for SCRIPT programs that
govern the distribution of Bitcoins. It advocates that weakest preconditions in
the context of Hoare triples are the appropriate notion for verifying access
control. It aims at obtaining human-readable descriptions of weakest
preconditions in order to close the validation gap between user requirements
and formal specification of smart contracts.
  As examples for the proposed approach, the paper focuses on two standard
SCRIPT programs that govern the distribution of Bitcoins, Pay to Public Key
Hash (P2PKH) and Pay to Multisig (P2MS). The paper introduces an operational
semantics of the SCRIPT commands used in P2PKH and P2MS, which is formalised in
the Agda proof assistant and reasoned about using Hoare triples. Two
methodologies for obtaining human-readable descriptions of weakest
preconditions are discussed:
  (1) a step-by-step approach, which works backwards instruction by instruction
through a script, sometimes grouping several instructions together;
  (2) symbolic execution of the code and translation into a nested case
distinction, which allows to read off weakest preconditions as the disjunction
of conjunctions of conditions along accepting paths.
  A syntax for equational reasoning with Hoare Triples is defined in order to
formalise those approaches in Agda.
  Keywords and phrases: Blockchain; Cryptocurrency; Bitcoin; Agda;
Verification; Hoare logic; Bitcoin script; P2PKH; P2MS; Access control; Weakest
precondition; Predicate transformer semantics; Provable correctness; Symbolic
execution; Smart contracts
","['\nFahad F. Alhabardi\n', '\nArnold Beckmann\n', '\nBogdan Lazar\n', '\nAnton Setzer\n']",27 pages,,http://dx.doi.org/10.4230/LIPIcs.TYPES.2021.1,cs.SC,"['cs.SC', 'cs.CR', 'cs.LO', 'cs.SI']",10.4230/LIPIcs.TYPES.2021.1,,[]
"On realizing differential-algebraic equations by rational dynamical
  systems",http://arxiv.org/abs/2203.03555v2,2022-03-07T17:56:27Z,2022-05-14T09:05:07Z,"  Real-world phenomena can often be conveniently described by dynamical systems
(that is, ODE systems in the state-space form). However, if one observes the
state of the system only partially, the observed quantities (outputs) and the
inputs of the system can typically be related by more complicated
differential-algebraic equations (DAEs). Therefore, a natural question
(referred to as the realizability problem) is: given a differential-algebraic
equation (say, fitted from data), does it come from a partially observed
dynamical system? A special case in which the functions involved in the
dynamical system are rational is of particular interest. For a single
differential-algebraic equation in a single output variable, Forsman has shown
that it is realizable by a rational dynamical system if and only if the
corresponding hypersurface is unirational, and he turned this into an algorithm
in the first-order case.
  In this paper, we study a more general case of single-input-single-output
equations. We show that if a realization by a rational dynamical system exists,
the system can be taken to have the dimension equal to the order of the DAE. We
provide a complete algorithm for first-order DAEs. We also show that the same
approach can be used for higher-order DAEs using several examples from the
literature.
","['\nDmitrii Pavlov\n', '\nGleb Pogudin\n']",,,http://dx.doi.org/10.1145/3476446.3535492,cs.SC,"['cs.SC', 'math.AG', 'math.DS', 'math.OC']",10.1145/3476446.3535492,,[]
"Deciding Cuspidality of Manipulators through Computer Algebra and
  Algorithms in Real Algebraic Geometry",http://arxiv.org/abs/2203.04578v2,2022-03-09T08:44:41Z,2022-07-25T11:17:19Z,"  Cuspidal robots are robots with at least two inverse kinematic solutions that
can be connected by a singularity-free path. Deciding the cuspidality of
generic 3R robots has been studied in the past, but extending the study to
six-degree-of-freedom robots can be a challenging problem. Many robots can be
modeled as a polynomial map together with a real algebraic set so that the
notion of cuspidality can be extended to these data. In this paper we design an
algorithm that, on input a polynomial map in $n$ indeterminates, and $s$
polynomials in the same indeterminates describing a real algebraic set of
dimension $d$, decides the cuspidality of the restriction of the map to the
real algebraic set under consideration. Moreover, if $D$ and $\tau$ are,
respectively the maximum degree and the bound on the bit size of the
coefficients of the input polynomials, this algorithm runs in time log-linear
in $\tau$ and polynomial in $((s+d)D)^{O(n^2)}$. It relies on many high-level
algorithms in computer algebra which use advanced methods on real algebraic
sets and critical loci of polynomial maps. As far as we know, this is the first
algorithm that tackles the cuspidality problem from a general point of view.
","['\nDamien Chablat\nLS2N\n', '\nRémi Prébet\nPolSys\n', '\nMohab Safey El Din\nPolSys\n', '\nDurgesh Salunkhe\nLS2N\n', '\nPhilippe Wenger\nLS2N\n']","10 pages, 4 figures, published in the Proceedings of ISSAC2022",,http://dx.doi.org/10.1145/3476446.3535477,cs.SC,"['cs.SC', 'cs.RO', 'math.AG', '68W30, 14Q30', 'I.1.2; I.1.4']",10.1145/3476446.3535477,,"['LS2N', 'PolSys', 'PolSys', 'LS2N', 'LS2N']"
Desingularization and p-Curvature of Recurrence Operators,http://arxiv.org/abs/2202.08931v1,2022-02-17T23:14:24Z,2022-02-17T23:14:24Z,"  Linear recurrence operators in characteristic $p$ are classified by their
$p$-curvature. For a recurrence operator $L$, denote by $\chi(L)$ the
characteristic polynomial of its $p$-curvature. We can obtain information about
the factorization of $L$ by factoring $\chi(L)$. The main theorem of this paper
gives an unexpected relation between $\chi(L)$ and the true singularities of
$L$. An application is to speed up a fast algorithm for computing $\chi(L)$ by
desingularizing $L$ first. Another contribution of this paper is faster
desingularization.
","['\nYi Zhou\n', '\nMark van Hoeij\n']",7 pages,,http://arxiv.org/abs/2202.08931v1,cs.SC,['cs.SC'],,,[]
A New Type of Gröbner Basis and Its Complexity,http://arxiv.org/abs/2202.09493v1,2022-02-19T02:11:48Z,2022-02-19T02:11:48Z,"  The new type of ideal basis introduced herein constitutes a compromise
between the Gr\""obner bases based on the Buchberger's algorithm and the
characteristic sets based on the Wu's method. It reduces the complexity of the
traditional Gr\""obner bases and subdues the notorious intermediate expression
swell problem and intermediate coefficient swell problem to a substantial
extent. The computation of an $S$-polynomial for the new bases requires at most
$O(m\ln^2m\ln\ln m)$ word operations whereas $O(m^6\ln^2m)$ word operations are
requisite in the Buchberger's algorithm. Here $m$ denotes the upper bound for
the numbers of terms both in the leading coefficients and for the rest of the
polynomials. The new bases are for zero-dimensional polynomial ideals and based
on univariate pseudo-divisions. However in contrast to the pseudo-divisions in
the Wu's method for the characteristic sets, the new bases retain the algebraic
information of the original ideal and in particular, solve the ideal membership
problem. In order to determine the authentic factors of the eliminant, we
analyze the multipliers of the pseudo-divisions and develop an algorithm over
principal quotient rings with zero divisors.
",['\nSheng-Ming Ma\n'],"15 pages. arXiv admin note: substantial text overlap with
  arXiv:2101.03482",,http://arxiv.org/abs/2202.09493v1,cs.SC,"['cs.SC', '13P10, 13B25', 'I.1.2']",,,[]
"Faster change of order algorithm for Gröbner bases under shape and
  stability assumptions",http://arxiv.org/abs/2202.09226v2,2022-02-18T14:49:43Z,2022-05-15T22:12:26Z,"  Solving zero-dimensional polynomial systems using Gr\""obner bases is usually
done by, first, computing a Gr\""obner basis for the degree reverse
lexicographic order, and next computing the lexicographic Gr\""obner basis with
a change of order algorithm. Currently, the change of order now takes a
significant part of the whole solving time for many generic instances.
  Like the fastest known change of order algorithms, this work focuses on the
situation where the ideal defined by the system satisfies natural properties
which can be recovered in generic coordinates. First, the ideal has a
\emph{shape} lexicographic Gr\""obner basis. Second, the set of leading terms
with respect to the degree reverse lexicographic order has a \emph{stability}
property; in particular, the multiplication matrix can be read on the input
Gr\""obner basis.
  The current fastest algorithms rely on the sparsity of this matrix. Actually,
this sparsity is a consequence of an algebraic structure, which can be
exploited to represent the matrix concisely as a univariate polynomial matrix.
We show that the Hermite normal form of that matrix yields the sought
lexicographic Gr\""obner basis, under assumptions which cover the shape position
case. Under some mild assumption implying $n \le t$, the arithmetic complexity
of our algorithm is $O\tilde{~}(t^{\omega-1}D)$, where $n$ is the number of
variables, $t$ is a sparsity indicator of the aforementioned matrix, $D$ is the
degree of the zero-dimensional ideal under consideration, and $\omega$ is the
exponent of matrix multiplication. This improves upon both state-of-the-art
complexity bounds $O\tilde{~}(tD^2)$ and $O\tilde{~}(D^\omega)$, since $\omega
< 3$ and $t\le D$. Practical experiments, based on the libraries msolve and
PML, confirm the high practical benefit.
","['\nJérémy Berthomieu\n', '\nVincent Neiger\n', '\nMohab Safey El Din\n']","10 pages, 2 tables",,http://arxiv.org/abs/2202.09226v2,cs.SC,"['cs.SC', 'math.AC']",,,[]
Rank-Sensitive Computation of the Rank Profile of a Polynomial Matrix,http://arxiv.org/abs/2202.09329v2,2022-02-18T17:44:18Z,2022-05-10T03:22:42Z,"  Consider a matrix $\mathbf{F} \in \mathbb{K}[x]^{m \times n}$ of univariate
polynomials over a field $\mathbb{K}$. We study the problem of computing the
column rank profile of $\mathbf{F}$. To this end we first give an algorithm
which improves the minimal kernel basis algorithm of Zhou, Labahn, and
Storjohann (Proceedings ISSAC 2012). We then provide a second algorithm which
computes the column rank profile of $\mathbf{F}$ with a rank-sensitive
complexity of $O\tilde{~}(r^{\omega-2} n (m+D))$ operations in $\mathbb{K}$.
Here, $D$ is the sum of row degrees of $\mathbf{F}$, $\omega$ is the exponent
of matrix multiplication, and $O\tilde{~}(\cdot)$ hides logarithmic factors.
","['\nGeorge Labahn\n', '\nVincent Neiger\n', '\nThi Xuan Vu\n', '\nWei Zhou\n']","10 pages, 2 algorithms, 1 figure",,http://dx.doi.org/10.1145/3476446.3535495,cs.SC,"['cs.SC', 'math.RA']",10.1145/3476446.3535495,,[]
Learning Program Synthesis for Integer Sequences from Scratch,http://arxiv.org/abs/2202.11908v3,2022-02-24T05:34:33Z,2022-11-29T17:00:52Z,"  We present a self-learning approach for synthesizing programs from integer
sequences. Our method relies on a tree search guided by a learned policy. Our
system is tested on the On-Line Encyclopedia of Integer Sequences. There, it
discovers, on its own, solutions for 27987 sequences starting from basic
operators and without human-written training examples.
","['\nThibault Gauthier\n', '\nJosef Urban\n']",,,http://arxiv.org/abs/2202.11908v3,cs.AI,"['cs.AI', 'cs.SC']",,,[]
Random primes without primality testing,http://arxiv.org/abs/2202.12073v1,2022-02-24T12:55:14Z,2022-02-24T12:55:14Z,"  Numerous algorithms call for computation over the integers modulo a
randomly-chosen large prime. In some cases, the quasi-cubic complexity of
selecting a random prime can dominate the total running time. We propose a new
variant of the classic D5 algorithm for ""dynamic evaluation"", applied to a
randomly-chosen (composite) integer. Unlike the D5 principle which has been
used in the past to compute over a direct product of fields, our method is
simpler as it only requires following a single path after any modulus splits.
The transformation we propose can apply to any algorithm in the algebraic RAM
model, even allowing randomization. The resulting transformed algorithm avoids
any primality tests and will, with constant positive probability, have the same
result as the original computation modulo a randomly-chosen prime. As an
application, we demonstrate how to compute the exact number of nonzero terms in
an unknown integer polynomial in quasi-linear time. We also show how the same
algorithmic transformation technique can be used for computing modulo random
irreducible polynomials over a finite field.
","['\nPascal Giorgi\n', '\nBruno Grenet\n', '\nArmelle Perret du Cray\n', '\nDaniel S. Roche\n']",,,http://arxiv.org/abs/2202.12073v1,cs.SC,"['cs.SC', 'cs.CC']",,,[]
"Extending Flat Motion Planning to Non-flat Systems. Experiments on
  Aircraft Models Using Maple",http://arxiv.org/abs/2202.09921v3,2022-02-20T22:33:03Z,2022-05-12T16:46:30Z,"  Aircraft models may be considered as flat if one neglects some terms
associated to aerodynamics. Computational experiments in Maple show that in
some cases a suitably designed feed-back allows to follow such trajectories,
when applied to the non-flat model. However some maneuvers may be hard or even
impossible to achieve with this flat approximation. In this paper, we propose
an iterated process to compute a more achievable trajectory, starting from the
flat reference trajectory. More precisely, the unknown neglected terms in the
flat model are iteratively re-evaluated using the values obtained at the
previous step. This process may be interpreted as a new trajectory
parametrization, using an infinite number of derivatives, a property that may
be called \emph{generalized flatness}. We illustrate the pertinence of this
approach in flight conditions of increasing difficulties, from single engine
flight, to aileron roll.
",['\nFrançois Ollivier\n'],"28 pages, 11 figures","ISSAC '22: Proceedings of the 2022 International Symposium on
  Symbolic and Algebraic Computation, ACM Press, July 2022, Pages 499--507",http://dx.doi.org/10.1145/3476446.3536179,cs.SC,"['cs.SC', 'cs.SY', 'eess.SY', '68W30, 93-08, 93B25, 93B51, 93B52', 'I.1']",10.1145/3476446.3536179,,[]
On the complexity of Chow and Hurwitz forms,http://arxiv.org/abs/2202.11582v2,2022-02-23T15:57:52Z,2022-12-25T16:04:47Z,"  We consider the bit complexity of computing Chow forms and their
generalization to multiprojective spaces. We develop a deterministic algorithm
using resultants and obtain a single exponential complexity upper bound.
Earlier computational results for Chow forms were in the arithmetic complexity
model, and our result represents the first bit complexity bound. We also extend
our algorithm to Hurwitz forms in projective space, and explore connections
between multiprojective Hurwitz forms and matroid theory. The motivation for
our work comes from incidence geometry where intriguing computational algebra
problems remain open.
","['\nMahmut Levent Doğan\n', '\nAlperen Ali Ergür\n', '\nElias Tsigaridas\n']",,,http://arxiv.org/abs/2202.11582v2,cs.CC,"['cs.CC', 'cs.SC', 'math.AG']",,,[]
"On the computation of Gröbner bases for matrix-weighted homogeneous
  systems",http://arxiv.org/abs/2202.05742v2,2022-02-11T16:32:11Z,2022-09-28T12:41:25Z,"  In this paper, we examine the structure of systems that are weighted
homogeneous for several systems of weights, and how it impacts the computation
of Gr\""obner bases. We present several linear algebra algorithms for computing
Gr\""obner bases for systems with this structure, either directly or by reducing
to existing structures. We also present suitable optimization techniques.
  As an opening towards complexity studies, we discuss potential definitions of
regularity and prove that they are generic if non-empty. Finally, we present
experimental data from a prototype implementation of the algorithms in
SageMath.
",['\nThibaut Verron\n'],23 pages,,http://arxiv.org/abs/2202.05742v2,cs.SC,['cs.SC'],,,[]
"Square-free Strong Triangular Decomposition of Zero-dimensional
  Polynomial Systems",http://arxiv.org/abs/2202.06044v1,2022-02-12T11:33:04Z,2022-02-12T11:33:04Z,"  Triangular decomposition with different properties has been used for various
types of problem solving, e.g. geometry theorem proving, real solution
isolation of zero-dimensional polynomial systems, etc. In this paper, the
concepts of strong chain and square-free strong triangular decomposition
(SFSTD) of zero-dimensional polynomial systems are defined. Because of its good
properties, SFSTD may be a key way to many problems related to zero-dimensional
polynomial systems, such as real solution isolation and computing radicals of
zero-dimensional ideals. Inspired by the work of Wang and of Dong and Mou, we
propose an algorithm for computing SFSTD based on Gr\""obner bases computation.
The novelty of the algorithm is that we make use of saturated ideals and
separant to ensure that the zero sets of any two strong chains have no
intersection and every strong chain is square-free, respectively. On one hand,
we prove that the arithmetic complexity of the new algorithm can be single
exponential in the square of the number of variables, which seems to be among
the rare complexity analysis results for triangular-decomposition methods. On
the other hand, we show experimentally that, on a large number of examples in
the literature, the new algorithm is far more efficient than a popular
triangular-decomposition method based on pseudo-division. Furthermore, it is
also shown that, on those examples, the methods based on SFSTD for real
solution isolation and for computing radicals of zero-dimensional ideals are
very efficient.
","['\nHaokun Li\n', '\nBican Xia\n', '\nTianqi Zhao\n']",,,http://arxiv.org/abs/2202.06044v1,cs.SC,['cs.SC'],,,[]
MMLN: Leveraging Domain Knowledge for Multimodal Diagnosis,http://arxiv.org/abs/2202.04266v1,2022-02-09T04:12:30Z,2022-02-09T04:12:30Z,"  Recent studies show that deep learning models achieve good performance on
medical imaging tasks such as diagnosis prediction. Among the models,
multimodality has been an emerging trend, integrating different forms of data
such as chest X-ray (CXR) images and electronic medical records (EMRs).
However, most existing methods incorporate them in a model-free manner, which
lacks theoretical support and ignores the intrinsic relations between different
data sources. To address this problem, we propose a knowledge-driven and
data-driven framework for lung disease diagnosis. By incorporating domain
knowledge, machine learning models can reduce the dependence on labeled data
and improve interpretability. We formulate diagnosis rules according to
authoritative clinical medicine guidelines and learn the weights of rules from
text data. Finally, a multimodal fusion consisting of text and image data is
designed to infer the marginal probability of lung disease. We conduct
experiments on a real-world dataset collected from a hospital. The results show
that the proposed method outperforms the state-of-the-art multimodal baselines
in terms of accuracy and interpretability.
","['\nHaodi Zhang\n', '\nChenyu Xu\n', '\nPeirou Liang\n', '\nKe Duan\n', '\nHao Ren\n', '\nWeibin Cheng\n', '\nKaishun Wu\n']",,,http://arxiv.org/abs/2202.04266v1,cs.LG,"['cs.LG', 'cs.SC']",,,[]
On the Identity Problem for Unitriangular Matrices of Dimension Four,http://arxiv.org/abs/2202.05225v3,2022-02-10T18:29:22Z,2022-06-24T13:56:29Z,"  We show that the Identity Problem is decidable in polynomial time for
finitely generated sub-semigroups of the group $\mathsf{UT}(4, \mathbb{Z})$ of
$4 \times 4$ unitriangular integer matrices. As a byproduct of our proof, we
also show the polynomial-time decidability of several subset reachability
problems in $\mathsf{UT}(4, \mathbb{Z})$.
",['\nRuiwen Dong\n'],"28 pages, 2 figures",,http://arxiv.org/abs/2202.05225v3,cs.DM,"['cs.DM', 'cs.SC']",,,[]
"The Factorial-Basis Method for Finding Definite-Sum Solutions of Linear
  Recurrences With Polynomial Coefficients",http://arxiv.org/abs/2202.05550v3,2022-02-11T11:07:39Z,2022-12-15T12:16:06Z,"  The problem of finding a nonzero solution of a linear recurrence $Ly = 0$
with polynomial coefficients where $y$ has the form of a definite
hypergeometric sum, related to the Inverse Creative Telescoping Problem of
[14][Sec. 8], has now been open for three decades. Here we present an algorithm
(implemented in a SageMath package) which, given such a recurrence and a
quasi-triangular, shift-compatible factorial basis $\mathcal{B} = \langle
P_k(n)\rangle_{k=0}^\infty$ of the polynomial space $\mathbb{K}[n]$ over a
field $\mathbb{K}$ of characteristic zero, computes a recurrence satisfied by
the coefficient sequence $c = \langle c_k\rangle_{k=0}^\infty$ of the solution
$y_n = \sum_{k=0}^\infty c_kP_k(n)$ (where, thanks to the quasi-triangularity
of $\mathcal{B}$, the sum on the right terminates for each $n \in \mathbb{N}$).
More generally, if $\mathcal{B}$ is $m$-sieved for some $m \in \mathbb{N}$, our
algorithm computes a system of $m$ recurrences satisfied by the $m$-sections of
the coefficient sequence $c$. If an explicit nonzero solution of this system
can be found, we obtain an explicit nonzero solution of $Ly = 0$.
","['\nAntonio Jiménez-Pastor\n', '\nMarko Petkovšek\n']",62 pages,Journal of Symbolic Computation 117 (2023) 15-50,http://dx.doi.org/10.1016/j.jsc.2022.11.002,cs.SC,"['cs.SC', 'cs.MS', '33F10, 39A06, 68W30']",10.1016/j.jsc.2022.11.002,,[]
"Exact SOHS decompositions of trigonometric univariate polynomials with
  Gaussian coefficients",http://arxiv.org/abs/2202.06544v2,2022-02-14T08:33:18Z,2023-10-04T12:30:59Z,"  Certifying the positivity of trigonometric polynomials is of first importance
for design problems in discrete-time signal processing. It is well known from
the Riesz-Fej\'ez spectral factorization theorem that any trigonometric
univariate polynomial positive on the unit circle can be decomposed as a
Hermitian square with complex coefficients. Here we focus on the case of
polynomials with Gaussian integer coefficients, i.e., with real and imaginary
parts being integers. We design, analyze and compare, theoretically and
practically,three hybrid numeric-symbolic algorithms computing weighted sums of
Hermitian squares decompositions for trigonometric univariate polynomials
positive on the unit circle with Gaussian coefficients. The numerical steps the
first and second algorithm rely on are complex root isolation and semidefinite
programming, respectively. An exact sum of Hermitian squares decomposition is
obtained thanks to compensation techniques. The third algorithm, also based on
complex semidefinite programming, is an adaptation of the rounding and
projection algorithm by Peyrl and Parrilo. For all three algorithms, we prove
bit complexity and output size estimates that are polynomial in the degree of
the input and linear in the maximum bitsize of its coefficients. We compare
their performance on randomly chosen benchmarks, and further design a certified
finite impulse filter.
","['\nVictor Magron\n', '\nMohab Safey El Din\n', '\nMarkus Schweighofer\n', '\nTrung Hieu Vu\n']","9 pages, 1 table",,http://arxiv.org/abs/2202.06544v2,cs.SC,"['cs.SC', 'math.OC']",,,[]
"The Membership Problem for Hypergeometric Sequences with Rational
  Parameters",http://arxiv.org/abs/2202.07416v2,2022-02-15T14:05:42Z,2022-05-24T13:13:41Z,"  We investigate the Membership Problem for hypergeometric sequences: given a
hypergeometric sequence $\langle u_n \rangle_{n=0}^\infty$ of rational numbers
and a target $t \in \mathbb{Q}$, decide whether $t$ occurs in the sequence. We
show decidability of this problem under the assumption that in the defining
recurrence $p(n)u_{n}=q(n)u_{n-1}$, the roots of the polynomials $p(x)$ and
$q(x)$ are all rational numbers. Our proof relies on bounds on the density of
primes in arithmetic progressions. We also observe a relationship between the
decidability of the Membership problem (and variants) and the Rohrlich-Lang
conjecture in transcendence theory.
","['\nKlara Nosan\n', '\nAmaury Pouly\n', '\nMahsa Shirmohammadi\n', '\nJames Worrell\n']",,,http://arxiv.org/abs/2202.07416v2,cs.LO,"['cs.LO', 'cs.SC']",,,[]
Bohemian Matrix Geometry,http://arxiv.org/abs/2202.07769v2,2022-02-15T22:43:30Z,2022-04-26T18:50:27Z,"  A Bohemian matrix family is a set of matrices all of whose entries are drawn
from a fixed, usually discrete and hence bounded, subset of a field of
characteristic zero. Originally these were integers -- hence the name, from the
acronym BOunded HEight Matrix of Integers (BOHEMI) -- but other kinds of
entries are also interesting. Some kinds of questions about Bohemian matrices
can be answered by numerical computation, but sometimes exact computation is
better. In this paper we explore some Bohemian families (symmetric, upper
Hessenberg, or Toeplitz) computationally, and answer some open questions posed
about the distributions of eigenvalue densities.
","['\nRobert M. Corless\n', '\nGeorge Labahn\n', '\nDan Piponi\n', '\nLeili Rafiee Sevyeri\n']",22 pages; 12 figures,,http://dx.doi.org/10.1145/3476446.3536177,cs.SC,"['cs.SC', 'math.CO', '15B05', 'I.1.4']",10.1145/3476446.3536177,,[]
"Differential Privacy for Symbolic Systems with Application to Markov
  Chains",http://arxiv.org/abs/2202.03325v2,2022-02-07T16:17:56Z,2022-08-11T14:25:48Z,"  Data-driven systems are gathering increasing amounts of data from users, and
sensitive user data requires privacy protections. In some cases, the data
gathered is non-numerical or symbolic, and conventional approaches to privacy,
e.g., adding noise, do not apply, though such systems still require privacy
protections. Accordingly, we present a novel differential privacy framework for
protecting trajectories generated by symbolic systems. These trajectories can
be represented as words or strings over a finite alphabet. We develop new
differential privacy mechanisms that approximate a sensitive word using a
random word that is likely to be near it. An offline mechanism is implemented
efficiently using a Modified Hamming Distance Automaton to generate whole
privatized output words over a finite time horizon. Then, an online mechanism
is implemented by taking in a sensitive symbol and generating a randomized
output symbol at each timestep. This work is extended to Markov chains to
generate differentially private state sequences that a given Markov chain could
have produced. Statistical accuracy bounds are developed to quantify the
accuracy of these mechanisms, and numerical results validate the accuracy of
these techniques for strings of English words.
","['\nBo Chen\n', '\nKevin Leahy\n', '\nAustin Jones\n', '\nMatthew Hale\n']","16 pages, 9 figures, submitted to Automatica",,http://arxiv.org/abs/2202.03325v2,cs.CR,"['cs.CR', 'cs.SC', 'cs.SY', 'eess.SY']",,,[]
Random primes in arithmetic progressions,http://arxiv.org/abs/2202.05955v2,2022-02-12T02:19:27Z,2022-04-29T15:46:15Z,"  We describe a straightforward method to generate a random prime q such that
the multiplicative group GF(q)* also has a random large prime-order subgroup.
The described algorithm also yields this order p as well as a p'th primitive
root of unity. The methods here are efficient asymptotically, but due to large
constants may not be very useful in practical settings.
","['\nPascal Giorgi\n', '\nBruno Grenet\n', '\nArmelle Perret du Cray\n', '\nDaniel S. Roche\n']",,,http://arxiv.org/abs/2202.05955v2,cs.CC,"['cs.CC', 'cs.SC', 'math.NT']",,,[]
"Faster Gröbner Bases via Domain-Specific Ordering in Parameter
  Identifiability of ODE Models",http://arxiv.org/abs/2202.06297v2,2022-02-13T12:40:11Z,2023-02-02T17:01:39Z,"  We consider a specific class of polynomial systems that arise in parameter
identifiability problems of models of ordinary differential equations (ODE) and
discover a method for speeding up the Gr\""obner basis computation by using
specific variable ordering and weights coming from the structure of the ODE
model. We provide empirical results that show improvement across different
symbolic computing frameworks.
","['\nMariya Bessonov\n', '\nIlia Ilmer\n', '\nTatiana Konstantinova\n', '\nAlexey Ovchinnikov\n', '\nGleb Pogudin\n', '\nPedro Soto\n']",,,http://arxiv.org/abs/2202.06297v2,cs.SC,"['cs.SC', 'cs.MS', 'q-bio.QM']",,,[]
Stability Problems in Symbolic Integration,http://arxiv.org/abs/2202.06305v1,2022-02-13T13:00:31Z,2022-02-13T13:00:31Z,"  This paper aims to initialize a dynamical aspect of symbolic integration by
studying stability problems in differential fields. We present some basic
properties of stable elementary functions and D-finite power series that enable
us to characterize three special families of stable elementary functions
involving rational functions, logarithmic functions, and exponential functions.
Some problems for future studies are proposed towards deeper dynamical studies
in differential and difference algebra.
",['\nShaoshi Chen\n'],18 pages,,http://arxiv.org/abs/2202.06305v1,cs.SC,"['cs.SC', 'math.AC', 'math.DS', '12H05, 37P15, 33F10,', 'I.1.2']",,,[]
Beyond Worst-Case Analysis for Root Isolation Algorithms,http://arxiv.org/abs/2202.06428v3,2022-02-13T22:28:12Z,2022-06-23T20:10:16Z,"  Isolating the real roots of univariate polynomials is a fundamental problem
in symbolic computation and it is arguably one of the most important problems
in computational mathematics. The problem has a long history decorated with
numerous ingenious algorithms and furnishes an active area of research.
However, the worst-case analysis of root-finding algorithms does not correlate
with their practical performance. We develop a smoothed analysis framework for
polynomials with integer coefficients to bridge the gap between the complexity
estimates and the practical performance. In this setting, we derive that the
expected bit complexity of DESCARTES solver to isolate the real roots of a
polynomial, with coefficients uniformly distributed, is
$\widetilde{\mathcal{O}}_B(d^2 + d \tau)$, where $d$ is the degree of the
polynomial and $\tau$ the bitsize of the coefficients.
","['\nAlperen A. Ergür\n', '\nJosué Tonelli-Cueto\n', '\nElias Tsigaridas\n']","9 pages, 2 figures. 2nd version: New title, corrections. 3rd version:
  Correction of typo in name","Proceedings of the 2022 International Symposium on Symbolic and
  Algebraic Computation (ISSAC 22). Association for Computing Machinery, New
  York, NY, USA, 139-148 (2022)",http://dx.doi.org/10.1145/3476446.3535475,cs.CC,"['cs.CC', 'cs.SC', 'math.AG', 'math.PR', '65H04, 14Q20, 68W30']",10.1145/3476446.3535475,,[]
"Fast Symbolic Algorithms for Omega-Regular Games under Strong Transition
  Fairness",http://arxiv.org/abs/2202.07480v4,2022-02-15T14:47:12Z,2023-02-23T16:42:01Z,"  We consider fixpoint algorithms for two-player games on graphs with
$\omega$-regular winning conditions, where the environment is constrained by a
strong transition fairness assumption. Strong transition fairness is a widely
occurring special case of strong fairness, which requires that any execution is
strongly fair with respect to a specified set of live edges: whenever the
source vertex of a live edge is visited infinitely often along a play, the edge
itself is traversed infinitely often along the play as well. We show that,
surprisingly, strong transition fairness retains the algorithmic
characteristics of the fixpoint algorithms for $\omega$-regular games -- the
new algorithms have the same alternation depth as the classical algorithms but
invoke a new type of predecessor operator. For Rabin games with $k$ pairs, the
complexity of the new algorithm is $O(n^{k+2}k!)$ symbolic steps, which is
independent of the number of live edges in the strong transition fairness
assumption. Further, we show that GR(1) specifications with strong transition
fairness assumptions can be solved with a 3-nested fixpoint algorithm, same as
the usual algorithm. In contrast, strong fairness necessarily requires
increasing the alternation depth depending on the number of fairness
assumptions. We get symbolic algorithms for (generalized) Rabin, parity and
GR(1) objectives under strong transition fairness assumptions as well as a
direct symbolic algorithm for qualitative winning in stochastic
$\omega$-regular games that runs in $O(n^{k+2}k!)$ symbolic steps, improving
the state of the art. Finally, we have implemented a BDD-based synthesis engine
based on our algorithm. We show on a set of synthetic and real benchmarks that
our algorithm is scalable, parallelizable, and outperforms previous algorithms
by orders of magnitude.
","['\nTamajit Banerjee\n', '\nRupak Majumdar\n', '\nKaushik Mallik\n', '\nAnne-Kathrin Schmuck\n', '\nSadegh Soudjani\n']",,"TheoretiCS, Volume 2 (February 24, 2023) theoretics:9088",http://dx.doi.org/10.46298/theoretics.23.4,cs.FL,"['cs.FL', 'cs.SC', 'cs.SY', 'eess.SY']",10.46298/theoretics.23.4,,[]
On Polynomial Ideals And Overconvergence In Tate Algebras,http://arxiv.org/abs/2202.07509v1,2022-02-15T15:37:30Z,2022-02-15T15:37:30Z,"  In this paper, we study ideals spanned by polynomials or overconvergent
series in a Tate algebra. With state-of-the-art algorithms for computing Tate
Gr{\""o}bner bases, even if the input is polynomials, the size of the output
grows with the required precision, both in terms of the size of the
coefficients and the size of the support of the series. We prove that ideals
which are spanned by polynomials admit a Tate Gr{\""o}bner basis made of
polynomials, and we propose an algorithm, leveraging Mora's weak normal form
algorithm, for computing it. As a result, the size of the output of this
algorithm grows linearly with the precision. Following the same ideas, we
propose an algorithm which computes an overconvergent basis for an ideal
spanned by overconvergent series. Finally, we prove the existence of a
universal analytic Gr{\""o}bner basis for polynomial ideals in Tate algebras,
compatible with all convergence radii.
","['\nXavier Caruso\nLFANT, IMB\n', '\nTristan Vaccon\nXLIM\n', '\nThibaut Verron\nJKU\n']",,,http://arxiv.org/abs/2202.07509v1,cs.SC,"['cs.SC', 'math.AG', 'math.NT']",,,"['LFANT, IMB', 'XLIM', 'JKU']"
Trinomials and Deterministic Complexity Limits for Real Solving,http://arxiv.org/abs/2202.06115v1,2022-02-12T18:07:51Z,2022-02-12T18:07:51Z,"  Consider a univariate polynomial f in Z[x] with degree d, exactly t monomial
terms, and coefficients in {-H,...,H}. Solving f over the reals, R, in
polynomial-time can be defined as counting the exact number of real roots of f
and then finding (for each such root z) an approximation w of logarithmic
height (log(dH))^{O(1)} such that the Newton iterates of w have error decaying
at a rate of O((1/2)^{2^i}). Solving efficiently in this sense, using
(log(dH))^{O(1)} deterministic bit operations, is arguably the most honest
formulation of solving a polynomial equation over R in time polynomial in the
input size. Unfortunately, deterministic algorithms this fast are known only
for t=2, unknown for t=3, and provably impossible for t=4. (One can of course
resort to older techniques with complexity (d\log H)^{O(1)} for t>=4.)
  We give evidence that polynomial-time real-solving in the strong sense above
is possible for t=3: We give a polynomial-time algorithm employing
A-hypergeometric series that works for all but a fraction of 1/Omega(log(dH))
of the input f. We also show an equivalence between fast trinomial solving and
sign evaluation at rational points of small height. As a consequence, we show
that for ""most"" trinomials f, we can compute the sign of f at a rational point
r in time polynomial in log(dH) and the logarithmic height of r. (This was
known only for binomials before.) We also mention a related family of
polynomial systems that should admit a similar speed-up for solving.
","['\nErick Boniface\n', '\nWeixun Deng\n', '\nJ. Maurice Rojas\n']",,,http://arxiv.org/abs/2202.06115v1,math.AG,"['math.AG', 'cs.CC', 'cs.NA', 'cs.SC', 'math.NA']",,,[]
"Locally Random Alloy Codes with Channel Coding Theorems for Distributed
  Matrix Multiplication",http://arxiv.org/abs/2202.03469v5,2022-02-07T19:20:00Z,2022-11-13T22:03:46Z,"  Matrix multiplication is a fundamental operation in machine learning and is
commonly distributed into multiple parallel tasks for large datasets.
Stragglers and other failures can severely impact the overall completion time.
Recent works in coded computing provide a novel strategy to mitigate stragglers
with coded tasks, with an objective of minimizing the number of tasks needed to
recover the overall result, known as the recovery threshold. However, we
demonstrate that this combinatorial definition does not directly optimize the
probability of failure.
  In this paper, we introduce a novel analytical metric, which focuses on the
most likely event and measures the optimality of a coding scheme by its
probability of decoding. Our general framework encompasses many other
computational schemes and metrics as a special case. Far from being a purely
theoretical construction, these definitions lead us to a practical construction
of random codes for matrix multiplication, i.e., locally random alloy codes,
which are optimal with respect to the measures. We present experimental results
on Amazon EC2 which empirically demonstrate the improvement in terms of running
time and numerical stability relative to well-established benchmarks.
","['\nPedro Soto\n', '\nHaibin Guan\n', '\nJun Li\n']","10 pages, preprint",,http://arxiv.org/abs/2202.03469v5,cs.IT,"['cs.IT', 'cs.DC', 'cs.LG', 'cs.NA', 'cs.SC', 'math.IT', 'math.NA', 'E.4; H.1.1; C.2.4; B.8.1; C.4; G.1.3; I.2.6; I.1.2']",,,[]
"Symbolic-Numeric Integration of Univariate Expressions based on Sparse
  Regression",http://arxiv.org/abs/2201.12468v2,2022-01-29T01:30:31Z,2022-02-06T14:07:12Z,"  Most computer algebra systems (CAS) support symbolic integration as core
functionality. The majority of the integration packages use a combination of
heuristic algebraic and rule-based (integration table) methods. In this paper,
we present a hybrid (symbolic-numeric) methodology to calculate the indefinite
integrals of univariate expressions. The primary motivation for this work is to
add symbolic integration functionality to a modern CAS (the symbolic
manipulation packages of SciML, the Scientific Machine Learning ecosystem of
the Julia programming language), which is mainly designed toward numerical and
machine learning applications and has a different set of features than
traditional CAS. The symbolic part of our method is based on the combination of
candidate terms generation (borrowed from the Homotopy operators theory) with
rule-based expression transformations provided by the underlying CAS. The
numeric part is based on sparse-regression, a component of Sparse
Identification of Nonlinear Dynamics (SINDy) technique. We show that this
system can solve a large variety of common integration problems using only a
few dozen basic integration rules.
","['\nShahriar Iravanian\n', '\nCarl Julius Martensen\n', '\nAlessandro Cheli\n', '\nShashi Gowda\n', '\nAnand Jain\n', '\nYingbo Ma\n', '\nChris Rackauckas\n']","8 pages. submitted to ISSAC 2022. Code at
  https://github.com/SciML/SymbolicNumericIntegration.jl",,http://arxiv.org/abs/2201.12468v2,cs.SC,"['cs.SC', 'I.1.0; I.1.2']",,,[]
"Rational Solutions of First Order Algebraic Ordinary Differential
  Equations",http://arxiv.org/abs/2201.11378v1,2022-01-27T08:38:04Z,2022-01-27T08:38:04Z,"  Let $f(t,y,y')=\sum_{i=0}^n a_i(t,y)y'^i=0$ be an irreducible first order
ordinary differential equation with polynomial coefficients. Eremenko in 1998
proved that there exists a constant $C$ such that every rational solution of
$f(t,y,y')=0$ is of degree not greater than $C$. Examples show that this degree
bound $C$ depends not only on the degrees of $f$ in $t,y,y'$ but also on the
coefficients of $f$ viewed as the polynomial in $t,y,y'$. In this paper, we
show that if $f$ satisfies $deg(f,y)<deg(f,y')$ or $\max_{i=0}^n
\{deg(a_i,y)-2(n-i)\}>0 $ then the degree bound $C$ only depends on the degrees
of $f$ in $t,y,y'$, and furthermore we present an explicit expression for $C$
in terms of the degrees of $f$ in $t,y,y'$.
","['\nShuang Feng\n', '\nLi-Yong Shen\n']",arXiv admin note: substantial text overlap with arXiv:2005.01289,,http://arxiv.org/abs/2201.11378v1,math.CA,"['math.CA', 'cs.SC']",,,[]
"Resultant Tools for Parametric Polynomial Systems with Application to
  Population Models",http://arxiv.org/abs/2201.13189v2,2022-01-31T12:52:59Z,2022-02-09T21:49:32Z,"  We are concerned with the problem of decomposing the parameter space of a
parametric system of polynomial equations, and possibly some polynomial
inequality constraints, with respect to the number of real solutions that the
system attains. Previous studies apply a two step approach to this problem,
where first the discriminant variety of the system is computed via a Groebner
Basis (GB), and then a Cylindrical Algebraic Decomposition (CAD) of this is
produced to give the desired computation. However, even on some reasonably
small applied examples this process is too expensive, with computation of the
discriminant variety alone infeasible. In this paper we develop new approaches
to build the discriminant variety using resultant methods (the Dixon resultant
and a new method using iterated univariate resultants). This reduces the
complexity compared to GB and allows for a previous infeasible example to be
tackled. We demonstrate the benefit by giving a symbolic solution to a problem
from population dynamics -- the analysis of the steady states of three
connected populations which exhibit Allee effects - which previously could only
be tackled numerically.
","['\nAmirHosein Sadeghimanesh\n', '\nMatthew England\n']",10 pages; typo from v1 fixed,,http://arxiv.org/abs/2201.13189v2,cs.SC,"['cs.SC', 'q-bio.PE', '92C42, 92D25, 13P15, 68W30', 'I.1.2; I.1.4; J.3']",,,[]
Reasoning Like Program Executors,http://arxiv.org/abs/2201.11473v2,2022-01-27T12:28:24Z,2022-10-22T13:46:24Z,"  Reasoning over natural language is a long-standing goal for the research
community. However, studies have shown that existing language models are
inadequate in reasoning. To address the issue, we present POET, a novel
reasoning pre-training paradigm. Through pre-training language models with
programs and their execution results, POET empowers language models to harvest
the reasoning knowledge possessed by program executors via a data-driven
approach. POET is conceptually simple and can be instantiated by different
kinds of program executors. In this paper, we showcase two simple instances
POET-Math and POET-Logic, in addition to a complex instance, POET-SQL.
Experimental results on six benchmarks demonstrate that POET can significantly
boost model performance in natural language reasoning, such as numerical
reasoning, logical reasoning, and multi-hop reasoning. POET opens a new gate on
reasoning-enhancement pre-training, and we hope our analysis would shed light
on the future research of reasoning like program executors.
","['\nXinyu Pi\n', '\nQian Liu\n', '\nBei Chen\n', '\nMorteza Ziyadi\n', '\nZeqi Lin\n', '\nQiang Fu\n', '\nYan Gao\n', '\nJian-Guang Lou\n', '\nWeizhu Chen\n']","To appear in EMNLP 2022 main conference. The first two authors
  contributed equally",,http://arxiv.org/abs/2201.11473v2,cs.CL,"['cs.CL', 'cs.AI', 'cs.SC']",,,[]
AI Research Associate for Early-Stage Scientific Discovery,http://arxiv.org/abs/2202.03199v1,2022-02-02T17:05:52Z,2022-02-02T17:05:52Z,"  Artificial intelligence (AI) has been increasingly applied in scientific
activities for decades; however, it is still far from an insightful and
trustworthy collaborator in the scientific process. Most existing AI methods
are either too simplistic to be useful in real problems faced by scientists or
too domain-specialized (even dogmatized), stifling transformative discoveries
or paradigm shifts. We present an AI research associate for early-stage
scientific discovery based on (a) a novel minimally-biased ontology for
physics-based modeling that is context-aware, interpretable, and generalizable
across classical and relativistic physics; (b) automatic search for viable and
parsimonious hypotheses, represented at a high-level (via domain-agnostic
constructs) with built-in invariants, e.g., postulated forms of conservation
principles implied by a presupposed spacetime topology; and (c) automatic
compilation of the enumerated hypotheses to domain-specific, interpretable, and
trainable/testable tensor-based computation graphs to learn phenomenological
relations, e.g., constitutive or material laws, from sparse (and possibly
noisy) data sets.
","['\nMorad Behandish\n', '\nJohn Maxwell III\n', '\nJohan de Kleer\n']",Paper #203,"AAAI-MLPS-2021: Association for the Advancement of Artificial
  Intelligence (AAAI) 2021 Spring Symposium on Combining Artificial
  Intelligence and Machine Learning with Physics Sciences (MLPS)",http://arxiv.org/abs/2202.03199v1,cs.AI,"['cs.AI', 'cs.LG', 'cs.SC']",,,[]
Exact linear reduction for rational dynamical systems,http://arxiv.org/abs/2201.13373v3,2022-01-31T17:34:04Z,2022-07-04T20:07:05Z,"  Detailed dynamical systems models used in life sciences may include dozens or
even hundreds of state variables. Models of large dimension are not only harder
from the numerical perspective (e.g., for parameter estimation or simulation),
but it is also becoming challenging to derive mechanistic insights from such
models. Exact model reduction is a way to address this issue by finding a
self-consistent lower-dimensional projection of the corresponding dynamical
system. A recent algorithm CLUE allows one to construct an exact linear
reduction of the smallest possible dimension such that the fixed variables of
interest are preserved. However, CLUE is restricted to systems with polynomial
dynamics. Since rational dynamics occurs frequently in the life sciences (e.g.,
Michaelis-Menten or Hill kinetics), it is desirable to extend CLUE to the
models with rational dynamics. In this paper, we present an extension of CLUE
to the case of rational dynamics and demonstrate its applicability on examples
from literature. Our implementation is available in version 1.5 of CLUE at
https://github.com/pogudingleb/CLUE.
","['\nAntonio Jiménez-Pastor\n', '\nJoshua Paul Jacob\n', '\nGleb Pogudin\n']","19 pages, 4 algorithms, 4 tables, 1 figure",,http://arxiv.org/abs/2201.13373v3,q-bio.QM,"['q-bio.QM', 'cs.SC', 'cs.SY', 'eess.SY', 'math.DS']",,,[]
"Interval-Memoized Backtracking on ZDDs for Fast Enumeration of All Lower
  Cost Solutions",http://arxiv.org/abs/2201.08118v2,2022-01-20T11:35:09Z,2022-04-27T23:28:17Z,"  In this paper, we propose a fast method for exactly enumerating a very large
number of all lower cost solutions for various combinatorial problems. Our
method is based on backtracking for a given decision diagram which represents
all the feasible solutions. The main idea is to memoize the intervals of cost
bounds to avoid duplicate search in the backtracking process. In contrast to
usual pseudo-polynomial-time dynamic programming approaches, the computation
time of our method does not directly depend on the total cost values, but is
bounded by the input and output size of the decision diagrams. Therefore, it
can be much faster if the cost values are large but the input/output decision
diagrams are well-compressed. We demonstrate its practical efficiency by
comparing our method to current available enumeration methods: for nontrivial
size instances of the Hamiltonian path problem, our method succeeded in exactly
enumerating billions of all lower cost solutions in a few seconds, which was
hundred or much more times faster. Our method can be regarded as a novel search
algorithm which integrates the two classical techniques, branch-and-bound and
dynamic programming. This method would have many applications in various
fields, including operations research, data mining, statistical testing,
hardware/software system design, etc.
","['\nShin-ichi Minato\n', '\nMutsunori Banbara\n', '\nTakashi Horiyama\n', '\nJun Kawahara\n', '\nIchigaku Takigawa\n', '\nYutaro Yamaguchi\n']","Related work (Section 6) is added. Experimental results for another
  set of problem instances (simple path problem) are added in Section 5.2. Some
  other minor corrections are made. Style file is changed to LIPIcs format",,http://arxiv.org/abs/2201.08118v2,cs.DS,"['cs.DS', 'cs.SC', '05C30, 05C85', 'I.1.3']",,,[]
ODEbase: A Repository of ODE Systems for Systems Biology,http://arxiv.org/abs/2201.08980v1,2022-01-22T07:22:01Z,2022-01-22T07:22:01Z,"  Recently, symbolic computation and computer algebra systems have been
successfully applied in systems biology, especially in chemical reaction
network theory. One advantage of symbolic computation is its potential for
qualitative answers to biological questions. Qualitative methods analyze
dynamical input systems as formal objects, in contrast to investigating only
part of the state space, as is the case with numerical simulation. However,
symbolic computation tools and libraries have a different set of requirements
for their input data than their numerical counterparts. A common format used in
mathematical modeling of biological processes is SBML. We illustrate that the
use of SBML data in symbolic computation requires significant pre-processing,
incorporating external biological and mathematical expertise. ODEbase provides
high quality symbolic computation input data derived from established existing
biomodels, covering in particular the BioModels database.
","['\nChristoph Lüders\n', '\nThomas Sturm\n', '\nOvidiu Radulescu\n']",,,http://arxiv.org/abs/2201.08980v1,q-bio.MN,"['q-bio.MN', 'cs.SC']",,,[]
Boosting Isomorphic Model Filtering with Invariants,http://arxiv.org/abs/2201.10516v1,2022-01-21T11:18:30Z,2022-01-21T11:18:30Z,"  The enumeration of finite models is very important to the working discrete
mathematician (algebra, graph theory, etc) and hence the search for effective
methods to do this task is a critical goal in discrete computational
mathematics. However, it is hindered by the possible existence of many
isomorphic models, which usually only add noise. Typically, they are filtered
out {\em a posteriori}, a step that might take a long time just to discard
redundant models. This paper proposes a novel approach to split the generated
models into mutually non-isomorphic blocks. To do that we use well-designed
hand-crafted invariants as well as randomly generated invariants. The blocks
are then tackled separately and possibly in parallel. This approach is
integrated into Mace4 (the most popular tool among mathematicians) where it
shows tremendous speed-ups for a large variety of algebraic structures.
","['\nJoão Araújo\n', '\nChoiwah Chow\n', '\nMikoláš Janota\n']",,,http://arxiv.org/abs/2201.10516v1,cs.SC,"['cs.SC', 'cs.LO']",,,[]
"DFORMPY: A Python Library for visualising and zooming on differential
  forms",http://arxiv.org/abs/2201.10517v1,2022-01-16T10:51:04Z,2022-01-16T10:51:04Z,"  We present the v1.0.1 release of DFormPy, the first Python library providing
an interactive visualisation of differential forms. DFormPy is also capable of
exterior algebra and vector calculus, building on the capabilities of NumPy and
matplotlib. This short paper will demonstrate the functionalities of the
library, briefly outlining the mathematics involved with our objects and the
methods available to the user. DFormPy is an open source library with
interactive GUI released under MIT license at
https://github.com/MostaphaG/Summer_project-df
","['\nMoustafa Gharamti\n', '\nMaciej Jarema\n', '\nSamuel Kirwin-Jones\n']",,,http://arxiv.org/abs/2201.10517v1,cs.SC,"['cs.SC', 'physics.comp-ph']",,,[]
Zeon and Idem-Clifford Formulations of Hypergraph Problems,http://arxiv.org/abs/2201.05895v1,2022-01-15T17:34:33Z,2022-01-15T17:34:33Z,"  Zeon algebras have proven to be useful for enumerating structures in graphs,
such as paths, trails, cycles, matchings, cliques, and independent sets. In
contrast to an ordinary graph, in which each edge connects exactly two
vertices, an edge (or, ""hyperedge"") can join any number of vertices in a
hypergraph. In game theory, hypergraphs are called simple games. Hypergraphs
have been used for problems in biology, chemistry, image processing, wireless
networks, and more. In the current work, zeon (""nil-Clifford"") and
""idem-Clifford"" graph-theoretic methods are generalized to hypergraphs. In
particular, zeon and idem-Clifford methods are used to enumerate paths, trails,
independent sets, cliques, and matchings in hypergraphs. An approach for
finding minimum hypergraph transversals is developed, and zeon formulations of
some open hypergraph problems are presented.
","['\nSamuel Ewing\n', '\nG. Stacey Staples\n']",,,http://arxiv.org/abs/2201.05895v1,math.CO,"['math.CO', 'cs.SC', 'math.RA']",,,[]
Simple algorithm for GCD of polynomials,http://arxiv.org/abs/2201.06940v1,2022-01-06T21:49:55Z,2022-01-06T21:49:55Z,"  Based on the Bezout approach we propose a simple algorithm to determine the
{\tt gcd} of two polynomials which doesn't need division, like the Euclidean
algorithm, or determinant calculations, like the Sylvester matrix algorithm.
The algorithm needs only $n$ steps for polynomials of degree $n$. Formal
manipulations give the discriminant or the resultant for any degree without
needing division nor determinant calculation.
","['\nPasquale Nardone\n', '\nGiorgio Sonnino\n']","9 pages, 0 Figures",,http://arxiv.org/abs/2201.06940v1,cs.SC,['cs.SC'],,,[]
Sparse trace tests,http://arxiv.org/abs/2201.04268v1,2022-01-12T01:55:48Z,2022-01-12T01:55:48Z,"  We establish how the coefficients of a sparse polynomial system influence the
sum (or the trace) of its zeros. As an application, we develop numerical tests
for verifying whether a set of solutions to a sparse system is complete. These
algorithms extend the classical trace test in numerical algebraic geometry. Our
results rely on both the analysis of the structure of sparse resultants as well
as an extension of Esterov's results on monodromy groups of sparse systems.
","['\nTaylor Brysiewicz\n', '\nMichael Burr\n']",,,http://arxiv.org/abs/2201.04268v1,math.AG,"['math.AG', 'cs.SC', '65H14, 14Q65, 14M25, 68W30']",,,[]
An algebraic attack to the Bluetooth stream cipher E0,http://arxiv.org/abs/2201.01262v2,2022-01-04T17:53:57Z,2022-08-08T11:59:15Z,"  In this paper we study the security of the Bluetooth stream cipher E0 from
the viewpoint it is a ""difference stream cipher"", that is, it is defined by a
system of explicit difference equations over the finite field GF(2). This
approach highlights some issues of the Bluetooth encryption such as the
invertibility of its state transition map, a special set of 14 bits of its
132-bit state which when guessed implies linear equations among the other bits
and finally a small number of spurious keys, with 83 guessed bits, which are
compatible with a keystream of about 60 bits. Exploiting these issues, we
implement an algebraic attack using Gr\""obner bases, SAT solvers and Binary
Decision Diagrams. Testing activities suggest that the version based on
Gr\""obner bases is the best one and it is able to attack E0 in about 2^79
seconds on an Intel i9 CPU. To the best of our knowledge, this work improves
any previous attack based on a short keystream, hence fitting with Bluetooth
specifications.
","['\nRoberto La Scala\n', '\nSergio Polese\n', '\nSharwan K. Tiwari\n', '\nAndrea Visconti\n']","24 pages, 1 figure. To appear in Finite Fields and Their Applications",,http://arxiv.org/abs/2201.01262v2,cs.CR,"['cs.CR', 'cs.SC', 'math.AC', 'math.RA', '11T71 (Primary) 12H10, 13P10 (Secondary)']",,,[]
DPCL: a Language Template for Normative Specifications,http://arxiv.org/abs/2201.04477v1,2022-01-12T13:51:11Z,2022-01-12T13:51:11Z,"  Several solutions for specifying normative artefacts (norms, contracts,
policies) in a computational processable way have been presented in the
literature. Legal core ontologies have been proposed to systematize concepts
and relationships relevant to normative reasoning. However, no solution amongst
those has achieved general acceptance, and no common ground (representational,
computational) has been identified enabling us to easily compare them. Yet, all
these efforts share the same motivation of representing normative directives,
therefore it is plausible that there may be a representational model
encompassing all of them. This presentation will introduce DPCL, a
domain-specific language (DSL) for specifying higher-level policies (including
norms, contracts, etc.), centred on Hohfeld's framework of fundamental legal
concepts. DPCL has to be seen primarily as a ""template"", i.e. as an
informational model for architectural reference, rather than a fully-fledged
formal language; it aims to make explicit the general requirements that should
be expected in a language for norm specification. In this respect, it goes
rather in the direction of legal core ontologies, but differently from those,
our proposal aims to keep the character of a DSL, rather than a set of axioms
in a logical framework: it is meant to be cross-compiled to underlying
languages/tools adequate to the type of target application. We provide here an
overview of some of the language features.
","['\nGiovanni Sileno\n', '\nThomas van Binsbergen\n', '\nMatteo Pascucci\n', '\nTom van Engers\n']",position paper at ProLaLa workshop @ POPL2022,,http://arxiv.org/abs/2201.04477v1,cs.AI,"['cs.AI', 'cs.FL', 'cs.MA', 'cs.PL', 'cs.SC']",,,[]
Automated Code Optimization with E-Graphs,http://arxiv.org/abs/2112.14714v2,2021-12-26T12:49:18Z,2021-12-30T17:50:02Z,"  This thesis proposes an advanced, generic and high-level code rewriting and
analysis system in the Julia programming language, providing applied equality
saturation in the presence of multiple dispatch and metaprogramming. We show
how our system can practically solve some challenging problems: Can programmers
implement their own high-level compiler optimizations for their domain-specific
scientific programs, without the requirement of them being compiler experts at
all? Can these optimizers be implemented by users in the same language and
inside the same programs they want to optimize, solving the two-language
problem? Can these compiler optimizers be written in a high-level fashion, as
equations, without the need to worry about the rewriting ordering? Thus, can
symbolic mathematics do high-level compiler optimizations or vice-versa?
",['\nAlessandro Cheli\n'],"Bachelor Thesis in Computer Science, University of Pisa",,http://arxiv.org/abs/2112.14714v2,cs.PL,"['cs.PL', 'cs.SC', 'I.1.0; I.1.2; I.1.3; D.3.2; D.3.3; D.3.4']",,,[]
"Stability analysis of heterogeneous oligopoly games of increasing
  players with quadratic costs",http://arxiv.org/abs/2112.13844v1,2021-12-24T03:52:20Z,2021-12-24T03:52:20Z,"  In this discussion draft, we explore heterogeneous oligopoly games of
increasing players with quadratic costs, where the market is supposed to have
the isoelastic demand. For each of the models considered in this draft, we
analytically investigate the necessary and sufficient condition of the local
stability of its positive equilibrium. Furthermore, we rigorously prove that
the stability regions are enlarged as the number of involved firms is
increasing.
",['\nXiaoliang Li\n'],,,http://arxiv.org/abs/2112.13844v1,econ.TH,"['econ.TH', 'cs.SC', 'math.DS']",,,[]
"Proceedings of the 13th International Conference on Automated Deduction
  in Geometry",http://arxiv.org/abs/2112.14770v1,2021-12-28T21:56:13Z,2021-12-28T21:56:13Z,"  Automated Deduction in Geometry (ADG) is a forum to exchange ideas and views,
to present research results and progress, and to demonstrate software tools at
the intersection between geometry and automated deduction. Relevant topics
include (but are not limited to): polynomial algebra, invariant and
coordinate-free methods; probabilistic, synthetic, and logic approaches,
techniques for automated geometric reasoning from discrete mathematics,
combinatorics, and numerics; interactive theorem proving in geometry; symbolic
and numeric methods for geometric computation, geometric constraint solving,
automated generation/reasoning and manipulation with diagrams; design and
implementation of geometry software, automated theorem provers, special-purpose
tools, experimental studies; applications of ADG in mechanics, geometric
modelling, CAGD/CAD, computer vision, robotics and education.
  Traditionally, the ADG conference is held every two years. The previous
editions of ADG were held in Nanning in 2018, Strasbourg in 2016, Coimbra in
2014, Edinburgh in 2012, Munich in 2010, Shanghai in 2008, Pontevedra in 2006,
Gainesville in 2004, Hagenberg in 2002, Zurich in 2000, Beijing in 1998, and
Toulouse in 1996. The 13th edition of ADG was supposed to be held in 2020 in
Hagenberg, Austria, but due to the COVID-19 pandemic, it was postponed for
2021, and held online (still hosted by RISC Institute, Hagenberg, Austria),
September 15-17, 2021 (https://www.risc.jku.at/conferences/adg2021).
","['\nPredrag Janičić\n', '\nZoltán Kovács\n']",,"EPTCS 352, 2021",http://dx.doi.org/10.4204/EPTCS.352,cs.AI,"['cs.AI', 'cs.LO', 'cs.MS', 'cs.SC']",10.4204/EPTCS.352,,[]
Subresultant of several univariate polynomials,http://arxiv.org/abs/2112.15370v4,2021-12-31T10:20:52Z,2023-04-27T00:39:41Z,"  Subresultant of two univariate polynomials is a fundamental object in
computational algebra and geometry with many applications (for instance,
parametric GCD and parametric multiplicity of roots). In this paper, we
generalize the theory of subresultants of two polynomials to arbitrary number
of polynomials, resulting in multi-polynomial subresultants. Specifically,
  1. we propose a definition of multi-polynomial subresultants, which is an
expression in terms of roots;
  2. we illustrate the usefulness of the proposed definition via the following
two fundamental applications:
  - parametric GCD of multi-polynomials, and
  - parametric multiplicity of roots of a polynomial;
  3. we provide several expressions for the multi-polynomials subresultants in
terms of coefficients, for computation.
","['\nHoon Hong\n', '\nJing Yang\n']",,,http://arxiv.org/abs/2112.15370v4,cs.SC,"['cs.SC', 'math.AC', 'math.AG']",,,[]
Proceedings Twelfth International Workshop on Graph Computational Models,http://arxiv.org/abs/2112.10217v1,2021-12-19T18:24:10Z,2021-12-19T18:24:10Z,"  This volume contains the post-proceedings of the Twelfth International
Workshop on Graph Computation Models (GCM 2021). The workshop was part of STAF
2021 (Software Technologies: Applications and Foundations) as an
online-workshop on 22nd June 2021.
  Graphs are common mathematical structures that are visual and intuitive. They
constitute a natural and seamless way for system modelling in science,
engineering and beyond, including computer science, biology, business process
modelling, etc. Graph computation models constitute a class of very high-level
models where graphs are first-class citizens. The aim of the International GCM
Workshop series is to bring together researchers interested in all aspects of
computation models based on graphs and graph transformation. It promotes the
cross-fertilizing exchange of ideas and experiences among senior and young
researchers from the different communities interested in the foundations,
applications, and implementations of graph computation models and related
areas.
","['\nBerthold Hoffmann\n', '\nMark Minas\n']",,"EPTCS 350, 2021",http://dx.doi.org/10.4204/EPTCS.350,cs.FL,"['cs.FL', 'cs.SC']",10.4204/EPTCS.350,,[]
"An algebraic attack on stream ciphers with application to nonlinear
  filter generators and WG-PRNG",http://arxiv.org/abs/2112.12268v2,2021-12-22T23:13:45Z,2022-01-21T17:34:06Z,"  In this paper, we propose a new algebraic attack on stream ciphers. Starting
from the well-known attack due to Courtois and Meier, we design an attack
especially effective against nonlinear filter generators. We test it on two toy
stream ciphers and we show that the level of security of one of stream ciphers
submitted to the NIST competition on Lightweight Cryptography, WG-PRNG, is less
than that stated before now.
","['\nCarla Mascia\n', '\nEnrico Piccione\n', '\nMassimiliano Sala\n']",,,http://arxiv.org/abs/2112.12268v2,cs.CR,"['cs.CR', 'cs.SC', '94A60, 13P10, 11T71, 06E30', 'E.3']",,,[]
The complexity of solving Weil restriction systems,http://arxiv.org/abs/2112.10506v2,2021-12-20T12:59:50Z,2023-02-03T14:04:25Z,"  The solving degree of a system of multivariate polynomial equations provides
an upper bound for the complexity of computing the solutions of the system via
Groebner bases methods. In this paper, we consider polynomial systems that are
obtained via Weil restriction of scalars. The latter is an arithmetic
construction which, given a finite Galois field extension $k\hookrightarrow K$,
associates to a system $\mathcal{F}$ defined over $K$ a system
$\mathrm{Weil}(\mathcal{F})$ defined over $k$, in such a way that the solutions
of $\mathcal{F}$ over $K$ and those of $\mathrm{Weil}(\mathcal{F})$ over $k$
are in natural bijection. In this paper, we find upper bounds for the
complexity of solving a polynomial system $\mathrm{Weil}(\mathcal{F})$ obtained
via Weil restriction in terms of algebraic invariants of the system
$\mathcal{F}$.
","['\nAlessio Caminata\n', '\nMichela Ceria\n', '\nElisa Gorla\n']",Final version. To appear in Journal of Algebra,,http://arxiv.org/abs/2112.10506v2,cs.CR,"['cs.CR', 'cs.SC', 'math.AC']",,,[]
FuSeBMC v.4: Smart Seed Generation for Hybrid Fuzzing,http://arxiv.org/abs/2112.10627v1,2021-12-20T15:41:57Z,2021-12-20T15:41:57Z,"  FuSeBMC is a test generator for finding security vulnerabilities in C
programs. In earlier work [4], we described a previous version that
incrementally injected labels to guide Bounded Model Checking (BMC) and
Evolutionary Fuzzing engines to produce test cases for code coverage and bug
finding. This paper introduces a new version of FuSeBMC that utilizes both
engines to produce smart seeds. First, the engines are run with a short time
limit on a lightly instrumented version of the program to produce the seeds.
The BMC engine is particularly useful in producing seeds that can pass through
complex mathematical guards. Then, FuSeBMC runs its engines with more extended
time limits using the smart seeds created in the previous round. FuSeBMC
manages this process in two main ways using its Tracer subsystem. Firstly, it
uses shared memory to record the labels covered by each test case. Secondly, it
evaluates test cases, and those of high impact are turned into seeds for
subsequent test fuzzing. As a result, we significantly increased our code
coverage score from last year, outperforming all tools that participated in
this year's competition in every single category.
","['\nKaled M. Alshmrany\n', '\nMohannad Aldughaim\n', '\nAhmed Bhayat\n', '\nLucas C. Cordeiro\n']","4 pages, 2 figures, International Conference on Fundamental
  Approaches to Software Engineering (FASE 2022)",,http://arxiv.org/abs/2112.10627v1,cs.CR,"['cs.CR', 'cs.CY', 'cs.SC', 'cs.SE']",,,[]
An ASP-based Approach to Answering Natural Language Questions for Texts,http://arxiv.org/abs/2112.11241v1,2021-12-21T14:13:06Z,2021-12-21T14:13:06Z,"  An approach based on answer set programming (ASP) is proposed in this paper
for representing knowledge generated from natural language texts. Knowledge in
a text is modeled using a Neo Davidsonian-like formalism, which is then
represented as an answer set program. Relevant commonsense knowledge is
additionally imported from resources such as WordNet and represented in ASP.
The resulting knowledge-base can then be used to perform reasoning with the
help of an ASP system. This approach can facilitate many natural language tasks
such as automated question answering, text summarization, and automated
question generation. ASP-based representation of techniques such as default
reasoning, hierarchical knowledge organization, preferences over defaults,
etc., are used to model commonsense reasoning methods required to accomplish
these tasks. In this paper, we describe the CASPR system that we have developed
to automate the task of answering natural language questions given English
text. CASPR can be regarded as a system that answers questions by
""understanding"" the text and has been tested on the SQuAD data set, with
promising results.
","['\nDhruva Pendharkar\n', '\nKinjal Basu\n', '\nFarhad Shakerin\n', '\nGopal Gupta\n']",,,http://arxiv.org/abs/2112.11241v1,cs.CL,"['cs.CL', 'cs.LO', 'cs.SC']",,,[]
"Analytical Modelling of Exoplanet Transit Specroscopy with Dimensional
  Analysis and Symbolic Regression",http://arxiv.org/abs/2112.11600v1,2021-12-22T00:52:56Z,2021-12-22T00:52:56Z,"  The physical characteristics and atmospheric chemical composition of newly
discovered exoplanets are often inferred from their transit spectra which are
obtained from complex numerical models of radiative transfer. Alternatively,
simple analytical expressions provide insightful physical intuition into the
relevant atmospheric processes. The deep learning revolution has opened the
door for deriving such analytical results directly with a computer algorithm
fitting to the data. As a proof of concept, we successfully demonstrate the use
of symbolic regression on synthetic data for the transit radii of generic hot
Jupiter exoplanets to derive a corresponding analytical formula. As a
preprocessing step, we use dimensional analysis to identify the relevant
dimensionless combinations of variables and reduce the number of independent
inputs, which improves the performance of the symbolic regression. The
dimensional analysis also allowed us to mathematically derive and properly
parametrize the most general family of degeneracies among the input atmospheric
parameters which affect the characterization of an exoplanet atmosphere through
transit spectroscopy.
","['\nKonstantin T. Matchev\n', '\nKatia Matcheva\n', '\nAlexander Roman\n']","Submitted to AAS Journals, 24 pages, 7 figures",,http://arxiv.org/abs/2112.11600v1,astro-ph.EP,"['astro-ph.EP', 'cs.LG', 'cs.SC', 'physics.data-an']",,,[]
Marginal Independence Models,http://arxiv.org/abs/2112.10287v2,2021-12-19T23:58:17Z,2022-05-10T19:10:22Z,"  We impose rank one constraints on marginalizations of a tensor, given by a
simplicial complex. Following work of Kirkup and Sullivant, such marginal
independence models can be made toric by a linear change of coordinates. We
study their toric ideals, with emphasis on random graph models and independent
set polytopes of matroids. We develop the numerical algebra of parameter
estimation, using both Euclidean distance and maximum likelihood, and we
present a comprehensive database of small models.
","['\nTobias Boege\n', '\nSonja Petrović\n', '\nBernd Sturmfels\n']",Revised to include more details in proof. Prepared for ISSAC '22,,http://dx.doi.org/10.1145/3476446.3536193,math.ST,"['math.ST', 'cs.SC', 'math.AG', 'math.OC', 'stat.TH', '62R01']",10.1145/3476446.3536193,,[]
Combining Sub-Symbolic and Symbolic Methods for Explainability,http://arxiv.org/abs/2112.01844v1,2021-12-03T10:57:00Z,2021-12-03T10:57:00Z,"  Similarly to other connectionist models, Graph Neural Networks (GNNs) lack
transparency in their decision-making. A number of sub-symbolic approaches have
been developed to provide insights into the GNN decision making process. These
are first important steps on the way to explainability, but the generated
explanations are often hard to understand for users that are not AI experts. To
overcome this problem, we introduce a conceptual approach combining
sub-symbolic and symbolic methods for human-centric explanations, that
incorporate domain knowledge and causality. We furthermore introduce the notion
of fidelity as a metric for evaluating how close the explanation is to the
GNN's internal decision making process. The evaluation with a chemical dataset
and ontology shows the explanatory value and reliability of our method.
","['\nAnna Himmelhuber\n', '\nStephan Grimm\n', '\nSonja Zillner\n', '\nMitchell Joblin\n', '\nMartin Ringsquandl\n', '\nThomas Runkler\n']",RuleML+RR 2021,,http://arxiv.org/abs/2112.01844v1,cs.AI,"['cs.AI', 'cs.LG', 'cs.SC']",,,[]
Neuro-Symbolic Inductive Logic Programming with Logical Neural Networks,http://arxiv.org/abs/2112.03324v1,2021-12-06T19:38:30Z,2021-12-06T19:38:30Z,"  Recent work on neuro-symbolic inductive logic programming has led to
promising approaches that can learn explanatory rules from noisy, real-world
data. While some proposals approximate logical operators with differentiable
operators from fuzzy or real-valued logic that are parameter-free thus
diminishing their capacity to fit the data, other approaches are only loosely
based on logic making it difficult to interpret the learned ""rules"". In this
paper, we propose learning rules with the recently proposed logical neural
networks (LNN). Compared to others, LNNs offer strong connection to classical
Boolean logic thus allowing for precise interpretation of learned rules while
harboring parameters that can be trained with gradient-based optimization to
effectively fit the data. We extend LNNs to induce rules in first-order logic.
Our experiments on standard benchmarking tasks confirm that LNN rules are
highly interpretable and can achieve comparable or higher accuracy due to their
flexible parameterization.
","['\nPrithviraj Sen\n', '\nBreno W. S. R. de Carvalho\n', '\nRyan Riegel\n', '\nAlexander Gray\n']",,,http://arxiv.org/abs/2112.03324v1,cs.AI,"['cs.AI', 'cs.LG', 'cs.LO', 'cs.SC']",,,[]
The VLSAT-3 Benchmark Suite,http://arxiv.org/abs/2112.03675v1,2021-12-07T13:23:51Z,2021-12-07T13:23:51Z,"  This report presents VLSAT-3 (an acronym for ""Very Large Boolean
SATisfiability problems""),the third part of a benchmark suite to be used in
scientific experimentsand software competitions addressing SAT and SMT
(Satisfiability Modulo Theories) solving issues.VLSAT-3 contains 1200
(600~satisfiable and 600~unsatisfiable) quantifier-free first-order logic
formulasof increasing complexity, proposed in SMT-LIB format under a permissive
Creative Commons license.More than 90% of these benchmarks have been used
during the 16th International Satisfiability Modulo TheoriesCompetition
(SMT-COMP~2021).
",['\nPierre Bouvier\nCONVECS\n'],"arXiv admin note: text overlap with arXiv:2011.11049,
  arXiv:2110.06336",,http://arxiv.org/abs/2112.03675v1,cs.DS,"['cs.DS', 'cs.FL', 'cs.SC']",,,['CONVECS']
"Explicit Bounds for Linear Forms in the Exponentials of Algebraic
  Numbers",http://arxiv.org/abs/2112.05004v2,2021-12-09T15:58:04Z,2022-05-16T05:20:39Z,"  In this paper, we study linear forms \[\lambda =
\beta_1\mathrm{e}^{\alpha_1}+\cdots+\beta_m\mathrm{e}^{\alpha_m},\] where
$\alpha_i$ and $\beta_i$ are algebraic numbers. An explicit lower bound for the
absolute value of $\lambda$ is proved, which is derived from ""th\'eor\`eme de
Lindemann--Weierstrass effectif"" via constructive methods in algebraic
computation. Besides, the existence of $\lambda$ with an explicit upper bound
is established on the result of counting algebraic numbers.
",['\nCheng-Chao Huang\n'],,,http://arxiv.org/abs/2112.05004v2,math.NT,"['math.NT', 'cs.CC', 'cs.SC']",,,[]
"Polynomial XL: A Variant of the XL Algorithm Using Macaulay Matrices
  over Polynomial Rings",http://arxiv.org/abs/2112.05023v1,2021-12-09T16:30:48Z,2021-12-09T16:30:48Z,"  Solving a system of $m$ multivariate quadratic equations in $n$ variables
(the $\mathcal MQ$ problem) is one of the main challenges of algebraic
cryptanalysis. The XL algorithm (XL for short) is a major approach for solving
the $\mathcal MQ$ problem with linearization over a coefficient field.
Furthermore, the hybrid approach with XL (h-XL) is a variant of XL guessing
some variables beforehand. In this paper, we present a variant of h-XL, which
we call the polynomial XL (PXL). In PXL, the whole $n$ variables are divided
into $k$ variables to be fixed and the remaining $n-k$ variables as ""main
variables"", and we generate the Macaulay matrix with respect to the $n-k$ main
variables over a polynomial ring of the $k$ variables. By eliminating some
columns of the Macaulay matrix over the polynomial ring before guessing $k$
variables, the amount of manipulations required for each guessed value can be
reduced. Our complexity analysis indicates that PXL is efficient on the system
with $n \approx m$. For example, on systems over ${\mathbb F}_{2^8}$ with
$n=m=80$, the number of manipulations required by the hybrid approaches with XL
and Wiedemann XL and PXL is estimated as $2^{252}$, $2^{234}$, and $2^{220}$,
respectively.
","['\nHiroki Furue\n', '\nMomonari Kudo\n']","28 pages, 1 figure",,http://arxiv.org/abs/2112.05023v1,cs.SC,"['cs.SC', 'cs.CR', 'math.AC']",,,[]
"Solving degree, last fall degree, and related invariants",http://arxiv.org/abs/2112.05579v2,2021-12-10T14:46:11Z,2022-06-01T13:41:42Z,"  In this paper we study and relate several invariants connected to the solving
degree of a polynomial system. This provides a rigorous framework for
estimating the complexity of solving a system of polynomial equations via
Groebner bases methods. Our main results include a connection between the
solving degree and the last fall degree and one between the degree of
regularity and the Castelnuovo-Mumford regularity.
","['\nAlessio Caminata\n', '\nElisa Gorla\n']",Final version. To appear in Journal of Symbolic Computation,,http://arxiv.org/abs/2112.05579v2,cs.CR,"['cs.CR', 'cs.SC', 'math.AC']",,,[]
"Stability of Cournot duopoly games with isoelastic demands and quadratic
  costs",http://arxiv.org/abs/2112.05948v2,2021-12-11T10:52:07Z,2022-03-18T04:25:06Z,"  In this discussion draft, we explore different duopoly games of players with
quadratic costs, where the market is supposed to have the isoelastic demand.
Different from the usual approaches based on numerical computations, the
methods used in the present work are built on symbolic computations, which can
produce analytical and rigorous results. Our investigations show that the
stability regions are enlarged for the games considered in this work compared
to their counterparts with linear costs, which generalizes the classical
results of ""F. M. Fisher. The stability of the Cournot oligopoly solution: The
effects of speeds of adjustment and increasing marginal costs. The Review of
Economic Studies, 28(2):125--135, 1961."".
","['\nXiaoliang Li\n', '\nLi Su\n']",,,http://arxiv.org/abs/2112.05948v2,cs.SC,"['cs.SC', 'econ.TH', 'math.DS']",,,[]
"Analysis of stability and bifurcation for two heterogeneous triopoly
  games with the isoelastic demand",http://arxiv.org/abs/2112.05950v1,2021-12-11T11:01:20Z,2021-12-11T11:01:20Z,"  In this paper, we investigate two heterogeneous triopoly games where the
demand function of the market is isoelastic. The local stability and the
bifurcation of these games are systematically analyzed using the symbolic
approach proposed by the author. The novelty of the present work is twofold. On
one hand, the results of this paper are analytical, which are different from
the existing results in the literature based on observations through numerical
simulations. In particular, we rigorously prove the existence of double routes
to chaos through the period-doubling bifurcation and through the Neimark-Sacker
bifurcation. On the other hand, for the special case of the involved firms
having identical marginal costs, we acquire the necessary and sufficient
conditions of the local stability for both models. By further analyzing these
conditions, it seems that that the presence of the local monopolistic
approximation (LMA) mechanism might have a stabilizing effect for heterogeneous
triopoly games with the isoelastic demand.
",['\nXiaoliang Li\n'],,,http://arxiv.org/abs/2112.05950v1,math.DS,"['math.DS', 'cs.SC', 'econ.TH']",,,[]
"Quasi-equivalence of heights in algebraic function fields of one
  variable",http://arxiv.org/abs/2111.13025v1,2021-11-25T11:09:39Z,2021-11-25T11:09:39Z,"  For points $(a,b)$ on an algebraic curve over a field $K$ with height
$\mathfrak{h}$, the asymptotic relation between $\mathfrak{h}(a)$ and
$\mathfrak{h}(b)$ has been extensively studied in diophantine geometry. When
$K=\overline{k(t)}$ is the field of algebraic functions in $t$ over a field $k$
of characteristic zero, Eremenko in 1998 proved the following quasi-equivalence
for an absolute logarithmic height $\mathfrak{h}$ in $K$: Given $P\in K[X,Y]$
irreducible over $K$ and $\epsilon>0$, there is a constant $C$ only depending
on $P$ and $\epsilon$ such that for each $(a,b)\in K^2$ with $P(a,b)=0$, $$
  (1-\epsilon) \deg(P,Y) \mathfrak{h}(b)-C \leq \deg(P,X) \mathfrak{h}(a) \leq
(1+\epsilon) \deg(P,Y) \mathfrak{h}(b)+C. $$ In this article, we shall give an
explicit bound for the constant $C$ in terms of the total degree of $P$, the
height of $P$ and $\epsilon$. This result is expected to have applications in
some other areas such as symbolic computation of differential and difference
equations.
","['\nRuyong Feng\n', '\nShuang Feng\n', '\nLi-Yong Shen\n']",,,http://arxiv.org/abs/2111.13025v1,cs.SC,['cs.SC'],,,[]
Computing with B-series,http://arxiv.org/abs/2111.11680v2,2021-11-23T06:55:29Z,2022-11-15T11:45:27Z,"  We present BSeries.jl, a Julia package for the computation and manipulation
of B-series, which are a versatile theoretical tool for understanding and
designing discretizations of differential equations. We give a short
introduction to the theory of B-series and associated concepts and provide
examples of their use, including method composition and backward error
analysis. The associated software is highly performant and makes it possible to
work with B-series of high order.
","['\nDavid I. Ketcheson\n', '\nHendrik Ranocha\n']",,"ACM Transactions on Mathematical Software, 2022",http://dx.doi.org/10.1145/3573384,math.NA,"['math.NA', 'cs.MS', 'cs.NA', 'cs.SC']",10.1145/3573384,,[]
From Kepler to Newton: Explainable AI for Science,http://arxiv.org/abs/2111.12210v7,2021-11-24T00:45:27Z,2023-01-23T17:58:01Z,"  The Observation--Hypothesis--Prediction--Experimentation loop paradigm for
scientific research has been practiced by researchers for years towards
scientific discoveries. However, with data explosion in both mega-scale and
milli-scale scientific research, it has been sometimes very difficult to
manually analyze the data and propose new hypotheses to drive the cycle for
scientific discovery. In this paper, we discuss the role of Explainable AI in
scientific discovery process by demonstrating an Explainable AI-based paradigm
for science discovery. The key is to use Explainable AI to help derive data or
model interpretations, hypotheses, as well as scientific discoveries or
insights. We show how computational and data-intensive methodology -- together
with experimental and theoretical methodology -- can be seamlessly integrated
for scientific research. To demonstrate the AI-based science discovery process,
and to pay our respect to some of the greatest minds in human history, we show
how Kepler's laws of planetary motion and Newton's law of universal gravitation
can be rediscovered by (Explainable) AI based on Tycho Brahe's astronomical
observation data, whose works were leading the scientific revolution in the
16-17th century. This work also highlights the important role of Explainable AI
(as compared to Blackbox AI) in science discovery to help humans prevent or
better prepare for the possible technological singularity that may happen in
the future, since science is not only about the know how, but also the know
why. Presentation of the work is available at
https://slideslive.com/38986142/from-kepler-to-newton-explainable-ai-for-science-discovery.
","['\nZelong Li\n', '\nJianchao Ji\n', '\nYongfeng Zhang\n']",Accepted by ICML-AI4Science 2022,,http://arxiv.org/abs/2111.12210v7,cs.AI,"['cs.AI', 'cs.LG', 'cs.SC']",,,[]
Hypergeometric Structures in Feynman Integrals,http://arxiv.org/abs/2111.15501v1,2021-11-30T15:37:55Z,2021-11-30T15:37:55Z,"  Hypergeometric structures in single and multiscale Feynman integrals emerge
in a wide class of topologies. Using integration-by-parts relations, associated
master or scalar integrals have to be calculated. For this purpose it appears
useful to devise an automated method which recognizes the respective (partial)
differential equations related to the corresponding higher transcendental
functions. We solve these equations through associated recursions of the
expansion coefficient of the multivalued formal Taylor series. The expansion
coefficients can be determined using either the package {\tt Sigma} in the case
of linear difference equations or by applying heuristic methods in the case of
partial linear difference equations. In the present context a new type of sums
occurs, the Hurwitz harmonic sums, and generalized versions of them. The code
{\tt HypSeries} transforming classes of differential equations into analytic
series expansions is described. Also partial difference equations having
rational solutions and rational function solutions of Pochhammer symbols are
considered, for which the code {\tt solvePartialLDE} is designed. Generalized
hypergeometric functions, Appell-,~Kamp\'e de F\'eriet-, Horn-,
Lauricella-Saran-, Srivasta-, and Exton--type functions are considered. We
illustrate the algorithms by examples.
","['\nJ. Blümlein\n', '\nM. Saragnese\n', '\nC. Schneider\n']","55 pages, several anc. files",,http://arxiv.org/abs/2111.15501v1,math-ph,"['math-ph', 'cs.SC', 'hep-ph', 'hep-th', 'math.MP']",,,[]
"A fast algorithm for computing the Smith normal form with multipliers
  for a nonsingular integer matrix",http://arxiv.org/abs/2111.09949v2,2021-11-18T21:26:47Z,2022-09-21T22:17:59Z,"  A Las Vegas randomized algorithm is given to compute the Smith multipliers
for a nonsingular integer matrix $A$, that is, unimodular matrices $U$ and $V$
such that $AV=US$, with $S$ the Smith normal form of $A$. The expected running
time of the algorithm is about the same as required to multiply together two
matrices of the same dimension and size of entries as $A$. Explicit bounds are
given for the size of the entries in both unimodular multipliers. The main tool
used by the algorithm is the Smith massager, a relaxed version of $V$, the
unimodular matrix specifying the column operations of the Smith computation.
From the perspective of efficiency, the main tools used are fast linear solving
and partial linearization of integer matrices. As an application of the Smith
with multipliers algorithm, a fast algorithm is given to find the fractional
part of the inverse of the input matrix.
","['\nStavros Birmpilis\n', '\nGeorge Labahn\n', '\nArne Storjohann\n']",41 pages,,http://arxiv.org/abs/2111.09949v2,cs.SC,"['cs.SC', '68W30']",,,[]
Solving sums of squares in global fields,http://arxiv.org/abs/2111.08558v1,2021-11-16T15:38:10Z,2021-11-16T15:38:10Z,"  The problem of writing a totally positive element as a sum of squares has a
long history in mathematics, going back to Bachet and Lagrange. While for some
specific rings (like integers or polynomials over the rationals), there are
known methods for decomposing an element into a sum of squares, in general, for
many other important rings and fields, the problem is still widely open. In
this paper, we present an explicit algorithm for decomposing an element of an
arbitrary global field (either a number field or a global function field) into
a sum of squares of minimal length.
",['\nPrzemysław Koprowski\n'],,,http://arxiv.org/abs/2111.08558v1,math.NT,"['math.NT', 'cs.SC', '11E25, 11Y40, 11E12']",,,[]
Isotropic vectors over global fields,http://arxiv.org/abs/2111.08569v1,2021-11-16T15:43:17Z,2021-11-16T15:43:17Z,"  We present a complete suite of algorithms for finding isotropic vectors of
quadratic forms (of any dimension) over an arbitrary global field of
characteristic different from 2.
",['\nPrzemysław Koprowski\n'],,,http://arxiv.org/abs/2111.08569v1,math.NT,"['math.NT', 'cs.SC', '11E12, 11E20, 11Y40, 11Y50']",,,[]
"NRPyLaTeX: A LaTeX interface to computer algebra systems for general
  relativity",http://arxiv.org/abs/2111.05861v2,2021-11-10T19:00:00Z,2022-05-12T18:19:08Z,"  While each computer algebra system (CAS) contains its own unique syntax for
inputting mathematical expressions, LaTeX is perhaps the most widespread
language for typesetting mathematics. NRPyLaTeX (NL) enables direct LaTeX input
of complex tensorial expressions (written in Einstein notation) relevant to
general relativity and differential geometry into the SymPy CAS. As SymPy also
supports output compatible with the Mathematica and Maple CASs, NL lowers the
learning curve for inputting and manipulating tensorial expressions in three
widely used CASs. LaTeX however is a typesetting language, and as such is not
designed to resolve ambiguities in mathematical expressions. To address this,
NL implements a convenient configuration interface that, e.g., defines
variables with certain attributes. Configuration commands appear as LaTeX
comments, so that entire NL workflows can fit seamlessly into the LaTeX source
code of scientific papers without interfering with the rendered mathematical
expressions. Further, NL adopts NRPy+'s rigid syntax for indexed symbols (e.g.,
tensors), which enables NL output to be directly converted into highly
optimized C/C++-code kernels using NRPy+. Finally NL has robust and
user-friendly error-handling, which catches common tensor indexing errors and
reports unresolved ambiguities, further expediting the input and validation of
LaTeX expressions into a CAS.
","['\nKenneth J. Sible\n', '\nZachariah B. Etienne\n']","17 pages, 10 figures",,http://arxiv.org/abs/2111.05861v2,gr-qc,"['gr-qc', 'astro-ph.HE', 'cs.SC']",,,[]
"A Maude Implementation of Rewritable Petri Nets: a Feasible Model for
  Dynamically Reconfigurable Systems",http://arxiv.org/abs/2111.08205v1,2021-11-16T03:10:10Z,2021-11-16T03:10:10Z,"  Petri Nets (PN) are a central, theoretically sound model for concurrent or
distributed systems but, at least in their classical definition, not expressive
enough to represent dynamic reconfiguration capabilities. On the other side,
Rewriting Logic has proved to be a natural semantic framework for several
formal models of concurrent/distributed systems. We propose a compact,
efficient Maude formalization of dynamically reconfigurable PT nets (with
inhibitor arcs), using as a running example the specification of a simple,
fault-tolerant manufacturing system. We discuss the advantages of such a
combined approach, as well as some concerns that it raises.
","['\nLorenzo Capra\nDipartimento di Informatica, Università degli Studi di Milano\n']","In Proceedings AppFM 2021, arXiv:2111.07538","EPTCS 349, 2021, pp. 31-49",http://dx.doi.org/10.4204/EPTCS.349.3,cs.LO,"['cs.LO', 'cs.SC', 'cs.SE']",10.4204/EPTCS.349.3,,"['Dipartimento di Informatica, Università degli Studi di Milano']"
Multiplicity structure of the arc space of a fat point,http://arxiv.org/abs/2111.10446v4,2021-11-19T21:42:24Z,2024-02-20T23:21:35Z,"  The equation $x^m = 0$ defines a fat point on a line. The algebra of regular
functions on the arc space of this scheme is the quotient of $k[x, x', x^{(2)},
\ldots]$ by all differential consequences of $x^m = 0$. This
infinite-dimensional algebra admits a natural filtration by finite dimensional
algebras corresponding to the truncations of arcs. We show that the generating
series for their dimensions equals $\frac{m}{1 - mt}$. We also determine the
lexicographic initial ideal of the defining ideal of the arc space. These
results are motivated by nonreduced version of the geometric motivic Poincar\'e
series, multiplicities in differential algebra, and connections between arc
spaces and the Rogers-Ramanujan identities. We also prove a recent conjecture
put forth by Afsharijoo in the latter context.
","['\nRida Ait El Manssour\n', '\nGleb Pogudin\n']",,,http://arxiv.org/abs/2111.10446v4,math.AG,"['math.AG', 'cs.SC', 'math.AC', 'math.CO', '12H05, 13D40, 05A17']",,,[]
"SimpleTensor -- a user-friendly Mathematica package for elementary
  tensor and differential-geometric calculations",http://arxiv.org/abs/2111.06718v1,2021-11-12T13:37:49Z,2021-11-12T13:37:49Z,"  In this paper we present a short overview of the new Wolfram Mathematica
package intended for elementary ""in-basis"" tensor and differential-geometric
calculations. In contrast to alternatives our package is designed to be
easy-to-use, short, all-purpose, and hackable. It supports tensor contractions
using Einstein notation, transformations between different bases, tensor
derivative operator, expansion in basis vectors and forms, exterior derivative,
and interior product.
",['\nD. O. Rybalka\n'],13 pages,,http://arxiv.org/abs/2111.06718v1,nucl-th,"['nucl-th', 'cs.MS', 'cs.SC', 'hep-th', 'physics.comp-ph']",,,[]
"A Symbolic Approach to Detecting Hardware Trojans Triggered by Don't
  Care Transitions",http://arxiv.org/abs/2111.03989v1,2021-11-07T03:08:02Z,2021-11-07T03:08:02Z,"  Due to the globalization of Integrated Circuit (IC) supply chain, hardware
trojans and the attacks that can trigger them have become an important security
issue. One type of hardware Trojans leverages the don't care transitions in
Finite State Machines (FSMs) of hardware designs. In this paper, we present a
symbolic approach to detecting don't care transitions and the hidden Trojans.
Our detection approach works at both RTL and gate-level, does not require a
golden design, and works in three stages. In the first stage, it explores the
reachable states. In the second stage, it performs an approximate analysis to
find the don't care transitions. In the third stage, it performs a state-space
exploration from reachable states that have incoming don't care transitions to
find behavioral discrepancies with respect to what has been observed in the
first stage. We also present a pruning technique based on the reachability of
FSM states. We present a methodology that leverages both RTL and gate-level for
soundness and efficiency. Specifically, we show that don't care transitions
must be detected at the gate-level, i.e., after synthesis has been performed,
for soundness. However, under specific conditions, Trojan detection can be
performed more efficiently at RTL. Evaluation of our approach on a set of
benchmarks from OpenCores and TrustHub and using gate-level representation
generated by two synthesis tools, Yosys and Synopsis Design Compiler (SDC),
shows that our approach is both efficient (up to 10X speedup w.r.t. no pruning)
and precise (0% false positives) in detecting don't care transitions and the
Trojans that leverage them. Additionally, the total analysis time can achieve
up to 3.40X (using Yosys) and 2.52X (SDC) speedup when synthesis preserves the
FSM structure and the Trojan detection is performed at RTL.
","['\nRuochen Dai\n', '\nTuba Yavuz\n']",,,http://arxiv.org/abs/2111.03989v1,cs.SC,['cs.SC'],,,[]
"MultiplexNet: Towards Fully Satisfied Logical Constraints in Neural
  Networks",http://arxiv.org/abs/2111.01564v1,2021-11-02T12:39:21Z,2021-11-02T12:39:21Z,"  We propose a novel way to incorporate expert knowledge into the training of
deep neural networks. Many approaches encode domain constraints directly into
the network architecture, requiring non-trivial or domain-specific engineering.
In contrast, our approach, called MultiplexNet, represents domain knowledge as
a logical formula in disjunctive normal form (DNF) which is easy to encode and
to elicit from human experts. It introduces a Categorical latent variable that
learns to choose which constraint term optimizes the error function of the
network and it compiles the constraints directly into the output of existing
learning algorithms. We demonstrate the efficacy of this approach empirically
on several classical deep learning tasks, such as density estimation and
classification in both supervised and unsupervised settings where prior
knowledge about the domains was expressed as logical constraints. Our results
show that the MultiplexNet approach learned to approximate unknown
distributions well, often requiring fewer data samples than the alternative
approaches. In some cases, MultiplexNet finds better solutions than the
baselines; or solutions that could not be achieved with the alternative
approaches. Our contribution is in encoding domain knowledge in a way that
facilitates inference that is shown to be both efficient and general; and
critically, our approach guarantees 100% constraint satisfaction in a network's
output.
","['\nNicholas Hoernle\n', '\nRafael Michael Karampatsis\n', '\nVaishak Belle\n', '\nKobi Gal\n']",Submitted to AAAI2022,,http://arxiv.org/abs/2111.01564v1,cs.LG,"['cs.LG', 'cs.SC']",,,[]
"A Design and an Implementation of an Inverse Kinematics Computation in
  Robotics Using Real Quantifier Elimination based on Comprehensive Gröbner
  Systems",http://arxiv.org/abs/2111.00384v2,2021-10-31T02:36:52Z,2023-04-21T12:18:04Z,"  The solution and implementation of the inverse kinematics computation of a
three degree-of-freedom (DOF) robot manipulator using an algorithm for real
quantifier elimination with Comprehensive Gr\""obner Systems (CGS) are
presented. The method enables us to verify if the given parameters are feasible
before solving the inverse kinematics problem. Furthermore, pre-computation of
CGS and substituting parameters in the CGS with the given values avoids the
repetitive computation of Gr\""obner basis. Experimental results compared with
our previous implementation are shown.
","['\nShuto Otaki\n', '\nAkira Terui\n', '\nMasahiko Mikawa\n']",26 pages,,http://arxiv.org/abs/2111.00384v2,cs.RO,"['cs.RO', 'cs.SC', 'math.AC', '68W30, 13P10, 13P25']",,,[]
"Differential elimination for dynamical models via projections with
  applications to structural identifiability",http://arxiv.org/abs/2111.00991v3,2021-11-01T14:56:52Z,2022-11-23T22:52:41Z,"  Elimination of unknowns in a system of differential equations is often
required when analysing (possibly nonlinear) dynamical systems models, where
only a subset of variables are observable. One such analysis, identifiability,
often relies on computing input-output relations via differential algebraic
elimination. Determining identifiability, a natural prerequisite for meaningful
parameter estimation, is often prohibitively expensive for medium to large
systems due to the computationally expensive task of elimination.
  We propose an algorithm that computes a description of the set of
differential-algebraic relations between the input and output variables of a
dynamical system model. The resulting algorithm outperforms general-purpose
software for differential elimination on a set of benchmark models from
literature.
  We use the designed elimination algorithm to build a new randomized algorithm
for assessing structural identifiability of a parameter in a parametric model.
A parameter is said to be identifiable if its value can be uniquely determined
from input-output data assuming the absence of noise and sufficiently exciting
inputs. Our new algorithm allows the identification of models that could not be
tackled before.
  Our implementation is publicly available as a Julia package at
https://github.com/SciML/StructuralIdentifiability.jl.
","['\nRuiwen Dong\n', '\nChristian Goodbrake\n', '\nHeather A Harrington\n', '\nGleb Pogudin\n']",,,http://arxiv.org/abs/2111.00991v3,math.AG,"['math.AG', 'cs.CG', 'cs.SC', 'cs.SY', 'eess.SY', 'q-bio.QM']",,,[]
"Computing elements of certain form in ideals to prove properties of
  operators",http://arxiv.org/abs/2110.12933v2,2021-10-25T13:09:27Z,2022-06-17T12:45:47Z,"  Proving statements about linear operators expressed in terms of identities
often leads to finding elements of certain form in noncommutative polynomial
ideals. We illustrate this by examples coming from actual operator statements
and discuss relevant algorithmic methods for finding such polynomials based on
noncommutative Gr\""obner bases. In particular, we present algorithms for
computing the intersection of a two-sided ideal with a one-sided ideal as well
as for computing homogeneous polynomials in two-sided ideals and monomials in
one-sided ideals. All methods presented in this work are implemented in the
Mathematica package OperatorGB.
","['\nClemens Hofstadler\n', '\nClemens G. Raab\n', '\nGeorg Regensburger\n']",26 pages,,http://dx.doi.org/10.1007/s11786-022-00536-5,cs.SC,"['cs.SC', '16Z10 (Primary), 03B35 (Secondary)']",10.1007/s11786-022-00536-5,,[]
"Towards a Theory of Domains for Harmonic Functions and its Symbolic
  Counterpart",http://arxiv.org/abs/2110.13743v1,2021-10-26T14:48:57Z,2021-10-26T14:48:57Z,"  In this paper, we begin by reviewing the calculus induced by the framework of
[10]. In there, we extended Polylogarithm functions over a subalgebra of
noncommutative rational power series, recognizable by finite state
(multiplicity) automata over the alphabet X = {x 0 , x 1 }. The stability of
this calculus under shuffle products relies on the nuclearity of the target
space [31]. We also concentrated on algebraic and analytic aspects of this
extension allowing to index polylogarithms, at non positive multi-indices, by
rational series and also allowing to regularize divergent polyzetas, at non
positive multi-indices [10]. As a continuation of works in [10] and in order to
understand the bridge between the extension of this ""polylogarithmic calculus""
and the world of harmonic sums, we propose a local theory, adapted to a full
calculus on indices of Harmonic Sums based on the Taylor expansions, around
zero, of polylogarithms with index x 1 on the rightmost end. This theory is not
only compatible with Stuffle products but also with the Analytic Model. In this
respect, it provides a stable and fully algorithmic model for Harmonic
calculus. Examples by computer are also provided 6 .
","['\nvan Chiên Bui\nLIPN\n', '\nGérard Duchamp\nLIPN\n', '\nQuoc Hoàn Ngo\n', '\nVincel Hoang Ngoc Minh\n', '\nVu Nguyen Dinh\n']",arXiv admin note: text overlap with arXiv:2009.05125,,http://arxiv.org/abs/2110.13743v1,cs.SC,"['cs.SC', 'math.CO']",,,"['LIPN', 'LIPN']"
On some combinatorial sequences associated to invariant theory,http://arxiv.org/abs/2110.13753v2,2021-10-26T15:04:50Z,2022-04-20T12:23:43Z,"  We study the enumerative and analytic properties of some sequences
constructed using tensor invariant theory. The octant sequences are constructed
from the exceptional Lie group $G_2$ and the quadrant sequences from the
special linear group $SL(3)$. In each case we show that the corresponding
sequences are related by binomial transforms. The first three octant sequences
and the first four quadrant sequences are listed in the On-Line Encyclopedia of
Integer Sequences (OEIS). These sequences all have interpretations as
enumerating two-dimensional lattice walks but for the octant sequences the
boundary conditions are unconventional. These sequences are all P-recursive and
we give the corresponding recurrence relations. In all cases the associated
differential operators are of third order and have the remarkable property that
they can be solved to give closed formulae for the ordinary generating
functions in terms of classical Gaussian hypergeometric functions. Moreover, we
show that the octant sequences and the quadrant sequences are related by the
branching rules for the inclusion of $SL(3)$ in $G_2$.
","['\nAlin Bostan\n', '\nJordan Tirrell\n', '\nBruce W. Westbury\n', '\nYi Zhang\n']",arXiv admin note: text overlap with arXiv:1911.10288,European Journal of Combinatorics (2022),http://arxiv.org/abs/2110.13753v2,math.CO,"['math.CO', 'cs.SC']",,,[]
"Pattern Division Random Access (PDRA) for M2M Communications with
  Massive MIMO Systems",http://arxiv.org/abs/2110.10586v2,2021-10-20T14:28:53Z,2021-11-17T13:25:29Z,"  In this work, we introduce the pattern-domain pilot design paradigm based on
a ""superposition of orthogonal-building-blocks"" with significantly larger
contention space to enhance the massive machine-type communications (mMTC)
random access (RA) performance in massive multiple-input multiple-output (MIMO)
systems.Specifically, the pattern-domain pilot is constructed based on the
superposition of $L$ cyclically-shifted Zadoff-Chu (ZC) sequences. The
pattern-domain pilots exhibit zero correlation values between non-colliding
patterns from the same root and low correlation values between patterns from
different roots. The increased contention space, i.e., from N to
$\binom{N}{L}$, where $\binom{N}{L}$ denotes the number of all L-combinations
of a set N, and low correlation valueslead to a significantly lower pilot
collision probability without compromising excessively on channel estimation
performance for mMTC RA in massive MIMO systems.We present the framework and
analysis of the RA success probability of the pattern-domain based scheme with
massive MIMO systems.Numerical results demonstrate that the proposed pattern
division random access (PDRA) scheme achieves an appreciable performance gain
over the conventional one,while preserving the existing physical layer
virtually unchanged. The extension of the ""superposition of
orthogonal-building-blocks"" scheme to ""superposition of
quasi-orthogonal-building-blocks"" is straightforward.
","['\nXiaoming Dai\n', '\nTiantian Yan\n', '\nQianqian Li\n', '\nHua Li\n', '\nXiyuan Wang\n']",,,http://arxiv.org/abs/2110.10586v2,cs.IT,"['cs.IT', 'cs.SC', 'math.IT']",,,[]
"An echelon form of weakly infeasible semidefinite programs and bad
  projections of the psd cone",http://arxiv.org/abs/2110.11437v3,2021-10-21T19:11:16Z,2022-07-07T20:09:50Z,"  A weakly infeasible semidefinite program (SDP) has no feasible solution, but
it has approximate solutions whose constraint violation is arbitrarily small.
These SDPs are ill-posed and numerically often unsolvable. They are also
closely related to ""bad"" linear projections that map the cone of positive
semidefinite matrices to a nonclosed set. We describe a simple echelon form of
weakly infeasible SDPs with the following properties: (i) it is obtained by
elementary row operations and congruence transformations, (ii) it makes weak
infeasibility evident, and (iii) it permits us to construct any weakly
infeasible SDP or bad linear projection by an elementary combinatorial
algorithm. Based on our echelon form we generate a challenging library of
weakly infeasible SDPs. Finally, we show that some SDPs in the literature are
in our echelon form, for example, the SDP from the sum-of-squares relaxation of
minimizing the famous Motzkin polynomial.
","['\nGábor Pataki\n', '\nAleksandr Touzov\n']",to appear,"Foundations of Computational Mathematics, 2022",http://arxiv.org/abs/2110.11437v3,math.OC,"['math.OC', 'cs.SC', 'math.AG', 'Primary: 90C22, 49N15, 15A21 Secondary: 47A52']",,,[]
"Computational aspects of finding a solution asymptotics for a singularly
  perturbed system of differential equations",http://arxiv.org/abs/2110.05082v1,2021-10-11T08:37:52Z,2021-10-11T08:37:52Z,"  We analyze the spatial structure of asymptotics of a solution to a singularly
perturbed system of mass transfer equations. The leading term of the
asymptotics is described by a parabolic equation with possibly degenerate
spatial part. We prove a theorem that establishes a relationship between the
degree of degeneracy and the numbers of equations in the system and spatial
variables in some particular cases. The work hardly depends on the calculation
of the eigenvalues of matrices that determine the spatial structure of the
asymptotics by the means of computer algebra system Wolfram Mathematica. We put
forward a hypothesis on the existence of the found connection for an arbitrary
number of equations and spatial variables.
","['\nVitaly A. Krasikov\n', '\nAndrey V. Nesterov\n']",5 pages,,http://arxiv.org/abs/2110.05082v1,math.AP,"['math.AP', 'cs.SC']",,,[]
"Algebraic and Puiseux series solutions of systems of autonomous
  algebraic ODEs of dimension one in several variables",http://arxiv.org/abs/2110.05558v2,2021-10-11T18:57:32Z,2022-02-09T10:09:30Z,"  In this paper we study systems of autonomous algebraic ODEs in several
differential indeterminates. We develop a notion of algebraic dimension of such
systems by considering them as algebraic systems. Afterwards we apply
differential elimination and analyze the behavior of the dimension in the
resulting Thomas decomposition. For such systems of algebraic dimension one, we
show that all formal Puiseux series solutions can be approximated up to an
arbitrary order by convergent solutions. We show that the existence of Puiseux
series and algebraic solutions can be decided algorithmically. Moreover, we
present a symbolic algorithm to compute all algebraic solutions. The output can
either be represented by triangular systems or by their minimal polynomials.
","['\nJose Cano\n', '\nSebastian Falkensteiner\n', '\nDaniel Robertz\n', '\nRafael Sendra\n']",,,http://arxiv.org/abs/2110.05558v2,math.AG,"['math.AG', 'cs.SC', '12H05 Differential algebra, 68W30 Symbolic computation and algebraic\n  computation, 34A25 Analytical theory of ordinary differential equations']",,,[]
Faster Modular Composition,http://arxiv.org/abs/2110.08354v2,2021-10-15T20:33:37Z,2023-07-20T15:18:49Z,"  A new Las Vegas algorithm is presented for the composition of two polynomials
modulo a third one, over an arbitrary field. When the degrees of these
polynomials are bounded by $n$, the algorithm uses $O(n^{1.43})$ field
operations, breaking through the $3/2$ barrier in the exponent for the first
time. The previous fastest algebraic algorithms, due to Brent and Kung in 1978,
require $O(n^{1.63})$ field operations in general, and ${n^{3/2+o(1)}}$ field
operations in the special case of power series over a field of large enough
characteristic. If cubic-time matrix multiplication is used, the new algorithm
runs in ${n^{5/3+o(1)}}$ operations, while previous ones run in $O(n^2)$
operations.
  Our approach relies on the computation of a matrix of algebraic relations
that is typically of small size. Randomization is used to reduce arbitrary
input to this favorable situation.
","['\nVincent Neiger\n', '\nBruno Salvy\n', '\nÉric Schost\n', '\nGilles Villard\n']",78 pages,,http://arxiv.org/abs/2110.08354v2,cs.SC,"['cs.SC', 'cs.CC']",,,[]
An Overview of Ontologies and Tool Support for COVID-19 Analytics,http://arxiv.org/abs/2110.06397v1,2021-10-12T23:20:37Z,2021-10-12T23:20:37Z,"  The outbreak of the SARS-CoV-2 pandemic of the new COVID-19 disease (COVID-19
for short) demands empowering existing medical, economic, and social emergency
backend systems with data analytics capabilities. An impediment in taking
advantages of data analytics in these systems is the lack of a unified
framework or reference model. Ontologies are highlighted as a promising
solution to bridge this gap by providing a formal representation of COVID-19
concepts such as symptoms, infections rate, contact tracing, and drug
modelling. Ontology-based solutions enable the integration of diverse data
sources that leads to a better understanding of pandemic data, management of
smart lockdowns by identifying pandemic hotspots, and knowledge-driven
inference, reasoning, and recommendations to tackle surrounding issues.
","['\nAakash Ahmad\n', '\nMadhushi Bandara\n', '\nMahdi Fahmideh\n', '\nHenderik A. Proper\n', '\nGiancarlo Guizzardi\n', '\nJeffrey Soar\n']",,,http://arxiv.org/abs/2110.06397v1,cs.SE,"['cs.SE', 'cs.CL', 'cs.SC']",,,[]
"Fast and Reliable Formal Verification of Smart Contracts with the Move
  Prover",http://arxiv.org/abs/2110.08362v3,2021-10-15T20:49:30Z,2022-02-13T02:56:40Z,"  The Move Prover (MVP) is a formal verifier for smart contracts written in the
Move programming language. MVP has an expressive specification language, and is
fast and reliable enough that it can be run routinely by developers and in
integration testing in a few minutes. Besides the simplicity of smart contracts
and the Move language, three transformations are responsible for the
practicality of MVP: (1) an alias-free memory model, (2) fine-grained invariant
checking, and (3) monomorphization. The entirety of the Move code for the Diem
blockchain has been extensively specified and can be completely verified by MVP
in a few minutes. Changes in the Diem framework must be successfully verified
before being integrated into the open source repository on GitHub.
","['\nDavid Dill\n', '\nWolfgang Grieskamp\n', '\nJunkil Park\n', '\nShaz Qadeer\n', '\nMeng Xu\n', '\nEmma Zhong\n']",,,http://arxiv.org/abs/2110.08362v3,cs.PL,"['cs.PL', 'cs.SC', 'cs.SE']",,,[]
VESPo: Verified Evaluation of Secret Polynomials,http://arxiv.org/abs/2110.02022v5,2021-10-05T13:11:04Z,2023-03-13T10:57:10Z,"  Proofs of Retrievability are protocols which allow a Client to store data
remotely and to efficiently ensure, via audits, that the entirety of that data
is still intact. Dynamic Proofs of Retrievability (DPoR) also support efficient
retrieval and update of any small portion of the data.We propose a novel
protocol for arbitrary outsourced data storage that achieves both low remote
storage size and audit complexity.A key ingredient, that can be also of
intrinsic interest, reduces to efficiently evaluating a secret polynomial at
given public points, when the (encrypted) polynomial is stored on an untrusted
Server.The Server performs the evaluations and also returns associated
certificates. A Client can check that the evaluations are correct using the
certificates and some pre-computed keys, more efficiently than re-evaluating
the polynomial.Our protocols support two important features: the polynomial
itself can be encrypted on the Server, and it can be dynamically updated by
changing individual coefficients cheaply without redoing the entire setup.Our
methods rely on linearly homomorphic encryption and pairings, and our
implementation shows good performance for polynomial evaluations with millions
of coefficients, and efficient DPoR with terabytes of data.For instance, for a
1TB database, compared to the state of art, we can reduce the Client storage by
5000x, communication size by 20x, and client-side audit time by 2x, at the cost
of one order of magnitude increase in server-side audit time.
","['\nJean-Guillaume Dumas\nCASC\n', '\nAude Maignan\nCASC\n', '\nClément Pernet\nCASC\n', '\nDaniel S. Roche\n']",,"Privacy Enhancing Technologies Symposium, Jul 2023, Lausanne (CH),
  Switzerland",http://arxiv.org/abs/2110.02022v5,cs.CR,"['cs.CR', 'cs.SC']",,,"['CASC', 'CASC', 'CASC']"
Not another computer algebra system: Highlighting wxMaxima in calculus,http://arxiv.org/abs/2109.13500v1,2021-09-28T05:41:56Z,2021-09-28T05:41:56Z,"  This article introduces and explains a computer algebra system (CAS) wxMaxima
for Calculus teaching and learning at the tertiary level. The didactic
reasoning behind this approach is the need to implement an element of
technology into classrooms to enhance students' understanding of Calculus
concepts. For many mathematics educators who have been using CAS, this material
is of great interest, particularly for secondary teachers and university
instructors who plan to introduce an alternative CAS into their classrooms. By
highlighting both the strengths and limitations of the software, we hope that
it will stimulate further debate not only among mathematics educators and
software users but also also among symbolic computation and software
developers.
","['\nN. Karjanto\n', '\nH. S. Husain\n']","19 pages, 4 figures, 5 tables, 37 references",Mathematics 9: 1317 (2021),http://dx.doi.org/10.3390/math9011317,math.HO,"['math.HO', 'cs.SC', 'physics.ed-ph', '68W30, 94-04, 97I40, 97I50, 97D40, 97D80, 97U50, 97U70']",10.3390/math9011317,,[]
Segre-Driven Radicality Testing,http://arxiv.org/abs/2110.01913v1,2021-10-05T10:06:06Z,2021-10-05T10:06:06Z,"  We present a probabilistic algorithm to test if a homogeneous polynomial
ideal $I$ defining a scheme $X$ in $\mathbb{P}^n$ is radical using Segre
classes and other geometric notions from intersection theory. Its worst case
complexity depends on the geometry of $X$. If the scheme $X$ has reduced
isolated primary components and no embedded components supported the singular
locus of $X_{\rm red}=V(\sqrt{I})$, then the worst case complexity is doubly
exponential in $n$; in all the other cases the complexity is singly
exponential. The realm of the ideals for which our radical testing procedure
requires only single exponential time includes examples which are often
considered pathological, such as the ones drawn from the famous Mayr-Meyer set
of ideals which exhibit doubly exponential complexity for the ideal membership
problem.
","['\nMartin Helmer\n', '\nElias Tsigaridas\n']",,,http://arxiv.org/abs/2110.01913v1,math.AG,"['math.AG', 'cs.CC', 'cs.SC', 'math.AC', '14Qxx, 13Pxx, 13H15, 14C17, 14C20, 68W30, 65H10']",,,[]
Bit Complexity of Jordan Normal Form and Spectral Factorization,http://arxiv.org/abs/2109.13956v2,2021-09-28T18:01:01Z,2022-11-25T21:27:24Z,"  We study the bit complexity of two related fundamental computational problems
in linear algebra and control theory. Our results are: (1) An
$\tilde{O}(n^{\omega+3}a+n^4a^2+n^\omega\log(1/\epsilon))$ time algorithm for
finding an $\epsilon-$approximation to the Jordan Normal form of an integer
matrix with $a-$bit entries, where $\omega$ is the exponent of matrix
multiplication. (2) An $\tilde{O}(n^6d^6a+n^4d^4a^2+n^3d^3\log(1/\epsilon))$
time algorithm for $\epsilon$-approximately computing the spectral
factorization $P(x)=Q^*(x)Q(x)$ of a given monic $n\times n$ rational matrix
polynomial of degree $2d$ with rational $a-$bit coefficients having $a-$bit
common denominators, which satisfies $P(x)\succeq 0$ for all real $x$. The
first algorithm is used as a subroutine in the second one.
  Despite its being of central importance, polynomial complexity bounds were
not previously known for spectral factorization, and for Jordan form the best
previous best running time was an unspecified polynomial in $n$ of degree at
least twelve \cite{cai1994computing}. Our algorithms are simple and judiciously
combine techniques from numerical and symbolic computation, yielding
significant advantages over either approach by itself.
","['\nPapri Dey\n', '\nRavi Kannan\n', '\nNick Ryder\n', '\nNikhil Srivastava\n']",19pp,ITCS 2023,http://arxiv.org/abs/2109.13956v2,cs.DS,"['cs.DS', 'cs.NA', 'cs.SC', 'math.NA', 'math.OC']",,,[]
On the representation of non-holonomic univariate power series,http://arxiv.org/abs/2109.09574v3,2021-09-20T14:29:02Z,2022-04-14T19:27:25Z,"  Holonomic functions play an essential role in Computer Algebra since they
allow the application of many symbolic algorithms. Among all algorithmic
attempts to find formulas for power series, the holonomic property remains the
most important requirement to be satisfied by the function under consideration.
The targeted functions mainly summarize that of meromorphic functions. However,
expressions like $\tan(z)$, $z/(\exp(z)-1)$, $\sec(z)$, etc., particularly,
reciprocals, quotients and compositions of holonomic functions, are generally
not holonomic. Therefore their power series are inaccessible by the holonomic
framework. From the mathematical dictionaries, one can observe that most of the
known closed-form formulas of non-holonomic power series involve another
sequence whose evaluation depends on some finite summations. In the case of
$\tan(z)$ and $\sec(z)$ the corresponding sequences are the Bernoulli and Euler
numbers, respectively. Thus providing a symbolic approach that yields complete
representations when linear summations for power series coefficients of
non-holonomic functions appear, might be seen as a step forward towards the
representation of non-holonomic power series.
  By adapting the method of ansatz with undetermined coefficients, we build an
algorithm that computes least-order quadratic differential equations with
polynomial coefficients for a large class of non-holonomic functions. A
differential equation resulting from this procedure is converted into a
recurrence equation by applying the Cauchy product formula and rewriting powers
into polynomials and derivatives into shifts. Finally, using enough initial
values we are able to give normal form representations to characterize several
non-holonomic power series and prove non-trivial identities. We discuss this
algorithm and its implementation for Maple 2022.
","['\nBertrand Teguia Tabuguia\n', '\nWolfram Koepf\n']",20 pages; 26 references. Update: revised version,,http://arxiv.org/abs/2109.09574v3,cs.SC,"['cs.SC', 'Primary: 30B99, 34K17, Secondary: 34A09, 11B68']",,,[]
"Conversational Multi-Hop Reasoning with Neural Commonsense Knowledge and
  Symbolic Logic Rules",http://arxiv.org/abs/2109.08544v1,2021-09-17T13:40:07Z,2021-09-17T13:40:07Z,"  One of the challenges faced by conversational agents is their inability to
identify unstated presumptions of their users' commands, a task trivial for
humans due to their common sense. In this paper, we propose a zero-shot
commonsense reasoning system for conversational agents in an attempt to achieve
this. Our reasoner uncovers unstated presumptions from user commands satisfying
a general template of if-(state), then-(action), because-(goal). Our reasoner
uses a state-of-the-art transformer-based generative commonsense knowledge base
(KB) as its source of background knowledge for reasoning. We propose a novel
and iterative knowledge query mechanism to extract multi-hop reasoning chains
from the neural KB which uses symbolic logic rules to significantly reduce the
search space. Similar to any KBs gathered to date, our commonsense KB is prone
to missing knowledge. Therefore, we propose to conversationally elicit the
missing knowledge from human users with our novel dynamic question generation
strategy, which generates and presents contextualized queries to human users.
We evaluate the model with a user study with human users that achieves a 35%
higher success rate compared to SOTA.
","['\nForough Arabshahi\n', '\nJennifer Lee\n', '\nAntoine Bosselut\n', '\nYejin Choi\n', '\nTom Mitchell\n']","Appearing in the 2021 Conference on Empirical Methods in Natural
  Language Processing (EMNLP)",,http://arxiv.org/abs/2109.08544v1,cs.AI,"['cs.AI', 'cs.CL', 'cs.LG', 'cs.SC']",,,[]
"Jacobi's Bound. Jacobi's results translated in K{Ö}nig's,
  Egerv{á}ry's and Ritt's mathematical languages",http://arxiv.org/abs/2109.03620v3,2021-09-07T16:03:38Z,2022-07-13T14:17:16Z,"  Jacobi's results on the computation of the order and of the normal forms of a
differential system are translated in the formalism of differential algebra. In
the quasi-regular case, we give complete proofs according to Jacobi's
arguments. The main result is {\it Jacobi's bound}, still conjectural in the
general case: the order of a differential system $P_{1}, \ldots, P_{n}$ is not
greater than the maximum $\cal O$ of the sums $\sum_{i=1}^{n} a_{i,\sigma(i)}$,
for all permutations $\sigma$ of the indices, where $a_{i,j}:={\rm
ord}_{x_{j}}P_{i}$, \emph{viz.}\ the \emph{tropical determinant of the matrix
$(a_{i,j})$}. The order is precisely equal to $\cal O$ iff Jacobi's
\emph{truncated determinant} does not vanish.
  Jacobi also gave a polynomial time algorithm to compute $\cal O$, similar to
Kuhn's ""Hungarian method"" and some variants of shortest path algorithms,
related to the computation of integers $\ell_{i}$ such that a normal form may
be obtained, in the generic case, by differentiating $\ell_{i}$ times equation
$P_{i}$.
  Fundamental results about changes of orderings and the various normal forms a
system may have, including differential resolvents, are also provided.
",['\nFrançois Ollivier\n'],"104 pages, 10 figures, index of words and names, index of notations","Applicable Algebra in Engineering, Communication and Computing 34
  (5), 793-885, September 2023",http://dx.doi.org/10.1007/s00200-022-00547-6,math.HO,"['math.HO', 'cs.SC', '12H05 (primary), 90C27 (secondary)', 'I.1.2']",10.1007/s00200-022-00547-6,,[]
Competition Report: CHC-COMP-21,http://arxiv.org/abs/2109.04635v1,2021-09-10T02:43:28Z,2021-09-10T02:43:28Z,"  CHC-COMP-21 is the fourth competition of solvers for Constrained Horn
Clauses. In this year, 7 solvers participated at the competition, and were
evaluated in 7 separate tracks on problems in linear integer arithmetic, linear
real arithmetic, arrays, and algebraic data-types. The competition was run in
March 2021 using the StarExec computing cluster. This report gives an overview
of the competition design, explains the organisation of the competition, and
presents the competition results.
","['\nGrigory Fedyukovich\nFlorida State University, USA\n', '\nPhilipp Rümmer\nUppsala University, Sweden\n']","In Proceedings HCVS 2021, arXiv:2109.03988. arXiv admin note:
  substantial text overlap with arXiv:2008.02939","EPTCS 344, 2021, pp. 91-108",http://dx.doi.org/10.4204/EPTCS.344.7,cs.LO,"['cs.LO', 'cs.SC', 'F.3.1']",10.4204/EPTCS.344.7,,"['Florida State University, USA', 'Uppsala University, Sweden']"
The VLSAT-2 Benchmark Suite,http://arxiv.org/abs/2110.06336v1,2021-09-08T06:46:18Z,2021-09-08T06:46:18Z,"  This report presents VLSAT-2 (an acronym for ""Very Large Boolean
SATisfiability problems),the second part of a benchmark suite to be used in
scientific experiments and softwarecompetitions addressing SAT-solving
issues.VLSAT-2 contains 100 benchmarks (50 satisfiable and 50 unsatisfiable
formulas)of increasing complexity, proposed in DIMACS CNF format undera
permissive Creative Commons license.25% of these benchmarks have been used
during the 2020 and 2021 editionsof the International SAT Competition.
","['\nPierre Bouvier\nCONVECS\n', '\nHubert Garavel\nCONVECS\n']",arXiv admin note: substantial text overlap with arXiv:2011.11049,,http://arxiv.org/abs/2110.06336v1,cs.DS,"['cs.DS', 'cs.SC']",,,"['CONVECS', 'CONVECS']"
"Proceedings of the 9th International Workshop on Verification and
  Program Transformation",http://arxiv.org/abs/2109.02001v1,2021-09-05T05:42:21Z,2021-09-05T05:42:21Z,"  The previous VPT 2020 workshop was organized in honour of Professor Alberto
Pettorossi on the occasion of his academic retirement from Universit\`a di Roma
Tor Vergata. Due to the pandemic the VPT 2020 meeting was cancelled but its
proceeding have already appeared in the EPTCS 320 volume. The joint VPT-20-21
event has subsumed the original programme of VPT 2020 and provided an
opportunity to meet and celebrate the achievements of Professor Alberto
Pettorossi; its programme was further expanded with the newly submitted
presentations for VPT 2021. The aim of the VPT workshop series is to provide a
forum where people from the areas of program transformation and program
verification can fruitfully exchange ideas and gain a deeper understanding of
the interactions between those two fields.
","['\nAlexei Lisitsa\nThe University of Liverpool, UK\n', '\nAndrei P. Nemytykh\nProgram Systems Institute of RAS, Russia\n']",,"EPTCS 341, 2021",http://dx.doi.org/10.4204/EPTCS.341,cs.SC,"['cs.SC', 'cs.PL', 'cs.SE']",10.4204/EPTCS.341,,"['The University of Liverpool, UK', 'Program Systems Institute of RAS, Russia']"
"Proceedings of the 9th International Symposium on Symbolic Computation
  in Software Science",http://arxiv.org/abs/2109.02501v1,2021-09-06T14:22:11Z,2021-09-06T14:22:11Z,"  This volume contains papers presented at the Ninth International Symposium on
Symbolic Computation in Software Science, SCSS 2021.
  Symbolic Computation is the science of computing with symbolic objects
(terms, formulae, programs, representations of algebraic objects, etc.).
Powerful algorithms have been developed during the past decades for the major
subareas of symbolic computation: computer algebra and computational logic.
These algorithms and methods are successfully applied in various fields,
including software science, which covers a broad range of topics about software
construction and analysis.
  Meanwhile, artificial intelligence methods and machine learning algorithms
are widely used nowadays in various domains and, in particular, combined with
symbolic computation. Several approaches mix artificial intelligence and
symbolic methods and tools deployed over large corpora to create what is known
as cognitive systems. Cognitive computing focuses on building systems that
interact with humans naturally by reasoning, aiming at learning at scale.
  The purpose of SCSS is to promote research on theoretical and practical
aspects of symbolic computation in software science, combined with modern
artificial intelligence techniques. These proceedings contain the keynote paper
by Bruno Buchberger and ten contributed papers. Besides, the conference program
included three invited talks, nine short and work-in-progress papers, and a
special session on computer algebra and computational logic. Due to the
COVID-19 pandemic, the symposium was held completely online. It was organized
by the Research Institute for Symbolic Computation (RISC) of the Johannes
Kepler University Linz on September 8--10, 2021.
",['\nTemur Kutsia\n'],,"EPTCS 342, 2021",http://dx.doi.org/10.4204/EPTCS.342,cs.SC,"['cs.SC', 'cs.AI', 'cs.LO', 'cs.SE']",10.4204/EPTCS.342,,[]
"Efficient diagonalization of symmetric matrices associated with graphs
  of small treewidth",http://arxiv.org/abs/2109.02515v2,2021-09-06T14:48:46Z,2021-10-26T21:23:29Z,"  Let $M=(m_{ij})$ be a symmetric matrix of order $n$ whose elements lie in an
arbitrary field $\mathbb{F}$, and let $G$ be the graph with vertex set
$\{1,\ldots,n\}$ such that distinct vertices $i$ and $j$ are adjacent if and
only if $m_{ij} \neq 0$. We introduce a dynamic programming algorithm that
finds a diagonal matrix that is congruent to $M$. If $G$ is given with a tree
decomposition $\mathcal{T}$ of width $k$, then this can be done in time
$O(k|\mathcal{T}| + k^2 n)$, where $|\mathcal{T}|$ denotes the number of nodes
in $\mathcal{T}$. Among other things, this allows one to compute the
determinant, the rank and the inertia of a symmetric matrix in time
$O(k|\mathcal{T}| + k^2 n)$.
","['\nMartin Fürer\n', '\nCarlos Hoppen\n', '\nVilmar Trevisan\n']",,,http://arxiv.org/abs/2109.02515v2,cs.DS,"['cs.DS', 'cs.SC', 'math.CO', '15A18', 'F.2.2; G.2.2']",,,[]
Symbolic Computation in Software Science: My Personal View,http://arxiv.org/abs/2109.02806v1,2021-09-07T01:41:41Z,2021-09-07T01:41:41Z,"  In this note, I develop my personal view on the scope and relevance of
symbolic computation in software science. For this, I discuss the interaction
and differences between symbolic computation, software science, automatic
programming, mathematical knowledge management, artificial intelligence,
algorithmic intelligence, numerical computation, and machine learning. In the
discussion of these notions, I allow myself to refer also to papers (1982,
1985, 2001, 2003, 2013) of mine in which I expressed my views on these areas at
early stages of some of these fields.
",['\nBruno Buchberger\nResearch Institute for Symbolic Computation\n'],"In Proceedings SCSS 2021, arXiv:2109.02501","EPTCS 342, 2021, pp. 1-13",http://dx.doi.org/10.4204/EPTCS.342.1,cs.SC,"['cs.SC', 'cs.AI', 'cs.SE']",10.4204/EPTCS.342.1,,['Research Institute for Symbolic Computation']
OGRe: An Object-Oriented General Relativity Package for Mathematica,http://arxiv.org/abs/2109.04193v1,2021-09-06T00:31:23Z,2021-09-06T00:31:23Z,"  We present OGRe, a modern Mathematica package for tensor calculus, designed
to be both powerful and user-friendly. The package can be used in a variety of
contexts where tensor calculations are needed, in both mathematics and physics,
but it is especially suitable for general relativity. By implementing an
object-oriented design paradigm, OGRe allows calculating arbitrarily
complicated tensor formulas easily, and automatically transforms between index
configurations and coordinate systems behind the scenes as needed, eliminating
user errors by making it impossible for the user to combine tensors in
inconsistent ways. Other features include displaying tensors in various forms,
automatic calculation of curvature tensors and geodesic equations, easy
importing and exporting of tensors between sessions, optimized algorithms and
parallelization for improved performance, and more.
",['\nBarak Shoshany\n'],"92 pages, source code available at https://github.com/bshoshany/OGRe","Journal of Open Source Software, 6(65), 3416 (2021)",http://dx.doi.org/10.21105/joss.03416,cs.MS,"['cs.MS', 'cs.SC', 'gr-qc', 'math.DG']",10.21105/joss.03416,,[]
"Knowledge-Assisted Reasoning of Model-Augmented System Requirements with
  Event Calculus and Goal-Directed Answer Set Programming",http://arxiv.org/abs/2109.04634v1,2021-09-10T02:43:08Z,2021-09-10T02:43:08Z,"  We consider requirements for cyber-physical systems represented in
constrained natural language. We present novel automated techniques for aiding
in the development of these requirements so that they are consistent and can
withstand perceived failures. We show how cyber-physical systems' requirements
can be modeled using the event calculus (EC), a formalism used in AI for
representing actions and change. We also show how answer set programming (ASP)
and its query-driven implementation s(CASP) can be used to directly realize the
event calculus model of the requirements. This event calculus model can be used
to automatically validate the requirements. Since ASP is an expressive
knowledge representation language, it can also be used to represent contextual
knowledge about cyber-physical systems, which, in turn, can be used to find
gaps in their requirements specifications. We illustrate our approach through
an altitude alerting system from the avionics domain.
","['\nBrendan Hall\nHoneywell Advanced Technology, Plymouth, USA\n', '\nSarat Chandra Varanasi\nThe University of Texas at Dallas, Richardson, USA\n', '\nJan Fiedor\nHoneywell Internation s.r.o & Brno University of Technology, Brno, Czech Republic\n', '\nJoaquín Arias\nUniversidad Rey Juan Carlos, Madrid, Spain\n', '\nKinjal Basu\nThe University of Texas at Dallas, Richardson, USA\n', '\nFang Li\nThe University of Texas at Dallas, Richardson, USA\n', '\nDevesh Bhatt\nHoneywell Advanced Technology, Plymouth, USA\n', '\nKevin Driscoll\nHoneywell Advanced Technology, Plymouth, USA\n', '\nElmer Salazar\nThe University of Texas at Dallas, Richardson, USA\n', '\nGopal Gupta\nThe University of Texas at Dallas, Richardson, USA\n']","In Proceedings HCVS 2021, arXiv:2109.03988","EPTCS 344, 2021, pp. 79-90",http://dx.doi.org/10.4204/EPTCS.344.6,cs.LO,"['cs.LO', 'cs.AI', 'cs.SC']",10.4204/EPTCS.344.6,,"['Honeywell Advanced Technology, Plymouth, USA', 'The University of Texas at Dallas, Richardson, USA', 'Honeywell Internation s.r.o & Brno University of Technology, Brno, Czech Republic', 'Universidad Rey Juan Carlos, Madrid, Spain', 'The University of Texas at Dallas, Richardson, USA', 'The University of Texas at Dallas, Richardson, USA', 'Honeywell Advanced Technology, Plymouth, USA', 'Honeywell Advanced Technology, Plymouth, USA', 'The University of Texas at Dallas, Richardson, USA', 'The University of Texas at Dallas, Richardson, USA']"
Adjoint Differentiation for generic matrix functions,http://arxiv.org/abs/2109.04913v1,2021-09-10T14:52:40Z,2021-09-10T14:52:40Z,"  We derive a formula for the adjoint $\overline{A}$ of a square-matrix
operation of the form $C=f(A)$, where $f$ is holomorphic in the neighborhood of
each eigenvalue. We then apply the formula to derive closed-form expressions in
particular cases of interest such as the case when we have a spectral
decomposition $A=UDU^{-1}$, the spectrum cut-off $C=A_+$ and the Nearest
Correlation Matrix routine. Finally, we explain how to simplify the computation
of adjoints for regularized linear regression coefficients.
","['\nAndrei Goloubentsev\n', '\nDmitri Goloubentsev\n', '\nEvgeny Lakshtanov\n']",,,http://arxiv.org/abs/2109.04913v1,q-fin.CP,"['q-fin.CP', 'cs.SC', 'q-fin.RM']",,,[]
Computer algebra in Julia,http://arxiv.org/abs/2108.12301v1,2021-08-27T14:24:51Z,2021-08-27T14:24:51Z,"  Recently, the place of the main programming language for scientific and
engineering computations has been little by little taken by Julia. Some users
want to work completely within the Julia framework as they work within the
Python framework. There are libraries for Julia that cover the majority of
scientific and engineering computations demands. The aim of this paper is to
combine the usage of the Julia framework for numerical computations and for
symbolic computations in mathematical modeling problems. The main functional
domains determining various variants of the application of computer algebra
systems are described. In each of these domains, generic representatives of
computer algebra systems in Julia are distinguished. The conclusion is that it
is possible (and even convenient) to use computer algebra systems within the
Julia framework.
","['\nDmitry S. Kulyabov\n', '\nAnna V. Korolkova\n']",in English; in Russian,,http://dx.doi.org/10.1134/S0361768821020079,cs.SC,['cs.SC'],10.1134/S0361768821020079,,[]
"Vivienne: Relational Verification of Cryptographic Implementations in
  WebAssembly",http://arxiv.org/abs/2109.01386v1,2021-09-03T09:11:08Z,2021-09-03T09:11:08Z,"  This paper explores the use of relational symbolic execution to counter
timing side channels in WebAssembly programs. We design and implement Vivienne,
an open-source tool to automatically analyze WebAssembly cryptographic
libraries for constant-time violations. Our approach features various
optimizations that leverage the structure of WebAssembly and automated theorem
provers, including support for loops via relational invariants. We evaluate
Vivienne on 57 real-world cryptographic implementations, including a previously
unverified implementation of the HACL* library in WebAssembly. The results
indicate that Vivienne is a practical solution for constant-time analysis of
cryptographic libraries in WebAssembly.
","['\nRodothea Myrsini Tsoupidi\n', '\nMusard Balliu\n', '\nBenoit Baudry\n']",,,http://arxiv.org/abs/2109.01386v1,cs.CR,"['cs.CR', 'cs.PL', 'cs.SC']",,,[]
Renormalized Wolfram model exhibiting non-relativistic quantum behavior,http://arxiv.org/abs/2108.08300v1,2021-08-18T02:19:40Z,2021-08-18T02:19:40Z,"  We show a Wolfram model whose renormalization generates a sequence of
approximations of a wave function having the Pauli-x matrix as Hamiltonian.
",['\nJosé Manuel Rodríguez Caballero\n'],2 figures,,http://arxiv.org/abs/2108.08300v1,quant-ph,"['quant-ph', 'cs.SC', 'math.DS']",,,[]
Beyond Linear Algebra,http://arxiv.org/abs/2108.09494v1,2021-08-21T11:41:45Z,2021-08-21T11:41:45Z,"  Our title challenges the reader to venture beyond linear algebra in designing
models and in thinking about numerical algorithms for identifying solutions.
This article accompanies the author's lecture at the International Congress of
Mathematicians 2022. It covers recent advances in the study of critical point
equations in optimization and statistics, and it explores the role of nonlinear
algebra in the study of linear PDE with constant coefficients.
",['\nBernd Sturmfels\n'],"19 pages, 3 figures",,http://arxiv.org/abs/2108.09494v1,math.AG,"['math.AG', 'cs.SC', 'math.OC', 'math.ST', 'stat.TH']",,,[]
Macaulay bases of modules,http://arxiv.org/abs/2108.03707v1,2021-08-08T18:34:01Z,2021-08-08T18:34:01Z,"  We define Macaulay bases of modules, which are a common generalization of
Groebner bases and Macaulay $H$-bases to suitably graded modules over a
commutative graded $\mathbf{k}$-algebra, where the index sets of the two
gradings may differ. This includes Groebner bases of modules as a special case,
in contrast to previous work on Macaulay bases of modules. We show that the
standard results on Groebner bases and Macaulay $H$-bases generalize in fields
of arbitrary characteristic to Macaulay bases, including the reduction
algorithm and Buchberger's criterion and algorithm. A key result is that
Macaulay bases, in contrast to Groebner bases, respect symmetries when there is
a group $G$ acting homogeneously on a graded module, in which case the
reduction algorithm is $G$-equivariant and the $\mathbf{k}$-span of a Macaulay
basis is $G$-invariant. We also show that some of the standard applications of
Groebner bases can be generalized to Macaulay bases, including elimination and
computation of syzygy modules, which require the generalization to modules that
was not present in previous work.
",['\nSujit Rao\n'],"20 pages, comments welcome",,http://arxiv.org/abs/2108.03707v1,math.AC,"['math.AC', 'cs.SC', 'math.RA']",,,[]
Linear equations for unordered data vectors in $[D]^k\to{}Z^d$,http://arxiv.org/abs/2109.03025v5,2021-08-09T16:27:52Z,2022-12-09T10:22:55Z,"  Following a recently considered generalisation of linear equations to
unordered-data vectors and to ordered-data vectors, we perform a further
generalisation to data vectors that are functions from k-element subsets of the
unordered-data set to vectors of integer numbers. These generalised equations
naturally appear in the analysis of vector addition systems (or Petri nets)
extended so that each token carries a set of unordered data. We show that
nonnegative-integer solvability of linear equations is in nondeterministic
exponential time while integer solvability is in polynomial time.
","['\nPiotr Hofman\n', '\nJakub Różycki\n']",,"Logical Methods in Computer Science, Volume 18, Issue 4 (December
  12, 2022) lmcs:8459",http://dx.doi.org/10.46298/lmcs-18(4:11)2022,cs.CC,"['cs.CC', 'cs.FL', 'cs.SC', '68Q85', 'F.4.2; G.2.2; F.3.1']",10.46298/lmcs-18(4:11)2022,,[]
Digital Collections of Examples in Mathematical Sciences,http://arxiv.org/abs/2107.12908v2,2021-07-27T16:05:36Z,2022-10-27T23:07:37Z,"  Some aspects of Computer Algebra (notably Computation Group Theory and
Computational Number Theory) have some good databases of examples, typically of
the form ""all the X up to size n"". But most of the others, especially on the
polynomial side, are lacking such, despite the utility they have demonstrated
in the related fields of SAT and SMT solving. We claim that the field would be
enhanced by such community-maintained databases, rather than each author
hand-selecting a few, which are often too large or error-prone to print, and
therefore difficult for subsequent authors to reproduce.
",['\nJames Harold Davenport\n'],"Presented at 8th European Congress of Mathematicians; this version to
  appear in proceedings",,http://arxiv.org/abs/2107.12908v2,cs.SC,"['cs.SC', '00A35, 12-04, 20-04']",,,[]
"ATLAS: Interactive and Educational Linear Algebra System Containing
  Non-Standard Methods",http://arxiv.org/abs/2107.13942v1,2021-07-29T13:03:34Z,2021-07-29T13:03:34Z,"  While there are numerous linear algebra teaching tools, they tend to be
focused on the basics, and not handle the more advanced aspects. This project
aims to fill that gap, focusing specifically on methods like Strassen's fast
matrix multiplication.
","['\nAkhilesh Pai\n', '\nJames Harold Davenport\n']","Presented at MathUI21 (part of Conferences on Intelligent Computer
  Mathematics 2021)",,http://arxiv.org/abs/2107.13942v1,cs.SC,['cs.SC'],,,[]
"Sum of Squares Decompositions of Polynomials over their Gradient Ideals
  with Rational Coefficients",http://arxiv.org/abs/2107.11825v1,2021-07-25T15:13:43Z,2021-07-25T15:13:43Z,"  Assessing non-negativity of multivariate polynomials over the reals, through
the computation of {\em certificates of non-negativity}, is a topical issue in
polynomial optimization. This is usually tackled through the computation of
{\em sums-of-squares decompositions} which rely on efficient numerical solvers
for semi-definite programming. This method faces two difficulties. The first
one is that the certificates obtained this way are {\em approximate} and then
non-exact. The second one is due to the fact that not all non-negative
polynomials are sums-of-squares. In this paper, we build on previous works by
Parrilo, Nie, Demmel and Sturmfels who introduced certificates of
non-negativity modulo {\em gradient ideals}. We prove that, actually, such
certificates can be obtained {\em exactly}, over the rationals if the
polynomial under consideration has rational coefficients and we provide {\em
exact} algorithms to compute them. We analyze the bit complexity of these
algorithms and deduce bit size bounds of such certificates.
","['\nVictor Magron\n', '\nMohab Safey El Din\n', '\nTrung-Hieu Vu\n']","24 pages, 2 tables",,http://arxiv.org/abs/2107.11825v1,cs.SC,"['cs.SC', 'math.OC']",,,[]
"Signature Gröbner bases, bases of syzygies and cofactor reconstruction
  in the free algebra",http://arxiv.org/abs/2107.14675v3,2021-07-30T14:52:12Z,2022-04-14T14:41:41Z,"  Signature-based algorithms have become a standard approach for computing
Gr\""obner bases in commutative polynomial rings. However, so far, it was not
clear how to extend this concept to the setting of noncommutative polynomials
in the free algebra. In this paper, we present a signature-based algorithm for
computing Gr\""obner bases in precisely this setting. The algorithm is an
adaptation of Buchberger's algorithm including signatures. We prove that our
algorithm correctly enumerates a signature Gr\""obner basis as well as a
Gr\""obner basis of the module generated by the leading terms of the generators'
syzygies, and that it terminates whenever the ideal admits a finite signature
Gr\""obner basis. Additionally, we adapt well-known signature-based criteria
eliminating redundant reductions, such as the syzygy criterion, the F5
criterion and the singular criterion, to the case of noncommutative
polynomials. We also generalize reconstruction methods from the commutative
setting that allow to recover, from partial information about signatures, the
coordinates of elements of a Gr\""obner basis in terms of the input polynomials,
as well as a basis of the syzygy module of the generators. We have written a
toy implementation of all the algorithms in the Mathematica package OperatorGB
and we compare our signature-based algorithm to the classical Buchberger
algorithm for noncommutative polynomials.
","['\nClemens Hofstadler\n', '\nThibaut Verron\n']","31 pages, 2 pages appendix, 1 figure",,http://dx.doi.org/10.1016/j.jsc.2022.04.001,cs.SC,"['cs.SC', 'math.RA']",10.1016/j.jsc.2022.04.001,,[]
"NeuralPDE: Automating Physics-Informed Neural Networks (PINNs) with
  Error Approximations",http://arxiv.org/abs/2107.09443v1,2021-07-19T12:38:31Z,2021-07-19T12:38:31Z,"  Physics-informed neural networks (PINNs) are an increasingly powerful way to
solve partial differential equations, generate digital twins, and create neural
surrogates of physical models. In this manuscript we detail the inner workings
of NeuralPDE.jl and show how a formulation structured around numerical
quadrature gives rise to new loss functions which allow for adaptivity towards
bounded error tolerances. We describe the various ways one can use the tool,
detailing mathematical techniques like using extended loss functions for
parameter estimation and operator discovery, to help potential users adopt
these PINN-based techniques into their workflow. We showcase how NeuralPDE uses
a purely symbolic formulation so that all of the underlying training code is
generated from an abstract formulation, and show how to make use of GPUs and
solve systems of PDEs. Afterwards we give a detailed performance analysis which
showcases the trade-off between training techniques on a large set of PDEs. We
end by focusing on a complex multiphysics example, the Doyle-Fuller-Newman
(DFN) Model, and showcase how this PDE can be formulated and solved with
NeuralPDE. Together this manuscript is meant to be a detailed and approachable
technical report to help potential users of the technique quickly get a sense
of the real-world performance trade-offs and use cases of the PINN techniques.
","['\nKirill Zubov\n', '\nZoe McCarthy\n', '\nYingbo Ma\n', '\nFrancesco Calisto\n', '\nValerio Pagliarino\n', '\nSimone Azeglio\n', '\nLuca Bottero\n', '\nEmmanuel Luján\n', '\nValentin Sulzer\n', '\nAshutosh Bharambe\n', '\nNand Vinchhi\n', '\nKaushik Balakrishnan\n', '\nDevesh Upadhyay\n', '\nChris Rackauckas\n']","74 pages, 20+ figures, 20+ tables",,http://arxiv.org/abs/2107.09443v1,cs.MS,"['cs.MS', 'cs.SC']",,,[]
Semantic Reasoning with Differentiable Graph Transformations,http://arxiv.org/abs/2107.09579v1,2021-07-20T15:48:54Z,2021-07-20T15:48:54Z,"  This paper introduces a differentiable semantic reasoner, where rules are
presented as a relevant set of graph transformations. These rules can be
written manually or inferred by a set of facts and goals presented as a
training set. While the internal representation uses embeddings in a latent
space, each rule can be expressed as a set of predicates conforming to a subset
of Description Logic.
",['\nAlberto Cetoli\n'],,"SEMAPRO 2021, The Fifteenth International Conference on Advances
  in Semantic Processing, pages 1-4",http://arxiv.org/abs/2107.09579v1,cs.AI,"['cs.AI', 'cs.SC']",,,[]
On generalizing Descartes' rule of signs to hypersurfaces,http://arxiv.org/abs/2107.10002v2,2021-07-21T10:46:42Z,2022-07-06T15:42:50Z,"  We give partial generalizations of the classical Descartes' rule of signs to
multivariate polynomials (with real exponents), in the sense that we provide
upper bounds on the number of connected components of the complement of a
hypersurface in the positive orthant. In particular, we give conditions based
on the geometrical configuration of the exponents and the sign of the
coefficients that guarantee that the number of connected components where the
polynomial attains a negative value is at most one or two. Our results fully
cover the cases where such an upper bound provided by the univariate Descartes'
rule of signs is one. This approach opens a new route to generalize Descartes'
rule of signs to the multivariate case, differing from previous works that aim
at counting the number of positive solutions of a system of multivariate
polynomial equations.
","['\nElisenda Feliu\n', '\nMáté L. Telek\n']",Final version to appear in Advances in Mathematics,,http://arxiv.org/abs/2107.10002v2,math.AG,"['math.AG', 'cs.SC']",,,[]
Using a template engine as a computer algebra tool,http://arxiv.org/abs/2107.07461v1,2021-07-15T17:04:02Z,2021-07-15T17:04:02Z,"  In research problems that involve the use of numerical methods for solving
systems of ordinary differential equations (ODEs), it is often required to
select the most efficient method for a particular problem. To solve a Cauchy
problem for a system of ODEs, Runge-Kutta methods (explicit or implicit ones,
with or without step-size control, etc.) are employed. In that case, it is
required to search through many implementations of the numerical method and
select coefficients or other parameters of its numerical scheme. This paper
proposes a library and scripts for automated generation of routine functions in
the Julia programming language for a set of numerical schemes of Runge-Kutta
methods. For symbolic manipulations, we use a template substitution tool. The
proposed approach to automated generation of program code allows us to use a
single template for editing, instead of modifying each individual function to
be compared. On the one hand, this provides universality in the implementation
of a numerical scheme and, on the other hand, makes it possible to minimize the
number of errors in the process of modifying the compared implementations of
the numerical method. We consider Runge-Kutta methods without step-size
control, embedded methods with step-size control, and Rosenbrock methods with
step-size control. The program codes for the numerical schemes, which are
generated automatically using the proposed library, are tested by numerical
solution of several well-known problems.
","['\nMigran N. Gevorkyan\n', '\nAnna V. Korolkova\n', '\nDmitry S. Kulyabov\n']",in English; in Russian,,http://dx.doi.org/10.1134/S0361768821010047,math.NA,"['math.NA', 'cs.MS', 'cs.NA', 'cs.SC']",10.1134/S0361768821010047,,[]
Learning Theorem Proving Components,http://arxiv.org/abs/2107.10034v1,2021-07-21T12:00:05Z,2021-07-21T12:00:05Z,"  Saturation-style automated theorem provers (ATPs) based on the given clause
procedure are today the strongest general reasoners for classical first-order
logic. The clause selection heuristics in such systems are, however, often
evaluating clauses in isolation, ignoring other clauses. This has changed
recently by equipping the E/ENIGMA system with a graph neural network (GNN)
that chooses the next given clause based on its evaluation in the context of
previously selected clauses. In this work, we describe several algorithms and
experiments with ENIGMA, advancing the idea of contextual evaluation based on
learning important components of the graph of clauses.
","['\nKarel Chvalovský\n', '\nJan Jakubův\n', '\nMiroslav Olšák\n', '\nJosef Urban\n']",Accepted to TABLEAUX'21,,http://arxiv.org/abs/2107.10034v1,cs.LO,"['cs.LO', 'cs.AI', 'cs.LG', 'cs.NE', 'cs.SC']",,,[]
"Polynomial-Division-Based Algorithms for Computing Linear Recurrence
  Relations",http://arxiv.org/abs/2107.02582v1,2021-07-06T12:50:57Z,2021-07-06T12:50:57Z,"  Sparse polynomial interpolation, sparse linear system solving or modular
rational reconstruction are fundamental problems in Computer Algebra. They come
down to computing linear recurrence relations of a sequence with the
Berlekamp-Massey algorithm. Likewise, sparse multivariate polynomial
interpolation and multidimensional cyclic code decoding require guessing linear
recurrence relations of a multivariate sequence.Several algorithms solve this
problem. The so-called Berlekamp-Massey-Sakata algorithm (1988) uses polynomial
additions and shifts by a monomial. The Scalar-FGLM algorithm (2015) relies on
linear algebra operations on a multi-Hankel matrix, a multivariate
generalization of a Hankel matrix. The Artinian Gorenstein border basis
algorithm (2017) uses a Gram-Schmidt process.We propose a new algorithm for
computing the Gr{\""o}bner basis of the ideal of relations of a sequence based
solely on multivariate polynomial arithmetic. This algorithm allows us to both
revisit the Berlekamp-Massey-Sakata algorithm through the use of polynomial
divisions and to completely revise the Scalar-FGLM algorithm without linear
algebra operations.A key observation in the design of this algorithm is to work
on the mirror of the truncated generating series allowing us to use polynomial
arithmetic modulo a monomial ideal. It appears to have some similarities with
Pad{\'e} approximants of this mirror polynomial.As an addition from the paper
published at the ISSAC conferance, we give an adaptive variant of this
algorithm taking into account the shape of the final Gr{\""o}bner basis
gradually as it is discovered. The main advantage of this algorithm is that its
complexity in terms of operations and sequence queries only depends on the
output Gr{\""o}bner basis.All these algorithms have been implemented in Maple
and we report on our comparisons.
","['\nJérémy Berthomieu\nPolSys\n', '\nJean-Charles Faugère\nPolSys\n']",,"Journal of Symbolic Computation, Elsevier, In press",http://arxiv.org/abs/2107.02582v1,cs.SC,['cs.SC'],,,"['PolSys', 'PolSys']"
"Testing Binomiality of Chemical Reaction Networks Using Comprehensive
  Gröbner Systems",http://arxiv.org/abs/2107.01706v2,2021-07-04T18:44:07Z,2021-07-07T09:15:25Z,"  We consider the problem of binomiality of the steady state ideals of
biochemical reaction networks. We are interested in finding polynomial
conditions on the parameters such that the steady state ideal of a chemical
reaction network is binomial under every specialisation of the parameters if
the conditions on the parameters hold. We approach the binomiality problem
using Comprehensive Gr\""obner systems. Considering rate constants as
parameters, we compute comprehensive Gr\""obner systems for various reactions.
In particular, we make automatic computations on n-site phosphorylations and
biomodels from the Biomodels repository using the grobcov library of the
computer algebra system Singular.
","['\nHamid Rahkooy\n', '\nThomas Sturm\n']",,,http://arxiv.org/abs/2107.01706v2,q-bio.MN,"['q-bio.MN', 'cs.SC']",,,[]
Decomposition algorithms for tensors and polynomials,http://arxiv.org/abs/2107.04097v1,2021-07-08T20:31:05Z,2021-07-08T20:31:05Z,"  We give algorithms to compute decompositions of a given polynomial, or more
generally mixed tensor, as sum of rank one tensors, and to establish whether
such a decomposition is unique. In particular, we present methods to compute
the decomposition of a general plane quintic in seven powers, and of a general
space cubic in five powers; the two decompositions of a general plane sextic of
rank nine, and the five decompositions of a general plane septic. Furthermore,
we give Magma implementations of all our algorithms.
","['\nAntonio Laface\n', '\nAlex Massarenti\n', '\nRick Rischter\n']",19 pages,,http://arxiv.org/abs/2107.04097v1,math.AG,"['math.AG', 'cs.MS', 'cs.SC', 'Primary 14N07, Secondary 14N05, 51N35, 14Q15, 14N15']",,,[]
"Sensitivity analysis in differentially private machine learning using
  hybrid automatic differentiation",http://arxiv.org/abs/2107.04265v2,2021-07-09T07:19:23Z,2021-08-17T06:21:58Z,"  In recent years, formal methods of privacy protection such as differential
privacy (DP), capable of deployment to data-driven tasks such as machine
learning (ML), have emerged. Reconciling large-scale ML with the closed-form
reasoning required for the principled analysis of individual privacy loss
requires the introduction of new tools for automatic sensitivity analysis and
for tracking an individual's data and their features through the flow of
computation. For this purpose, we introduce a novel \textit{hybrid} automatic
differentiation (AD) system which combines the efficiency of reverse-mode AD
with an ability to obtain a closed-form expression for any given quantity in
the computational graph. This enables modelling the sensitivity of arbitrary
differentiable function compositions, such as the training of neural networks
on private data. We demonstrate our approach by analysing the individual DP
guarantees of statistical database queries. Moreover, we investigate the
application of our technique to the training of DP neural networks. Our
approach can enable the principled reasoning about privacy loss in the setting
of data processing, and further the development of automatic sensitivity
analysis and privacy budgeting systems.
","['\nAlexander Ziller\n', '\nDmitrii Usynin\n', '\nMoritz Knolle\n', '\nKritika Prakash\n', '\nAndrew Trask\n', '\nRickmer Braren\n', '\nMarcus Makowski\n', '\nDaniel Rueckert\n', '\nGeorgios Kaissis\n']","Accepted to the ICML 2021 Theory and Practice of Differential Privacy
  Workshop",,http://arxiv.org/abs/2107.04265v2,cs.LG,"['cs.LG', 'cs.CR', 'cs.SC']",,,[]
"Systematic human learning and generalization from a brief tutorial with
  explanatory feedback",http://arxiv.org/abs/2107.06994v2,2021-07-10T00:14:41Z,2023-03-29T02:15:18Z,"  Neural networks have long been used to model human intelligence, capturing
elements of behavior and cognition, and their neural basis. Recent advancements
in deep learning have enabled neural network models to reach and even surpass
human levels of intelligence in many respects, yet unlike humans, their ability
to learn new tasks quickly remains a challenge. People can reason not only in
familiar domains, but can also rapidly learn to reason through novel problems
and situations, raising the question of how well modern neural network models
capture human intelligence and in which ways they diverge. In this work, we
explore this gap by investigating human adults' ability to learn an abstract
reasoning task based on Sudoku from a brief instructional tutorial with
explanatory feedback for incorrect responses using a narrow range of training
examples. We find that participants who master the task do so within a small
number of trials and generalize well to puzzles outside of the training range.
We also find that most of those who master the task can describe a valid
solution strategy, and such participants perform better on transfer puzzles
than those whose strategy descriptions are vague or incomplete. Interestingly,
fewer than half of our human participants were successful in acquiring a valid
solution strategy, and this ability is associated with high school mathematics
education. We consider the challenges these findings pose for building
computational models that capture all aspects of our findings and point toward
a possible role for learning to engage in explanation-based reasoning to
support rapid learning and generalization.
","['\nAndrew J. Nam\nStanford University\n', '\nJames L. McClelland\nStanford University\n']","27 pages, 108 references, 8 Figures, and one Table, plus
  Supplementary Materials",,http://arxiv.org/abs/2107.06994v2,cs.LG,"['cs.LG', 'cs.AI', 'cs.SC']",,,"['Stanford University', 'Stanford University']"
"Certifying a probabilistic parallel modular algorithm for rational
  univariate representation",http://arxiv.org/abs/2106.10912v3,2021-06-21T08:24:22Z,2021-08-31T08:12:31Z,"  This paper is about solving polynomial systems. It first recalls how to do
that efficiently with a very high probability of correctness by reconstructing
a rational univariate representation (rur) using Groebner revlex computation,
Berlekamp-Massey algorithm and Hankel linear system solving modulo several
primes in parallel. Then it introduces a new method (theorem \ref{prop:check})
for rur certification that is effective for most polynomial systems.These
algorithms are implemented in
https://www-fourier.univ-grenoble-alpes.fr/~parisse/giac.html since version
1.7.0-13 or 1.7.0-17 for certification, it has (July 2021) leading performances
on multiple CPU, at least for an open-source software.
",['\nBernard Parisse\nIF\n'],,,http://arxiv.org/abs/2106.10912v3,cs.SC,['cs.SC'],,,['IF']
Multivariate Power Series in Maple,http://arxiv.org/abs/2106.15519v1,2021-06-21T16:54:04Z,2021-06-21T16:54:04Z,"  We present MultivariatePowerSeries, a Maple library introduced in Maple 2021,
providing a variety of methods to study formal multivariate power series and
univariate polynomials over such series. This library offers a simple and
easy-to-use user interface. Its implementation relies on lazy evaluation
techniques and takes advantage of Maple's features for object-oriented
programming. The exposed methods include Weierstrass Preparation Theorem and
factorization via Hensel's lemma. The computational performance is demonstrated
by means of an experimental comparison with software counterparts.
","['\nMohammadali Asadi\n', '\nAlexander Brandt\n', '\nMahsa Kazemi\n', '\nMarc Moreno Maza\n', '\nErik Postma\n']",,,http://arxiv.org/abs/2106.15519v1,cs.SC,['cs.SC'],,,[]
"Computing Characteristic Polynomials of p-Curvatures in Average
  Polynomial Time",http://arxiv.org/abs/2106.14637v2,2021-06-28T12:40:27Z,2021-07-02T06:40:22Z,"  We design a fast algorithm that computes, for a given linear differential
operator with coefficients in $Z[x ]$, all the characteristic polynomials of
its p-curvatures, for all primes $p < N$ , in asymptotically quasi-linear bit
complexity in N. We discuss implementations and applications of our algorithm.
We shall see in particular that the good performances of our algorithm are
quickly visible.
","['\nRaphaël Pagès\nIMB, LFANT, SPECFUN\n']",,"ISSAC 2021 - International Symposium on Symbolic and Algebraic
  Computation, Jul 2021, Saint-Petersbourg / Virtual, Russia",http://dx.doi.org/10.1145/3452143.3465524,cs.SC,"['cs.SC', 'math.FA']",10.1145/3452143.3465524,,"['IMB, LFANT, SPECFUN']"
Recovery from Power Sums,http://arxiv.org/abs/2106.13981v1,2021-06-26T09:31:22Z,2021-06-26T09:31:22Z,"  We study the problem of recovering a collection of $n$ numbers from the
evaluation of $m$ power sums. This yields a system of polynomial equations,
which can be underconstrained ($m < n$), square ($m = n$), or overconstrained
($m > n$). Fibers and images of power sum maps are explored in all three
regimes, and in settings that range from complex and projective to real and
positive. This involves surprising deviations from the B\'ezout bound, and the
recovery of vectors from length measurements by $p$-norms.
","['\nHana Melánová\n', '\nBernd Sturmfels\n', '\nRosa Winter\n']",15 pages,,http://arxiv.org/abs/2106.13981v1,math.AG,"['math.AG', 'cs.SC', 'math.CO']",,,[]
SymbolicGPT: A Generative Transformer Model for Symbolic Regression,http://arxiv.org/abs/2106.14131v1,2021-06-27T03:26:35Z,2021-06-27T03:26:35Z,"  Symbolic regression is the task of identifying a mathematical expression that
best fits a provided dataset of input and output values. Due to the richness of
the space of mathematical expressions, symbolic regression is generally a
challenging problem. While conventional approaches based on genetic evolution
algorithms have been used for decades, deep learning-based methods are
relatively new and an active research area. In this work, we present
SymbolicGPT, a novel transformer-based language model for symbolic regression.
This model exploits the advantages of probabilistic language models like GPT,
including strength in performance and flexibility. Through comprehensive
experiments, we show that our model performs strongly compared to competing
models with respect to the accuracy, running time, and data efficiency.
","['\nMojtaba Valipour\n', '\nBowen You\n', '\nMaysum Panju\n', '\nAli Ghodsi\n']","11 pages, 4 figures",,http://arxiv.org/abs/2106.14131v1,cs.LG,"['cs.LG', 'cs.CL', 'cs.SC']",,,[]
Conormal Spaces and Whitney Stratifications,http://arxiv.org/abs/2106.14555v4,2021-06-28T10:23:15Z,2022-12-27T04:51:04Z,"  We describe a new algorithm for computing Whitney stratifications of complex
projective varieties. The main ingredients are (a) an algebraic criterion, due
to L\^e and Teissier, which reformulates Whitney regularity in terms of
conormal spaces and maps, and (b) a new interpretation of this conormal
criterion via primary decomposition, which can be practically implemented on a
computer. We show that this algorithm improves upon the existing state of the
art by several orders of magnitude, even for relatively small input varieties.
En route, we introduce related algorithms for efficiently stratifying affine
varieties, flags on a given variety, and algebraic maps.
","['\nMartin Helmer\n', '\nVidit Nanda\n']","There is an error in the published version of the article (Found
  Comput Math, 2022) which has been fixed in this update. Section 3 is entirely
  new, but the downstream results Sections 4-6 remain largely the same. We have
  also updated the Runtimes and Complexity estimates in Section 7. The def. of
  the integral closure of an ideal has also been corrected",,http://dx.doi.org/10.1007/s10208-022-09574-8,math.AG,"['math.AG', 'cs.SC', 'math.AC', 'math.AT', '14B05, 14Q20, 32S60, 32S15']",10.1007/s10208-022-09574-8,,[]
Web-based Structural Identifiability Analyzer,http://arxiv.org/abs/2106.15066v1,2021-06-29T02:57:34Z,2021-06-29T02:57:34Z,"  Parameter identifiability describes whether, for a given differential model,
one can determine parameter values from model equations. Knowing global or
local identifiability properties allows construction of better practical
experiments to identify parameters from experimental data. In this work, we
present a web-based software tool that allows to answer specific
identifiability queries. Concretely, our toolbox can determine identifiability
of individual parameters of the model and also provide all functions of
parameters that are identifiable (also called identifiable combinations) from
single or multiple experiments. The program is freely available at
https://maple.cloud/app/6509768948056064.
","['\nIlia Ilmer\n', '\nAlexey Ovchinnikov\n', '\nGleb Pogudin\n']",,,http://arxiv.org/abs/2106.15066v1,cs.MS,"['cs.MS', 'cs.SC', 'cs.SY', 'eess.SY', 'q-bio.QM']",,,[]
"Automatic Differentiation With Higher Infinitesimals, or Computational
  Smooth Infinitesimal Analysis in Weil Algebra",http://arxiv.org/abs/2106.14153v2,2021-06-27T06:17:26Z,2021-07-05T03:22:50Z,"  We propose an algorithm to compute the $C^\infty$-ring structure of arbitrary
Weil algebra. It allows us to do some analysis with higher infinitesimals
numerically and symbolically. To that end, we first give a brief description of
the (Forward-mode) automatic differentiation (AD) in terms of $C^\infty$-rings.
The notion of a $C^\infty$-ring was introduced by Lawvere and used as the
fundamental building block of smooth infinitesimal analysis and synthetic
differential geometry. We argue that interpreting AD in terms of
$C^\infty$-rings gives us a unifying theoretical framework and modular ways to
express multivariate partial derivatives. In particular, we can ""package""
higher-order Forward-mode AD as a Weil algebra, and take tensor products to
compose them to achieve multivariate higher-order AD. The algorithms in the
present paper can also be used for a pedagogical purpose in learning and
studying smooth infinitesimal analysis as well.
",['\nHiromi Ishii\n'],to appear in Computer Algebra in Scientific Computing 2021,"Computer Algebra in Scientific Computing, pp. 174-191. CASC 2021.
  Lecture Notes in Computer Science, vol 12865. Springer, Cham",http://dx.doi.org/10.1007/978-3-030-85165-1_11,cs.SC,"['cs.SC', 'cs.MS', 'cs.NA', 'math.CT', 'math.DG', 'math.NA']",10.1007/978-3-030-85165-1_11,,[]
"The DEWCAD Project: Pushing Back the Doubly Exponential Wall of
  Cylindrical Algebraic Decomposition",http://arxiv.org/abs/2106.08740v1,2021-06-16T12:33:09Z,2021-06-16T12:33:09Z,"  This abstract seeks to introduce the ISSAC community to the DEWCAD project,
which is based at Coventry University and the University of Bath, in the United
Kingdom. The project seeks to push back the Doubly Exponential Wall of
Cylindrical Algebraic Decomposition, through the integration of SAT/SMT
technology, the extension of Lazard projection theory, and the development of
new algorithms based on CAD technology but without producing CADs themselves.
The project also seeks to develop applications of CAD and will focus on
applications in the domains of economics and bio-network analysis.
","['\nR. Bradford\n', '\nJ. H. Davenport\n', '\nM. England\n', '\nA. Sadeghimanesh\n', '\nA. Uncu\n']",5 pages. Accepted as short communication at ISSAC 2021,"ACM Communications in Computer Algebra 55:3 (issue 217), pp.
  107-111, ACM, 2021",http://dx.doi.org/10.1145/3511528.3511538,cs.SC,"['cs.SC', '68W30, 03C10', 'I.1.2; I.1.4; G.4; J.3; J.4']",10.1145/3511528.3511538,,[]
Fast evaluation of some p-adic transcendental functions,http://arxiv.org/abs/2106.09315v1,2021-06-17T08:30:29Z,2021-06-17T08:30:29Z,"  We design algorithms for computing values of many p-adic elementary and
special functions, including logarithms, exponentials, polylogarithms, and
hypergeometric functions. All our algorithms feature a quasi-linear complexity
with respect to the target precision and most of them are based on an
adaptation to the-adic setting of the binary splitting and bit-burst
strategies.
","['\nXavier Caruso\nIMB, LFANT\n', '\nMarc Mezzarobba\nLIX\n', '\nNobuki Takayama\nXLIM\n', '\nTristan Vaccon\nXLIM\n']",,,http://arxiv.org/abs/2106.09315v1,cs.SC,"['cs.SC', 'math.NT']",,,"['IMB, LFANT', 'LIX', 'XLIM', 'XLIM']"
Verifying Quantized Neural Networks using SMT-Based Model Checking,http://arxiv.org/abs/2106.05997v2,2021-06-10T18:27:45Z,2021-09-16T17:42:34Z,"  Artificial Neural Networks (ANNs) are being deployed for an increasing number
of safety-critical applications, including autonomous cars and medical
diagnosis. However, concerns about their reliability have been raised due to
their black-box nature and apparent fragility to adversarial attacks. These
concerns are amplified when ANNs are deployed on restricted system, which limit
the precision of mathematical operations and thus introduce additional
quantization errors. Here, we develop and evaluate a novel symbolic
verification framework using software model checking (SMC) and satisfiability
modulo theories (SMT) to check for vulnerabilities in ANNs. More specifically,
we propose several ANN-related optimizations for SMC, including invariant
inference via interval analysis, slicing, expression simplifications, and
discretization of non-linear activation functions. With this verification
framework, we can provide formal guarantees on the safe behavior of ANNs
implemented both in floating- and fixed-point arithmetic. In this regard, our
verification approach was able to verify and produce adversarial examples for
$52$ test cases spanning image classification and general machine learning
applications. Furthermore, for small- to medium-sized ANN, our approach
completes most of its verification runs in minutes. Moreover, in contrast to
most state-of-the-art methods, our approach is not restricted to specific
choices regarding activation functions and non-quantized representations. Our
experiments show that our approach can analyze larger ANN implementations and
substantially reduce the verification time compared to state-of-the-art
techniques that use SMT solving.
","['\nLuiz Sena\n', '\nXidan Song\n', '\nErickson Alves\n', '\nIury Bessa\n', '\nEdoardo Manino\n', '\nLucas Cordeiro\n', '\nEddie de Lima Filho\n']","Changes with respect to the previous version: improved explanation of
  our methodology in Section 3; improved and extended experimental evaluation
  in Section 4; added comparison with the state of the art in Section 4.5",,http://arxiv.org/abs/2106.05997v2,cs.LG,"['cs.LG', 'cs.CR', 'cs.LO', 'cs.SC']",,,[]
LNN-EL: A Neuro-Symbolic Approach to Short-text Entity Linking,http://arxiv.org/abs/2106.09795v1,2021-06-17T20:22:45Z,2021-06-17T20:22:45Z,"  Entity linking (EL), the task of disambiguating mentions in text by linking
them to entities in a knowledge graph, is crucial for text understanding,
question answering or conversational systems. Entity linking on short text
(e.g., single sentence or question) poses particular challenges due to limited
context. While prior approaches use either heuristics or black-box neural
methods, here we propose LNN-EL, a neuro-symbolic approach that combines the
advantages of using interpretable rules based on first-order logic with the
performance of neural learning. Even though constrained to using rules, LNN-EL
performs competitively against SotA black-box neural approaches, with the added
benefits of extensibility and transferability. In particular, we show that we
can easily blend existing rule templates given by a human expert, with multiple
types of features (priors, BERT encodings, box embeddings, etc), and even
scores resulting from previous EL methods, thus improving on such methods. For
instance, on the LC-QuAD-1.0 dataset, we show more than $4$\% increase in F1
score over previous SotA. Finally, we show that the inductive bias offered by
using logic results in learned rules that transfer well across datasets, even
without fine tuning, while maintaining high accuracy.
","['\nHang Jiang\n', '\nSairam Gurajada\n', '\nQiuhao Lu\n', '\nSumit Neelam\n', '\nLucian Popa\n', '\nPrithviraj Sen\n', '\nYunyao Li\n', '\nAlexander Gray\n']",Accepted to ACL 2021,,http://arxiv.org/abs/2106.09795v1,cs.CL,"['cs.CL', 'cs.AI', 'cs.SC']",,,[]
Faster Sparse Matrix Inversion and Rank Computation in Finite Fields,http://arxiv.org/abs/2106.09830v2,2021-06-17T22:01:46Z,2022-12-10T00:03:33Z,"  We improve the current best running time value to invert sparse matrices over
finite fields, lowering it to an expected $O\big(n^{2.2131}\big)$ time for the
current values of fast rectangular matrix multiplication. We achieve the same
running time for the computation of the rank and nullspace of a sparse matrix
over a finite field. This improvement relies on two key techniques. First, we
adopt the decomposition of an arbitrary matrix into block Krylov and Hankel
matrices from Eberly et al. (ISSAC 2007). Second, we show how to recover the
explicit inverse of a block Hankel matrix using low displacement rank
techniques for structured matrices and fast rectangular matrix multiplication
algorithms. We generalize our inversion method to block structured matrices
with other displacement operators and strengthen the best known upper bounds
for explicit inversion of block Toeplitz-like and block Hankel-like matrices,
as well as for explicit inversion of block Vandermonde-like matrices with
structured blocks. As a further application, we improve the complexity of
several algorithms in topological data analysis and in finite group theory.
","['\nSílvia Casacuberta\n', '\nRasmus Kyng\n']","Appeared at ITCS 2022. Fixed the runtimes in Section 3 by using the
  correct generalization of the BA80 algorithm to arbitrary matrices with large
  displacement rank",,http://arxiv.org/abs/2106.09830v2,cs.DS,"['cs.DS', 'cs.NA', 'cs.SC', 'math.NA']",,,[]
Resultant-based Elimination in Ore Algebra,http://arxiv.org/abs/2105.14799v3,2021-05-31T08:49:42Z,2022-10-07T02:36:36Z,"  We consider resultant-based methods for elimination of indeterminates of Ore
polynomial systems in Ore algebra. We start with defining the concept of
resultant for bivariate Ore polynomials then compute it by the Dieudonne
determinant of the polynomial coefficients. Additionally, we apply
noncommutative versions of evaluation and interpolation techniques to the
computation process to improve the efficiency of the method. The implementation
of the algorithms will be performed in Maple to evaluate the performance of the
approaches.
",['\nRaqeeb Rasheed\n'],"An updated (and shorter) version published in the SYNASC '21
  proceedings (IEEE CS) with the title ""Resultant-based Elimination for Skew
  Polynomials""","Proceedings of the 23rd International Symposium on Symbolic and
  Numeric Algorithms for Scientific Computing (SYNASC '21), pp. 11-18. IEEE,
  2021",http://dx.doi.org/10.1109/SYNASC54541.2021.00014,cs.SC,['cs.SC'],10.1109/SYNASC54541.2021.00014,,[]
"A Computer Program for the Numerical Analysis of Economic Cycles Within
  the Framework of the Dubovsky Generalized Model",http://arxiv.org/abs/2106.01827v1,2021-06-03T13:23:50Z,2021-06-03T13:23:50Z,"  The article proposes a computer program for calculating economic crises
according to the generalized mathematical model of S.V. Dubovsky. This model is
represented by a system of ordinary nonlinear differential equations with
fractional derivatives in the sense of Gerasimov-Caputo with initial
conditions. Furthermore, according to a numerical algorithm based on an
explicit nonlocal finite-difference scheme, oscillograms and phase trajectories
were constructed. It is shown that changing the orders of fractional
derivatives in the model can give rise to various modes, for example, damped
modes with a steady-state amplitude. It is concluded that the orders of
fractional derivatives are responsible for the intensity of the process.
","['\nDanil Makarov\n', '\nRoman Parovik\n']",,,http://dx.doi.org/10.1063/5.0092376,math.NA,"['math.NA', 'cs.NA', 'cs.SC', '37N30', 'G.1.7']",10.1063/5.0092376,,[]
"New data structure for univariate polynomial approximation and
  applications to root isolation, numerical multipoint evaluation, and other
  problems",http://arxiv.org/abs/2106.02505v4,2021-06-04T14:20:05Z,2021-11-29T10:03:51Z,"  We present a new data structure to approximate accurately and efficiently a
polynomial $f$ of degree $d$ given as a list of coefficients. Its properties
allow us to improve the state-of-the-art bounds on the bit complexity for the
problems of root isolation and approximate multipoint evaluation. This data
structure also leads to a new geometric criterion to detect ill-conditioned
polynomials, implying notably that the standard condition number of the zeros
of a polynomial is at least exponential in the number of roots of modulus less
than $1/2$ or greater than $2$.Given a polynomial $f$ of degree $d$ with
$\|f\|_1 \leq 2^\tau$ for $\tau \geq 1$, isolating all its complex roots or
evaluating it at $d$ points can be done with a quasi-linear number of
arithmetic operations. However, considering the bit complexity, the
state-of-the-art algorithms require at least $d^{3/2}$ bit operations even for
well-conditioned polynomials and when the accuracy required is low. Given a
positive integer $m$, we can compute our new data structure and evaluate $f$ at
$d$ points in the unit disk with an absolute error less than $2^{-m}$ in
$\widetilde O(d(\tau+m))$ bit operations, where $\widetilde O(\cdot)$ means
that we omit logarithmic factors. We also show that if $\kappa$ is the absolute
condition number of the zeros of $f$, then we can isolate all the roots of $f$
in $\widetilde O(d(\tau + \log \kappa))$ bit operations. Moreover, our
algorithms are simple to implement. For approximating the complex roots of a
polynomial, we implemented a small prototype in \verb|Python/NumPy| that is an
order of magnitude faster than the state-of-the-art solver \verb/MPSolve/ for
high degree polynomials with random coefficients.
",['\nGuillaume Moroz\nGAMBLE\n'],,"FOCS 2021 - 62nd Annual IEEE Symposimum on Foundations of Computer
  Science, Feb 2022, Denver, United States",http://arxiv.org/abs/2106.02505v4,cs.SC,"['cs.SC', 'cs.NA', 'math.NA']",,,['GAMBLE']
Learning a performance metric of Buchberger's algorithm,http://arxiv.org/abs/2106.03676v2,2021-06-07T14:57:57Z,2022-05-31T18:56:27Z,"  What can be (machine) learned about the complexity of Buchberger's algorithm?
  Given a system of polynomials, Buchberger's algorithm computes a Gr\""obner
basis of the ideal these polynomials generate using an iterative procedure
based on multivariate long division. The runtime of each step of the algorithm
is typically dominated by a series of polynomial additions, and the total
number of these additions is a hardware independent performance metric that is
often used to evaluate and optimize various implementation choices. In this
work we attempt to predict, using just the starting input, the number of
polynomial additions that take place during one run of Buchberger's algorithm.
Good predictions are useful for quickly estimating difficulty and understanding
what features make Gr\""obner basis computation hard. Our features and methods
could also be used for value models in the reinforcement learning approach to
optimize Buchberger's algorithm introduced in [Peifer, Stillman, and
Halpern-Leistner, 2020].
  We show that a multiple linear regression model built from a set of
easy-to-compute ideal generator statistics can predict the number of polynomial
additions somewhat well, better than an uninformed model, and better than
regression models built on some intuitive commutative algebra invariants that
are more difficult to compute. We also train a simple recursive neural network
that outperforms these linear models. Our work serves as a proof of concept,
demonstrating that predicting the number of polynomial additions in
Buchberger's algorithm is a feasible problem from the point of view of machine
learning.
","['\nJelena Mojsilović\n', '\nDylan Peifer\n', '\nSonja Petrović\n']",,Involve 16 (2023) 227-248,http://dx.doi.org/10.2140/involve.2023.16.227,math.AC,"['math.AC', 'cs.LG', 'cs.SC', 'math.AG', 'stat.ML']",10.2140/involve.2023.16.227,,[]
Computing the dimension of real algebraic sets,http://arxiv.org/abs/2105.10255v2,2021-05-21T10:12:39Z,2021-06-13T18:15:04Z,"  Let $V$ be the set of real common solutions to $F = (f_1, \ldots, f_s)$ in
$\mathbb{R}[x_1, \ldots, x_n]$ and $D$ be the maximum total degree of the
$f_i$'s. We design an algorithm which on input $F$ computes the dimension of
$V$. Letting $L$ be the evaluation complexity of $F$ and $s=1$, it runs using
$O^\sim \big (L D^{n(d+3)+1}\big )$ arithmetic operations in $\mathbb{Q}$ and
at most $D^{n(d+1)}$ isolations of real roots of polynomials of degree at most
$D^n$. Our algorithm depends on the real geometry of $V$; its practical
behavior is more governed by the number of topology changes in the fibers of
some well-chosen maps. Hence, the above worst-case bounds are rarely reached in
practice, the factor $D^{nd}$ being in general much lower on practical
examples. We report on an implementation showing its ability to solve problems
which were out of reach of the state-of-the-art implementations.
","['\nPiere Lairez\n', '\nMohab Safey El Din\n']",v2: title change,ISSAC 2021,http://dx.doi.org/10.1145/3452143.3465551,cs.SC,['cs.SC'],10.1145/3452143.3465551,,[]
Binomial Determinants for Tiling Problems Yield to the Holonomic Ansatz,http://arxiv.org/abs/2105.08539v2,2021-05-18T14:14:56Z,2021-09-21T09:31:00Z,"  We present and prove closed form expressions for some families of binomial
determinants with signed Kronecker deltas that are located along an arbitrary
diagonal in the corresponding matrix. They count cyclically symmetric rhombus
tilings of hexagonal regions with triangular holes. We extend a previous
systematic study of these families, where the locations of the Kronecker deltas
depended on an additional parameter, to families with negative Kronecker
deltas. By adapting Zeilberger's holonomic ansatz to make it work for our
problems, we can take full advantage of computer algebra tools for symbolic
summation. This, together with the combinatorial interpretation, allows us to
realize some new determinantal relationships. From there, we are able to
resolve all remaining open conjectures related to these determinants, including
one from 2005 due to Lascoux and Krattenthaler.
","['\nHao Du\n', '\nChristoph Koutschan\n', '\nThotsaporn Thanatipanonda\n', '\nElaine Wong\n']","45 pages; Supplementary material at
  https://wongey.github.io/binom-det","European Journal of Combinatorics, Volume 99, January 2022, 103437",http://dx.doi.org/10.1016/j.ejc.2021.103437,math.CO,"['math.CO', 'cs.SC']",10.1016/j.ejc.2021.103437,,[]
A Flawed Dataset for Symbolic Equation Verification,http://arxiv.org/abs/2105.11479v4,2021-05-24T18:05:38Z,2021-05-28T03:05:46Z,"  Arabshahi, Singh, and Anandkumar (2018) propose a method for creating a
dataset of symbolic mathematical equations for the tasks of symbolic equation
verification and equation completion. Unfortunately, a dataset constructed
using the method they propose will suffer from two serious flaws. First, the
class of true equations that the procedure can generate will be very limited.
Second, because true and false equations are generated in completely different
ways, there are likely to be artifactual features that allow easy
discrimination.
  Moreover, over the class of equations they consider, there is an extremely
simple probabilistic procedure that solves the problem of equation verification
with extremely high reliability. The usefulness of this problem in general as a
testbed for AI systems is therefore doubtful.
",['\nErnest Davis\n'],,,http://arxiv.org/abs/2105.11479v4,cs.AI,"['cs.AI', 'cs.SC']",,,[]
"Implementation and Evaluation of a Multivariate Abstraction-Based,
  Interval-Based Dynamic Time-Warping Method as a Similarity Measure for
  Longitudinal Medical Records",http://arxiv.org/abs/2105.08450v2,2021-05-18T11:41:42Z,2021-08-26T10:59:13Z,"  We extended dynamic time warping (DTW) into interval-based dynamic time
warping (iDTW), including (A) interval-based representation (iRep): [1]
abstracting raw, time-stamped data into interval-based abstractions, [2]
comparison-period scoping, [3] partitioning abstract intervals into a given
temporal granularity; (B) interval-based matching (iMatch): matching
partitioned, abstract-concepts records, using a modified DTW. Using domain
knowledge, we abstracted the raw data of medical records, for up to three
concepts out of four or five relevant concepts, into two interval types: State
abstractions (e.g. LOW, HIGH) and Gradient abstractions (e.g. INCREASING,
DECREASING). We created all uni-dimensional (State or Gradient) or
multi-dimensional (State and Gradient) abstraction combinations. Tasks:
Classifying 161 oncology patients records as autologous or allogenic
bone-marrow transplantation; classifying 125 hepatitis patients records as B or
C hepatitis; predicting micro- or macro-albuminuria in the next year for 151
Type 2 diabetes patients. We used a k-Nearest-Neighbors majority, k = an odd
number from 1 to SQRT(N), N = set size. 75,936 10-fold cross-validation
experiments were performed: 33,600 (Oncology), 28,800 (Hepatitis), 13,536
(Diabetes). Measures: Area Under the Curve (AUC), optimal Youden's Index.
Paired t-tests compared result vectors for equivalent configurations other than
a tested variable, to determine a significant mean accuracy difference
(P<0.05). Mean classification and prediction using abstractions was
significantly better than using only raw time-stamped data. In each domain, at
least one abstraction combination led to a significantly better mean
performance than raw data. Increasing feature number and using
Multi-dimensional abstractions enhanced performance. Unlike when using raw
data, optimal mean performance was often reached with k=5, using abstractions.
","['\nYuval Shahar\n', '\nMatan Lion\n']","38 pages; 5 figures; 8 tables including three in the Appendix.
  Compared to the previous version, we have recomputed more accurately the
  number of experimental instances, and added two tables of summary and
  examples to the Results",,http://dx.doi.org/10.1016/j.jbi.2021.103919,cs.AI,"['cs.AI', 'cs.LG', 'cs.SC', 'I.2.6']",10.1016/j.jbi.2021.103919,,[]
Yet another eigenvalue algorithm for solving polynomial systems,http://arxiv.org/abs/2105.08472v2,2021-05-18T12:29:11Z,2022-02-10T21:35:00Z,"  In latest years, several advancements have been made in symbolic-numerical
eigenvalue techniques for solving polynomial systems. In this article, we add
to this list. We design an algorithm which solves systems with isolated
solutions reliably and efficiently. In overdetermined cases, it reduces the
task to an eigenvalue problem in a simpler and considerably faster way than in
previous methods, and it can outperform the homotopy continuation approach. We
provide many examples and an implementation in the proof-of-concept Julia
package EigenvalueSolver.jl.
","['\nMatías R. Bender\n', '\nSimon Telen\n']",30 pages,,http://arxiv.org/abs/2105.08472v2,math.NA,"['math.NA', 'cs.NA', 'cs.SC', 'math.AG', '65H04 (Primary), 65H10']",,,[]
"SANM: A Symbolic Asymptotic Numerical Solver with Applications in Mesh
  Deformation",http://arxiv.org/abs/2105.08535v1,2021-05-18T14:04:06Z,2021-05-18T14:04:06Z,"  Solving nonlinear systems is an important problem. Numerical continuation
methods efficiently solve certain nonlinear systems. The Asymptotic Numerical
Method (ANM) is a powerful continuation method that usually converges faster
than Newtonian methods. ANM explores the landscape of the function by following
a parameterized solution curve approximated with a high-order power series.
Although ANM has successfully solved a few graphics and engineering problems,
prior to our work, applying ANM to new problems required significant effort
because the standard ANM assumes quadratic functions, while manually deriving
the power series expansion for nonquadratic systems is a tedious and
challenging task.
  This paper presents a novel solver, SANM, that applies ANM to solve
symbolically represented nonlinear systems. SANM solves such systems in a fully
automated manner. SANM also extends ANM to support many nonquadratic operators,
including intricate ones such as singular value decomposition. Furthermore,
SANM generalizes ANM to support the implicit homotopy form. Moreover, SANM
achieves high computing performance via optimized system design and
implementation.
  We deploy SANM to solve forward and inverse elastic force equilibrium
problems and controlled mesh deformation problems with a few constitutive
models. Our results show that SANM converges faster than Newtonian solvers,
requires little programming effort for new problems, and delivers comparable or
better performance than a hand-coded, specialized ANM solver. While we
demonstrate on mesh deformation problems, SANM is generic and potentially
applicable to many tasks.
",['\nKai Jia\n'],SIGGRAPH 2021,,http://arxiv.org/abs/2105.08535v1,math.NA,"['math.NA', 'cs.GR', 'cs.NA', 'cs.SC']",,,[]
Recursion formulas for integrated products of Jacobi polynomials,http://arxiv.org/abs/2105.08989v1,2021-05-19T08:34:57Z,2021-05-19T08:34:57Z,"  From the literature it is known that orthogonal polynomials as the Jacobi
polynomials can be expressed by hypergeometric series. In this paper, the
authors derive several contiguous relations for terminating multivariate
hypergeometric series. With these contiguous relations one can prove several
recursion formulas of those series. This theoretical result allows to compute
integrals over products of Jacobi polynomials in a very efficient recursive
way. Moreover, the authors present an application to numerical analysis where
it can be used in algorithms which compute the approximate solution of boundary
value problem of partial differential equations by means of the finite elements
method (FEM). With the aid of the contiguous relations, the approximate
solution can be computed much faster than using numerical integration. A
numerical example illustrates this effect.
","['\nSven Beuchler\n', '\nTim Haubold\n', '\nVeronika Pillwein\n']",,,http://dx.doi.org/10.1007/s00365-023-09655-z,math.NA,"['math.NA', 'cs.NA', 'cs.SC', '33C45, 33C70, 65N30']",10.1007/s00365-023-09655-z,,[]
"On the Complexity and Parallel Implementation of Hensel's Lemma and
  Weierstrass Preparation",http://arxiv.org/abs/2105.10798v2,2021-05-22T19:26:52Z,2021-07-02T18:50:05Z,"  Hensel's lemma, combined with repeated applications of Weierstrass
preparation theorem, allows for the factorization of polynomials with
multivariate power series coefficients. We present a complexity analysis for
this method and leverage those results to guide the load-balancing of a
parallel implementation to concurrently update all factors. In particular, the
factorization creates a pipeline where the terms of degree k of the first
factor are computed simultaneously with the terms of degree k-1 of the second
factor, etc. An implementation challenge is the inherent irregularity of
computational work between factors, as our complexity analysis reveals.
Additional resource utilization and load-balancing is achieved through the
parallelization of Weierstrass preparation. Experimental results show the
efficacy of this mixed parallel scheme, achieving up to 9x parallel speedup on
12 cores.
","['\nAlexander Brandt\n', '\nMarc Moreno Maza\n']","21 pages, 3 figures, submitted to Computer Algebra in Scientific
  Computing CASC 2021",,http://arxiv.org/abs/2105.10798v2,cs.SC,"['cs.SC', 'cs.DC', 'cs.MS']",,,[]
Parametric Toricity of Steady State Varieties of Reaction Networks,http://arxiv.org/abs/2105.10853v2,2021-05-23T04:09:21Z,2021-07-05T06:42:35Z,"  We study real steady state varieties of the dynamics of chemical reaction
networks. The dynamics are derived using mass action kinetics with parametric
reaction rates. The models studied are not inherently parametric in nature.
Rather, our interest in parameters is motivated by parameter uncertainty, as
reaction rates are typically either measured with limited precision or
estimated. We aim at detecting toricity and shifted toricity, using a framework
that has been recently introduced and studied for the non-parametric case over
both the real and the complex numbers. While toricity requires that the variety
specifies a subgroup of the direct power of the multiplicative group of the
underlying field, shifted toricity requires only a coset. In the non-parametric
case these requirements establish real decision problems. In the presence of
parameters we must go further and derive necessary and sufficient conditions in
the parameters for toricity or shifted toricity to hold. Technically, we use
real quantifier elimination methods. Our computations on biological networks
here once more confirm shifted toricity as a relevant concept, while toricity
holds only for degenerate parameter choices.
","['\nHamid Rahkooy\n', '\nThomas Sturm\n']",Computations available as ancillary files,,http://arxiv.org/abs/2105.10853v2,q-bio.MN,"['q-bio.MN', 'cs.LO', 'cs.SC', '92C42 (Primary), 68W30, 14P05 (Secondary)']",,,[]
"Koszul-type determinantal formulas for families of mixed multilinear
  systems",http://arxiv.org/abs/2105.13188v1,2021-05-26T08:54:14Z,2021-05-26T08:54:14Z,"  Effective computation of resultants is a central problem in elimination
theory and polynomial system solving. Commonly, we compute the resultant as a
quotient of determinants of matrices and we say that there exists a
determinantal formula when we can express it as a determinant of a matrix whose
elements are the coefficients of the input polynomials. We study the resultant
in the context of mixed multilinear polynomial systems, that is multilinear
systems with polynomials having different supports, on which determinantal
formulas were not known. We construct determinantal formulas for two kind of
multilinear systems related to the Multiparameter Eigenvalue Problem (MEP):
first, when the polynomials agree in all but one block of variables; second,
when the polynomials are bilinear with different supports, related to a
bipartite graph. We use the Weyman complex to construct Koszul-type
determinantal formulas that generalize Sylvester-type formulas. We can use the
matrices associated to these formulas to solve square systems without computing
the resultant. The combination of the resultant matrices with the eigenvalue
and eigenvector criterion for polynomial systems leads to a new approach for
solving MEP.
","['\nMatías R. Bender\n', '\nJean-Charles Faugère\n', '\nAngelos Mantzaflaris\n', '\nElias Tsigaridas\n']","29 pages, accepted for publication in SIAGA",,http://arxiv.org/abs/2105.13188v1,math.AC,"['math.AC', 'cs.NA', 'cs.SC', 'math.NA', '13P15 (Primary) 14Q20 15A18']",,,[]
Wilf classes of non-symmetric operads,http://arxiv.org/abs/2105.08880v1,2021-05-19T01:57:12Z,2021-05-19T01:57:12Z,"  Two operads are said to belong to the same Wilf class if they have the same
generating series. We discuss possible Wilf classifications of non-symmetric
operads with monomial relations. As a corollary, this would give the same
classification for the operads with a finite Groebner basis.
  Generally, there is no algorithm to decide whether two finitely presented
operads belong to the same Wilf class. Still, we show that if an operad has a
finite Groebner basis, then the monomial basis of the operad forms an
unambiguous context-free language. Moreover, we discuss the deterministic
grammar which defines the language. The generating series of the operad can be
obtained as a result of an algorithmic elimination of variables from the
algebraic system of equations defined by the Chomsky--Schutzenberger
enumeration theorem. We then focus on the case of binary operads with a single
relation. The approach is based on the results by Rowland on pattern avoidance
in binary trees. We improve and refine Rowland's calculations and empirically
confirm his conjecture. Here we use both the algebraic elimination and the
direct calculation of formal power series from algebraic systems of equations.
Finally, we discuss the connection of Wilf classes with algorithms for the
Quillen homology of operads calculation.
","['\nAndrey T. Cherkasov\n', '\nDmitri Piontkovski\n']",8 pages,"ISSAC '21: Proceedings of the 2021 on International Symposium on
  Symbolic and Algebraic Computation, July 2021, pp. 91--98",http://dx.doi.org/10.1145/3452143.3465555,math.CO,"['math.CO', 'cs.FL', 'cs.SC', 'math.AT', 'math.RA', '18M65 (Primary) 68Q45, 05A15, 05C30 (Secondary)']",10.1145/3452143.3465555,,[]
The D-plus Discriminant and Complexity of Root Clustering,http://arxiv.org/abs/2105.03856v2,2021-05-09T07:13:21Z,2021-05-19T09:30:32Z,"  Let $p(x)$ be an integer polynomial with $m\ge 2$ distinct roots
$\rho_1,\ldots,\rho_m$ whose multiplicities are
$\boldsymbol{\mu}=(\mu_1,\ldots,\mu_m)$. We define the D-plus discriminant of
$p(x)$ to be $D^+(p):= \prod_{1\le i<j\le m}(\rho_i-\rho_j)^{\mu_i+\mu_j}$. We
first prove a conjecture that $D^+(p)$ is a $\boldsymbol{\mu}$-symmetric
function of its roots $\rho_1,\ldots,\rho_m$. Our main result gives an explicit
formula for $D^+(p)$, as a rational function of its coefficients. Our proof is
ideal-theoretic, based on re-casting the classic Poisson resultant as the
""symbolic Poisson formula"". The D-plus discriminant first arose in the
complexity analysis of a root clustering algorithm from Becker et al. (ISSAC
2016). The bit-complexity of this algorithm is proportional to a quantity
$\log(|D^+(p)|^{-1})$. As an application of our main result, we give an
explicit upper bound on this quantity in terms of the degree of $p$ and its
leading coefficient.
","['\nJing Yang\n', '\nChee K. Yap\n']",,,http://arxiv.org/abs/2105.03856v2,cs.SC,"['cs.SC', '68W30, 11R29, 68Q25']",,,[]
Complexity Analysis of Root Clustering for a Complex Polynomial,http://arxiv.org/abs/2105.05183v1,2021-05-11T16:36:19Z,2021-05-11T16:36:19Z,"  Let $F(z)$ be an arbitrary complex polynomial. We introduce the local root
clustering problem, to compute a set of natural $\varepsilon$-clusters of roots
of $F(z)$ in some box region $B_0$ in the complex plane. This may be viewed as
an extension of the classical root isolation problem. Our contribution is
two-fold: we provide an efficient certified subdivision algorithm for this
problem, and we provide a bit-complexity analysis based on the local geometry
of the root clusters.
  Our computational model assumes that arbitrarily good approximations of the
coefficients of $F$ are provided by means of an oracle at the cost of reading
the coefficients. Our algorithmic techniques come from a companion paper
(Becker et al., 2018) and are based on the Pellet test, Graeffe and Newton
iterations, and are independent of Sch\""onhage's splitting circle method. Our
algorithm is relatively simple and promises to be efficient in practice.
","['\nRuben Becker\n', '\nMichael Sagraloff\n', '\nVikram Sharma\n', '\nJuan Xu\n', '\nChee Yap\n']",,,http://arxiv.org/abs/2105.05183v1,cs.SC,"['cs.SC', 'cs.CG']",,,[]
On the probability of generating a primitive matrix,http://arxiv.org/abs/2105.05383v2,2021-05-12T01:03:20Z,2023-03-16T10:26:00Z,"  Given a $k\times n$ integer primitive matrix $\bf{A}$ (i.e., a matrix can be
extended to an $n\times n$ unimodular matrix over the integers) with the
maximal absolute value of entries $\|\bf{A}\|$ bounded by {an integer}
$\lambda$ from above, we study the probability that the $m\times n$ matrix
extended from $\bf{A}$ by appending other $m-k$ row vectors of dimension $n$
with entries chosen randomly and independently from the uniform distribution
over $\{0, 1,\ldots, \lambda-1\}$ is still primitive. We present a complete and
rigorous proof of a lower bound on the probability, which is at least a
constant for fixed $m$ in the range $[k+1, n-4]$. As an application, we prove
that there exists a fast Las Vegas algorithm that completes a $k\times n$
primitive matrix $\bf{A}$ to an $n\times n$ unimodular matrix within expected
$\tilde{O}(n^{\omega}\log \|\bf{A}\|)$ bit operations, where $\tilde{O}$ is
big-$O$ but without log factors, $\omega$ is the exponent on the arithmetic
operations of matrix multiplication.
","['\nJingwei Chen\n', '\nYong Feng\n', '\nYang Liu\n', '\nWenyuan Wu\n']",,,http://arxiv.org/abs/2105.05383v2,cs.SC,"['cs.SC', 'cs.DS', '15B36, 15A83', 'F.2.1; I.1.2']",,,[]
High-performance symbolic-numerics via multiple dispatch,http://arxiv.org/abs/2105.03949v3,2021-05-09T14:22:43Z,2022-02-05T07:46:51Z,"  As mathematical computing becomes more democratized in high-level languages,
high-performance symbolic-numeric systems are necessary for domain scientists
and engineers to get the best performance out of their machine without deep
knowledge of code optimization. Naturally, users need different term types
either to have different algebraic properties for them, or to use efficient
data structures. To this end, we developed Symbolics.jl, an extendable symbolic
system which uses dynamic multiple dispatch to change behavior depending on the
domain needs. In this work we detail an underlying abstract term interface
which allows for speed without sacrificing generality. We show that by
formalizing a generic API on actions independent of implementation, we can
retroactively add optimized data structures to our system without changing the
pre-existing term rewriters. We showcase how this can be used to optimize term
construction and give a 113x acceleration on general symbolic transformations.
Further, we show that such a generic API allows for complementary
term-rewriting implementations. We demonstrate the ability to swap between
classical term-rewriting simplifiers and e-graph-based term-rewriting
simplifiers. We showcase an e-graph ruleset which minimizes the number of CPU
cycles during expression evaluation, and demonstrate how it simplifies a
real-world reaction-network simulation to halve the runtime. Additionally, we
show a reaction-diffusion partial differential equation solver which is able to
be automatically converted into symbolic expressions via multiple dispatch
tracing, which is subsequently accelerated and parallelized to give a 157x
simulation speedup. Together, this presents Symbolics.jl as a next-generation
symbolic-numeric computing environment geared towards modeling and simulation.
","['\nShashi Gowda\n', '\nYingbo Ma\n', '\nAlessandro Cheli\n', '\nMaja Gwozdz\n', '\nViral B. Shah\n', '\nAlan Edelman\n', '\nChristopher Rackauckas\n']",,,http://arxiv.org/abs/2105.03949v3,cs.CL,"['cs.CL', 'cs.MS', 'cs.PL', 'cs.SC', 'D.3.3; I.1.1; I.1.3']",,,[]
Tuple Interpretations for Higher-Order Rewriting,http://arxiv.org/abs/2105.01112v1,2021-05-03T18:34:37Z,2021-05-03T18:34:37Z,"  We develop a class of algebraic interpretations for many-sorted and
higher-order term rewriting systems that takes type information into account.
Specifically, base-type terms are mapped to \emph{tuples} of natural numbers
and higher-order terms to functions between those tuples. Tuples may carry
information relevant to the type; for instance, a term of type $\mathsf{nat}$
may be associated to a pair $(\mathsf{cost}, \mathsf{size})$ representing its
evaluation cost and size. This class of interpretations results in a more
fine-grained notion of complexity than runtime or derivational complexity,
which makes it particularly useful to obtain complexity bounds for higher-order
rewriting systems. We show that rewriting systems compatible with tuple
interpretations admit finite bounds on derivation height. Furthermore, we
demonstrate how to mechanically construct tuple interpretations and how to
orient $\beta$ and $\eta$ reductions within our technique. Finally, we relate
our method to runtime complexity and prove that specific interpretation shapes
imply certain runtime complexity bounds.
","['\nDeivid Vale\n', '\nCynthia Kop\n']",,,http://arxiv.org/abs/2105.01112v1,cs.SC,"['cs.SC', 'cs.LO']",,,[]
CPS Engineering: Gap Analysis and Perspectives,http://arxiv.org/abs/2104.13210v1,2021-04-26T13:45:26Z,2021-04-26T13:45:26Z,"  Virtualization of computing and networking, IT-OT convergence, cybersecurity
and AI-based enhancement of autonomy are significantly increasing the
complexity of CPS and CPSoS. New challenges have emerged to demonstrate that
these systems are safe and secure. We emphasize the role of control and
emerging fields therein, like symbolic control or set-based fault-tolerant and
decentralized control, to address safety. We have chosen three open
verification problems we deem central in cost-effective development and
certification of safety critical CPSoS. We review some promising threads of
research that could lead in the long term to a scalable and powerful
verification strategy. Its main components are set-based and invariant-based
design, contracts, adversarial testing, algorithmic geometry of dynamics, and
probabilistic estimation derived from compositional massive testing. To explore
these orientations in collaborative projects, and to promote them in
certification arenas, we propose to continue and upgrade an open innovation
drone-based use case that originated from a collaborative research project in
aeronautic certification reformation
",['\nEmmanuel Ledinot\n'],"41 pages, 19 figures, submitted to Leibniz Journal special issue on
  CPS Enigineering",,http://arxiv.org/abs/2104.13210v1,eess.SY,"['eess.SY', 'cs.SC', 'cs.SY']",,,[]
"Multigraded Sylvester forms, Duality and Elimination Matrices",http://arxiv.org/abs/2104.08941v2,2021-04-18T19:40:08Z,2022-07-04T06:59:30Z,"  In this paper we study the equations of the elimination ideal associated with
$n+1$ generic multihomogeneous polynomials defined over a product of projective
spaces of dimension $n$. We first prove a duality property and then make this
duality explicit by introducing multigraded Sylvester forms. These results
provide a partial generalization of similar properties that are known in the
setting of homogeneous polynomial systems defined over a single projective
space. As an important consequence, we derive a new family of elimination
matrices that can be used for solving zero-dimensional multiprojective
polynomial systems by means of linear algebra methods.
","['\nLaurent Busé\n', '\nMarc Chardin\n', '\nNavid Nemati\n']",To appear in Journal of Algebra,,http://arxiv.org/abs/2104.08941v2,math.AC,"['math.AC', 'cs.SC', 'math.AG']",,,[]
Linear PDE with Constant Coefficients,http://arxiv.org/abs/2104.10146v3,2021-04-20T17:38:56Z,2021-10-13T01:18:06Z,"  We discuss practical methods for computing the space of solutions to an
arbitrary homogeneous linear system of partial differential equations with
constant coefficients. These rest on the Fundamental Principle of
Ehrenpreis-Palamodov from the 1960s. We develop this further using recent
advances in computational commutative algebra.
","['\nRida Ait El Manssour\n', '\nMarc Härkönen\n', '\nBernd Sturmfels\n']","31 pages, 1 figure",,http://arxiv.org/abs/2104.10146v3,math.AC,"['math.AC', 'cs.SC', 'math.AP', '13N10, 14-04, 14Q15, 35G35, 35C15']",,,[]
"EXplainable Neural-Symbolic Learning (X-NeSyL) methodology to fuse deep
  learning representations with expert knowledge graphs: the MonuMAI cultural
  heritage use case",http://arxiv.org/abs/2104.11914v2,2021-04-24T09:06:08Z,2021-10-13T06:17:12Z,"  The latest Deep Learning (DL) models for detection and classification have
achieved an unprecedented performance over classical machine learning
algorithms. However, DL models are black-box methods hard to debug, interpret,
and certify. DL alone cannot provide explanations that can be validated by a
non technical audience. In contrast, symbolic AI systems that convert concepts
into rules or symbols -- such as knowledge graphs -- are easier to explain.
However, they present lower generalisation and scaling capabilities. A very
important challenge is to fuse DL representations with expert knowledge. One
way to address this challenge, as well as the performance-explainability
trade-off is by leveraging the best of both streams without obviating domain
expert knowledge. We tackle such problem by considering the symbolic knowledge
is expressed in form of a domain expert knowledge graph. We present the
eXplainable Neural-symbolic learning (X-NeSyL) methodology, designed to learn
both symbolic and deep representations, together with an explainability metric
to assess the level of alignment of machine and human expert explanations. The
ultimate objective is to fuse DL representations with expert domain knowledge
during the learning process to serve as a sound basis for explainability.
X-NeSyL methodology involves the concrete use of two notions of explanation at
inference and training time respectively: 1) EXPLANet: Expert-aligned
eXplainable Part-based cLAssifier NETwork Architecture, a compositional CNN
that makes use of symbolic representations, and 2) SHAP-Backprop, an
explainable AI-informed training procedure that guides the DL process to align
with such symbolic representations in form of knowledge graphs. We showcase
X-NeSyL methodology using MonuMAI dataset for monument facade image
classification, and demonstrate that our approach improves explainability and
performance.
","['\nNatalia Díaz-Rodríguez\n', '\nAlberto Lamas\n', '\nJules Sanchez\n', '\nGianni Franchi\n', '\nIvan Donadello\n', '\nSiham Tabik\n', '\nDavid Filliat\n', '\nPolicarpo Cruz\n', '\nRosana Montes\n', '\nFrancisco Herrera\n']",,,http://dx.doi.org/10.1016/j.inffus.2021.09.022,cs.LG,"['cs.LG', 'cs.AI', 'cs.CV', 'cs.SC']",10.1016/j.inffus.2021.09.022,,[]
"Decomposition of a Quantum System Into Subsystems in Finite Quantum
  Mechanics",http://arxiv.org/abs/2104.11992v1,2021-04-24T17:55:23Z,2021-04-24T17:55:23Z,"  Any Hilbert space with composite dimension can be factorized into a tensor
product of smaller Hilbert spaces. This allows to decompose a quantum system
into subsystems. We propose a simple tractable model for a constructive study
of decompositions of quantum systems.
",['\nVladimir V. Kornyak\n'],"8 pages, extended abstract for PCA '2021, April 19-24, 2021,
  St.Petersburg",,http://arxiv.org/abs/2104.11992v1,quant-ph,"['quant-ph', 'cs.SC', 'hep-th']",,,[]
"Exceptional points and domains of unitarity for a class of strongly
  non-Hermitian real-matrix Hamiltonians",http://arxiv.org/abs/2104.11016v1,2021-04-22T12:27:09Z,2021-04-22T12:27:09Z,"  A phenomenological Hamiltonian of a closed (i.e., unitary) quantum system is
assumed to have an $N$ by $N$ real-matrix form composed of a unperturbed
diagonal-matrix part $H^{(N)}_0$ and of a tridiagonal-matrix perturbation
$\lambda\,W^{(N)}(\lambda)$. The requirement of the unitarity of the evolution
of the system (i.e., of the diagonalizability and of the reality of the
spectrum) restricts, naturally, the variability of the matrix elements to a
""physical"" domain ${\cal D}^{[N]} \subset \mathbb{R}^d$. We fix the unperturbed
matrix (simulating a non-equidistant, square-well-type unperturbed spectrum)
and we only admit the maximally non-Hermitian antisymmetric-matrix
perturbations. This yields the hiddenly Hermitian model with the measure of
perturbation $\lambda$ and with the $d=N$ matrix elements which are, inside
${\cal D}^{[N]}$, freely variable. Our aim is to describe the quantum
phase-transition boundary $\partial {\cal D}^{[N]}$ (alias exceptional-point
boundary) at which the unitarity of the system is lost. Our main attention is
paid to the strong-coupling extremes of stability, i.e., to the Kato's
exceptional points of order $N$ (EPN) and to the (sharply spiked) shape of the
boundary $\partial {\cal D}^{[N]}$ in their vicinity. The feasibility of our
constructions is based on the use of the high-precision arithmetics in
combination with the computer-assisted symbolic manipulations (including, in
particular, the Gr\""{o}bner basis elimination technique).
",['\nMiloslav Znojil\n'],27 pp,J. Math. Phys. 62 (2021) 052103,http://dx.doi.org/10.1063/5.0041185,math-ph,"['math-ph', 'cs.NA', 'cs.SC', 'math.MP', 'math.NA', 'quant-ph']",10.1063/5.0041185,,[]
"Computing the Characteristic Polynomial of Generic Toeplitz-like and
  Hankel-like Matrices",http://arxiv.org/abs/2104.02497v1,2021-04-06T13:29:36Z,2021-04-06T13:29:36Z,"  New algorithms are presented for computing annihilating polynomials of
Toeplitz, Hankel, and more generally Toeplitz+ Hankel-like matrices over a
field. Our approach follows works on Coppersmith's block Wiedemann method with
structured projections, which have been recently successfully applied for
computing the bivariate resultant. A first baby-step/giant step approach --
directly derived using known techniques on structured matrices -- gives a
randomized Monte Carlo algorithm for the minimal polynomial of an $n\times n$
Toeplitz or Hankel-like matrix of displacement rank $\alpha$ using $\tilde
O(n^{\omega - c(\omega)} \alpha^{c(\omega)})$ arithmetic operations, where
$\omega$ is the exponent of matrix multiplication and $c(2.373)\approx 0.523$
for the best known value of $\omega$. For generic Toeplitz+Hankel-like matrices
a second algorithm computes the characteristic polynomial in $\tilde
O(n^{2-1/\omega})$ operations when the displacement rank is considered
constant. Previous algorithms required $O(n^2)$ operations while the exponents
presented here are respectively less than $1.86$ and $1.58$ with the best known
estimate for $\omega$.
","['\nClément Pernet\nCASC\n', '\nHippolyte Signargout\nARIC, CASC\n', '\nPierre Karpman\nCASC\n', '\nGilles Villard\nARIC\n']",,,http://arxiv.org/abs/2104.02497v1,cs.SC,['cs.SC'],,,"['CASC', 'ARIC, CASC', 'CASC', 'ARIC']"
msolve: A Library for Solving Polynomial Systems,http://arxiv.org/abs/2104.03572v2,2021-04-08T07:37:02Z,2021-05-19T08:46:29Z,"  We present a new open source C library \texttt{msolve} dedicated to solving
multivariate polynomial systems of dimension zero through computer algebra
methods. The core algorithmic framework of \texttt{msolve} relies on Gr\''obner
bases and linear algebra based algorithms for polynomial system solving. It
relies on Gr\''obner basis computation w.r.t.\ the degree reverse
lexicographical order, Gr\''obner conversion to a lexicographical Gr\''obner
basis and real solving of univariate polynomials. We explain in detail how
these three main steps of the solving process are implemented, how we exploit
\texttt{AVX2} instruction processors and the more general implementation ideas
we put into practice to better exploit the computational capabilities of this
algorithmic framework. We compare the practical performances of \texttt{msolve}
with leading computer algebra systems such as \textsc{Magma}, \textsc{Maple},
\textsc{Singular} on a wide range of systems with finitely many complex
solutions, showing that \texttt{msolve} can tackle systems which were out of
reach by the computer algebra software state-of-the-art.
","['\nJérémy Berthomieu\nPolSys\n', '\nChristian Eder\nPolSys\n', '\nMohab Safey El Din\nPolSys\n']","2021 International Symposium on Symbolic and Algebraic Computation,
  Jul 2021, Saint-P{\'e}tersbourg, Russia",,http://dx.doi.org/10.1145/3452143.3465545,cs.SC,['cs.SC'],10.1145/3452143.3465545,,"['PolSys', 'PolSys', 'PolSys']"
Polynomial Circuit Verification using BDDs,http://arxiv.org/abs/2104.03024v1,2021-04-07T09:56:42Z,2021-04-07T09:56:42Z,"  Verification is one of the central tasks during circuit design. While most of
the approaches have exponential worst-case behaviour, in the following
techniques are discussed for proving polynomial circuit verification based on
Binary Decision Diagrams (BDDs). It is shown that for circuits with specific
structural properties, like e.g. tree-like circuits, and circuits based on
multiplexers derived from BDDs complete formal verification can be carried out
in polynomial time and space.
",['\nRolf Drechsler\n'],"8 pages, 5 figures",,http://arxiv.org/abs/2104.03024v1,cs.AR,"['cs.AR', 'cs.DS', 'cs.SC', '68W30, 68M07, 68W35', 'B.6.3; B.2.1; F.2.2']",,,[]
"A Logical Programming Language as an Instrument for Specifying and
  Verifying Dynamic Memory",http://arxiv.org/abs/2104.01667v1,2021-04-04T19:18:07Z,2021-04-04T19:18:07Z,"  This work proposes a Prolog-dialect for the found and prioritised problems on
expressibility and automation. Given some given C-like program, if dynamic
memory is allocated, altered and freed on runtime, then a description of
desired dynamic memory is a heap specification. The check of calculated memory
state against a given specification is dynamic memory verification. This
contribution only considers formal specification and verification in a Hoare
calculus. Issues found include: invalid assignment, (temporary) unavailable
data in memory cells, excessive memory allocation, (accidental) heap alteration
in unexpected regions and others. Excessive memory allocation is nowadays
successfully resolved by memory analysers like Valgrind. Essentially, papers in
those areas did not bring any big breakthrough. Possible reasons may also
include the decrease of tension due to more available memory and parallel
threads. However, starting with Apt, problems related to variable modes have
not yet been resolved -- neither entirely nor in an acceptable way. Research
contributions over the last decades show again and again that heap issues
remain and remain complex and still important. A significant contribution was
reached in 2016 by Peter O'Hearn, who accepted the G\""{o}del prize for his
parallel approach on a spatial heap operation.
",['\nRené Haberland\n'],"209 pages, 97 figures, 6 appendices","Dissertation, Thesis, 2017",http://arxiv.org/abs/2104.01667v1,cs.LO,"['cs.LO', 'cs.FL', 'cs.PL', 'cs.SC', 'cs.SE']",,,[]
"On the computation of asymptotic critical values of polynomial maps and
  applications",http://arxiv.org/abs/2104.00913v1,2021-04-02T07:05:35Z,2021-04-02T07:05:35Z,"  Let $\mathbf{f} = \left(f_1, \dots, f_p\right) $ be a polynomial tuple in
$\mathbb{Q}[z_1, \dots, z_n]$ and let $d = \max_{1 \leq i \leq p} \deg f_i$. We
consider the problem of computing the set of asymptotic critical values of the
polynomial mapping, with the assumption that this mapping is dominant,
$\mathbf{f}: z \in \mathbb{K}^n \to (f\_1(z), \dots, f\_p(z)) \in \mathbb{K}^p$
where $\mathbb{K}$ is either $\mathbb{R}$ or $\mathbb{C}$. This is the set of
values $c$ in the target space of $\mathbf{f}$ such that there exists a
sequence of points $(\mathbf{x}_i)_{i\in \mathbb{N}}$ for which
$\mathbf{f}(\mathbf{x}_i)$ tends to $c$ and $\|\mathbf{x}_i\| \kappa {\rm d}
\mathbf{f}(\mathbf{x}_i))$ tends to $0$ when $i$ tends to infinity where ${\rm
d} \mathbf{f}$ is the differential of $\mathbf{f}$ and $\kappa$ is a function
measuring the distance of a linear operator to the set of singular linear
operators from $\mathbb{K}^n$ to $\mathbb{K}^p$. Computing the union of the
classical and asymptotic critical values allows one to put into practice
generalisations of Ehresmann's fibration theorem. This leads to natural and
efficient applications in polynomial optimisation and computational real
algebraic geometry. Going back to previous works by Kurdyka, Orro and Simon, we
design new algorithms to compute asymptotic critical values. Through
randomisation, we introduce new geometric characterisations of asymptotic
critical values. This allows us to dramatically reduce the complexity of
computing such values to a cost that is essentially $O(d^{2n(p+1)})$ arithmetic
operations in $\mathbb{Q}$. We also obtain tighter degree bounds on a
hypersurface containing the asymptotic critical values, showing that the degree
is at most $p^{n-p+1}(d-1)^{n-p}(d+1)^{p}$. Next, we show how to apply these
algorithms to unconstrained polynomial optimisation problems and the problem of
computing sample points per connected component of a semi-algebraic set defined
by a single inequality/inequation. We report on the practical capabilities of
our implementation of this algorithm. It shows how the practical efficiency
surpasses the current state-of-the-art algorithms for computing asymptotic
critical values by tackling examples that were previously out of reach.
","['\nJérémy Berthomieu\nPolSys\n', '\nAndrew Ferguson\nPolSys\n', '\nMohab Safey El Din\n']",,,http://arxiv.org/abs/2104.00913v1,cs.SC,['cs.SC'],,,"['PolSys', 'PolSys']"
"Faster One Block Quantifier Elimination for Regular Polynomial Systems
  of Equations",http://arxiv.org/abs/2103.13735v3,2021-03-25T10:26:51Z,2021-05-24T11:59:07Z,"  Quantifier elimination over the reals is a central problem in computational
real algebraic geometry, polynomial system solving and symbolic computation.
Given a semi-algebraic formula (whose atoms are polynomial constraints) with
quantifiers on some variables, it consists in computing a logically equivalent
formula involving only unquantified variables. When there is no alternation of
quantifiers, one has a one block quantifier elimination problem.
  This paper studies a variant of the one block quantifier elimination in which
we compute an almost equivalent formula of the input. We design a new
probabilistic efficient algorithm for solving this variant when the input is a
system of polynomial equations satisfying some regularity assumptions. When the
input is generic, involves $s$ polynomials of degree bounded by $D$ with $n$
quantified variables and $t$ unquantified ones, we prove that this algorithm
outputs semi-algebraic formulas of degree bounded by $\mathcal{D}$ using $O\
{\widetilde{~}}\left ((n-s+1)\ 8^{t}\ \mathcal{D}^{3t+2}
\binom{t+\mathcal{D}}{t} \right )$ arithmetic operations in the ground field
where $\mathcal{D} = 2(n+s)\ D^s(D-1)^{n-s+1}\ \binom{n}{s}$. In practice, it
allows us to solve quantifier elimination problems which are out of reach of
the state-of-the-art (up to $8$ variables).
","['\nHuu Phuoc Le\n', '\nMohab Safey El Din\n']","International Symposium on Symbolic and Algebraic Computation 2021,
  Jul. 2021, Saint-Petersbourg, Russia",,http://dx.doi.org/10.1145/3452143.3465546,cs.SC,"['cs.SC', 'cs.CG', 'I.1.2']",10.1145/3452143.3465546,,[]
Interpolation by decomposable univariate polynomials,http://arxiv.org/abs/2103.15926v1,2021-03-29T19:57:59Z,2021-03-29T19:57:59Z,"  The usual univariate interpolation problem of finding a monic polynomial f of
degree n that interpolates n given values is well understood. This paper
studies a variant where f is required to be composite, say, a composition of
two polynomials of degrees d and e, respectively, with de=n, and therefore
d+e-1 given values. Some special cases are easy to solve, and for the general
case, we construct a homotopy between it and a special case. We compute a
geometric solution of the algebraic curve presenting this homotopy, and this
also provides an answer to the interpolation task. The computing time is
polynomial in the geometric data, like the degree, of this curve. A consequence
is that for almost all inputs, a decomposable interpolation polynomial exists.
","['\nJoachim von zur Gathen\n', '\nGuillermo Matera\n']",29 pages,,http://arxiv.org/abs/2103.15926v1,math.AG,"['math.AG', 'cs.SC', '12E05 (Primary) 68W30, 14Q05, 14Q20 (Secondary)']",,,[]
How to hunt wild constants,http://arxiv.org/abs/2103.16720v3,2021-03-30T23:19:16Z,2022-02-03T00:06:00Z,"  There are now several comprehensive web applications, stand-alone computer
programs and computer algebra functions that, given a floating point number
such as 6.518670730718491, can return concise nonfloat constants such as 3
arctan 2 + ln 9 + 1, that closely approximate the float. Examples include
AskConstants, Inverse Symbolic Calculator, the Maple identify function,
MESearch, OEIS, RIES, and WolframAlpha. Usefully often such a result is the
exact limit as the float is computed with increasing precision. Therefore these
program results are candidates for proving an exact result that you could not
otherwise compute or conjecture without the program. Moreover, candidates that
are not the exact limit can be provable bounds, or convey qualitative insight,
or suggest series that they truncate, or provide sufficiently close efficient
approximations for subsequent computation. This article describes some of these
programs, how they work, and how best to use each of them. Almost everyone who
uses or should use mathematical software can benefit from acquaintance with
several such programs, because these programs differ in the sets of constants
that they can return.
",['\nDavid R. Stoutemyer\n'],"39 pages, 3 figures",,http://arxiv.org/abs/2103.16720v3,cs.SC,"['cs.SC', 'cs.NA', 'math.NA', '65-04', 'G.4']",,,[]
"Grounding Physical Concepts of Objects and Events Through Dynamic Visual
  Reasoning",http://arxiv.org/abs/2103.16564v1,2021-03-30T17:59:48Z,2021-03-30T17:59:48Z,"  We study the problem of dynamic visual reasoning on raw videos. This is a
challenging problem; currently, state-of-the-art models often require dense
supervision on physical object properties and events from simulation, which are
impractical to obtain in real life. In this paper, we present the Dynamic
Concept Learner (DCL), a unified framework that grounds physical objects and
events from video and language. DCL first adopts a trajectory extractor to
track each object over time and to represent it as a latent, object-centric
feature vector. Building upon this object-centric representation, DCL learns to
approximate the dynamic interaction among objects using graph networks. DCL
further incorporates a semantic parser to parse questions into semantic
programs and, finally, a program executor to run the program to answer the
question, levering the learned dynamics model. After training, DCL can detect
and associate objects across the frames, ground visual properties, and physical
events, understand the causal relationship between events, make future and
counterfactual predictions, and leverage these extracted presentations for
answering queries. DCL achieves state-of-the-art performance on CLEVRER, a
challenging causal video reasoning dataset, even without using ground-truth
attributes and collision labels from simulations for training. We further test
DCL on a newly proposed video-retrieval and event localization dataset derived
from CLEVRER, showing its strong generalization capacity.
","['\nZhenfang Chen\n', '\nJiayuan Mao\n', '\nJiajun Wu\n', '\nKwan-Yee Kenneth Wong\n', '\nJoshua B. Tenenbaum\n', '\nChuang Gan\n']",ICLR 2021. Project page: http://dcl.csail.mit.edu/,,http://arxiv.org/abs/2103.16564v1,cs.CV,"['cs.CV', 'cs.AI', 'cs.CL', 'cs.LG', 'cs.SC']",,,[]
ChronoR: Rotation Based Temporal Knowledge Graph Embedding,http://arxiv.org/abs/2103.10379v1,2021-03-18T17:08:33Z,2021-03-18T17:08:33Z,"  Despite the importance and abundance of temporal knowledge graphs, most of
the current research has been focused on reasoning on static graphs. In this
paper, we study the challenging problem of inference over temporal knowledge
graphs. In particular, the task of temporal link prediction. In general, this
is a difficult task due to data non-stationarity, data heterogeneity, and its
complex temporal dependencies. We propose Chronological Rotation embedding
(ChronoR), a novel model for learning representations for entities, relations,
and time. Learning dense representations is frequently used as an efficient and
versatile method to perform reasoning on knowledge graphs. The proposed model
learns a k-dimensional rotation transformation parametrized by relation and
time, such that after each fact's head entity is transformed using the
rotation, it falls near its corresponding tail entity. By using high
dimensional rotation as its transformation operator, ChronoR captures rich
interaction between the temporal and multi-relational characteristics of a
Temporal Knowledge Graph. Experimentally, we show that ChronoR is able to
outperform many of the state-of-the-art methods on the benchmark datasets for
temporal knowledge graph link prediction.
","['\nAli Sadeghian\n', '\nMohammadreza Armandpour\n', '\nAnthony Colas\n', '\nDaisy Zhe Wang\n']",,AAAI 2021,http://arxiv.org/abs/2103.10379v1,cs.LG,"['cs.LG', 'cs.SC']",,,[]
Optimal monomial quadratization for ODE systems,http://arxiv.org/abs/2103.08013v3,2021-03-14T19:49:05Z,2021-05-13T01:16:18Z,"  Quadratization problem is, given a system of ODEs with polynomial right-hand
side, transform the system to a system with quadratic right-hand side by
introducing new variables. Such transformations have been used, for example, as
a preprocessing step by model order reduction methods and for transforming
chemical reaction networks.
  We present an algorithm that, given a system of polynomial ODEs, finds a
transformation into a quadratic ODE system by introducing new variables which
are monomials in the original variables. The algorithm is guaranteed to produce
an optimal transformation of this form (that is, the number of new variables is
as small as possible), and it is the first algorithm with such a guarantee we
are aware of. Its performance compares favorably with the existing software,
and it is capable to tackle problems that were out of reach before.
","['\nAndrey Bychkov\n', '\nGleb Pogudin\n']",,,http://arxiv.org/abs/2103.08013v3,cs.SC,"['cs.SC', 'cs.DM', 'cs.NA', 'math.NA']",,,[]
Iterated integrals over letters induced by quadratic forms,http://arxiv.org/abs/2103.08330v1,2021-03-15T12:20:31Z,2021-03-15T12:20:31Z,"  An automated treatment of iterated integrals based on letters induced by
real-valued quadratic forms and Kummer--Poincar\'e letters is presented. These
quantities emerge in analytic single and multi--scale Feynman diagram
calculations. To compactify representations, one wishes to apply general
properties of these quantities in computer-algebraic implementations. We
provide the reduction to basis representations, expansions, analytic
continuation and numerical evaluation of these quantities.
","['\nJ. Ablinger\n', '\nJ. Blümlein\n', '\nC. Schneider\n']","14 pages LATEX, 1 anc. file","Phys. Rev. D 103, 096025 (2021)",http://dx.doi.org/10.1103/PhysRevD.103.096025,hep-th,"['hep-th', 'cs.SC', 'hep-ph', 'math-ph', 'math.MP']",10.1103/PhysRevD.103.096025,,[]
"A Succinct Multivariate Lazy Multivariate Tower AD for Weil Algebra
  Computation",http://arxiv.org/abs/2103.11615v2,2021-03-22T06:54:32Z,2021-07-05T03:21:00Z,"  We propose a functional implementation of \emph{Multivariate Tower Automatic
Differentiation}. Our implementation is intended to be used in implementing
$C^\infty$-structure computation of an arbitrary Weil algebra, which we
discussed in the previous work.
",['\nHiromi Ishii\n'],,"Computer Algebra - Theory and its Applications,RIMS K\^oky\^uroku,
  No.2185 (2021). pp.104-112",http://arxiv.org/abs/2103.11615v2,cs.SC,"['cs.SC', 'cs.MS', 'cs.NA', 'math.DG', 'math.NA']",,,[]
"Puiseux Series and Algebraic Solutions of First Order Autonomous AODEs
  -- A MAPLE Package",http://arxiv.org/abs/2103.03646v1,2021-03-05T13:20:47Z,2021-03-05T13:20:47Z,"  There exist several methods for computing exact solutions of algebraic
differential equations. Most of the methods, however, do not ensure existence
and uniqueness of the solutions and might fail after several steps, or are
restricted to linear equations. The authors have presented in previous works a
method to overcome this problem for autonomous first order algebraic ordinary
differential equations and formal Puiseux series solutions and algebraic
solutions. In the first case, all solutions can uniquely be represented by a
sufficiently large truncation and in the latter case by its minimal polynomial.
The main contribution of this paper is the implementation, in a MAPLE-package
named FirstOrderSolve, of the algorithmic ideas presented therein. More
precisely, all formal Puiseux series and algebraic solutions, including the
generic and singular solutions, are computed and described uniquely. The
computation strategy is to reduce the given differential equation to a simpler
one by using local parametrizations and the already known degree bounds.
","['\nFrancois Boulier\n', '\nJose Cano\n', '\nSebastian Falkensteiner\n', '\nRafael Sendra\n']",,,http://arxiv.org/abs/2103.03646v1,cs.MS,"['cs.MS', 'cs.SC', '34-04']",,,[]
"DI2: prior-free and multi-item discretization ofbiomedical data and its
  applications",http://arxiv.org/abs/2103.04356v1,2021-03-07T13:45:30Z,2021-03-07T13:45:30Z,"  Motivation: A considerable number of data mining approaches for biomedical
data analysis, including state-of-the-art associative models, require a form of
data discretization. Although diverse discretization approaches have been
proposed, they generally work under a strict set of statistical assumptions
which are arguably insufficient to handle the diversity and heterogeneity of
clinical and molecular variables within a given dataset. In addition, although
an increasing number of symbolic approaches in bioinformatics are able to
assign multiple items to values occurring near discretization boundaries for
superior robustness, there are no reference principles on how to perform
multi-item discretizations.
  Results: In this study, an unsupervised discretization method, DI2, for
variables with arbitrarily skewed distributions is proposed. DI2 provides
robust guarantees of generalization by placing data corrections using the
Kolmogorov-Smirnov test before statistically fitting distribution candidates.
DI2 further supports multi-item assignments. Results gathered from biomedical
data show its relevance to improve classic discretization choices.
  Software: available at https://github.com/JupitersMight/DI2
","['\nLeonardo Alexandre\n', '\nRafael S. Costa\n', '\nRui Henriques\n']",,,http://arxiv.org/abs/2103.04356v1,q-bio.QM,"['q-bio.QM', 'cs.SC']",,,[]
"Symbolic integration by integrating learning models with different
  strengths and weaknesses",http://arxiv.org/abs/2103.05497v1,2021-03-09T15:46:36Z,2021-03-09T15:46:36Z,"  Integration is indispensable, not only in mathematics, but also in a wide
range of other fields. A deep learning method has recently been developed and
shown to be capable of integrating mathematical functions that could not
previously be integrated on a computer. However, that method treats integration
as equivalent to natural language translation and does not reflect mathematical
information. In this study, we adjusted the learning model to take mathematical
information into account and developed a wide range of learning models that
learn the order of numerical operations more robustly. In this way, we achieved
a 98.80% correct answer rate with symbolic integration, a higher rate than that
of any existing method. We judged the correctness of the integration based on
whether the derivative of the primitive function was consistent with the
integrand. By building an integrated model based on this strategy, we achieved
a 99.79% rate of correct answers with symbolic integration.
","['\nHazumi Kubota\n', '\nYuta Tokuoka\n', '\nTakahiro G. Yamada\n', '\nAkira Funahashi\n']",,"in IEEE Access, vol. 10, pp. 47000-47010, 2022",http://dx.doi.org/10.1109/ACCESS.2022.3171329,cs.LG,"['cs.LG', 'cs.SC']",10.1109/ACCESS.2022.3171329,,[]
Provability in BI's Sequent Calculus is Decidable,http://arxiv.org/abs/2103.02343v4,2021-03-03T11:48:10Z,2023-10-19T10:57:19Z,"  Warning: This paper contains a mistake, rendering the proof of the main
theorem invalid. The logic of Bunched Implications (BI) combines both additive
and multiplicative connectives, which include two primitive intuitionistic
implications. As a consequence, contexts in the sequent presentation are not
lists, nor multisets, but rather tree-like structures called bunches. This
additional complexity notwithstanding, the logic has a well-behaved metatheory
admitting all the familiar forms of semantics and proof systems. However, the
presentation of an effective proof-search procedure has been elusive since the
logic's debut. We show that one can reduce the proof-search space for any given
sequent to a primitive recursive set, the argument generalizing Gentzen's
decidability argument for classical propositional logic and combining key
features of Dyckhoff's contraction-elimination argument for intuitionistic
logic. An effective proof-search procedure, and hence decidability of
provability, follows as a corollary.
","['\nAlexander Gheorghiu\n', '\nSimon Docherty\n', '\nDavid Pym\n']",The paper contains an error so the result is invalid,,http://arxiv.org/abs/2103.02343v4,cs.LO,"['cs.LO', 'cs.SC', 'math.LO', '03B25 (Primary) 03D99, 68W68, 68Q68 (Secondary)', 'F.0; I.1; F.3']",,,[]
"ModelingToolkit: A Composable Graph Transformation System For
  Equation-Based Modeling",http://arxiv.org/abs/2103.05244v3,2021-03-09T06:31:24Z,2022-02-09T10:49:22Z,"  Getting good performance out of numerical equation solvers requires that the
user has provided stable and efficient functions representing their model.
However, users should not be trusted to write good code. In this manuscript we
describe ModelingToolkit (MTK), a symbolic equation-based modeling system which
allows for composable transformations to generate stable, efficient, and
parallelized model implementations. MTK blurs the lines of traditional symbolic
computing by acting directly on a user's numerical code. We show the ability to
apply graph algorithms for automatically parallelizing and performing index
reduction on code written for differential-algebraic equation (DAE) solvers,
""fixing"" the performance and stability of the model without requiring any
changes to on the user's part. We demonstrate how composable model
transformations can be combined with automated data-driven surrogate generation
techniques, allowing machine learning methods to generate accelerated
approximate models within an acausal modeling framework. These reduced models
are shown to outperform the Dymola Modelica compiler on an HVAC model by 590x
at 3\% error. Together, this demonstrates MTK as a system for bringing the
latest research in graph transformations directly to modeling applications.
","['\nYingbo Ma\n', '\nShashi Gowda\n', '\nRanjan Anantharaman\n', '\nChris Laughman\n', '\nViral Shah\n', '\nChris Rackauckas\n']","10 pages, 3 figures, 1 table",,http://arxiv.org/abs/2103.05244v3,cs.MS,"['cs.MS', 'cs.SC', 'cs.SE']",,,[]
Classification of higher Mobility closed-loop Linkages,http://arxiv.org/abs/2103.04799v2,2021-03-08T14:49:03Z,2022-07-27T10:43:01Z,"  We provide a complete classification of paradoxical closed-loop $n$-linkages,
where $n\geq6$, of mobility $n-4$ or higher, containing revolute, prismatic or
helical joints. We also explicitly write down strong necessary conditions for
$nR$-linkages of mobility $n-5$. Our main new tool is a geometric relation
between a linkage $L$ and another linkage $L'$ resulting from adding equations
to the configuration space of $L$. We then lift known classification results
for $L'$ to $L$ using this relation.
","['\nTiago Duarte Guerreiro\n', '\nZijia Li\n', '\nJosef Schicho\n']",20 pp. Final version. To appear on Ann. Mat. Pura Appl,,http://arxiv.org/abs/2103.04799v2,cs.CG,"['cs.CG', 'cs.RO', 'cs.SC', 'math.AG', 'math.RA']",,,[]
Root Radii and Subdivision for Polynomial Root-Finding,http://arxiv.org/abs/2102.10821v2,2021-02-22T08:17:13Z,2021-07-02T09:27:07Z,"  We depart from our approximation of 2000 of all root radii of a polynomial,
which has readily extended Sch{\""o}nhage's efficient algorithm of 1982 for a
single root radius. We revisit this extension, advance it, based on our simple
but novel idea, and yield significant practical acceleration of the known near
optimal subdivision algorithms for complex and real root-finding of user's
choice. We achieve this by means of significant saving of exclusion tests and
Taylor's shifts, which are the bottleneck of subdivision root-finders. This
saving relies on our novel recipes for the initialization of root-finding
iterations of independent interest. We demonstrate our practical progress with
numerical tests, provide extensive analysis of the resulting algorithms, and
show that, like the preceding subdivision root-finders, they support near
optimal Boolean complexity bounds.
","['\nRémi Imbach\n', '\nVictor Y. Pan\n']",,,http://arxiv.org/abs/2102.10821v2,cs.SC,['cs.SC'],,,[]
An algorithm to determine regular singular Mahler systems,http://arxiv.org/abs/2102.10842v1,2021-02-22T09:13:11Z,2021-02-22T09:13:11Z,"  This paper is devoted to the study of the analytic properties of Mahler
systems at 0. We give an effective characterisation of Mahler systems that are
regular singular at 0, that is, systems which are equivalent to constant ones.
Similar characterisations already exist for differential and (q-)difference
systems but they do not apply in the Mahler case. This work fill in the gap by
giving an algorithm which decides whether or not a Mahler system is regular
singular at 0.
","['\nColin Faverjon\nICJ\n', '\nMarina Poulet\nICJ\n']",,,http://arxiv.org/abs/2102.10842v1,cs.SC,"['cs.SC', 'math.NT']",,,"['ICJ', 'ICJ']"
"Affine equivalences of surfaces of translation and minimal surfaces, and
  applications to symmetry detection and design",http://arxiv.org/abs/2103.00151v3,2021-02-27T07:36:21Z,2021-12-17T16:09:18Z,"  We introduce a characterization for affine equivalence of two surfaces of
translation defined by either rational or meromorphic generators. In turn, this
induces a similar characterization for minimal surfaces. In the rational case,
our results provide algorithms for detecting affine equivalence of these
surfaces, and therefore, in particular, the symmetries of a surface of
translation or a minimal surface of the considered types. Additionally, we
apply our results to designing surfaces of translation and minimal surfaces
with symmetries, and to computing the symmetries of the higher-order Enneper
surfaces.
","['\nJuan Gerardo Alcázar\n', '\nGeorg Muntingh\n']",24 pages,,http://arxiv.org/abs/2103.00151v3,math.AG,"['math.AG', 'cs.CG', 'cs.SC', '14Q10, 68W30', 'F.2.2; I.1.2']",,,[]
Frobenius Groups with Perfect Order Classes,http://arxiv.org/abs/2103.00425v2,2021-02-28T08:28:14Z,2023-07-08T02:44:03Z,"  The purpose of this paper is to investigate the finite Frobenius groups with
""perfect order classes""; that is, those for which the number of elements of
each order is a divisor of the order of the group. If a finite Frobenius group
has perfect order classes then so too does its Frobenius complement, the
Frobenius kernel is a homocyclic group of odd prime power order, and the
Frobenius complement acts regularly on the elements of prime order in the
Frobenius kernel. The converse is also true.
  Combined with elementary number-theoretic arguments, we use this to provide
characterisations of several important classes of Frobenius groups. The
insoluble Frobenius groups with perfect order classes are fully characterised.
These turn out to be the perfect Frobenius groups whose Frobenius kernel is a
homocyclic $11$-group of rank $2$.
  We also determine precisely which nilpotent Frobenius complements have
perfect order classes, from which it follows that a Frobenius group with
nilpotent complement has perfect order classes only if the Frobenius complement
is a cyclic $\{2,3\}$-group of even order.
  Those Frobenius groups for which the Frobenius complement is a biprimary
group are also described fully, and we show that no soluble Frobenius group
whose Frobenius complement is a $\{2,3,5\}$-group with order divisible by $30$
has perfect order classes.
",['\nJames McCarron\n'],"v1: 46 pages, no figures. Preliminary draft. Feedback welcome. v2: 43
  pages, no figures. Removed appendix and other textual emendations",,http://arxiv.org/abs/2103.00425v2,math.GR,"['math.GR', 'cs.SC', 'math.NT', '20D60 (Primary), 20D99, 20E45, 20E99, 11A05, 11A07, 11D99\n  (Secondary)']",,,[]
"Symbolic computation of hypergeometric type and non-holonomic power
  series",http://arxiv.org/abs/2102.04157v1,2021-02-08T12:19:39Z,2021-02-08T12:19:39Z,"  A term $a_n$ is $m$-fold hypergeometric, for a given positive integer $m$, if
the ratio $a_{n+m}/a_n$ is a rational function over a field $K$ of
characteristic zero. We establish the structure of holonomic recurrence
equation, i.e. linear and homogeneous recurrence equations having polynomial
coefficients, that have $m$-fold hypergeometric term solutions over $K$, for
any positive integer $m$. Consequently, we describe an algorithm, say
$mfoldHyper$, that extends van Hoeij's algorithm (1998) which computes a basis
of the subspace of hypergeometric $(m=1)$ term solutions of holonomic
recurrence equations to the more general case of $m$-fold hypergeometric terms.
  We generalize the concept of hypergeometric type power series introduced by
Koepf (1992), by considering linear combinations of Laurent-Puiseux series
whose coefficients are $m$-fold hypergeometric terms. Thus thanks to
$mfoldHyper$, we deduce a complete procedure to compute these power series;
indeed, it turns out that every linear combination of power series with
$m$-fold hypergeometric term coefficients, for finitely many values of $m$, is
detected.
  On the other hand, we investigate an algorithm to represent power series of
non-holonomic functions. The algorithm follows the same steps of Koepf's
algorithm, but instead of seeking holonomic differential equations, quadratic
differential equations are computed and the Cauchy product rule is used to
deduce recurrence equations for the power series coefficients. This algorithm
defines a normal function that yields together with enough initial values
normal forms for many power series of non-holonomic functions. Therefore,
non-trivial identities are automatically proved using this approach.
  This paper is accompanied by implementations in the Computer Algebra Systems
(CAS) Maxima 5.44.0 and Maple 2019.
","['\nBertrand Teguia Tabuguia\n', '\nWolfram Koepf\n']",68 pages,"Program. Comput. Soft. 48, 125--146 (2022)",http://dx.doi.org/10.1134/S0361768822020104,cs.SC,"['cs.SC', 'Primary: 33F10, 30B60, Secondary: 33C20, 39A99']",10.1134/S0361768822020104,,[]
"Fast real and complex root-finding methods for well-conditioned
  polynomials",http://arxiv.org/abs/2102.04180v1,2021-02-08T13:24:08Z,2021-02-08T13:24:08Z,"  Given a polynomial $p$ of degree $d$ and a bound $\kappa$ on a condition
number of $p$, we present the first root-finding algorithms that return all its
real and complex roots with a number of bit operations quasi-linear in $d
\log^2(\kappa)$. More precisely, several condition numbers can be defined
depending on the norm chosen on the coefficients of the polynomial. Let $p(x) =
\sum\_{k=0}^d a\_k x^k = \sum\_{k=0}^d \sqrt{\binom d k} b\_k x^k$. We call the
condition number associated with a perturbation of the $a\_k$ the hyperbolic
condition number $\kappa\_h$, and the one associated with a perturbation of the
$b\_k$ the elliptic condition number $\kappa\_e$. For each of these condition
numbers, we present algorithms that find the real and the complex roots of $p$
in $O\left(d\log^2(d\kappa)\ \text{polylog}(\log(d\kappa))\right)$ bit
operations.Our algorithms are well suited for random polynomials since
$\kappa\_h$ (resp. $\kappa\_e$) is bounded by a polynomial in $d$ with high
probability if the $a\_k$ (resp. the $b\_k$) are independent, centered Gaussian
variables of variance $1$.
",['\nGuillaume Moroz\nGAMBLE\n'],,,http://arxiv.org/abs/2102.04180v1,cs.SC,['cs.SC'],,,['GAMBLE']
On exact division and divisibility testing for sparse polynomials,http://arxiv.org/abs/2102.04826v2,2021-02-09T13:54:38Z,2021-05-19T19:45:10Z,"  No polynomial-time algorithm is known to test whether a sparse polynomial G
divides another sparse polynomial $F$. While computing the quotient Q=F quo G
can be done in polynomial time with respect to the sparsities of F, G and Q,
this is not yet sufficient to get a polynomial-time divisibility test in
general. Indeed, the sparsity of the quotient Q can be exponentially larger
than the ones of F and G. In the favorable case where the sparsity #Q of the
quotient is polynomial, the best known algorithm to compute Q has a non-linear
factor #G#Q in the complexity, which is not optimal.
  In this work, we are interested in the two aspects of this problem. First, we
propose a new randomized algorithm that computes the quotient of two sparse
polynomials when the division is exact. Its complexity is quasi-linear in the
sparsities of F, G and Q. Our approach relies on sparse interpolation and it
works over any finite field or the ring of integers. Then, as a step toward
faster divisibility testing, we provide a new polynomial-time algorithm when
the divisor has a specific shape. More precisely, we reduce the problem to
finding a polynomial S such that QS is sparse and testing divisibility by S can
be done in polynomial time. We identify some structure patterns in the divisor
G for which we can efficiently compute such a polynomial~S.
","['\nPascal Giorgi\n', '\nBruno Grenet\n', '\nArmelle Perret du Cray\n']",,Proceedings of ISSAC 2021,http://dx.doi.org/10.1145/3452143.3465539,cs.SC,['cs.SC'],10.1145/3452143.3465539,,[]
On FGLM Algorithms with Tate Algebras,http://arxiv.org/abs/2102.05324v1,2021-02-10T08:53:55Z,2021-02-10T08:53:55Z,"  Tate introduced in [Ta71] the notion of Tate algebras to serve, in the
context of analytic geometry over the-adics, as a counterpart of polynomial
algebras in classical algebraic geometry. In [CVV19, CVV20] the formalism of
Gr{\""o}bner bases over Tate algebras has been introduced and advanced
signature-based algorithms have been proposed. In the present article, we
extend the FGLM algorithm of [FGLM93] to Tate algebras. Beyond allowing for
fast change of ordering, this strategy has two other important benefits. First,
it provides an efficient algorithm for changing the radii of convergence which,
in particular, makes effective the bridge between the polynomial setting and
the Tate setting and may help in speeding up the computation of Gr{\""o}bner
basis over Tate algebras. Second, it gives the foundations for designing a fast
algorithm for interreduction, which could serve as basic primitive in our
previous algorithms and accelerate them significantly.
","['\nXavier Caruso\nIMB, CNRS\n', '\nTristan Vaccon\nXLIM\n', '\nThibaut Verron\nJKU\n']",,,http://arxiv.org/abs/2102.05324v1,cs.SC,['cs.SC'],,,"['IMB, CNRS', 'XLIM', 'JKU']"
Lazy Hermite Reduction and Creative Telescoping for Algebraic Functions,http://arxiv.org/abs/2102.06538v2,2021-02-12T14:04:15Z,2021-02-15T06:00:31Z,"  Bronstein's lazy Hermite reduction is a symbolic integration technique that
reduces algebraic functions to integrands with only simple poles without the
prior computation of an integral basis. We sharpen the lazy Hermite reduction
by combining it with the polynomial reduction to solve the decomposition
problem of algebraic functions. The sharpened reduction is then used to design
a reduction-based telescoping algorithm for algebraic functions in two
variables.
","['\nShaoshi Chen\n', '\nLixin Du\n', '\nManuel Kauers\n']",20 pages,,http://arxiv.org/abs/2102.06538v2,cs.SC,['cs.SC'],,,[]
"Metatheory.jl: Fast and Elegant Algebraic Computation in Julia with
  Extensible Equality Saturation",http://arxiv.org/abs/2102.07888v1,2021-02-15T22:58:18Z,2021-02-15T22:58:18Z,"  We introduce Metatheory.jl: a lightweight and performant general purpose
symbolics and metaprogramming framework meant to simplify the act of writing
complex Julia metaprograms and to significantly enhance Julia with a native
term rewriting system, based on state-of-the-art equality saturation
techniques, and a dynamic first class Abstract Syntax Tree (AST) pattern
matching system that is dynamically composable in an algebraic fashion, taking
full advantage of the language's powerful reflection capabilities. Our
contribution allows to perform general purpose symbolic mathematics,
manipulation, optimization, synthesis or analysis of syntactically valid Julia
expressions with a clean and concise programming interface, both during
compilation or execution of programs.
",['\nAlessandro Cheli\n'],"3 pages, 1 figure","The Open Journal, 2021",http://dx.doi.org/10.21105/joss.03078,cs.PL,"['cs.PL', 'cs.SC', 'I.1.0; I.1.2; I.1.3; D.3.2; D.3.3; D.3.4']",10.21105/joss.03078,,[]
"Polynomial Linear System Solving with Random Errors: new bounds and
  early termination technique",http://arxiv.org/abs/2102.04182v1,2021-02-08T13:27:46Z,2021-02-08T13:27:46Z,"  This paper deals with the polynomial linear system solving with errors
(PLSwE) problem. Specifically, we focus on the evaluation-interpolation
technique for solving polynomial linear systems and we assume that errors can
occur in the evaluation step. In this framework, the number of evaluations
needed to recover the solution of the linear system is crucial since it affects
the number of computations. It depends on the parameters of the linear system
(degrees, size) and on a bound on the number of errors.
  Our work is part of a series of papers about PLSwE aiming to reduce this
number of evaluations. We proved in [Guerrini et al., Proc. ISIT'19] that if
errors are randomly distributed, the bound of the number of evaluations can be
lowered for large error rate.
  In this paper, following the approach of [Kaltofen et al., Proc. ISSAC'17],
we improve the results of [Guerrini et al., Proc. ISIT'19] in two directions.
First, we propose a new bound of the number of evaluations, lowering the
dependency on the parameters of the linear system, based on work of [Cabay,
Proc. SYMSAC'71]. Second, we introduce an early termination strategy in order
to handle the unnecessary increase of the number of evaluations due to
overestimation of the parameters of the system and on the bound on the number
of errors.
","['\nGuerrini Eleonora\n', '\nLebreton Romain\n', '\nZappatore Ilaria\n']",,,http://arxiv.org/abs/2102.04182v1,cs.SC,"['cs.SC', 'cs.IT', 'math.IT']",,,[]
"Choosing the Variable Ordering for Cylindrical Algebraic Decomposition
  via Exploiting Chordal Structure",http://arxiv.org/abs/2102.00823v2,2021-02-01T13:29:17Z,2021-02-04T16:57:05Z,"  Cylindrical algebraic decomposition (CAD) plays an important role in the
field of real algebraic geometry and many other areas. As is well-known, the
choice of variable ordering while computing CAD has a great effect on the time
and memory use of the computation as well as the number of sample points
computed. In this paper, we indicate that typical CAD algorithms, if executed
with respect to a special kind of variable orderings (called ""the perfect
elimination orderings""), naturally preserve chordality, which is an important
property on sparsity of variables. Experimentation suggests that if the
associated graph of the polynomial system in question is chordal (\emph{resp.},
is nearly chordal), then a perfect elimination ordering of the associated graph
(\emph{resp.}, of a minimal chordal completion of the associated graph) can be
a good variable ordering for the CAD computation. That is, by using the perfect
elimination orderings, the CAD computation may produce a much smaller full set
of projection polynomials than by using other naive variable orderings. More
importantly, for the complexity analysis of the CAD computation via a perfect
elimination ordering, a so-called $(m,d)$-property of the full set of
projection polynomials obtained via such an ordering is given, through which
the ""size"" of this set is characterized. This property indicates that when the
corresponding perfect elimination tree has a lower height, the full set of
projection polynomials also tends to have a smaller ""size"". This is well
consistent with the experimental results, hence the perfect elimination
orderings with lower elimination tree height are further recommended to be used
in the CAD projection.
","['\nHaokun Li\n', '\nBican Xia\n', '\nHuiying Zhang\n', '\nTao Zheng\n']",,,http://arxiv.org/abs/2102.00823v2,cs.SC,['cs.SC'],,,[]
Computing Limits of Quotients of Multivariate Real Analytic Functions,http://arxiv.org/abs/2102.01242v1,2021-02-02T00:57:48Z,2021-02-02T00:57:48Z,"  We present an algorithm for computing limits of quotients of real analytic
functions. The algorithm is based on computation of a bound on the Lojasiewicz
exponent and requires the denominator to have an isolated zero at the limit
point.
",['\nAdam Strzebonski\n'],6 pages,,http://arxiv.org/abs/2102.01242v1,cs.SC,"['cs.SC', 'I.1.2; G.4']",,,[]
"Term Algebras, Canonical Representations and Difference Ring Theory for
  Symbolic Summation",http://arxiv.org/abs/2102.01471v3,2021-02-02T12:44:01Z,2021-05-03T14:05:35Z,"  A general overview of the existing difference ring theory for symbolic
summation is given. Special emphasis is put on the user interface: the
translation and back translation of the corresponding representations within
the term algebra and the formal difference ring setting. In particular,
canonical (unique) representations and their refinements in the introduced term
algebra are explored by utilizing the available difference ring theory. Based
on that, precise input-output specifications of the available tools of the
summation package Sigma are provided.
",['\nCarsten Schneider\n'],Various typos removed; improved presentation II,,http://arxiv.org/abs/2102.01471v3,cs.SC,['cs.SC'],,,[]
"Proceedings 11th International Workshop on Computing with Terms and
  Graphs",http://arxiv.org/abs/2102.01804v1,2021-02-02T23:58:54Z,2021-02-02T23:58:54Z,"  Graphs, and graph transformation systems, are used in many areas within
Computer Science: to represent data structures and algorithms, to define
computation models, as a general modelling tool to study complex systems, etc.
Research in term and graph rewriting ranges from theoretical questions to
practical implementation issues. Relevant research areas include: the modelling
of first- and higher-order term rewriting by graph rewriting, graphical
frameworks such as interaction nets and sharing graphs (optimal reduction),
rewrite calculi for the analysis of functional programs, graph reduction
implementations of programming languages, graphical calculi modelling
concurrent and mobile computations, object-oriented systems, graphs as a model
of biological or chemical systems, and automated reasoning and symbolic
computation systems working on shared structures. The aim of the TERMGRAPH
workshop is to bring together researchers working in these different domains
and to foster their interaction, to provide a forum for presenting new ideas
and work in progress, and to enable newcomers to learn about current activities
in this area.
",['\nPatrick Bahr\nIT University of Copenhagen\n'],"This volume contains a selection of the papers presented at TERMGRAPH
  2020, the 11th International Workshop on Computing with Terms and Graphs","EPTCS 334, 2021",http://dx.doi.org/10.4204/EPTCS.334,cs.SC,['cs.SC'],10.4204/EPTCS.334,,['IT University of Copenhagen']
"Solving linear difference equations with coefficients in rings with
  idempotent representations",http://arxiv.org/abs/2102.03307v1,2021-02-05T17:28:04Z,2021-02-05T17:28:04Z,"  We introduce a general reduction strategy that enables one to search for
solutions of parameterized linear difference equations in difference rings.
Here we assume that the ring itself can be decomposed by a direct sum of
integral domains (using idempotent elements) that enjoys certain technical
features and that the coefficients of the difference equation are not
degenerated. Using this mechanism we can reduce the problem to find solutions
in a ring (with zero-divisors) to search solutions in several copies of
integral domains. Utilizing existing solvers in this integral domain setting,
we obtain a general solver where the components of the linear difference
equations and the solutions can be taken from difference rings that are built
e.g., by $R\Pi\Sigma$-extensions over $\Pi\Sigma$-fields. This class of
difference rings contains, e.g., nested sums and products, products over roots
of unity and nested sums defined over such objects.
","['\nJakob Ablinger\n', '\nCarsten Schneider\n']",,,http://arxiv.org/abs/2102.03307v1,cs.SC,['cs.SC'],,,[]
Algorithms for Linearly Recurrent Sequences of Truncated Polynomials,http://arxiv.org/abs/2102.03583v2,2021-02-06T13:21:03Z,2021-06-08T20:23:09Z,"  Linear recurrent sequences are those whose elements are defined as linear
combinations of preceding elements, and finding recurrence relations is a
fundamental problem in computer algebra. In this paper, we focus on sequences
whose elements are vectors over the ring $\mathbb{A} = \mathbb{K}[x]/(x^d)$ of
truncated polynomials. Finding the ideal of their recurrence relations has
applications such as the computation of minimal polynomials and determinants of
sparse matrices over $\mathbb{A}$. We present three methods for finding this
ideal: a Berlekamp-Massey-like approach due to Kurakin, one which computes the
kernel of some block-Hankel matrix over $\mathbb{A}$ via a minimal approximant
basis, and one based on bivariate Pad\'e approximation. We propose complexity
improvements for the first two methods, respectively by avoiding the
computation of redundant relations and by exploiting the Hankel structure to
compress the approximation problem. Then we confirm these improvements
empirically through a C++ implementation, and we discuss the above-mentioned
applications.
","['\nSeung Gyu Hyun\n', '\nVincent Neiger\n', '\nÉric Schost\n']","8 pages, ISSAC 2021",,http://arxiv.org/abs/2102.03583v2,cs.SC,['cs.SC'],,,[]
"On Two Signature Variants Of Buchberger's Algorithm Over Principal Ideal
  Domains",http://arxiv.org/abs/2102.03339v2,2021-02-05T18:40:51Z,2021-05-25T10:53:50Z,"  Signature-based algorithms have brought large improvements in the
performances of Gr\""obner bases algorithms for polynomial systems over fields.
Furthermore, they yield additional data which can be used, for example, to
compute the module of syzygies of an ideal or to compute coefficients in terms
of the input generators.
  In this paper, we examine two variants of Buchberger's algorithm to compute
Gr\""obner bases over principal ideal domains, with the addition of signatures.
The first one is adapted from Kandri-Rody and Kapur's algorithm, whereas the
second one uses the ideas developed in the algorithms by L. Pan (1989) and D.
Lichtblau (2012). The differences in constructions between the algorithms
entail differences in the operations which are compatible with the signatures,
and in the criteria which can be used to discard elements.
  We prove that both algorithms are correct and discuss their relative
performances in a prototype implementation in Magma.
","['\nMaria Francis\n', '\nThibaut Verron\n']","9 pages, 0 figures, accepted at ISSAC'21",,http://dx.doi.org/10.1145/3452143.3465522,cs.SC,"['cs.SC', 'math.AC']",10.1145/3452143.3465522,,[]
A Companion Curve Tracing Method for Rank-deficient Polynomial Systems,http://arxiv.org/abs/2101.12453v1,2021-01-29T07:53:40Z,2021-01-29T07:53:40Z,"  We propose a method for tracing implicit real algebraic curves defined by
polynomials with rank-deficient Jacobians.
  For a given curve $f^{-1}(0)$, it first utilizes a regularization technique
to compute at least one witness point per connected component of the curve.
  We improve this step by establishing a sufficient condition for testing the
emptiness of $f^{-1}(0)$.
  We also analyze the convergence rate and carry out an error analysis for
refining the witness points.
  The witness points are obtained by computing the minimum distance of a random
point to a smooth manifold embedding the curve while at the same time
penalizing the residual of $f$ at the local minima.
  To trace the curve starting from these witness points, we prove that if one
drags the random point along a trajectory inside a tubular neighborhood of the
embedded manifold of the curve, the projection of the trajectory on the
manifold is unique and can be computed by numerical continuation.
  We then show how to choose such a trajectory to approximate the curve by
computing eigenvectors of certain matrices.
  Effectiveness of the method is illustrated by examples.
","['\nWenyuan Wu\n', '\nChangbo Chen\n']",,,http://arxiv.org/abs/2101.12453v1,math.NA,"['math.NA', 'cs.NA', 'cs.SC']",,,[]
"Certified evaluations of Hölder continuous functions at roots of
  polynomials",http://arxiv.org/abs/2102.00115v1,2021-01-30T00:14:39Z,2021-01-30T00:14:39Z,"  Various methods can obtain certified estimates for roots of polynomials. Many
applications in science and engineering additionally utilize the value of
functions evaluated at roots. For example, critical values are obtained by
evaluating an objective function at critical points. For analytic evaluation
functions, Newton's method naturally applies to yield certified estimates.
These estimates no longer apply, however, for H\""older continuous functions,
which are a generalization of Lipschitz continuous functions where continuous
derivatives need not exist. This work develops and analyzes an alternative
approach for certified estimates of evaluating locally H\""older continuous
functions at roots of polynomials. An implementation of the method in Maple
demonstrates efficacy and efficiency.
","['\nParker B. Edwards\n', '\nJonathan D. Hauenstein\n', '\nClifford D. Smyth\n']","15 pages, 1 figure, associated software package at
  https://github.com/P-Edwards/EvalCertification",,http://arxiv.org/abs/2102.00115v1,cs.SC,"['cs.SC', 'cs.NA', 'math.NA', '65H14 (Primary) 65H04, 14-04 (Secondary)', 'I.1.2']",,,[]
Combinatorial Differential Algebra of $x^p$,http://arxiv.org/abs/2102.03182v3,2021-02-05T14:02:07Z,2022-03-09T12:22:24Z,"  We link $n$-jets of the affine monomial scheme defined by $x^p$ to the stable
set polytope of some perfect graph. We prove that, as $p$ varies, the dimension
of the coordinate ring of a certain subscheme of the scheme of $n$-jets as a
$\mathbb{C}$-vector space is a polynomial of degree $n+1$, namely the Ehrhart
polynomial of the stable set polytope of that graph. One main ingredient for
our proof is a result of Zobnin who determined a differential Gr\""{o}bner basis
of the differential ideal generated by $x^p$. We generalize Zobnin's result to
the bivariate case. We study $(m,n)$-jets, a higher-dimensional analog of jets,
and relate them to regular unimodular triangulations.
","['\nRida Ait El Manssour\n', '\nAnna-Laura Sattelberger\n']",16 pages,,http://arxiv.org/abs/2102.03182v3,math.AG,"['math.AG', 'cs.SC', 'math.CO', '05E40, 13P10, 12H05 (primary), 52B20 (secondary)']",,,[]
Telescopers for differential forms with one parameter,http://arxiv.org/abs/2101.06576v2,2021-01-17T02:53:04Z,2021-01-19T07:37:50Z,"  Telescopers for a function are linear differential (resp. difference)
operators annihilated by the definite integral (resp. definite sum) of this
function. They play a key role in Wilf-Zeilberger theory and algorithms for
computing them have been extensively studied in the past thirty years. In this
paper, we introduce the notion of telescopers for differential forms with
$D$-finite function coefficients. These telescopers appear in several areas of
mathematics, for instance parametrized differential Galois theory and mirror
symmetry. We give a sufficient and necessary condition for the existence of
telescopers for a differential form and describe a method to compute them if
they exist. Algorithms for verifying this condition are also given.
","['\nShaoshi Chen\n', '\nRuyong Feng\n', '\nZiming Li\n', '\nMichael F. Singer\n', '\nStephen Watt\n']",26 pages,,http://arxiv.org/abs/2101.06576v2,cs.SC,"['cs.SC', '68W30']",,,[]
Covering rational surfaces with rational parametrization images,http://arxiv.org/abs/2101.07011v1,2021-01-18T11:25:48Z,2021-01-18T11:25:48Z,"  Let $S$ be a rational projective surface given by means of a projective
rational parametrization whose base locus satisfies a mild assumption. In this
paper we present an algorithm that provides three rational maps
$f,g,h:\mathbb{A}^2 --\to S\subset \mathbb{P}^n$ such that the union of the
three images covers $S$. As a consequence, we present a second algorithm that
generates two rational maps $f,\tilde{g}:\mathbb{A}^2 --\to S$, such that the
union of their images covers the affine surface $S\cap \mathbb{A}^n$. In the
affine case, the number of rational maps involved in the cover is in general
optimal.
","['\nJorge Caravantes\n', '\nJ. Rafael Sendra\n', '\nDavid Sevilla\n', '\nCarlos Villarino\n']",16 pages. Submitted,,http://arxiv.org/abs/2101.07011v1,math.AG,"['math.AG', 'cs.SC', '14Q10 (Primary) 68W30 (Secondary)']",,,[]
"There are EXACTLY 1493804444499093354916284290188948031229880469556 Ways
  to Derange a Standard Deck of Cards (ignoring suits) [and many other such
  useful facts]",http://arxiv.org/abs/2101.10147v1,2021-01-25T14:53:38Z,2021-01-25T14:53:38Z,"  In this memorial tribute to Joe Gillis, who taught us that Special Functions
count, we show how the seminal Even-Gillis integral formula for the number of
derangements of a multiset, in terms of Laguerre polynomials, can be used to
efficiently compute not only the number of the title, but much harder ones,
when it is interfaced with Wilf-Zeilberger algorithmic proof theory.
","['\nShalosh B. Ekhad\n', '\nChristoph Koutschan\n', '\nDoron Zeilberger\n']","Accompanied by a Maple package and output files that can be gotten
  from
  https://sites.math.rutgers.edu/~zeilberg/mamarim/mamarimhtml/multider.html",,http://arxiv.org/abs/2101.10147v1,math.CO,"['math.CO', 'cs.SC', '33F10, 05A15, 33D45, 68W30']",,,[]
MultivariateApart: Generalized Partial Fractions,http://arxiv.org/abs/2101.08283v1,2021-01-20T19:00:20Z,2021-01-20T19:00:20Z,"  We present a package to perform partial fraction decompositions of
multivariate rational functions. The algorithm allows to systematically avoid
spurious denominator factors and is capable of producing unique results also
when being applied to terms of a sum separately. The package is designed to
work in Mathematica, but also provides interfaces to the Form and Singular
computer algebra systems.
","['\nMatthias Heller\n', '\nAndreas von Manteuffel\n']","30 pages, 2 tables, code available at
  https://gitlab.msu.edu/vmante/multivariateapart",,http://dx.doi.org/10.1016/j.cpc.2021.108174,cs.SC,"['cs.SC', 'hep-ph', 'hep-th']",10.1016/j.cpc.2021.108174,,[]
"Accelerated Polynomial Evaluation and Differentiation at Power Series in
  Multiple Double Precision",http://arxiv.org/abs/2101.10881v3,2021-01-22T19:42:43Z,2021-03-13T00:22:10Z,"  The problem is to evaluate a polynomial in several variables and its gradient
at a power series truncated to some finite degree with multiple double
precision arithmetic. To compensate for the cost overhead of multiple double
precision and power series arithmetic, data parallel algorithms for general
purpose graphics processing units are presented. The reverse mode of
algorithmic differentiation is organized into a massively parallel computation
of many convolutions and additions of truncated power series. Experimental
results demonstrate that teraflop performance is obtained in deca double
precision with power series truncated at degree 152. The algorithms scale well
for increasing precision and increasing degrees.
",['\nJan Verschelde\n'],"Improved the introduction, adding two citations to related work;
  fixed error, added table on the fluctuations of wall clock times. To appear
  in the Proceedings of the 2021 IEEE International Parallel and Distributed
  Processing Symposium Workshops (IPDPSW)",,http://arxiv.org/abs/2101.10881v3,cs.MS,"['cs.MS', 'cs.DC', 'cs.NA', 'cs.SC', 'math.AG', 'math.NA']",,,[]
PTOPO: Computing the Geometry and the Topology of Parametric Curves,http://arxiv.org/abs/2101.01925v2,2021-01-06T08:48:25Z,2022-02-17T17:26:42Z,"  We consider the problem of computing the topology and describing the geometry
of a parametric curve in $\mathbb{R}^n$. We present an algorithm, PTOPO, that
constructs an abstract graph that is isotopic to the curve in the embedding
space. Our method exploits the benefits of the parametric representation and
does not resort to implicitization.
  Most importantly, we perform all computations in the parameter space and not
in the implicit space. When the parametrization involves polynomials of degree
at most $d$ and maximum bitsize of coefficients $\tau$, then the worst case bit
complexity of PTOPO is $
\tilde{\mathcal{O}}_B(nd^6+nd^5\tau+d^4(n^2+n\tau)+d^3(n^2\tau+
n^3)+n^3d^2\tau)$. This bound matches the current record bound
$\tilde{\mathcal{O}}_B(d^6+d^5\tau)$ for the problem of computing the topology
of a plane algebraic curve given in implicit form. For plane and space curves,
if $N = \max\{d, \tau \}$, the complexity of PTOPO becomes
$\tilde{\mathcal{O}}_B(N^6)$, which improves the state-of-the-art result, due
to Alc\'azar and D\'iaz-Toca [CAGD'10], by a factor of $N^{10}$. In the same
time complexity, we obtain a graph whose straight-line embedding is isotopic to
the curve. However, visualizing the curve on top of the abstract graph
construction, increases the bound to $\tilde{\mathcal{O}}_B(N^7)$. For curves
of general dimension, we can also distinguish between ordinary and non-ordinary
real singularities and determine their multiplicities in the same expected
complexity of PTOPO by employing the algorithm of Blasco and P\'erez-D\'iaz
[CAGD'19]. We have implemented PTOPO in Maple for the case of plane and space
curves. Our experiments illustrate its practical nature.
","['\nChristina Katsamaki\n', '\nFabrice Rouillier\n', '\nElias Tsigaridas\n']",,,http://arxiv.org/abs/2101.01925v2,cs.SC,['cs.SC'],,,[]
Polynomial modular product verification and its implications,http://arxiv.org/abs/2101.02142v1,2021-01-06T17:19:14Z,2021-01-06T17:19:14Z,"  Polynomial multiplication is known to have quasi-linear complexity in both
the dense and the sparse cases. Yet no truly linear algorithm has been given in
any case for the problem, and it is not clear whether it is even possible. This
leaves room for a better algorithm for the simpler problem of verifying a
polynomial product. While finding deterministic methods seems out of reach,
there exist probabilistic algorithms for the problem that are optimal in number
of algebraic operations.
  We study the generalization of the problem to the verification of a
polynomial product modulo a sparse divisor. We investigate its bit complexity
for both dense and sparse multiplicands. In particular, we are able to show the
primacy of the verification over modular multiplication when the divisor has a
constant sparsity and a second highest-degree monomial that is not too large.
We use these results to obtain new bounds on the bit complexity of the standard
polynomial multiplication verification. In particular, we provide optimal
algorithms in the bit complexity model in the dense case by improving a result
of Kaminski and develop the first quasi-optimal algorithm for verifying sparse
polynomial product.
","['\nPascal Giorgi\n', '\nBruno Grenet\n', '\nArmelle Perret du Cray\n']",,,http://arxiv.org/abs/2101.02142v1,cs.SC,"['cs.SC', 'cs.CC']",,,[]
Object-Level Reasoning with Logics Encoded in HOL Light,http://arxiv.org/abs/2101.03808v1,2021-01-11T10:51:36Z,2021-01-11T10:51:36Z,"  We present a generic framework that facilitates object level reasoning with
logics that are encoded within the Higher Order Logic theorem proving
environment of HOL Light. This involves proving statements in any logic using
intuitive forward and backward chaining in a sequent calculus style. It is made
possible by automated machinery that take care of the necessary structural
reasoning and term matching automatically. Our framework can also handle type
theoretic correspondences of proofs, effectively allowing the type checking and
construction of computational processes via proof. We demonstrate our
implementation using a simple propositional logic and its Curry-Howard
correspondence to the lambda-calculus, and argue its use with linear logic and
its various correspondences to session types.
","['\nPetros Papapanagiotou\nUniversity of Edinburgh\n', '\nJacques Fleuriot\nUniversity of Edinburgh\n']","In Proceedings LFMTP 2020, arXiv:2101.02835","EPTCS 332, 2021, pp. 18-34",http://dx.doi.org/10.4204/EPTCS.332.2,cs.LO,"['cs.LO', 'cs.SC']",10.4204/EPTCS.332.2,,"['University of Edinburgh', 'University of Edinburgh']"
Theorem Proving and Algebra,http://arxiv.org/abs/2101.02690v2,2021-01-07T18:52:08Z,2021-01-16T22:29:37Z,"  This book can be seen either as a text on theorem proving that uses
techniques from general algebra, or else as a text on general algebra
illustrated and made concrete by practical exercises in theorem proving. The
book considers several different logical systems, including first-order logic,
Horn clause logic, equational logic, and first-order logic with equality.
Similarly, several different proof paradigms are considered. However, we do
emphasize equational logic, and for simplicity we use only the OBJ3 software
system, though it is used in a rather flexible manner. We do not pursue the
lofty goal of mechanizing proofs like those of which mathematicians are justly
so proud; instead, we seek to take steps towards providing mechanical
assistance for proofs that are useful for computer scientists in developing
software and hardware. This more modest goal has the advantage of both being
achievable and having practical benefits.
  The following topics are covered: many-sorted signature, algebra and
homomorphism; term algebra and substitution; equation and satisfaction;
conditional equations; equational deduction and its completeness; deduction for
conditional equations; the theorem of constants; interpretation and equivalence
of theories; term rewriting, termination, confluence and normal form; abstract
rewrite systems; standard models, abstract data types, initiality, and
induction; rewriting and deduction modulo equations; first-order logic, models,
and proof planning; second-order algebra; order-sorted algebra and rewriting;
modules; unification and completion; and hidden algebra. In parallel with these
are a gradual introduction to OBJ3, applications to group theory, various
abstract data types (such as number systems, lists, and stacks), propositional
calculus, hardware verification, the {\lambda}-calculus, correctness of
functional programs, and other topics.
",['\nJoseph A. Goguen\n'],"427+ xviii pages, 38 figures, Unfinished book by Joseph A. Goguen,
  Edited by Kokichi Futatsugi, Narciso Mart\'i-Oliet and Jos\'e Meseguer;
  revised version corrects some strange characters in page xv",,http://arxiv.org/abs/2101.02690v2,cs.LO,"['cs.LO', 'cs.PL', 'cs.SC', '68Q65, 03B70 (Primary)', 'F.3.1; F.3.2; F.4.1; F.1.1; I.1.3']",,,[]
The Proper Basis for a Zero-dimensional Polynomial Ideal,http://arxiv.org/abs/2101.03482v2,2021-01-10T06:29:49Z,2023-12-23T09:13:37Z,"  The proper basis formulated herein constitutes an improvement on the
Gr\""obner basis for a zero-dimensional polynomial ideal. Let $K[\mathbf{x}]$ be
a polynomial ring over a field $K$ with $\mathbf{x}:=(x_1,\dotsc,x_n)$. With
$x_1$ being the least variable, a zero-dimensional polynomial ideal $I\subset
K[\mathbf{x}]$ always has an eliminant $\chi\in K[x_1]\setminus K$ such that
$I\cap K[x_1]=(\chi)$ after eliminating the other variables
$\tilde{\mathbf{x}}:=(x_2,\dotsc,x_n)$. Hence it is excessive computation for
the elimination process involving the variable $x_1$ in Buchberger's algorithm
for the Gr\""obner basis. It is natural to treat $K[\mathbf{x}]$ as the algebra
$K[x_1][\tilde{\mathbf{x}}]$ and define a new type of basis over $K[x_1]$ for
$I$ called the proper basis. The proper basis is based on a new type of
polynomial division called the proper division, which improves the division
mechanism in M\""oller's algorithm over $K[x_1]$ for the Gr\""obner basis. We
develop a modular algorithm over a principal ideal ring with zero divisors. The
convincing efficiency of the proper basis over both Buchberger's Gr\""obner
basis over $K$ and M\""oller's one over $K[x_1]$ is corroborated by a series of
benchmark testings with respect to the typical \textnormal{\textsc{lex}}
ordering.
",['\nSheng-Ming Ma\n'],"29 pages. I name the new type of basis in the old version as the
  proper basis in the new version. The length of the paper is shortened from
  more than 70 pages to less than 30 pages in its current form. I also add the
  benchmark testing results to corroborate the impressive efficiency of the
  proper basis",,http://arxiv.org/abs/2101.03482v2,math.AC,"['math.AC', 'cs.SC', 'math.AG', '13P10, 13B25']",,,[]
Multi-Source Anomaly Detection in Distributed IT Systems,http://arxiv.org/abs/2101.04977v1,2021-01-13T10:11:32Z,2021-01-13T10:11:32Z,"  The multi-source data generated by distributed systems, provide a holistic
description of the system. Harnessing the joint distribution of the different
modalities by a learning model can be beneficial for critical applications for
maintenance of the distributed systems. One such important task is the task of
anomaly detection where we are interested in detecting the deviation of the
current behaviour of the system from the theoretically expected. In this work,
we utilize the joint representation from the distributed traces and system log
data for the task of anomaly detection in distributed systems. We demonstrate
that the joint utilization of traces and logs produced better results compared
to the single modality anomaly detection methods. Furthermore, we formalize a
learning task - next template prediction NTP, that is used as a generalization
for anomaly detection for both logs and distributed trace. Finally, we
demonstrate that this formalization allows for the learning of template
embedding for both the traces and logs. The joint embeddings can be reused in
other applications as good initialization for spans and logs.
","['\nJasmin Bogatinovski\n', '\nSasho Nedelkoski\n']",12 pages. Presented at AIOPS 2020 workshop,,http://arxiv.org/abs/2101.04977v1,cs.LG,"['cs.LG', 'cs.DC', 'cs.SC', 'cs.SE']",,,[]
Notes on Computational Graph and Jacobian Accumulation,http://arxiv.org/abs/2012.15034v1,2020-12-30T04:28:37Z,2020-12-30T04:28:37Z,"  The optimal calculation order of a computational graph can be represented by
a set of algebraic expressions. Computational graph and algebraic expression
both have close relations and significant differences, this paper looks into
these relations and differences, making plain their interconvertibility. By
revealing different types of multiplication relations in algebraic expressions
and their elimination dependencies in line-graph, we establish a theoretical
limit on the efficiency of face elimination.
",['\nYichong Zhou\n'],,,http://arxiv.org/abs/2012.15034v1,cs.SC,['cs.SC'],,,[]
Some fast algorithms multiplying a matrix by its adjoint,http://arxiv.org/abs/2101.01025v1,2021-01-04T15:24:25Z,2021-01-04T15:24:25Z,"  We present a non-commutative algorithm for the multiplication of a 2 x 2
block-matrix by its adjoint, defined by a matrix ring anti-homomorphism. This
algorithm uses 5 block products (3 recursive calls and 2 general products)over
C or in positive characteristic. The resulting algorithm for arbitrary
dimensions is a reduction of multiplication of a matrix by its adjoint to
general matrix product, improving by a constant factor previously known
reductions. We prove also that there is no algorithm derived from bilinear
forms using only four products and the adjoint of one of them. Second we give
novel dedicated algorithms for the complex field and the quaternions to
alternatively compute the multiplication taking advantage of the structure of
the matrix-polynomial arithmetic involved. We then analyze the respective
ranges of predominance of the two strategies. Finally we propose schedules with
low memory footprint that support a fast and memory efficient practical
implementation over a prime field.
","['\nJean-Guillaume Dumas\nCASC\n', '\nClément Pernet\nCASC\n', '\nAlexandre Sedoglavic\nCRIStAL\n']",,,http://arxiv.org/abs/2101.01025v1,cs.SC,['cs.SC'],,,"['CASC', 'CASC', 'CRIStAL']"
Exploring tropical differential equations,http://arxiv.org/abs/2012.14067v4,2020-12-28T02:29:05Z,2021-11-14T02:20:32Z,"  The purpose of this paper is fourfold. The first is to develop the theory of
tropical differential algebraic geometry from scratch; the second is to present
the tropical fundamental theorem for differential algebraic geometry, and show
how it may be used to extract combinatorial information about the set of power
series solutions to a given system of differential equations, both in the
archimedean (complex analytic) and in the non-archimedean (e.g., $p$-adic)
settings. A third and subsidiary aim is to show how tropical differential
algebraic geometry is a natural application of semiring theory, and in so
doing, contribute to the valuative study of differential algebraic geometry. We
use this formalism to extend the fundamental theorem of partial differential
algebraic geometry to the differential fraction field of the ring of formal
power series in arbitrarily (finitely) many variables; in doing so we produce
new examples of non-Krull valuations that merit further study in their own
right.
","['\nEthan Cotterill\n', '\nCristhian Garay\n', '\nJohana Luviano\n']","28 pages, 3 figures. Some proofs were corrected",,http://arxiv.org/abs/2012.14067v4,math.AG,"['math.AG', 'cs.SC', '13N99, 14T10 (Primary) 13P15, 52B20 (Secondary)']",,,[]
Monomial-agnostic computation of vanishing ideals,http://arxiv.org/abs/2101.00243v6,2021-01-01T14:27:45Z,2023-12-31T14:44:26Z,"  In the last decade, the approximate basis computation of vanishing ideals has
been studied extensively in computational algebra and data-driven applications
such as machine learning. However, symbolic computation and the dependency on
term order remain essential gaps between the two fields. In this study, we
present the first $\textit{monomial-agnostic}$ basis computation, which works
fully numerically with proper normalization and without term order. This is
realized by gradient normalization, a newly proposed data-dependent
normalization that normalizes a polynomial with the magnitude of gradients at
given points. The data-dependent nature of gradient normalization brings
various significant advantages: i) efficient resolution of the spurious
vanishing problem, the scale-variance issue of approximately vanishing
polynomials, without accessing coefficients of terms, ii) scaling-consistent
basis computation, ensuring that input scaling does not lead to an essential
change in the output, and iii) robustness against input perturbations, where
the upper bound of error is determined only by the magnitude of the
perturbations. Existing studies did not achieve any of these. As further
applications of gradient information, we propose a monomial-agnostic basis
reduction method and a regularization method to manage positive-dimensional
ideals.
","['\nHiroshi Kera\n', '\nYoshihiko Hasegawa\n']","25 pages, 1 figures",,http://arxiv.org/abs/2101.00243v6,cs.SC,"['cs.SC', 'math.AC']",,,[]
"Proceedings 6th International Workshop on Symbolic-Numeric methods for
  Reasoning about CPS and IoT",http://arxiv.org/abs/2101.05256v1,2021-01-01T06:32:01Z,2021-01-01T06:32:01Z,"  The proceedings of the 6th International Workshop on Symbolic-Numeric Methods
for Reasoning about CPS and IoT (SNR 2020) contains papers underlying talks
presented at the workshop. SNR focuses on the combination of symbolic and
numeric methods for reasoning about Cyber-Physical Systems and the Internet of
Things to facilitate model identification, specification, verification, and
control synthesis for these systems.
","['\nThao Dang\nVerimag/CNRS, France\n', '\nStefan Ratschan\nInstitute of Computer Science, Czech Academy of Sciences\n']",,"EPTCS 331, 2021",http://dx.doi.org/10.4204/EPTCS.331,cs.LO,"['cs.LO', 'cs.SC']",10.4204/EPTCS.331,,"['Verimag/CNRS, France', 'Institute of Computer Science, Czech Academy of Sciences']"
Border basis computation with gradient-weighted normalization,http://arxiv.org/abs/2101.00401v4,2021-01-02T08:29:51Z,2022-07-01T01:36:26Z,"  Normalization of polynomials plays a vital role in the approximate basis
computation of vanishing ideals. Coefficient normalization, which normalizes a
polynomial with its coefficient norm, is the most common method in computer
algebra. This study proposes the gradient-weighted normalization method for the
approximate border basis computation of vanishing ideals, inspired by recent
developments in machine learning. The data-dependent nature of
gradient-weighted normalization leads to better stability against perturbation
and consistency in the scaling of input points, which cannot be attained by
coefficient normalization. Only a subtle change is needed to introduce gradient
normalization in the existing algorithms with coefficient normalization. The
analysis of algorithms still works with a small modification, and the order of
magnitude of time complexity of algorithms remains unchanged. We also prove
that, with coefficient normalization, which does not provide the scaling
consistency property, scaling of points (e.g., as a preprocessing) can cause an
approximate basis computation to fail. This study is the first to theoretically
highlight the crucial effect of scaling in approximate basis computation and
presents the utility of data-dependent normalization.
",['\nHiroshi Kera\n'],"20 pages, 1 figure",,http://arxiv.org/abs/2101.00401v4,cs.SC,"['cs.SC', 'cs.LG', 'math.AC']",,,[]
"Fast Computation of the $N$-th Term of a $q$-Holonomic Sequence and
  Applications",http://arxiv.org/abs/2012.08656v1,2020-12-15T22:51:12Z,2020-12-15T22:51:12Z,"  In 1977, Strassen invented a famous baby-step/giant-step algorithm that
computes the factorial $N!$ in arithmetic complexity quasi-linear in
$\sqrt{N}$. In 1988, the Chudnovsky brothers generalized Strassen's algorithm
to the computation of the $N$-th term of any holonomic sequence in essentially
the same arithmetic complexity. We design $q$-analogues of these algorithms. We
first extend Strassen's algorithm to the computation of the $q$-factorial of
$N$, then Chudnovskys' algorithm to the computation of the $N$-th term of any
$q$-holonomic sequence. Both algorithms work in arithmetic complexity
quasi-linear in $\sqrt{N}$; surprisingly, they are simpler than their analogues
in the holonomic case. We provide a detailed cost analysis, in both arithmetic
and bit complexity models. Moreover, we describe various algorithmic
consequences, including the acceleration of polynomial and rational solving of
linear $q$-differential equations, and the fast evaluation of large classes of
polynomials, including a family recently considered by Nogneng and Schost.
","['\nAlin Bostan\n', '\nSergey Yurkevich\n']",,,http://arxiv.org/abs/2012.08656v1,cs.SC,"['cs.SC', '68W30, 68Q25, 05A30, 33F10', 'I.1.2']",,,[]
"SymFields: An Open Source Symbolic Fields Analysis Tool for General
  Curvilinear Coordinates in Python",http://arxiv.org/abs/2012.10723v1,2020-12-19T16:08:15Z,2020-12-19T16:08:15Z,"  An open source symbolic tool for vector fields analysis 'SymFields' is
developed in Python. The SymFields module is constructed upon Python symbolic
module sympy, which could only conduct scaler field analysis. With SymFields
module, you can conduct vector analysis for general curvilinear coordinates
regardless whether it is orthogonal or not. In SymFields, the differential
operators based on metric tensor are normalized to real physical values, which
means your can use real physical value of the vector fields as inputs. This
could greatly free the physicists from the tedious calculation under
complicated coordinates.
",['\nNan Chu\n'],"12 pages, 11 figures, open source code on GitHub:
  https://github.com/DocNan/SymFields",,http://arxiv.org/abs/2012.10723v1,cs.SC,['cs.SC'],,,[]
"A variant of van Hoeij's algorithm to compute hypergeometric term
  solutions of holonomic recurrence equations",http://arxiv.org/abs/2012.11513v1,2020-12-21T17:28:05Z,2020-12-21T17:28:05Z,"  Linear homogeneous recurrence equations with polynomial coefficients are said
to be holonomic. Such equations have been introduced in the last century for
proving and discovering combinatorial and hypergeometric identities. Given a
field K of characteristic zero, a term a(n) is called hypergeometric with
respect to K, if the ratio a(n+1)/a(n) is a rational function over K. The
solutions space of holonomic recurrence equations gained more interest in the
1990s from the well known Zeilberger's algorithm. In particular, algorithms
computing the subspace of hypergeometric term solutions which covers
polynomial, rational, and some algebraic solutions of these equations were
investigated by Marko Petkov\v{s}ek (1993) and Mark van Hoeij (1999). The
algorithm proposed by the latter is characterized by a much better efficiency
than that of the other; it computes, in Gamma representations, a basis of the
subspace of hypergeometric term solutions of any given holonomic recurrence
equation, and is considered as the current state of the art in this area. Mark
van Hoeij implemented his algorithm in the Computer Algebra System (CAS) Maple
through the command $LREtools[hypergeomsols]$.
  We propose a variant of van Hoeij's algorithm that performs the same
efficiency and gives outputs in terms of factorials and shifted factorials,
without considering certain recommendations of the original version. We have
implementations of our algorithm for the CASs Maxima and Maple. Such an
implementation is new for Maxima which is therefore used for general-purpose
examples. Our Maxima code is currently available as a third-party package for
Maxima. A comparison between van Hoeij's implementation and ours is presented
for Maple 2020. It appears that both have the same efficiency, and moreover,
for some particular cases, our code finds results where
$LREtools[hypergeomsols]$ fails.
",['\nBertrand Teguia Tabuguia\n'],25 pages,"J. Algorithm Comput., 53, 2021, 1--32",http://dx.doi.org/10.22059/JAC.2021.85170,cs.SC,"['cs.SC', 'math.CO', 'Primary: 33F10, 39A06, Secondary: 33C20, 68W30']",10.22059/JAC.2021.85170,,[]
"Investigating ADR mechanisms with knowledge graph mining and explainable
  AI",http://arxiv.org/abs/2012.09077v1,2020-12-16T16:59:25Z,2020-12-16T16:59:25Z,"  Adverse Drug Reactions (ADRs) are characterized within randomized clinical
trials and postmarketing pharmacovigilance, but their molecular mechanism
remains unknown in most cases. Aside from clinical trials, many elements of
knowledge about drug ingredients are available in open-access knowledge graphs.
In addition, drug classifications that label drugs as either causative or not
for several ADRs, have been established. We propose to mine knowledge graphs
for identifying biomolecular features that may enable reproducing automatically
expert classifications that distinguish drug causative or not for a given type
of ADR. In an explainable AI perspective, we explore simple classification
techniques such as Decision Trees and Classification Rules because they provide
human-readable models, which explain the classification itself, but may also
provide elements of explanation for molecular mechanisms behind ADRs. In
summary, we mine a knowledge graph for features; we train classifiers at
distinguishing, drugs associated or not with ADRs; we isolate features that are
both efficient in reproducing expert classifications and interpretable by
experts (i.e., Gene Ontology terms, drug targets, or pathway names); and we
manually evaluate how they may be explanatory. Extracted features reproduce
with a good fidelity classifications of drugs causative or not for DILI and
SCAR. Experts fully agreed that 73% and 38% of the most discriminative features
are possibly explanatory for DILI and SCAR, respectively; and partially agreed
(2/3) for 90% and 77% of them. Knowledge graphs provide diverse features to
enable simple and explainable models to distinguish between drugs that are
causative or not for ADRs. In addition to explaining classifications, most
discriminative features appear to be good candidates for investigating ADR
mechanisms further.
","['\nEmmanuel Bresso\n', '\nPierre Monnin\n', '\nCédric Bousquet\n', '\nFrançois-Elie Calvier\n', '\nNdeye-Coumba Ndiaye\n', '\nNadine Petitpain\n', '\nMalika Smaïl-Tabbone\n', '\nAdrien Coulet\n']",,,http://arxiv.org/abs/2012.09077v1,cs.LG,"['cs.LG', 'cs.AI', 'cs.SC']",,,[]
"Repairing dynamic models: a method to obtain identifiable and observable
  reparameterizations with mechanistic insights",http://arxiv.org/abs/2012.09826v2,2020-12-17T18:49:27Z,2020-12-18T15:08:44Z,"  Mechanistic dynamic models allow for a quantitative and systematic
interpretation of data and the generation of testable hypotheses. However,
these models are often over-parameterized, leading to non-identifiability and
non-observability, i.e. the impossibility of inferring their parameters and
state variables. The lack of structural identifiability and observability (SIO)
compromises a model's ability to make predictions and provide insight. Here we
present a methodology, AutoRepar, that corrects SIO deficiencies automatically,
yielding reparameterized models that are structurally identifiable and
observable. The reparameterization preserves the mechanistic meaning of
selected variables, and has the exact same dynamics and input-output mapping as
the original model. We implement AutoRepar as an extension of the STRIKE-GOLDD
software toolbox for SIO analysis, applying it to several models from the
literature to demonstrate its ability to repair their structural deficiencies.
AutoRepar increases the applicability of mechanistic models, enabling them to
provide reliable information about their parameters and dynamics.
","['\nGemma Massonis\n', '\nJulio R. Banga\n', '\nAlejandro F. Villaverde\n']","36 pages, 6 figures",,http://arxiv.org/abs/2012.09826v2,eess.SY,"['eess.SY', 'cs.SC', 'cs.SY']",,,[]
"Augmenting Policy Learning with Routines Discovered from a Single
  Demonstration",http://arxiv.org/abs/2012.12469v4,2020-12-23T03:15:21Z,2021-05-02T06:55:54Z,"  Humans can abstract prior knowledge from very little data and use it to boost
skill learning. In this paper, we propose routine-augmented policy learning
(RAPL), which discovers routines composed of primitive actions from a single
demonstration and uses discovered routines to augment policy learning. To
discover routines from the demonstration, we first abstract routine candidates
by identifying grammar over the demonstrated action trajectory. Then, the best
routines measured by length and frequency are selected to form a routine
library. We propose to learn policy simultaneously at primitive-level and
routine-level with discovered routines, leveraging the temporal structure of
routines. Our approach enables imitating expert behavior at multiple temporal
scales for imitation learning and promotes reinforcement learning exploration.
Extensive experiments on Atari games demonstrate that RAPL improves the
state-of-the-art imitation learning method SQIL and reinforcement learning
method A2C. Further, we show that discovered routines can generalize to unseen
levels and difficulties on the CoinRun benchmark.
","['\nZelin Zhao\n', '\nChuang Gan\n', '\nJiajun Wu\n', '\nXiaoxiao Guo\n', '\nJoshua B. Tenenbaum\n']","AAAI 2021. Code is available at
  https://github.com/sjtuytc/AAAI21-RoutineAugmentedPolicyLearning",,http://arxiv.org/abs/2012.12469v4,cs.LG,"['cs.LG', 'cs.AI', 'cs.SC']",,,[]
"Method for estimating hidden structures determined by unidentifiable
  state-space models and time-series data based on the Groebner basis",http://arxiv.org/abs/2012.11906v1,2020-12-22T10:09:42Z,2020-12-22T10:09:42Z,"  In this study, we propose a method for extracting the hidden algebraic
structures of model parameters that are uniquely determined by observed
time-series data and unidentifiable state-space models, explicitly and
exhaustively. State-space models are often constructed based on the domain, for
example, physical or biological. Such models include parameters that are
assigned specific meanings in relation to the system under consideration, which
is examined by estimating the parameters using the corresponding data. As the
parameters of unidentifiable models cannot be uniquely determined from the
given data, it is difficult to examine the systems described by such models. To
overcome this difficulty, multiple possible sets of parameters are estimated
and analysed in the exiting approaches; however, in general, all the possible
parameters cannot be explored; therefore, considerations on the system using
the estimated parameters become insufficient. In this study, focusing on
certain structures determined by the observed data and models uniquely, even if
they are unidentifiable, we introduce the concept of parameter variety. This is
newly defined and proven to form algebraic varieties, in general. A
computational algebraic method that relies on the Groebner basis for deriving
the explicit representation of the varieties is presented along with the
supporting theory. Furthermore, its application in the analysis of a model that
describes virus dynamics is presented. With this, new insight on the dynamics
overlooked by the conventional approach are discovered, confirming the
applicability of our idea and the proposed method.
","['\nMizuka Komatsu\n', '\nTakaharu Yaguchi\n']",,,http://arxiv.org/abs/2012.11906v1,eess.SY,"['eess.SY', 'cs.SC', 'cs.SY', 'math.DS', 'q-bio.QM', '93B25, 93B30']",,,[]
Counting Real Roots in Polynomial-Time for Systems Supported on Circuits,http://arxiv.org/abs/2012.04868v5,2020-12-09T05:11:20Z,2021-06-11T16:37:18Z,"  Suppose $A=\{a_1,\ldots,a_{n+2}\}\subset\mathbb{Z}^n$ has cardinality $n+2$,
with all the coordinates of the $a_j$ having absolute value at most $d$, and
the $a_j$ do not all lie in the same affine hyperplane. Suppose
$F=(f_1,\ldots,f_n)$ is an $n\times n$ polynomial system with generic integer
coefficients at most $H$ in absolute value, and $A$ the union of the sets of
exponent vectors of the $f_i$. We give the first algorithm that, for any fixed
$n$, counts exactly the number of real roots of $F$ in in time polynomial in
$\log(dH)$.
",['\nJ. Maurice Rojas\n'],"29 pages, 1 figure, accepted for presentation at MEGA (Effective
  Methods in Algebraic Geometry) 2021. You can see a recording of my talk at
  MEGA 2021 (June 9, 2021) at this YouTube link:
  https://www.youtube.com/watch?v=KKKmTctxbs4",,http://arxiv.org/abs/2012.04868v5,math.AG,"['math.AG', 'cs.CC', 'cs.SC']",,,[]
Hexapods with a small linear span,http://arxiv.org/abs/2012.05120v1,2020-12-09T15:50:38Z,2020-12-09T15:50:38Z,"  The understanding of mobile hexapods, i.e., parallel manipulators with six
legs, is one of the driving questions in theoretical kinematics. We aim at
contributing to this understanding by employing techniques from algebraic
geometry. The set of configurations of a mobile hexapod with one degree of
freedom has the structure of a projective curve, which hence has a degree and
an embedding dimension. Our main result is a classification of configuration
curves of hexapods that satisfy some restrictions on their embedding dimension.
","['\nHans-Christian Graf von Bothmer\n', '\nMatteo Gallet\n', '\nJosef Schicho\n']",41 pages,,http://arxiv.org/abs/2012.05120v1,math.AG,"['math.AG', 'cs.RO', 'cs.SC']",,,[]
A SAT-based Resolution of Lam's Problem,http://arxiv.org/abs/2012.04715v1,2020-12-08T20:06:25Z,2020-12-08T20:06:25Z,"  In 1989, computer searches by Lam, Thiel, and Swiercz experimentally resolved
Lam's problem from projective geometry$\unicode{x2014}$the long-standing
problem of determining if a projective plane of order ten exists. Both the
original search and an independent verification in 2011 discovered no such
projective plane. However, these searches were each performed using highly
specialized custom-written code and did not produce nonexistence certificates.
In this paper, we resolve Lam's problem by translating the problem into Boolean
logic and use satisfiability (SAT) solvers to produce nonexistence certificates
that can be verified by a third party. Our work uncovered consistency issues in
both previous searches$\unicode{x2014}$highlighting the difficulty of relying
on special-purpose search code for nonexistence results.
","['\nCurtis Bright\n', '\nKevin K. H. Cheung\n', '\nBrett Stevens\n', '\nIlias Kotsireas\n', '\nVijay Ganesh\n']","To appear at the Thirty-Fifth AAAI Conference on Artificial
  Intelligence",,http://dx.doi.org/10.1609/aaai.v35i5.16483,cs.DM,"['cs.DM', 'cs.AI', 'cs.LO', 'cs.SC', 'math.CO']",10.1609/aaai.v35i5.16483,,[]
Parallel Software to Offset the Cost of Higher Precision,http://arxiv.org/abs/2012.06607v1,2020-12-11T19:23:55Z,2020-12-11T19:23:55Z,"  Hardware double precision is often insufficient to solve large scientific
problems accurately. Computing in higher precision defined by software causes
significant computational overhead. The application of parallel algorithms
compensates for this overhead. Newton's method to develop power series
expansions of algebraic space curves is the use case for this application.
",['\nJan Verschelde\n'],"The paper corresponds to a talk given by the author at the HILT 2020
  Workshop on Safe Languages and Technologies for Structured and Efficient
  Parallel and Distributed/Cloud Computing, 16-17 November 2020",,http://arxiv.org/abs/2012.06607v1,cs.MS,"['cs.MS', 'cs.DC', 'cs.NA', 'cs.SC', 'math.AG', 'math.NA']",,,[]
"Why Charles Can Pen-test: an Evolutionary Approach to Vulnerability
  Testing",http://arxiv.org/abs/2011.13213v2,2020-11-26T10:15:53Z,2020-12-09T14:14:59Z,"  Discovering vulnerabilities in applications of real-world complexity is a
daunting task: a vulnerability may affect a single line of code, and yet it
compromises the security of the entire application. Even worse, vulnerabilities
may manifest only in exceptional circumstances that do not occur in the normal
operation of the application. It is widely recognized that state-of-the-art
penetration testing tools play a crucial role, and are routinely used, to dig
up vulnerabilities. Yet penetration testing is still primarily a human-driven
activity, and its effectiveness still depends on the skills and ingenuity of
the security analyst driving the tool. In this paper, we propose a technique
for the automatic discovery of vulnerabilities in event-based systems, such as
web and mobile applications. Our approach is based on a collaborative,
co-evolutionary and contract-driven search strategy that iteratively (i)
executes a pool of test cases, (ii) identifies the most promising ones, and
(iii) generates new test cases from them. The approach makes a synergistic
combination of evolutionary algorithms where several ""species"" contribute to
solving the problem: one species, the test species, evolves to find the target
test case, i.e., the set of instruction whose execution lead to the vulnerable
statement, whereas the other species, called contract species, evolve to select
the parameters for the procedure calls needed to trigger the vulnerability. To
assess the effectiveness of our approach, we implemented a working prototype
and ran it against both a case study and a benchmark web application. The
experimental results confirm that our tool automatically discovers and executes
a number of injection flaw attacks that are out of reach for state-of-the-art
web scanners.
","['\nGabriele Costa\n', '\nAndrea Valenza\n']",,,http://arxiv.org/abs/2011.13213v2,cs.CR,"['cs.CR', 'cs.SC']",,,[]
"Solving parametric systems of polynomial equations over the reals
  through Hermite matrices",http://arxiv.org/abs/2011.14136v2,2020-11-28T14:09:06Z,2021-12-16T20:07:46Z,"  We design a new algorithm for solving parametric systems having finitely many
complex solutions for generic values of the parameters. More precisely, let $f
= (f_1, \ldots, f_m)\subset \mathbb{Q}[y][x]$ with $y = (y_1, \ldots, y_t)$ and
$x = (x_1, \ldots, x_n)$, $V\subset \mathbb{C}^{t+n}$ be the algebraic set
defined by $f$ and $\pi$ be the projection $(y, x) \to y$. Under the
assumptions that $f$ admits finitely many complex roots for generic values of
$y$ and that the ideal generated by $f$ is radical, we solve the following
problem. On input $f$, we compute semi-algebraic formulas defining
semi-algebraic subsets $S_1, \ldots, S_l$ of the $y$-space such that
$\cup_{i=1}^l S_i$ is dense in $\mathbb{R}^t$ and the number of real points in
$V\cap \pi^{-1}(\eta)$ is invariant when $\eta$ varies over each $S_i$.
  This algorithm exploits properties of some well chosen monomial bases in the
algebra $\mathbb{Q}(y)[x]/I$ where $I$ is the ideal generated by $f$ in
$\mathbb{Q}(y)[x]$ and the specialization property of the so-called Hermite
matrices. This allows us to obtain compact representations of the sets $S_i$ by
means of semi-algebraic formulas encoding the signature of a symmetric matrix.
When $f$ satisfies extra genericity assumptions, we derive complexity bounds on
the number of arithmetic operations in $\mathbb{Q}$ and the degree of the
output polynomials. Let $d$ be the maximal degree of the $f_i$'s and $D =
n(d-1)d^n$, we prove that, on a generic $f=(f_1,\ldots,f_n)$, one can compute
those semi-algebraic formulas with $O^~( \binom{t+D}{t}2^{3t}n^{2t+1}
d^{3nt+2(n+t)+1})$ operations in $\mathbb{Q}$ and that the polynomials involved
have degree bounded by $D$.
  We report on practical experiments which illustrate the efficiency of our
algorithm on generic systems and systems from applications. It allows us to
solve problems which are out of reach of the state-of-the-art.
","['\nHuu Phuoc Le\n', '\nMohab Safey El Din\n']",,"Journal of Symbolic Computation, 2021",http://dx.doi.org/10.1016/j.jsc.2021.12.002,cs.SC,"['cs.SC', 'cs.CG', 'I.1.2']",10.1016/j.jsc.2021.12.002,,[]
"Proceedings of the Eleventh International Workshop on Graph Computation
  Models",http://arxiv.org/abs/2012.01181v1,2020-12-02T13:11:35Z,2020-12-02T13:11:35Z,"  Graphs are common mathematical structures that are visual and intuitive. They
constitute a natural and seamless way for system modelling in science,
engineering and beyond, including computer science, biology, business process
modelling, etc. Graph computation models constitute a class of very high-level
models where graphs are first-class citizens. The aim of the International GCM
Workshop series is to bring together researchers interested in all aspects of
computation models based on graphs and graph transformation. It promotes the
cross-fertilizing exchange of ideas and experiences among senior and young
researchers from the different communities interested in the foundations,
applications, and implementations of graph computation models and related
areas.
","['\nBerthold Hoffmann\nUniversität Bremen\n', '\nMark Minas\nUniversität der Bundeswehr München\n']","This volume contains the post-proceedings of the Eleventh
  International Workshop on Graph Computation Models (GCM 2020). The workshop
  was originally planned as part of STAF 2020 (Software Technologies:
  Applications and Foundations), to be held in Bergen, Norway, but then held as
  an online-workshop on 24th June 2020, because of the COVID-19 pandemic","EPTCS 330, 2020",http://dx.doi.org/10.4204/EPTCS.330,cs.FL,"['cs.FL', 'cs.SC']",10.4204/EPTCS.330,,"['Universität Bremen', 'Universität der Bundeswehr München']"
"Representation of hypergeometric products of higher nesting depths in
  difference rings",http://arxiv.org/abs/2011.08775v1,2020-11-17T17:01:56Z,2020-11-17T17:01:56Z,"  A non-trivial symbolic machinery is presented that can rephrase
algorithmically a finite set of nested hypergeometric products in appropriately
designed difference rings. As a consequence, one obtains an alternative
representation in terms of one single product defined over a root of unity and
nested hypergeometric products which are algebraically independent among each
other. In particular, one can solve the zero-recognition problem: the input
expression of nested hypergeometric products evaluates to zero if and only if
the output expression is the zero expression. Combined with available symbolic
summation algorithms in the setting of difference rings, one obtains a general
machinery that can represent (and simplify) nested sums defined over nested
products.
","['\nEvans Doe Ocansey\n', '\nCarsten Schneider\n']",,,http://arxiv.org/abs/2011.08775v1,cs.SC,['cs.SC'],,,[]
An effective method for computing Grothendieck point residue mappings,http://arxiv.org/abs/2011.09092v1,2020-11-18T05:14:17Z,2020-11-18T05:14:17Z,"  Grothendieck point residue is considered in the context of computational
complex analysis. A new effective method is proposed for computing Grothendieck
point residues mappings and residues. Basic ideas of our approach are the use
of Grothendieck local duality and a transformation law for local cohomology
classes. A new tool is devised for efficiency to solve the extended ideal
membership problems in local rings. The resulting algorithms are described with
an example to illustrate them. An extension of the proposed method to
parametric cases is also discussed as an application.
","['\nShinichi Tajima\n', '\nKatsusuke Nabeshima\n']",,,http://arxiv.org/abs/2011.09092v1,cs.SC,"['cs.SC', 'math.AG', '32A27, 32C36, 13P10, 14B15']",,,[]
Invariants of Self-Intersected N-Periodics in the Elliptic Billiard,http://arxiv.org/abs/2011.06640v3,2020-11-12T20:28:53Z,2021-01-19T13:00:24Z,"  We study self-intersected N-periodics in the elliptic billiard, describing
new facts about their geometry (e.g., self-intersected 4-periodics have
vertices concyclic with the foci). We also check if some invariants listed in
""Eighty New Invariants of N-Periodics in the Elliptic Billiard"" (2020),
arXiv:2004.12497, remain invariant in the self-intersected case. Toward that
end, we derive explicit expressions for many low-N simple and self-intersected
cases. We identify two special cases (one simple, one self-intersected) where a
quantity prescribed to be invariant is actually variable.
","['\nRonaldo Garcia\n', '\nDan Reznik\n']","24 pages, 14 figures, 3 tables, and 21 videos",,http://arxiv.org/abs/2011.06640v3,math.MG,"['math.MG', 'cs.CG', 'cs.RO', 'cs.SC', '51M04 51N20 51N35 68T20']",,,[]
Symbolically Solving Partial Differential Equations using Deep Learning,http://arxiv.org/abs/2011.06673v1,2020-11-12T22:16:03Z,2020-11-12T22:16:03Z,"  We describe a neural-based method for generating exact or approximate
solutions to differential equations in the form of mathematical expressions.
Unlike other neural methods, our system returns symbolic expressions that can
be interpreted directly. Our method uses a neural architecture for learning
mathematical expressions to optimize a customizable objective, and is scalable,
compact, and easily adaptable for a variety of tasks and configurations. The
system has been shown to effectively find exact or approximate symbolic
solutions to various differential equations with applications in natural
sciences. In this work, we highlight how our method applies to partial
differential equations over multiple variables and more complex boundary and
initial value conditions.
","['\nMaysum Panju\n', '\nKourosh Parand\n', '\nAli Ghodsi\n']",10 pages,,http://arxiv.org/abs/2011.06673v1,cs.LG,"['cs.LG', 'cs.NE', 'cs.SC']",,,[]
"Sequence Positivity Through Numeric Analytic Continuation: Uniqueness of
  the Canham Model for Biomembranes",http://arxiv.org/abs/2011.08155v1,2020-11-16T18:26:34Z,2020-11-16T18:26:34Z,"  We prove solution uniqueness for the genus one Canham variational problem
arising in the shape prediction of biomembranes. The proof builds on a result
of Yu and Chen that reduces the variational problem to proving non-negativity
of a sequence defined by a linear recurrence relation with polynomial
coefficients. We combine rigorous numeric analytic continuation of D-finite
functions with classic bounds from singularity analysis to derive an effective
index where the asymptotic behaviour of the sequence, which is positive,
dominates the sequence behaviour. Positivity of the finite number of remaining
terms is then checked computationally.
","['\nStephen Melczer\n', '\nMarc Mezzarobba\n']",,,http://arxiv.org/abs/2011.08155v1,math.CO,"['math.CO', 'cs.SC', 'math.DG']",,,[]
Multi-experiment parameter identifiability of ODEs and model theory,http://arxiv.org/abs/2011.10868v2,2020-11-21T21:05:23Z,2021-08-17T11:35:19Z,"  Structural identifiability is a property of an ODE model with parameters that
allows for the parameters to be determined from continuous noise-free data.
This is a natural prerequisite for practical identifiability. Conducting
multiple independent experiments could make more parameters or functions of
parameters identifiable, which is a desirable property to have. How many
experiments are sufficient? In the present paper, we provide an algorithm to
determine the exact number of experiments for multi-experiment local
identifiability and obtain an upper bound that is off at most by one for the
number of experiments for multi-experiment global identifiability.
  Interestingly, the main theoretical ingredient of the algorithm has been
discovered and proved using model theory (in the sense of mathematical logic).
We hope that this unexpected connection will stimulate interactions between
applied algebra and model theory, and we provide a short introduction to model
theory in the context of parameter identifiability. As another related
application of model theory in this area, we construct a nonlinear ODE system
with one output such that single-experiment and multiple-experiment
identifiability are different for the system. This contrasts with recent
results about single-output linear systems.
  We also present a Monte Carlo randomized version of the algorithm with a
polynomial arithmetic complexity. Implementation of the algorithm is provided
and its performance is demonstrated on several examples. The source code is
available at https://github.com/pogudingleb/ExperimentsBound.
","['\nAlexey Ovchinnikov\n', '\nAnand Pillay\n', '\nGleb Pogudin\n', '\nThomas Scanlon\n']",,,http://arxiv.org/abs/2011.10868v2,math.AG,"['math.AG', 'cs.SC', 'cs.SY', 'eess.SY', 'math.LO']",,,[]
LDU factorization,http://arxiv.org/abs/2011.04108v1,2020-11-08T23:47:44Z,2020-11-08T23:47:44Z,"  LU-factorization of matrices is one of the fundamental algorithms of linear
algebra. The widespread use of supercomputers with distributed memory requires
a review of traditional algorithms, which were based on the common memory of a
computer. Matrix block recursive algorithms are a class of algorithms that
provide coarse-grained parallelization. The block recursive LU factorization
algorithm was obtained in 2010. This algorithm is called LEU-factorization. It,
like the traditional LU-algorithm, is designed for matrices over number fields.
However, it does not solve the problem of numerical instability. We propose a
generalization of the LEU algorithm to the case of a commutative domain and its
field of quotients. This LDU factorization algorithm decomposes the matrix over
the commutative domain into a product of three matrices, in which the matrices
L and U belong to the commutative domain, and the elements of the weighted
truncated permutation matrix D are the elements inverse to the product of some
pair of minors. All elements are calculated without errors, so the problem of
instability does not arise.
",['\nGennadi Malaschonok\n'],"16 pages, 1 figures, presented at conference CASC-2020",,http://arxiv.org/abs/2011.04108v1,cs.SC,"['cs.SC', '15B33', 'F.2.1']",,,[]
Calcium: computing in exact real and complex fields,http://arxiv.org/abs/2011.01728v1,2020-11-03T14:22:18Z,2020-11-03T14:22:18Z,"  Calcium is a C library for real and complex numbers in a form suitable for
exact algebraic and symbolic computation. Numbers are represented as elements
of fields $\mathbb{Q}(a_1,\ldots,a_n)$ where the extensions numbers $a_k$ may
be algebraic or transcendental. The system combines efficient field operations
with automatic discovery and certification of algebraic relations, resulting in
a practical computational model of $\mathbb{R}$ and $\mathbb{C}$ in which
equality is rigorously decidable for a large class of numbers.
",['\nFredrik Johansson\nLFANT\n'],,,http://arxiv.org/abs/2011.01728v1,cs.MS,"['cs.MS', 'cs.SC']",,,['LFANT']
Connectivity in Semi-Algebraic Sets I,http://arxiv.org/abs/2011.02162v2,2020-11-04T07:24:31Z,2020-11-13T02:14:19Z,"  A semi-algebraic set is a subset of the real space defined by polynomial
equations and inequalities having real coefficients and is a union of finitely
many maximally connected components. We consider the problem of deciding
whether two given points in a semi-algebraic set are connected; that is,
whether the two points lie in the same connected component. In particular, we
consider the semi-algebraic set defined by f <> 0 where f is a given polynomial
with integer coefficients. The motivation comes from the observation that many
important or non-trivial problems in science and engineering can be often
reduced to that of connectivity. Due to its importance, there has been intense
research effort on the problem. We will describe a symbolic-numeric method
based on gradient ascent. The method will be described in two papers. The first
paper (the present one) will describe the symbolic part and the forthcoming
second paper will describe the numeric part. In the present paper, we give
proofs of correctness and termination for the symbolic part and illustrate the
efficacy of the method using several non-trivial examples.
","['\nHoon Hong\n', '\nJames Rohal\n', '\nMohab Safey El Din\n', '\nEric Schost\n']",,,http://arxiv.org/abs/2011.02162v2,math.AG,"['math.AG', 'cs.SC', '14Q30, 68W30, 14P10, 14P25, 37D15']",,,[]
Rounding Error Analysis of Linear Recurrences Using Generating Series,http://arxiv.org/abs/2011.00827v5,2020-11-02T08:54:22Z,2023-03-01T13:41:29Z,"  We develop a toolbox for the error analysis of linear recurrences with
constant or polynomial coefficients, based on generating series, Cauchy's
method of majorants, and simple results from analytic combinatorics. We
illustrate the power of the approach by several nontrivial application
examples. Among these examples are a new worst-case analysis of an algorithm
for computing Bernoulli numbers, and a new algorithm for evaluating
differentially finite functions in interval arithmetic while avoiding interval
blow-up.
",['\nMarc Mezzarobba\nPEQUAN\n'],,"Electronic Transactions on Numerical Analysis, 2023, 58,
  pp.196--227",http://dx.doi.org/10.1553/etna_vol58s196,math.NA,"['math.NA', 'cs.NA', 'cs.SC']",10.1553/etna_vol58s196,,['PEQUAN']
"A Neuro-Symbolic Method for Solving Differential and Functional
  Equations",http://arxiv.org/abs/2011.02415v1,2020-11-04T17:13:25Z,2020-11-04T17:13:25Z,"  When neural networks are used to solve differential equations, they usually
produce solutions in the form of black-box functions that are not directly
mathematically interpretable. We introduce a method for generating symbolic
expressions to solve differential equations while leveraging deep learning
training methods. Unlike existing methods, our system does not require learning
a language model over symbolic mathematics, making it scalable, compact, and
easily adaptable for a variety of tasks and configurations. As part of the
method, we propose a novel neural architecture for learning mathematical
expressions to optimize a customizable objective. The system is designed to
always return a valid symbolic formula, generating a useful approximation when
an exact analytic solution to a differential equation is not or cannot be
found. We demonstrate through examples how our method can be applied on a
number of differential equations, often obtaining symbolic approximations that
are useful or insightful. Furthermore, we show how the system can be
effortlessly generalized to find symbolic solutions to other mathematical
tasks, including integration and functional equations.
","['\nMaysum Panju\n', '\nAli Ghodsi\n']",8 pages,,http://arxiv.org/abs/2011.02415v1,cs.LG,"['cs.LG', 'cs.AI', 'cs.SC']",,,[]
Quadratization of ODEs: Monomial vs. Non-Monomial,http://arxiv.org/abs/2011.03959v1,2020-11-08T11:42:10Z,2020-11-08T11:42:10Z,"  Quadratization is a transform of a system of ODEs with polynomial right-hand
side into a system of ODEs with at most quadratic right-hand side via the
introduction of new variables. It has been recently used as a pre-processing
step for new model order reduction methods, so it is important to keep the
number of new variables small. Several algorithms have been designed to search
for a quadratization with the new variables being monomials in the original
variables. To understand the limitations and potential ways of improving such
algorithms, we study the following question: can quadratizations with not
necessarily monomial new variables produce a model of substantially smaller
dimension than quadratization with only monomial new variables?
  To do this, we restrict our attention to scalar polynomial ODEs. Our first
result is that a scalar polynomial ODE
$\dot{x}=p(x)=a_nx^n+a_{n-1}x^{n-1}+\ldots + a_0$ with $n\geqslant 5$ and
$a_n\neq0$ can be quadratized using exactly one new variable if and only if
$p(x-\frac{a_{n-1}}{n\cdot a_n})=a_nx^n+ax^2+bx$ for some $a, b \in
\mathbb{C}$. In fact, the new variable can be taken
$z:=(x-\frac{a_{n-1}}{n\cdot a_n})^{n-1}$. Our second result is that two
non-monomial new variables are enough to quadratize all degree $6$ scalar
polynomial ODEs. Based on these results, we observe that a quadratization with
not necessarily monomial new variables can be much smaller than a monomial
quadratization even for scalar ODEs.
  The main results of the paper have been discovered using computational
methods of applied nonlinear algebra (Gr\""obner bases), and we describe these
computations.
",['\nFoyez Alauddin\n'],,,http://arxiv.org/abs/2011.03959v1,math.DS,"['math.DS', 'cs.SC', 'math.AG']",,,[]
"A Graph Theoretical Approach for Testing Binomiality of Reversible
  Chemical Reaction Networks",http://arxiv.org/abs/2010.12615v2,2020-10-23T19:02:55Z,2021-01-14T12:35:29Z,"  We study binomiality of the steady state ideals of chemical reaction
networks. Considering rate constants as indeterminates, the concept of
unconditional binomiality has been introduced and an algorithm based on linear
algebra has been proposed in a recent work for reversible chemical reaction
networks, which has a polynomial time complexity upper bound on the number of
species and reactions. In this article, using a modified version of
species--reaction graphs, we present an algorithm based on graph theory which
performs by adding and deleting edges and changing the labels of the edges in
order to test unconditional binomiality. We have implemented our graph
theoretical algorithm as well as the linear algebra one in Maple and made
experiments on biochemical models. Our experiments show that the performance of
the graph theoretical approach is similar to or better than the linear algebra
approach, while it is drastically faster than Groebner basis and quantifier
elimination methods.
","['\nHamid Rahkooy\n', '\nCristian Vargas Montero\n']",,,http://arxiv.org/abs/2010.12615v2,cs.SC,"['cs.SC', 'math.AC']",,,[]
"Lexicographic Groebner bases of bivariate polynomials modulo a
  univariate one",http://arxiv.org/abs/2010.14775v3,2020-10-28T05:37:20Z,2021-09-29T06:06:15Z,"  Let T(x) in k[x] be a monic non-constant polynomial and write R=k[x] / (T)
the quotient ring. Consider two bivariate polynomials a(x, y), b(x, y) in R[y].
In a first part, T = p^e is assumed to be the power of an irreducible
polynomial p. A new algorithm that computes a minimal lexicographic Groebner
basis of the ideal ( a, b, p^e), is introduced. A second part extends this
algorithm when T is general through the ""local/global"" principle realized by a
generalization of ""dynamic evaluation"", restricted so far to a polynomial T
that is squarefree. The algorithm produces splittings according to the case
distinction ""invertible/nilpotent"", extending the usual ""invertible/zero"" in
classic dynamic evaluation. This algorithm belongs to the Euclidean family, the
core being a subresultant sequence of a and b modulo T. In particular no
factorization or Groebner basis computations are necessary. The theoretical
background relies on Lazard's structural theorem for lexicographic Groebner
bases in two variables. An implementation is realized in Magma. Benchmarks show
clearly the benefit, sometimes important, of this approach compared to the
Groebner bases approach.
",['\nXavier Dahan\n'],"Accepted at JSC. 50 pages, 6 tables, 8 figures",,http://arxiv.org/abs/2010.14775v3,math.AC,"['math.AC', 'cs.SC']",,,[]
The New Rewriting Engine of Dedukti,http://arxiv.org/abs/2010.16115v2,2020-10-30T08:19:19Z,2022-02-15T14:24:55Z,"  Dedukti is a type-checker for the $\lambda$$\Pi$-calculus modulo rewriting,
an extension of Edinburgh's logicalframework LF where functions and type
symbols can be defined by rewrite rules. It thereforecontains an engine for
rewriting LF terms and types according to the rewrite rules given by the user.A
key component of this engine is the matching algorithm to find which rules can
be fired. In thispaper, we describe the class of rewrite rules supported by
Dedukti and the new implementation ofthe matching algorithm. Dedukti supports
non-linear rewrite rules on terms with binders usinghigher-order
pattern-matching as in Combinatory Reduction Systems (CRS). The new
matchingalgorithm extends the technique of decision trees introduced by Luc
Maranget in the OCamlcompiler to this more general context.
","['\nGabriel Hondet\nDEDUCTEAM, LSV, ENS Paris Saclay, CNRS\n', '\nFrédéric Blanqui\nDEDUCTEAM, LSV, ENS Paris Saclay, CNRS\n']",,"FSCD 2020 - 5th International Conference on Formal Structures for
  Computation and Deduction, Jun 2020, Paris, France. pp.16",http://dx.doi.org/10.4230/LIPIcs.FSCD.2020.35,cs.PL,"['cs.PL', 'cs.SC']",10.4230/LIPIcs.FSCD.2020.35,,"['DEDUCTEAM, LSV, ENS Paris Saclay, CNRS', 'DEDUCTEAM, LSV, ENS Paris Saclay, CNRS']"
Logic Guided Genetic Algorithms,http://arxiv.org/abs/2010.11328v1,2020-10-21T21:57:12Z,2020-10-21T21:57:12Z,"  We present a novel Auxiliary Truth enhanced Genetic Algorithm (GA) that uses
logical or mathematical constraints as a means of data augmentation as well as
to compute loss (in conjunction with the traditional MSE), with the aim of
increasing both data efficiency and accuracy of symbolic regression (SR)
algorithms. Our method, logic-guided genetic algorithm (LGGA), takes as input a
set of labelled data points and auxiliary truths (ATs) (mathematical facts
known a priori about the unknown function the regressor aims to learn) and
outputs a specially generated and curated dataset that can be used with any SR
method. Three key insights underpin our method: first, SR users often know
simple ATs about the function they are trying to learn. Second, whenever an SR
system produces a candidate equation inconsistent with these ATs, we can
compute a counterexample to prove the inconsistency, and further, this
counterexample may be used to augment the dataset and fed back to the SR system
in a corrective feedback loop. Third, the value addition of these ATs is that
their use in both the loss function and the data augmentation process leads to
better rates of convergence, accuracy, and data efficiency. We evaluate LGGA
against state-of-the-art SR tools, namely, Eureqa and TuringBot on 16 physics
equations from ""The Feynman Lectures on Physics"" book. We find that using these
SR tools in conjunction with LGGA results in them solving up to 30.0% more
equations, needing only a fraction of the amount of data compared to the same
tool without LGGA, i.e., resulting in up to a 61.9% improvement in data
efficiency.
","['\nDhananjay Ashok\n', '\nJoseph Scott\n', '\nSebastian Wetzel\n', '\nMaysum Panju\n', '\nVijay Ganesh\n']",,,http://arxiv.org/abs/2010.11328v1,cs.NE,"['cs.NE', 'cs.AI', 'cs.LG', 'cs.SC']",,,[]
Optimized Multivariate Polynomial Determinant on GPU,http://arxiv.org/abs/2010.12117v1,2020-10-23T00:48:32Z,2020-10-23T00:48:32Z,"  We present an optimized algorithm calculating determinant for multivariate
polynomial matrix on GPU. The novel algorithm provides precise determinant for
input multivariate polynomial matrix in controllable time. Our approach is
based on modular methods and split into Fast Fourier Transformation,
Condensation method and Chinese Remainder Theorem where each algorithm is
paralleled on GPU. The experiment results show that our parallel method owns
substantial speedups compared to Maple, allowing memory overhead and time
expedition in steady increment. We are also able to deal with complex matrix
which is over the threshold on Maple and constrained on CPU. In addition,
calculation during the process could be recovered without losing accuracy at
any point regardless of disruptions. Furthermore, we propose a time prediction
for calculation of polynomial determinant according to some basic matrix
attributes and we solve an open problem relating to harmonic elimination
equations on the basis of our GPU implementation.
","['\nJianjun Wei\n', '\nLiangyu Chen\n']",,,http://arxiv.org/abs/2010.12117v1,math.NA,"['math.NA', 'cs.DC', 'cs.NA', 'cs.SC']",,,[]
"On Linear Representation, Complexity and Inversion of maps over finite
  fields",http://arxiv.org/abs/2010.14601v4,2020-10-26T12:34:27Z,2022-04-09T10:52:03Z,"  The paper primarily addressed the problem of linear representation,
invertibility, and construction of the compositional inverse for non-linear
maps over finite fields. Though there is vast literature available for the
invertibility of polynomials and construction of inverses of permutation
polynomials over $\mathbb{F}$, this paper explores a completely new approach
using the dual map defined through the Koopman operator. This helps define the
linear representation of the non-linear map,, which helps translate the map's
non-linear compositions to a linear algebraic framework. The linear
representation, defined over the space of functions, naturally defines a notion
of linear complexity for non-linear maps, which can be viewed as a measure of
computational complexity associated with such maps. The framework of linear
representation is then extended to parameter dependent maps over $\mathbb{F}$,
and the conditions on parametric invertibility of such maps are established,
leading to a construction of a parametric inverse map (under composition). It
is shown that the framework can be extended to multivariate maps over
$\mathbb{F}^n$, and the conditions are established for invertibility of such
maps, and the inverse is constructed using the linear representation. Further,
the problem of linear representation of a group generated by a finite set of
permutation maps over $\mathbb{F}^n$ under composition is also solved by
extending the theory of linear representation of a single map.
","['\nRamachandran Anantharaman\n', '\nVirendra Sule\n']",22 pages,,http://arxiv.org/abs/2010.14601v4,cs.SC,"['cs.SC', 'cs.DM', 'math.RT']",,,[]
Fast Minimal Presentations of Bi-graded Persistence Modules,http://arxiv.org/abs/2010.15623v1,2020-10-29T14:11:01Z,2020-10-29T14:11:01Z,"  Multi-parameter persistent homology is a recent branch of topological data
analysis. In this area, data sets are investigated through the lens of homology
with respect to two or more scale parameters. The high computational cost of
many algorithms calls for a preprocessing step to reduce the input size. In
general, a minimal presentation is the smallest possible representation of a
persistence module. Lesnick and Wright proposed recently an algorithm (the
LW-algorithm) for computing minimal presentations based on matrix reduction. In
this work, we propose, implement and benchmark several improvements over the
LW-algorithm. Most notably, we propose the use of priority queues to avoid
extensive scanning of the matrix columns, which constitutes the computational
bottleneck in the LW-algorithm, and we combine their algorithm with ideas from
the multi-parameter chunk algorithm by Fugacci and Kerber. Our extensive
experiments show that our algorithm outperforms the LW-algorithm and computes
the minimal presentation for data sets with millions of simplices within a few
seconds. Our software is publicly available.
","['\nMichael Kerber\n', '\nAlexander Rolle\n']","This is an extended version of a paper that will appear at ALENEX
  2021",,http://arxiv.org/abs/2010.15623v1,math.AT,"['math.AT', 'cs.SC', 'math.AC', '55N99, 13D02']",,,[]
"On Minor Left Prime Factorization Problem for Multivariate Polynomial
  Matrices",http://arxiv.org/abs/2010.06998v1,2020-10-14T12:19:10Z,2020-10-14T12:19:10Z,"  A new necessary and sufficient condition for the existence of minor left
prime factorizations of multivariate polynomial matrices without full row rank
is presented. The key idea is to establish a relationship between a matrix and
its full row rank submatrix. Based on the new result, we propose an algorithm
for factorizing matrices and have implemented it on the computer algebra system
Maple. Two examples are given to illustrate the effectiveness of the algorithm,
and experimental data shows that the algorithm is efficient.
","['\nDong Lu\n', '\nDingkang Wang\n', '\nFanghui Xiao\n']",,,http://arxiv.org/abs/2010.06998v1,cs.SC,['cs.SC'],,,[]
"On Factor Left Prime Factorization Problems for Multivariate Polynomial
  Matrices",http://arxiv.org/abs/2010.07007v1,2020-10-14T12:31:09Z,2020-10-14T12:31:09Z,"  This paper is concerned with factor left prime factorization problems for
multivariate polynomial matrices without full row rank. We propose a necessary
and sufficient condition for the existence of factor left prime factorizations
of a class of multivariate polynomial matrices, and then design an algorithm to
compute all factor left prime factorizations if they exist. We implement the
algorithm on the computer algebra system Maple, and two examples are given to
illustrate the effectiveness of the algorithm. The results presented in this
paper are also true for the existence of factor right prime factorizations of
multivariate polynomial matrices without full column rank.
","['\nDong Lu\n', '\nDingkang Wang\n', '\nFanghui Xiao\n']",,,http://arxiv.org/abs/2010.07007v1,cs.SC,['cs.SC'],,,[]
"New Remarks on the Factorization and Equivalence Problems for a Class of
  Multivariate Polynomial Matrices",http://arxiv.org/abs/2010.07088v1,2020-10-14T13:42:10Z,2020-10-14T13:42:10Z,"  This paper is concerned with the factorization and equivalence problems of
multivariate polynomial matrices. We present some new criteria for the
existence of matrix factorizations for a class of multivariate polynomial
matrices, and obtain a necessary and sufficient condition for the equivalence
of a square polynomial matrix and a diagonal matrix. Based on the constructive
proof of the new criteria, we give a factorization algorithm and prove the
uniqueness of the factorization. We implement the algorithm on Maple, and two
illustrative examples are given to show the effectiveness of the algorithm.
","['\nDong Lu\n', '\nDingkang Wang\n', '\nFanghui Xiao\n']",,,http://arxiv.org/abs/2010.07088v1,cs.SC,['cs.SC'],,,[]
Creative Telescoping on Multiple Sums,http://arxiv.org/abs/2010.08889v2,2020-10-18T00:00:02Z,2021-03-17T08:51:17Z,"  We showcase a collection of practical strategies to deal with a problem
arising from an analysis of integral estimators derived via quasi-Monte Carlo
methods. The problem reduces to a triple binomial sum, thereby enabling us to
open up the holonomic toolkit, which contains tools such as creative
telescoping that can be used to deduce a recurrence satisfied by the sum. While
applying these techniques, a host of issues arose that partly needed to be
resolved by hand. In other words, no creative telescoping implementation
currently exists that can resolve all these issues automatically. Thus, we felt
the need to compile the different strategies we tried and the difficulties that
we encountered along the way. In particular, we highlight the necessity of the
certificate in these computations and how its complexity can greatly influence
the computation time.
","['\nChristoph Koutschan\n', '\nElaine Wong\n']","22 pages; Supplementary material at
  https://wongey.github.io/digital-nets-walsh/","Mathematics in Computer Science, Vol. 15(3), Pages 483-498 (2021)",http://dx.doi.org/10.1007/s11786-021-00514-3,cs.SC,['cs.SC'],10.1007/s11786-021-00514-3,,[]
Projective isomorphisms between rational surfaces,http://arxiv.org/abs/2010.08393v2,2020-10-16T13:52:47Z,2021-12-17T12:04:15Z,"  We present a method for computing projective isomorphisms between rational
surfaces that are given in terms of their parametrizations. The main idea is to
reduce the computation of such projective isomorphisms to five base cases by
modifying the parametric maps such that the components of the resulting maps
have lower degree. Our method can be used to compute affine, Euclidean and
M\""obius isomorphisms between surfaces.
","['\nBert Jüttler\n', '\nNiels Lubbes\n', '\nJosef Schicho\n']",,J. Algebra 594 (2022) 571-596,http://dx.doi.org/10.1016/j.jalgebra.2021.11.045,math.AG,"['math.AG', 'cs.SC', '14J50, 14J26']",10.1016/j.jalgebra.2021.11.045,,[]
Optimal Descartes' Rule of Signs for Circuits,http://arxiv.org/abs/2010.09165v2,2020-10-19T01:28:15Z,2022-05-25T19:29:20Z,"  We present an optimal version of Descartes' rule of signs to bound the number
of positive real roots of a sparse system of polynomial equations in n
variables with n+2 monomials. This sharp upper bound is given in terms of the
sign variation of a sequence associated to the exponents and the coefficients
of the system.
","['\nFrédéric Bihan\n', '\nAlicia Dickenstein\n', '\nJens Forsgård\n']","21 pages, 5 figures. We improved the proof of Theorem 2.4 by adding
  Proposition 2.7",,http://arxiv.org/abs/2010.09165v2,math.AG,"['math.AG', 'cs.SC']",,,[]
A Categorical Programming Language,http://arxiv.org/abs/2010.05167v1,2020-10-11T04:44:19Z,2020-10-11T04:44:19Z,"  A theory of data types based on category theory is presented. We organize
data types under a new categorical notion of F,G-dialgebras which is an
extension of the notion of adjunctions as well as that of T-algebras.
T-algebras are also used in domain theory, but while domain theory needs some
primitive data types, like products, to start with, we do not need any.
Products, coproducts and exponentiations (i.e. function spaces) are defined
exactly like in category theory using adjunctions. F,G-dialgebras also enable
us to define the natural number object, the object for finite lists and other
familiar data types in programming. Furthermore, their symmetry allows us to
have the dual of the natural number object and the object for infinite lists
(or lazy lists). We also introduce a programming language in a categorical
style using F,G-dialgebras as its data type declaration mechanism. We define
the meaning of the language operationally and prove that any program terminates
using Tait's computability method.
",['\nTatsuya Hagino\n'],,,http://arxiv.org/abs/2010.05167v1,cs.PL,"['cs.PL', 'cs.LO', 'cs.SC']",,,[]
Exploiting Knowledge Graphs for Facilitating Product/Service Discovery,http://arxiv.org/abs/2010.05213v1,2020-10-11T10:22:10Z,2020-10-11T10:22:10Z,"  Most of the existing techniques to product discovery rely on syntactic
approaches, thus ignoring valuable and specific semantic information of the
underlying standards during the process. The product data comes from different
heterogeneous sources and formats giving rise to the problem of
interoperability. Above all, due to the continuously increasing influx of data,
the manual labeling is getting costlier. Integrating the descriptions of
different products into a single representation requires organizing all the
products across vendors in a single taxonomy. Practically relevant and quality
product categorization standards are still limited in number; and that too in
academic research projects where we can majorly see only prototypes as compared
to industry. This work presents a cost-effective solution for e-commerce on the
Data Web by employing an unsupervised approach for data classification and
exploiting the knowledge graphs for matching. The proposed architecture
describes available products in web ontology language OWL and stores them in a
triple store. User input specifications for certain products are matched
against the available product categories to generate a knowledge graph. This
mullti-phased top-down approach to develop and improve existing, if any,
tailored product recommendations will be able to connect users with the exact
product/service of their choice.
",['\nSarika Jain\n'],"13 pages, 4 figures",,http://arxiv.org/abs/2010.05213v1,cs.AI,"['cs.AI', 'cs.LG', 'cs.SC', 'H.1.2; H.2.4; I.2; J.1; J.7']",,,[]
An Algorithm for the Factorization of Split Quaternion Polynomials,http://arxiv.org/abs/2010.05751v2,2020-10-12T14:45:53Z,2021-03-09T13:55:11Z,"  We present an algorithm to compute all factorizations into linear factors of
univariate polynomials over the split quaternions, provided such a
factorization exists. Failure of the algorithm is equivalent to
non-factorizability for which we present also geometric interpretations in
terms of rulings on the quadric of non-invertible split quaternions. However,
suitable real polynomial multiples of split quaternion polynomials can still be
factorized and we describe how to find these real polynomials. Split quaternion
polynomials describe rational motions in the hyperbolic plane. Factorization
with linear factors corresponds to the decomposition of the rational motion
into hyperbolic rotations. Since multiplication with a real polynomial does not
change the motion, this decomposition is always possible. Some of our ideas can
be transferred to the factorization theory of motion polynomials. These are
polynomials over the dual quaternions with real norm polynomial and they
describe rational motions in Euclidean kinematics. We transfer techniques
developed for split quaternions to compute new factorizations of certain dual
quaternion polynomials.
","['\nDaniel F. Scharler\n', '\nHans-Peter Schröcker\n']",,"Scharler, D.F., Schr\""ocker, HP. An Algorithm for the
  Factorization of Split Quaternion Polynomials. Adv. Appl. Clifford Algebras
  31, 29 (2021)",http://dx.doi.org/10.1007/s00006-021-01133-8,math.RA,"['math.RA', 'cs.SC', 'math.MG', '12D05, 16S36, 51M09, 51M10, 70B10']",10.1007/s00006-021-01133-8,,[]
On lattice point counting in $Δ$-modular polyhedra,http://arxiv.org/abs/2010.05768v4,2020-10-12T15:05:06Z,2021-05-10T18:03:48Z,"  Let a polyhedron $P$ be defined by one of the following ways:
  (i) $P = \{x \in R^n \colon A x \leq b\}$, where $A \in Z^{(n+k) \times n}$,
$b \in Z^{(n+k)}$ and $rank\, A = n$;
  (ii) $P = \{x \in R_+^n \colon A x = b\}$, where $A \in Z^{k \times n}$, $b
\in Z^{k}$ and $rank\, A = k$.
  And let all rank order minors of $A$ be bounded by $\Delta$ in absolute
values. We show that the short rational generating function for the power
series $$ \sum\limits_{m \in P \cap Z^n} x^m $$ can be computed with the
arithmetic complexity $ O\left(T_{SNF}(d) \cdot d^{k} \cdot d^{\log_2
\Delta}\right), $ where $k$ and $\Delta$ are fixed, $d = \dim P$, and
$T_{SNF}(m)$ is the complexity to compute the Smith Normal Form for $m \times
m$ integer matrix. In particular, $d = n$ for the case (i) and $d = n-k$ for
the case (ii). The simplest examples of polyhedra that meet conditions (i) or
(ii) are the simplicies, the subset sum polytope and the knapsack or
multidimensional knapsack polytopes. We apply these results to parametric
polytopes, and show that the step polynomial representation of the function
$c_P(y) = |P_{y} \cap Z^n|$, where $P_{y}$ is parametric polytope, can be
computed by a polynomial time even in varying dimension if $P_{y}$ has a close
structure to the cases (i) or (ii). As another consequence, we show that the
coefficients $e_i(P,m)$ of the Ehrhart quasi-polynomial $$ \left| mP \cap
Z^n\right| = \sum\limits_{j = 0}^n e_i(P,m)m^j $$ can be computed by a
polynomial time algorithm for fixed $k$ and $\Delta$.
","['\nD. V. Gribanov\n', '\nN. Yu. Zolotykh\n']",,"Optim Lett 16, 1991-2018 (2022)",http://dx.doi.org/10.1007/s11590-021-01744-x,cs.CC,"['cs.CC', 'cs.DM', 'cs.SC', 'math.CO']",10.1007/s11590-021-01744-x,,[]
"A variational autoencoder for music generation controlled by tonal
  tension",http://arxiv.org/abs/2010.06230v2,2020-10-13T08:37:22Z,2020-10-14T08:24:25Z,"  Many of the music generation systems based on neural networks are fully
autonomous and do not offer control over the generation process. In this
research, we present a controllable music generation system in terms of tonal
tension. We incorporate two tonal tension measures based on the Spiral Array
Tension theory into a variational autoencoder model. This allows us to control
the direction of the tonal tension throughout the generated piece, as well as
the overall level of tonal tension. Given a seed musical fragment, stemming
from either the user input or from directly sampling from the latent space, the
model can generate variations of this original seed fragment with altered tonal
tension. This altered music still resembles the seed music rhythmically, but
the pitch of the notes are changed to match the desired tonal tension as
conditioned by the user.
","['\nRui Guo\n', '\nIvor Simpson\n', '\nThor Magnusson\n', '\nChris Kiefer\n', '\nDorien Herremans\n']",2020 Joint Conference on AI Music Creativity,,http://arxiv.org/abs/2010.06230v2,cs.SD,"['cs.SD', 'cs.SC', 'eess.AS']",,,[]
"Formal Verification of Arithmetic RTL: Translating Verilog to C++ to
  ACL2",http://arxiv.org/abs/2009.13761v1,2020-09-29T04:09:53Z,2020-09-29T04:09:53Z,"  We present a methodology for formal verification of arithmetic RTL designs
that combines sequential logic equivalence checking with interactive theorem
proving. An intermediate model of a Verilog module is hand-coded in Restricted
Algorithmic C (RAC), a primitive subset of C augmented by the integer and
fixed-point register class templates of Algorithmic C. The model is designed to
be as abstract and compact as possible, but sufficiently faithful to the RTL to
allow efficient equivalence checking with a commercial tool. It is then
automatically translated to the logic of ACL2, enabling a mechanically checked
proof of correctness with respect to a formal architectural specification. In
this paper, we describe the RAC language, the translation process, and some
techniques that facilitate formal analysis of the resulting ACL2 code.
",['\nDavid M. Russinoff\nArm\n'],"In Proceedings ACL2 2020, arXiv:2009.12521","EPTCS 327, 2020, pp. 1-15",http://dx.doi.org/10.4204/EPTCS.327.1,cs.LO,"['cs.LO', 'cs.SC']",10.4204/EPTCS.327.1,,['Arm']
Iteration in ACL2,http://arxiv.org/abs/2009.13762v1,2020-09-29T04:10:05Z,2020-09-29T04:10:05Z,"  Iterative algorithms are traditionally expressed in ACL2 using recursion. On
the other hand, Common Lisp provides a construct, loop, which -- like most
programming languages -- provides direct support for iteration. We describe an
ACL2 analogue loop$ of loop that supports efficient ACL2 programming and
reasoning with iteration.
","['\nMatt Kaufmann\nUniv. of Texas at Austin\n', '\nJ Strother Moore\nUniv. of Texas at Austin\n']","In Proceedings ACL2 2020, arXiv:2009.12521","EPTCS 327, 2020, pp. 16-31",http://dx.doi.org/10.4204/EPTCS.327.2,cs.LO,"['cs.LO', 'cs.SC']",10.4204/EPTCS.327.2,,"['Univ. of Texas at Austin', 'Univ. of Texas at Austin']"
"A Deep Genetic Programming based Methodology for Art Media
  Classification Robust to Adversarial Perturbations",http://arxiv.org/abs/2010.01238v1,2020-10-03T00:36:34Z,2020-10-03T00:36:34Z,"  Art Media Classification problem is a current research area that has
attracted attention due to the complex extraction and analysis of features of
high-value art pieces. The perception of the attributes can not be subjective,
as humans sometimes follow a biased interpretation of artworks while ensuring
automated observation's trustworthiness. Machine Learning has outperformed many
areas through its learning process of artificial feature extraction from images
instead of designing handcrafted feature detectors. However, a major concern
related to its reliability has brought attention because, with small
perturbations made intentionally in the input image (adversarial attack), its
prediction can be completely changed. In this manner, we foresee two ways of
approaching the situation: (1) solve the problem of adversarial attacks in
current neural networks methodologies, or (2) propose a different approach that
can challenge deep learning without the effects of adversarial attacks. The
first one has not been solved yet, and adversarial attacks have become even
more complex to defend. Therefore, this work presents a Deep Genetic
Programming method, called Brain Programming, that competes with deep learning
and studies the transferability of adversarial attacks using two artworks
databases made by art experts. The results show that the Brain Programming
method preserves its performance in comparison with AlexNet, making it robust
to these perturbations and competing to the performance of Deep Learning.
","['\nGustavo Olague\n', '\nGerardo Ibarra-Vazquez\n', '\nMariana Chan-Ley\n', '\nCesar Puente\n', '\nCarlos Soubervielle-Montalvo\n', '\nAxel Martinez\n']","13 pages, 3 figures, International Symposium on Visual Computing 2020",,http://arxiv.org/abs/2010.01238v1,cs.CV,"['cs.CV', 'cs.SC']",,,[]
Factorization of Dual Quaternion Polynomials Without Study's Condition,http://arxiv.org/abs/2010.01945v2,2020-10-05T12:17:44Z,2021-01-19T15:13:07Z,"  In this paper we investigate factorizations of polynomials over the ring of
dual quaternions into linear factors. While earlier results assume that the
norm polynomial is real (""motion polynomials""), we only require the absence of
real polynomial factors in the primal part and factorizability of the norm
polynomial over the dual numbers into monic quadratic factors. This obviously
necessary condition is also sufficient for existence of factorizations. We
present an algorithm to compute factorizations of these polynomials and use it
for new constructions of mechanisms which cannot be obtained by existing
factorization algorithms for motion polynomials. While they produce mechanisms
with rotational or translational joints, our approach yields mechanisms
consisting of ""vertical Darboux joints"". They exhibit mechanical deficiencies
so that we explore ways to replace them by cylindrical joints while keeping the
overall mechanism sufficiently constrained.
","['\nJohannes Siegele\n', '\nMartin Pfurner\n', '\nHans-Peter Schröcker\n']",,,http://dx.doi.org/10.1007/s00006-021-01123-w,math.RA,"['math.RA', 'cs.SC', '16S36, 70B15']",10.1007/s00006-021-01123-w,,[]
A Simple and Efficient Tensor Calculus for Machine Learning,http://arxiv.org/abs/2010.03313v1,2020-10-07T10:18:56Z,2020-10-07T10:18:56Z,"  Computing derivatives of tensor expressions, also known as tensor calculus,
is a fundamental task in machine learning. A key concern is the efficiency of
evaluating the expressions and their derivatives that hinges on the
representation of these expressions. Recently, an algorithm for computing
higher order derivatives of tensor expressions like Jacobians or Hessians has
been introduced that is a few orders of magnitude faster than previous
state-of-the-art approaches. Unfortunately, the approach is based on Ricci
notation and hence cannot be incorporated into automatic differentiation
frameworks from deep learning like TensorFlow, PyTorch, autograd, or JAX that
use the simpler Einstein notation. This leaves two options, to either change
the underlying tensor representation in these frameworks or to develop a new,
provably correct algorithm based on Einstein notation. Obviously, the first
option is impractical. Hence, we pursue the second option. Here, we show that
using Ricci notation is not necessary for an efficient tensor calculus and
develop an equally efficient method for the simpler Einstein notation. It turns
out that turning to Einstein notation enables further improvements that lead to
even better efficiency.
  The methods that are described in this paper have been implemented in the
online tool www.MatrixCalculus.org for computing derivatives of matrix and
tensor expressions.
  An extended abstract of this paper appeared as ""A Simple and Efficient Tensor
Calculus"", AAAI 2020.
","['\nSören Laue\n', '\nMatthias Mitterreiter\n', '\nJoachim Giesen\n']",,,http://arxiv.org/abs/2010.03313v1,cs.LG,"['cs.LG', 'cs.SC']",,,[]
"On Differentially Algebraic Generating Series for Walks in the Quarter
  Plane",http://arxiv.org/abs/2010.00963v1,2020-10-02T12:51:24Z,2020-10-02T12:51:24Z,"  We refine necessary and sufficient conditions for the generating series of a
weighted model of a quarter plane walk to be differentially algebraic. In
addition, we give algorithms based on the theory of Mordell-Weil lattices,
that, for each weighted model, yield polynomial conditions on the weights
determining this property of the associated generating series.
","['\nCharlotte Hardouin\n', '\nMichael F Singer\n']",37 Pages,,http://arxiv.org/abs/2010.00963v1,math.CO,"['math.CO', 'cs.SC', 'math.NT', '05A15, 11G05, 30D05, 39A06']",,,[]
SPPL: Probabilistic Programming with Fast Exact Symbolic Inference,http://arxiv.org/abs/2010.03485v3,2020-10-07T15:42:37Z,2021-06-11T12:21:13Z,"  We present the Sum-Product Probabilistic Language (SPPL), a new probabilistic
programming language that automatically delivers exact solutions to a broad
range of probabilistic inference queries. SPPL translates probabilistic
programs into sum-product expressions, a new symbolic representation and
associated semantic domain that extends standard sum-product networks to
support mixed-type distributions, numeric transformations, logical formulas,
and pointwise and set-valued constraints. We formalize SPPL via a novel
translation strategy from probabilistic programs to sum-product expressions and
give sound exact algorithms for conditioning on and computing probabilities of
events. SPPL imposes a collection of restrictions on probabilistic programs to
ensure they can be translated into sum-product expressions, which allow the
system to leverage new techniques for improving the scalability of translation
and inference by automatically exploiting probabilistic structure. We implement
a prototype of SPPL with a modular architecture and evaluate it on benchmarks
the system targets, showing that it obtains up to 3500x speedups over
state-of-the-art symbolic systems on tasks such as verifying the fairness of
decision tree classifiers, smoothing hidden Markov models, conditioning
transformed random variables, and computing rare event probabilities.
","['\nFeras A. Saad\n', '\nMartin C. Rinard\n', '\nVikash K. Mansinghka\n']",,"Proceedings of the 42nd ACM SIGPLAN International Conference on
  Programming Language Design and Implementation (PLDI '21), June 20-25, 2021,
  Virtual, Canada. ACM, New York, NY, USA",http://dx.doi.org/10.1145/3453483.3454078,cs.PL,"['cs.PL', 'cs.LG', 'cs.SC', 'stat.CO', 'stat.ML']",10.1145/3453483.3454078,,[]
"Enhancing Linear Algebraic Computation of Logic Programs Using Sparse
  Representation",http://arxiv.org/abs/2009.10247v1,2020-09-22T00:50:05Z,2020-09-22T00:50:05Z,"  Algebraic characterization of logic programs has received increasing
attention in recent years. Researchers attempt to exploit connections between
linear algebraic computation and symbolic computation in order to perform
logical inference in large scale knowledge bases. This paper proposes further
improvement by using sparse matrices to embed logic programs in vector spaces.
We show its great power of computation in reaching the fixpoint of the
immediate consequence operator from the initial vector. In particular,
performance for computing the least models of definite programs is dramatically
improved in this way. We also apply the method to the computation of stable
models of normal programs, in which the guesses are associated with initial
matrices, and verify its effect when there are small numbers of negation. These
results show good enhancement in terms of performance for computing
consequences of programs and depict the potential power of tensorized logic
programs.
","['\nTuan Nguyen Quoc\nNational Institute of Informatics\n', '\nKatsumi Inoue\nNational Institute of Informatics\n', '\nChiaki Sakama\nWakayama University\n']","In Proceedings ICLP 2020, arXiv:2009.09158","EPTCS 325, 2020, pp. 192-205",http://dx.doi.org/10.4204/EPTCS.325.24,cs.LO,"['cs.LO', 'cs.SC']",10.4204/EPTCS.325.24,,"['National Institute of Informatics', 'National Institute of Informatics', 'Wakayama University']"
"KG-BART: Knowledge Graph-Augmented BART for Generative Commonsense
  Reasoning",http://arxiv.org/abs/2009.12677v2,2020-09-26T19:57:49Z,2021-01-21T06:02:59Z,"  Generative commonsense reasoning which aims to empower machines to generate
sentences with the capacity of reasoning over a set of concepts is a critical
bottleneck for text generation. Even the state-of-the-art pre-trained language
generation models struggle at this task and often produce implausible and
anomalous sentences. One reason is that they rarely consider incorporating the
knowledge graph which can provide rich relational information among the
commonsense concepts. To promote the ability of commonsense reasoning for text
generation, we propose a novel knowledge graph augmented pre-trained language
generation model KG-BART, which encompasses the complex relations of concepts
through the knowledge graph and produces more logical and natural sentences as
output. Moreover, KG-BART can leverage the graph attention to aggregate the
rich concept semantics that enhances the model generalization on unseen concept
sets. Experiments on benchmark CommonGen dataset verify the effectiveness of
our proposed approach by comparing with several strong pre-trained language
generation models, particularly KG-BART outperforms BART by 5.80, 4.60, in
terms of BLEU-3, 4. Moreover, we also show that the generated context by our
model can work as background scenarios to benefit downstream commonsense QA
tasks.
","['\nYe Liu\n', '\nYao Wan\n', '\nLifang He\n', '\nHao Peng\n', '\nPhilip S. Yu\n']","10 pages, 7 figures, Appear in AAAI 2021",,http://arxiv.org/abs/2009.12677v2,cs.CL,"['cs.CL', 'cs.SC']",,,[]
"Deriving Theorems in Implicational Linear Logic, Declaratively",http://arxiv.org/abs/2009.10241v1,2020-09-22T00:48:45Z,2020-09-22T00:48:45Z,"  The problem we want to solve is how to generate all theorems of a given size
in the implicational fragment of propositional intuitionistic linear logic. We
start by filtering for linearity the proof terms associated by our Prolog-based
theorem prover for Implicational Intuitionistic Logic. This works, but using
for each formula a PSPACE-complete algorithm limits it to very small formulas.
We take a few walks back and forth over the bridge between proof terms and
theorems, provided by the Curry-Howard isomorphism, and derive step-by-step an
efficient algorithm requiring a low polynomial effort per generated theorem.
The resulting Prolog program runs in O(N) space for terms of size N and
generates in a few hours 7,566,084,686 theorems in the implicational fragment
of Linear Intuitionistic Logic together with their proof terms in normal form.
As applications, we generate datasets for correctness and scalability testing
of linear logic theorem provers and training data for neural networks working
on theorem proving challenges. The results in the paper, organized as a
literate Prolog program, are fully replicable.
  Keywords: combinatorial generation of provable formulas of a given size,
intuitionistic and linear logic theorem provers, theorems of the implicational
fragment of propositional linear intuitionistic logic, Curry-Howard
isomorphism, efficient generation of linear lambda terms in normal form, Prolog
programs for lambda term generation and theorem proving.
","['\nPaul Tarau\nUniversity of North Texas\n', '\nValeria de Paiva\nTopos Institute\n']","In Proceedings ICLP 2020, arXiv:2009.09158","EPTCS 325, 2020, pp. 110-123",http://dx.doi.org/10.4204/EPTCS.325.18,cs.LO,"['cs.LO', 'cs.AI', 'cs.PL', 'cs.SC']",10.4204/EPTCS.325.18,,"['University of North Texas', 'Topos Institute']"
A Low-Level Index for Distributed Logic Programming,http://arxiv.org/abs/2009.10255v1,2020-09-22T00:52:15Z,2020-09-22T00:52:15Z,"  A distributed logic programming language with support for meta-programming
and stream processing offers a variety of interesting research problems, such
as: How can a versatile and stable data structure for the indexing of a large
number of expressions be implemented with simple low-level data structures? Can
low-level programming help to reduce the number of occur checks in Robinson's
unification algorithm? This article gives the answers.
","['\nThomas Prokosch\nInstitute for Informatics, Ludwig-Maximilian University of Munich, Germany\n']","In Proceedings ICLP 2020, arXiv:2009.09158","EPTCS 325, 2020, pp. 303-312",http://dx.doi.org/10.4204/EPTCS.325.40,cs.SC,"['cs.SC', 'cs.DS', 'cs.IR']",10.4204/EPTCS.325.40,,"['Institute for Informatics, Ludwig-Maximilian University of Munich, Germany']"
"Guessing Gr{ö}bner Bases of Structured Ideals of Relations of
  Sequences",http://arxiv.org/abs/2009.05248v2,2020-09-11T06:23:15Z,2021-11-18T15:09:39Z,"  Assuming sufficiently many terms of a n-dimensional table defined over a
field are given, we aim at guessing the linear recurrence relations with either
constant or polynomial coefficients they satisfy. In many applications, the
table terms come along with a structure: for instance, they may be zero outside
of a cone, they may be built from a Gr{\""o}bner basis of an ideal invariant
under the action of a finite group. Thus, we show how to take advantage of this
structure to both reduce the number of table queries and the number of
operations in the base field to recover the ideal of relations of the table. In
applications like in combinatorics, where all these zero terms make us guess
many fake relations, this allows us to drastically reduce these wrong guesses.
These algorithms have been implemented and, experimentally, they let us handle
examples that we could not manage otherwise. Furthermore, we show which kind of
cone and lattice structures are preserved by skew polynomial multiplication.
This allows us to speed up the guessing of linear recurrence relations with
polynomial coefficients by computing sparse Gr{\""o}bner bases or Gr{\""o}bner
bases of an ideal invariant under the action of a finite group in a ring of
skew polynomials.
","['\nJérémy Berthomieu\nPolSys\n', '\nMohab Safey El Din\nPolSys\n']","Journal of Symbolic Computation, Elsevier, In press",,http://arxiv.org/abs/2009.05248v2,cs.SC,['cs.SC'],,,"['PolSys', 'PolSys']"
Modeling Hierarchical System with Operads,http://arxiv.org/abs/2009.09848v1,2020-09-15T02:14:02Z,2020-09-15T02:14:02Z,"  This paper applies operads and functorial semantics to address the problem of
failure diagnosis in complex systems. We start with a concrete example,
developing a hierarchical interaction model for the Length Scale
Interferometer, a high-precision measurement system operated by the US National
Institute of Standards and Technology. The model is expressed in terms of
combinatorial/diagrammatic structures called port-graphs, and we explain how to
extract an operad LSI from a collection of these diagrams. Next we show how
functors to the operad of probabilities organize and constrain the relative
probabilities of component failure in the system. Finally, we show how to
extend the analysis from general component failure to specific failure modes.
","['\nSpencer Breiner\nNIST\n', '\nBlake Pollard\nNIST\n', '\nEswaran Subrahmanian\nCMU\n', '\nOlivier Marie-Rose\nPrometheus Computing\n']","In Proceedings ACT 2019, arXiv:2009.06334","EPTCS 323, 2020, pp. 72-83",http://dx.doi.org/10.4204/EPTCS.323.5,cs.LO,"['cs.LO', 'cs.SC']",10.4204/EPTCS.323.5,,"['NIST', 'NIST', 'CMU', 'Prometheus Computing']"
PolyAdd: Polynomial Formal Verification of Adder Circuits,http://arxiv.org/abs/2009.03242v3,2020-09-07T17:10:21Z,2021-04-03T10:20:50Z,"  Only by formal verification approaches functional correctness can be ensured.
While for many circuits fast verification is possible, in other cases the
approaches fail. In general no efficient algorithms can be given, since the
underlying verification problem is NP-complete. In this paper we prove that for
different types of adder circuits polynomial verification can be ensured based
on BDDs. While it is known that the output functions for addition are
polynomially bounded, we show in the following that the entire construction
process can be carried out in polynomial time. This is shown for the simple
Ripple Carry Adder, but also for fast adders like the Conditional Sum Adder and
the Carry Look Ahead Adder. Properties about the adder function are proven and
the core principle of polynomial verification is described that can also be
extended to other classes of functions and circuit realizations.
",['\nRolf Drechsler\n'],"7 pages, 8 figures, published at 24th International Symposium on
  Design and Diagnostics of Electronic Circuits and Systems (DDECS), 2021",,http://arxiv.org/abs/2009.03242v3,cs.AR,"['cs.AR', 'cs.DS', 'cs.SC', '68W30, 68M07, 68W35', 'B.6.3; B.2.1; F.2.2']",,,[]
"Characterizing Positively Invariant Sets: Inductive and Topological
  Methods",http://arxiv.org/abs/2009.09797v2,2020-09-08T09:02:20Z,2021-04-19T10:29:12Z,"  We present two characterizations of positive invariance of sets under the
flow of systems of ordinary differential equations. The first characterization
uses inward sets which intuitively collect those points from which the flow
evolves within the set for a short period of time, whereas the second
characterization uses the notion of exit sets, which intuitively collect those
points from which the flow immediately leaves the set. Our proofs emphasize the
use of the real induction principle as a generic and unifying proof technique
that captures the essence of the formal reasoning justifying our results and
provides cleaner alternative proofs of known results. The two characterizations
presented in this article, while essentially equivalent, lead to two rather
different decision procedures (termed respectively LZZ and ESE) for checking
whether a given semi-algebraic set is positively invariant under the flow of a
system of polynomial ordinary differential equations. The procedure LZZ
improves upon the original work by Liu, Zhan and Zhao (EMSOFT 2011). The
procedure ESE, introduced in this article, works by splitting the problem, in a
principled way, into simpler sub-problems that are easier to check, and is
shown to exhibit substantially better performance compared to LZZ on problems
featuring semi-algebraic sets described by formulas with non-trivial Boolean
structure.
","['\nKhalil Ghorbal\n', '\nAndrew Sogokon\n']",,,http://dx.doi.org/10.1016/j.jsc.2022.01.004,cs.CG,"['cs.CG', 'cs.LO', 'cs.SC', 'F.2.2; I.1.2']",10.1016/j.jsc.2022.01.004,,[]
"Robots, computer algebra and eight connected components",http://arxiv.org/abs/2008.13392v1,2020-08-31T06:57:26Z,2020-08-31T06:57:26Z,"  Answering connectivity queries in semi-algebraic sets is a long-standing and
challenging computational issue with applications in robotics, in particular
for the analysis of kinematic singularities. One task there is to compute the
number of connected components of the complementary of the singularities of the
kinematic map. Another task is to design a continuous path joining two given
points lying in the same connected component of such a set. In this paper, we
push forward the current capabilities of computer algebra to obtain
computer-aided proofs of the analysis of the kinematic singularities of various
robots used in industry. We first show how to combine mathematical reasoning
with easy symbolic computations to study the kinematic singularities of an
infinite family (depending on paramaters) modelled by the UR-series produced by
the company ``Universal Robots''. Next, we compute roadmaps (which are curves
used to answer connectivity queries) for this family of robots. We design an
algorithm for ``solving'' positive dimensional polynomial system depending on
parameters. The meaning of solving here means partitioning the parameter's
space into semi-algebraic components over which the number of connected
components of the semi-algebraic set defined by the input system is invariant.
Practical experiments confirm our computer-aided proof and show that such an
algorithm can already be used to analyze the kinematic singularities of the
UR-series family. The number of connected components of the complementary of
the kinematic singularities of generic robots in this family is $8$.
","['\nJose Capco\nJKU\n', '\nMohab Safey El Din\nPolSys\n', '\nJosef Schicho\nRISC\n']",,"ISSAC '20: International Symposium on Symbolic and Algebraic
  Computation, Jul 2020, Kalamata Greece, France. pp.62-69",http://dx.doi.org/10.1145/3373207.3404048,cs.SC,['cs.SC'],10.1145/3373207.3404048,,"['JKU', 'PolSys', 'RISC']"
"Homotopy techniques for solving sparse column support determinantal
  polynomial systems",http://arxiv.org/abs/2009.00844v1,2020-09-02T06:50:46Z,2020-09-02T06:50:46Z,"  Let $\mathbf{K}$ be a field of characteristic zero with
$\overline{\mathbf{K}}$ its algebraic closure. Given a sequence of polynomials
$\mathbf{g} = (g_1, \ldots, g_s) \in \mathbf{K}[x_1, \ldots , x_n]^s$ and a
polynomial matrix $\mathbf{F} = [f_{i,j}] \in \mathbf{K}[x_1, \ldots, x_n]^{p
\times q}$, with $p \leq q$, we are interested in determining the isolated
points of $V_p(\mathbf{F},\mathbf{g})$, the algebraic set of points in
$\overline{\mathbf{K}}$ at which all polynomials in $\mathbf{g}$ and all
$p$-minors of $\mathbf{F}$ vanish, under the assumption $n = q - p + s + 1$.
Such polynomial systems arise in a variety of applications including for
example polynomial optimization and computational geometry. We design a
randomized sparse homotopy algorithm for computing the isolated points in
$V_p(\mathbf{F},\mathbf{g})$ which takes advantage of the determinantal
structure of the system defining $V_p(\mathbf{F}, \mathbf{g})$. Its complexity
is polynomial in the maximum number of isolated solutions to such systems
sharing the same sparsity pattern and in some combinatorial quantities attached
to the structure of such systems. It is the first algorithm which takes
advantage both on the determinantal structure and sparsity of input
polynomials. We also derive complexity bounds for the particular but important
case where $\mathbf{g}$ and the columns of $\mathbf{F}$ satisfy weighted degree
constraints. Such systems arise naturally in the computation of critical points
of maps restricted to algebraic sets when both are invariant by the action of
the symmetric group.
","['\nGeorge Labahn\nSCG\n', '\nMohab Safey El Din\nPolSys\n', '\nÉric Schost\nSCG\n', '\nThi Xuan Vu\nPolSys, SCG\n']",,,http://arxiv.org/abs/2009.00844v1,cs.SC,['cs.SC'],,,"['SCG', 'PolSys', 'SCG', 'PolSys, SCG']"
Computing critical points for invariant algebraic systems,http://arxiv.org/abs/2009.00847v1,2020-09-02T06:52:49Z,2020-09-02T06:52:49Z,"  Let $\mathbf{K}$ be a field and $\phi$, $\mathbf{f} = (f_1, \ldots, f_s)$ in
$\mathbf{K}[x_1, \dots, x_n]$ be multivariate polynomials (with $s < n$)
invariant under the action of $\mathcal{S}_n$, the group of permutations of
$\{1, \dots, n\}$. We consider the problem of computing the points at which
$\mathbf{f}$ vanish and the Jacobian matrix associated to $\mathbf{f}, \phi$ is
rank deficient provided that this set is finite. We exploit the invariance
properties of the input to split the solution space according to the orbits of
$\mathcal{S}_n$. This allows us to design an algorithm which gives a triangular
description of the solution space and which runs in time polynomial in $d^s$,
${{n+d}\choose{d}}$ and $\binom{n}{s+1}$ where $d$ is the maximum degree of the
input polynomials. When $d,s$ are fixed, this is polynomial in $n$ while when
$s$ is fixed and $d \simeq n$ this yields an exponential speed-up with respect
to the usual polynomial system solving algorithms.
","['\nJean-Charles Faugère\nPolSys\n', '\nGeorge Labahn\nSCG\n', '\nMohab Safey El Din\nPolSys\n', '\nÉric Schost\nSCG\n', '\nThi Xuan Vu\nPolSys, SCG\n']",,,http://arxiv.org/abs/2009.00847v1,cs.SC,['cs.SC'],,,"['PolSys', 'SCG', 'PolSys', 'SCG', 'PolSys, SCG']"
On FGLM Algorithms with Tropical Gröbner bases,http://arxiv.org/abs/2009.02067v1,2020-09-04T08:38:34Z,2020-09-04T08:38:34Z,"  Let K be a field equipped with a valuation. Tropical varieties over K can be
defined with a theory of Gr{\""o}bner bases taking into account the valuation of
K. Because of the use of the valuation, the theory of tropical Gr{\""o}bner
bases has proved to provide settings for computations over polynomial rings
over a p-adic field that are more stable than that of classical Gr{\""o}bner
bases. In this article, we investigate how the FGLM change of ordering
algorithm can be adapted to the tropical setting. As the valuations of the
polynomial coefficients are taken into account, the classical FGLM algorithm's
incremental way, monomo-mial by monomial, to compute the multiplication
matrices and the change of basis matrix can not be transposed at all to the
tropical setting. We mitigate this issue by developing new linear algebra
algorithms and apply them to our new tropical FGLM algorithms. Motivations are
twofold. Firstly, to compute tropical varieties, one usually goes through the
computation of many tropical Gr{\""o}bner bases defined for varying weights (and
then varying term orders). For an ideal of dimension 0, the tropical FGLM
algorithm provides an efficient way to go from a tropical Gr{\""o}bner basis
from one weight to one for another weight. Secondly, the FGLM strategy can be
applied to go from a tropical Gr{\""o}bner basis to a classical Gr{\""o}bner
basis. We provide tools to chain the stable computation of a tropical
Gr{\""o}bner basis (for weight [0,. .. , 0]) with the p-adic stabilized variants
of FGLM of [RV16] to compute a lexicographical or shape position basis. All our
algorithms have been implemented into SageMath. We provide numerical examples
to illustrate time-complexity. We then illustrate the superiority of our
strategy regarding to the stability of p-adic numerical computations.
","['\nYuki Ishihara\nXLIM\n', '\nTristan Vaccon\nXLIM\n', '\nKazuhiro Yokoyama\n']",,"ISSAC '20: International Symposium on Symbolic and Algebraic
  Computation, Jul 2020, Kalamata Greece, France. pp.257-264",http://dx.doi.org/10.1145/3373207.3404037,cs.SC,"['cs.SC', 'math.AC']",10.1145/3373207.3404037,,"['XLIM', 'XLIM']"
On a non-archimedean broyden method,http://arxiv.org/abs/2009.01511v1,2020-09-03T08:28:04Z,2020-09-03T08:28:04Z,"  Newton's method is an ubiquitous tool to solve equations, both in the
archimedean and non-archimedean settings -- for which it does not really
differ. Broyden was the instigator of what is called ""quasi-Newton methods"".
These methods use an iteration step where one does not need to compute a
complete Jacobian matrix nor its inverse. We provide an adaptation of Broyden's
method in a general non-archimedean setting, compatible with the lack of inner
product, and study its Q and R convergence. We prove that our adapted method
converges at least Q-linearly and R-superlinearly with R-order
$2^{\frac{1}{2m}}$ in dimension m. Numerical data are provided.
","['\nXavier Dahan\nXLIM\n', '\nTristan Vaccon\nXLIM\n']",,"ISSAC '20: International Symposium on Symbolic and Algebraic
  Computation, Jul 2020, Kalamata Greece, France. pp.114-121",http://dx.doi.org/10.1145/3373207.3404045,cs.SC,"['cs.SC', 'cs.NA', 'math.NA', 'math.NT']",10.1145/3373207.3404045,,"['XLIM', 'XLIM']"
"Strong Consistency and Thomas Decomposition of Finite Difference
  Approximations to Systems of Partial Differential Equations",http://arxiv.org/abs/2009.01731v1,2020-09-03T15:08:51Z,2020-09-03T15:08:51Z,"  For a wide class of polynomially nonlinear systems of partial differential
equations we suggest an algorithmic approach that combines differential and
difference algebra to analyze s(trong)-consistency of finite difference
approximations. Our approach is applicable to regular solution grids. For the
grids of this type we give a new definition of s-consistency for finite
difference approximations which generalizes our definition given earlier for
Cartesian grids. The algorithmic verification of s-consistency presented in the
paper is based on the use of both differential and difference Thomas
decomposition. First, we apply the differential decomposition to the input
system, resulting in a partition of its solution space. Then, to the output
subsystem that contains a solution of interest we apply a difference analogue
of the differential Thomas decomposition which allows to check the
s-consistency. For linear and some quasi-linear differential systems one can
also apply difference \Gr bases for the s-consistency analysis. We illustrate
our methods and algorithms by a number of examples, which include Navier-Stokes
equations for viscous incompressible flow.
","['\nVladimir P. Gerdt\n', '\nDaniel Robertz\n', '\nYuri A. Blinkov\n']","47 pages, 8 figures",,http://arxiv.org/abs/2009.01731v1,cs.SC,"['cs.SC', 'cs.NA', 'math.AP', 'math.NA', 'math.RA', 'physics.flu-dyn', '12H05, 12H10', 'F.2.1; I.1.4']",,,[]
"A Simple and Fast Algorithm for Computing the $N$-th Term of a Linearly
  Recurrent Sequence",http://arxiv.org/abs/2008.08822v1,2020-08-20T07:54:01Z,2020-08-20T07:54:01Z,"  We present a simple and fast algorithm for computing the $N$-th term of a
given linearly recurrent sequence. Our new algorithm uses $O(\mathsf{M}(d) \log
N)$ arithmetic operations, where $d$ is the order of the recurrence, and
$\mathsf{M}(d)$ denotes the number of arithmetic operations for computing the
product of two polynomials of degree $d$. The state-of-the-art algorithm, due
to Charles Fiduccia (1985), has the same arithmetic complexity up to a constant
factor. Our algorithm is simpler, faster and obtained by a totally different
method. We also discuss several algorithmic applications, notably to polynomial
modular exponentiation, powering of matrices and high-order lifting.
","['\nAlin Bostan\n', '\nRyuhei Mori\n']",34 pages,,http://arxiv.org/abs/2008.08822v1,cs.SC,['cs.SC'],,,[]
Computing the Real Isolated Points of an Algebraic Hypersurface,http://arxiv.org/abs/2008.10331v1,2020-08-24T11:44:56Z,2020-08-24T11:44:56Z,"  Let $\mathbb{R}$ be the field of real numbers. We consider the problem of
computing the real isolated points of a real algebraic set in $\mathbb{R}^n$
given as the vanishing set of a polynomial system. This problem plays an
important role for studying rigidity properties of mechanism in material
designs. In this paper, we design an algorithm which solves this problem. It is
based on the computations of critical points as well as roadmaps for answering
connectivity queries in real algebraic sets. This leads to a probabilistic
algorithm of complexity $(nd)^{O(n\log(n))}$ for computing the real isolated
points of real algebraic hypersurfaces of degree $d$. It allows us to solve in
practice instances which are out of reach of the state-of-the-art.
","['\nHuu Phuoc Le\n', '\nMohab Safey El Din\n', '\nTimo de Wolff\n']",Conference paper ISSAC 2020,,http://dx.doi.org/10.1145/3373207.3404049,cs.CG,"['cs.CG', 'cs.SC']",10.1145/3373207.3404049,,[]
Computing singular elements modulo squares,http://arxiv.org/abs/2008.10335v1,2020-08-24T11:50:03Z,2020-08-24T11:50:03Z,"  The group of singular elements was first introduced by Helmut Hasse and later
it has been studied by numerous authors including such well known
mathematicians as: Cassels, Furtw\""{a}ngler, Hecke, Knebusch, Takagi and of
course Hasse himself; to name just a few. The aim of the present paper is to
present algorithms that explicitly construct groups of singular and
$S$-singular elements (modulo squares) in a global function field.
",['\nPrzemysław Koprowski\n'],,,http://arxiv.org/abs/2008.10335v1,math.NT,"['math.NT', 'cs.SC', '68W30, 11Y40', 'I.1.2']",,,[]
Towards a noncommutative Picard-Vessiot theory,http://arxiv.org/abs/2008.10872v5,2020-08-25T08:06:48Z,2022-09-18T16:26:42Z,"  A Chen generating series, along a path and with respect to $m$ differential
forms,is a noncommutative series on $m$ letters and with coefficients which are
holomorphic functionsover a simply connected manifold in other words a series
with variable (holomorphic) coefficients.Such a series satisfies a first order
noncommutative differential equation which is considered, bysome authors, as
the universal differential equation, (i.e.) universality can beseen by
replacing each letter by constant matrices (resp. analytic vector fields)and
then solving a system of linear (resp. nonlinear) differential equations.Via
rational series, on noncommutative indeterminates and with coefficients in
rings, andtheir non-trivial combinatorial Hopf algebras, we give the first step
of a noncommutativePicard-Vessiot theory and we illustrate it with the case of
linear differential equationswith singular regular singularities thanks to the
universal equation previously mentioned.
","['\nG. Duchamp\nLIPN\n', '\nViincel Hoang Ngoc Minh\n', '\nVu Nguyen Dinh\n', '\nPierre Simonnet\n']",,,http://arxiv.org/abs/2008.10872v5,math.AG,"['math.AG', 'cs.SC']",,,['LIPN']
Exact $p$-adic computation in Magma,http://arxiv.org/abs/2008.11063v1,2020-08-24T11:01:01Z,2020-08-24T11:01:01Z,"  We describe a new arithmetic system for the Magma computer algebra system for
working with $p$-adic numbers exactly, in the sense that numbers are
represented lazily to infinite $p$-adic precision. This is the first highly
featured such implementation. This has the benefits of increasing
user-friendliness and speeding up some computations, as well as forcibly
producing provable results. We give theoretical and practical justification for
its design and describe some use cases. The intention is that this article will
be of benefit to anyone wanting to implement similar functionality in other
languages.
",['\nChristopher Doris\n'],"22 pages, 2 figures. arXiv admin note: text overlap with
  arXiv:1805.09794",,http://dx.doi.org/10.1016/j.jsc.2020.08.005,math.NT,"['math.NT', 'cs.SC']",10.1016/j.jsc.2020.08.005,,[]
Competition Report: CHC-COMP-20,http://arxiv.org/abs/2008.02939v1,2020-08-07T01:24:57Z,2020-08-07T01:24:57Z,"  CHC-COMP-20 is the third competition of solvers for Constrained Horn Clauses.
In this year, 9 solvers participated at the competition, and were evaluated in
four separate tracks on problems in linear integer arithmetic, linear real
arithmetic, and arrays. The competition was run in the first week of May 2020
using the StarExec computing cluster. This report gives an overview of the
competition design, explains the organisation of the competition, and presents
the competition results.
","['\nPhilipp Rümmer\nUppsala University, Sweden\n']","In Proceedings VPT/HCVS 2020, arXiv:2008.02483","EPTCS 320, 2020, pp. 197-219",http://dx.doi.org/10.4204/EPTCS.320.15,cs.LO,"['cs.LO', 'cs.SC', 'F.3.1']",10.4204/EPTCS.320.15,,"['Uppsala University, Sweden']"
"Proceedings 8th International Workshop on Verification and Program
  Transformation and 7th Workshop on Horn Clauses for Verification and
  Synthesis",http://arxiv.org/abs/2008.02483v1,2020-08-06T07:11:26Z,2020-08-06T07:11:26Z,"  The proceedings consist of a keynote paper by Alberto followed by 6 invited
papers written by Lorenzo Clemente (U. Warsaw), Alain Finkel (U. Paris-Saclay),
John Gallagher (Roskilde U. and IMDEA Software Institute) et al., Neil Jones
(U. Copenhagen) et al., Michael Leuschel (Heinrich-Heine U.) and Maurizio
Proietti (IASI-CNR) et al.. These invited papers are followed by 4 regular
papers accepted at VPT 2020 and the papers of HCVS 2020 which consist of three
contributed papers and an invited paper on the third competition of solvers for
Constrained Horn Clauses.
  In addition, the abstracts (in HTML format) of 3 invited talks at VPT 2020 by
Andrzej Skowron (U. Warsaw), Sophie Renault (EPO) and Moa Johansson (Chalmers
U.), are included.
","['\nLaurent Fribourg\nCNRS & ENS Paris-Saclay, France\n', '\nMatthias Heizmann\nUniversity of Freiburg, Germany\n']",,"EPTCS 320, 2020",http://dx.doi.org/10.4204/EPTCS.320,cs.LO,"['cs.LO', 'cs.PL', 'cs.SC']",10.4204/EPTCS.320,,"['CNRS & ENS Paris-Saclay, France', 'University of Freiburg, Germany']"
"From Big-Step to Small-Step Semantics and Back with Interpreter
  Specialisation",http://arxiv.org/abs/2008.02931v1,2020-08-07T01:23:04Z,2020-08-07T01:23:04Z,"  We investigate representations of imperative programs as constrained Horn
clauses. Starting from operational semantics transition rules, we proceed by
writing interpreters as constrained Horn clause programs directly encoding the
rules. We then specialise an interpreter with respect to a given source program
to achieve a compilation of the source language to Horn clauses (an instance of
the first Futamura projection). The process is described in detail for an
interpreter for a subset of C, directly encoding the rules of big-step
operational semantics for C. A similar translation based on small-step
semantics could be carried out, but we show an approach to obtaining a
small-step representation using a linear interpreter for big-step Horn clauses.
This interpreter is again specialised to achieve the translation from big-step
to small-step style. The linear small-step program can be transformed back to a
big-step non-linear program using a third interpreter. A regular path
expression is computed for the linear program using Tarjan's algorithm, and
this regular expression then guides an interpreter to compute a program path.
The transformation is realised by specialisation of the path interpreter. In
all of the transformation phases, we use an established partial evaluator and
exploit standard logic program transformation to remove redundant data
structures and arguments in predicates and rename predicates to make clear
their link to statements in the original source program.
","['\nJohn P. Gallagher\nRoskilde University, Denmark and IMDEA Software Institute, Spain\n', '\nManuel Hermenegildo\nIMDEA Software Institute, Spain\n', '\nBishoksan Kafle\nIMDEA Software Institute, Spain\n', '\nMaximiliano Klemen\nIMDEA Software Institute, Spain\n', '\nPedro López García\nIMDEA Software Institute, Spain\n', '\nJosé Morales\nIMDEA Software Institute, Spain\n']","In Proceedings VPT/HCVS 2020, arXiv:2008.02483","EPTCS 320, 2020, pp. 50-64",http://dx.doi.org/10.4204/EPTCS.320.4,cs.PL,"['cs.PL', 'cs.LO', 'cs.SC']",10.4204/EPTCS.320.4,,"['Roskilde University, Denmark and IMDEA Software Institute, Spain', 'IMDEA Software Institute, Spain', 'IMDEA Software Institute, Spain', 'IMDEA Software Institute, Spain', 'IMDEA Software Institute, Spain', 'IMDEA Software Institute, Spain']"
An Experiment Combining Specialization with Abstract Interpretation,http://arxiv.org/abs/2008.02937v1,2020-08-07T01:24:31Z,2020-08-07T01:24:31Z,"  It was previously shown that control-flow refinement can be achieved by a
program specializer incorporating property-based abstraction, to improve
termination and complexity analysis tools. We now show that this purpose-built
specializer can be reconstructed in a more modular way, and that the previous
results can be achieved using an off-the-shelf partial evaluation tool, applied
to an abstract interpreter. The key feature of the abstract interpreter is the
abstract domain, which is the product of the property-based abstract domain
with the concrete domain. This language-independent framework provides a
practical approach to implementing a variety of powerful specializers, and
contributes to a stream of research on using interpreters and specialization to
achieve program transformations.
","['\nJohn P. Gallagher\nRoskilde University, Denmark and IMDEA Software Institute, Spain\n', '\nRobert Glück\nCopenhagen University, Denmark\n']","In Proceedings VPT/HCVS 2020, arXiv:2008.02483","EPTCS 320, 2020, pp. 155-158",http://dx.doi.org/10.4204/EPTCS.320.11,cs.PL,"['cs.PL', 'cs.LO', 'cs.SC']",10.4204/EPTCS.320.11,,"['Roskilde University, Denmark and IMDEA Software Institute, Spain', 'Copenhagen University, Denmark']"
Cyclotomic Identity Testing and Applications,http://arxiv.org/abs/2007.13179v2,2020-07-26T17:02:30Z,2021-05-04T15:51:53Z,"  We consider the cyclotomic identity testing (CIT) problem: given a polynomial
$f(x_1,\ldots,x_k)$, decide whether $f(\zeta_n^{e_1},\ldots,\zeta_n^{e_k})$ is
zero, where $\zeta_n = e^{2\pi i/n}$ is a primitive complex $n$-th root of
unity and $e_1,\ldots,e_k$ are integers, represented in binary. When $f$ is
given by an algebraic circuit, we give a randomized polynomial-time algorithm
for CIT assuming the generalised Riemann hypothesis (GRH), and show that the
problem is in coNP unconditionally. When $f$ is given by a circuit of
polynomially bounded degree, we give a randomized NC algorithm. In case $f$ is
a linear form we show that the problem lies in NC. Towards understanding when
CIT can be solved in deterministic polynomial-time, we consider so-called
diagonal depth-3 circuits, i.e., polynomials $f=\sum_{i=1}^m g_i^{d_i}$, where
$g_i$ is a linear form and $d_i$ a positive integer given in unary. We observe
that a polynomial-time algorithm for CIT on this class would yield a
sub-exponential-time algorithm for polynomial identity testing. However,
assuming GRH, we show that if the linear forms~$g_i$ are all identical then CIT
can be solved in polynomial time. Finally, we use our results to give a new
proof that equality of compressed strings, i.e., strings presented using
context-free grammars, can be decided in randomized NC.
","['\nNikhil Balaji\n', '\nSylvain Perifel\n', '\nMahsa Shirmohammadi\n', '\nJames Worrell\n']",,,http://arxiv.org/abs/2007.13179v2,cs.CC,"['cs.CC', 'cs.SC']",,,[]
On Bergman's Diamond Lemma for Ring Theory,http://arxiv.org/abs/2007.13845v2,2020-07-27T20:21:25Z,2023-09-21T09:02:03Z,"  This expository and review paper deals with the Diamond Lemma for ring
theory, which is proved in the first section of G. M. Bergman, The Diamond
Lemma for Ring Theory, Advances in Mathematics, 29 (1978), pp. 178-218. No
originality of the present note is claimed on the part of the author, except
for some suggestions and figures. Throughout this paper, I shall mostly use
Bergman's expressions in his paper. In Remarks and Notes, the reader will find
some useful information on this topic.
",['\nTakao Inoué\n'],15 pages,,http://arxiv.org/abs/2007.13845v2,math.RT,"['math.RT', 'cs.SC', '16S15, 16-02']",,,[]
Formal Power Series on Algebraic Cryptanalysis,http://arxiv.org/abs/2007.14729v2,2020-07-29T10:36:20Z,2022-04-10T10:06:24Z,"  In cryptography, by attacks reducing a cryptosystem to the problem that
solves the system of polynomial equations, several cryptosystems have been
broken. The efficiency of an algorithm to solve the polynomial system depends
on the solving degree, and its complexity is estimated using a theoretical
proxy for the solving degree. If a polynomial system generated by an attack is
semi-regular, the degree of regularity determined from a certain univariate
formal power series is used as such a proxy, otherwise, the first fall degree
is used. In this article, we mainly investigate an upper bound for the proxy
defined using non-constructive syzygies such as the first fall degree. In
particular, for a sufficiently large field, we prove that the first fall degree
of a non-semi-regular system is bounded above by the degree of regularity of
the semi-regular system, and that the first fall degree of a multi-graded
polynomial system is bounded above by a certain value determined from a
multivariate formal power series. Moreover, we show that the assumption for the
order of a coefficient field in our results is satisfied in actual attacks
against multivariate cryptosystems. Furthermore, we provide the theoretical
assumption for the algorithm with a kernel search to solve a multi-graded
polynomial system and compute a certain proxy for the solving degree by the
multivariate formal power series. Consequently, we clarify the relationship
between the first fall degree and the degree of regularity over a sufficiently
large field, and provide a theoretical method using a multivariate power series
for cryptanalysis.
",['\nShuhei Nakamura\n'],,,http://arxiv.org/abs/2007.14729v2,cs.SC,"['cs.SC', 'cs.CR']",,,[]
Parameter identifiability and input-output equations,http://arxiv.org/abs/2007.14787v2,2020-07-28T01:16:51Z,2020-12-27T15:06:35Z,"  Structural parameter identifiability is a property of a differential model
with parameters that allows for the parameters to be determined from the model
equations in the absence of noise. One of the standard approaches to assessing
this problem is via input-output equations and, in particular, characteristic
sets of differential ideals. The precise relation between identifiability and
input-output identifiability is subtle. The goal of this note is to clarify
this relation. The main results are:
  1) identifiability implies input-output identifiability;
  2) these notions coincide if the model does not have rational first
integrals;
  3) the field of input-output identifiable functions is generated by the
coefficients of a ""minimal"" characteristic set of the corresponding
differential ideal.
  We expect that some of these facts may be known to the experts in the area,
but we are not aware of any articles in which these facts are stated precisely
and rigorously proved.
","['\nAlexey Ovchinnikov\n', '\nGleb Pogudin\n', '\nPeter Thompson\n']",arXiv admin note: substantial text overlap with arXiv:1910.03960,,http://arxiv.org/abs/2007.14787v2,math.AG,"['math.AG', 'cs.SC', 'cs.SY', 'eess.SY', 'math.DS']",,,[]
"On the Complexity of Quadratization for Polynomial Differential
  Equations",http://arxiv.org/abs/2007.08910v2,2020-07-17T11:39:09Z,2020-07-27T13:46:35Z,"  Chemical reaction networks (CRNs) are a standard formalism used in chemistry
and biology to reason about the dynamics of molecular interaction networks. In
their interpretation by ordinary differential equations, CRNs provide a
Turing-complete model of analog computattion, in the sense that any computable
function over the reals can be computed by a finite number of molecular species
with a continuous CRN which approximates the result of that function in one of
its components in arbitrary precision. The proof of that result is based on a
previous result of Bournez et al. on the Turing-completeness of polyno-mial
ordinary differential equations with polynomial initial conditions (PIVP). It
uses an encoding of real variables by two non-negative variables for
concentrations, and a transformation to an equivalent quadratic PIVP (i.e. with
degrees at most 2) for restricting ourselves to at most bimolecular reactions.
In this paper, we study the theoretical and practical complexities of the
quadratic transformation. We show that both problems of minimizing either the
number of variables (i.e., molecular species) or the number of monomials (i.e.
elementary reactions) in a quadratic transformation of a PIVP are NP-hard. We
present an encoding of those problems in MAX-SAT and show the practical
complexity of this algorithm on a benchmark of quadratization problems inspired
from CRN design problems.
","['\nMathieu Hemery\nLifeware\n', '\nFrançois Fages\nLifeware\n', '\nSylvain Soliman\nLifeware\n']",,"CMSB 2020: The 18th International Conference on Computational
  Methods in Systems Biology, Sep 2020, Konstanz, Germany",http://arxiv.org/abs/2007.08910v2,q-bio.QM,"['q-bio.QM', 'cs.SC']",,,"['Lifeware', 'Lifeware', 'Lifeware']"
"On Algorithmic Estimation of Analytic Complexity for Polynomial
  Solutions to Hypergeometric Systems",http://arxiv.org/abs/2007.09407v1,2020-07-18T11:16:28Z,2020-07-18T11:16:28Z,"  The paper deals with the analytic complexity of solutions to bivariate
holonomic hypergeometric systems of the Horn type. We obtain estimates on the
analytic complexity of Puiseux polynomial solutions to the hypergeometric
systems defined by zonotopes. We also propose algorithms of the analytic
complexity estimation for polynomials.
",['\nVitaly A. Krasikov\n'],,,http://arxiv.org/abs/2007.09407v1,cs.SC,"['cs.SC', 'math.AP']",,,[]
"Computing Regular Meromorphic Differential Forms via Saito's Logarithmic
  Residues",http://arxiv.org/abs/2007.09950v4,2020-07-20T08:57:00Z,2021-02-28T04:01:45Z,"  Logarithmic differential forms and logarithmic vector fields associated to a
hypersurface with an isolated singularity are considered in the context of
computational complex analysis. As applications, based on the concept of
torsion differential forms due to A.G. Aleksandrov, regular meromorphic
differential forms introduced by D. Barlet and M. Kersken, and Brieskorn
formulae on Gauss-Manin connections are investigated. (i) A method is given to
describe singular parts of regular meromorphic differential forms in terms of
non-trivial logarithmic vector fields via Saito's logarithmic residues. The
resulting algorithm is illustrated by using examples. (ii) A new link between
Brieskorn formulae and logarithmic vector fields is discovered and an
expression that rewrites Brieskorn formulae in terms of non-trivial logarithmic
vector fields is presented. A new effective method is described to compute non
trivial logarithmic vector fields which are suitable for the computation of
Gauss-Manin connections. Some examples are given for illustration.
","['\nShinichi Tajima\n', '\nKatsusuke Nabeshima\n']",,"SIGMA 17 (2021), 019, 21 pages",http://dx.doi.org/10.3842/SIGMA.2021.019,math.AG,"['math.AG', 'cs.SC', '32S05, 32A27']",10.3842/SIGMA.2021.019,,[]
Computing stable resultant-based minimal solvers by hiding a variable,http://arxiv.org/abs/2007.10100v1,2020-07-17T07:40:10Z,2020-07-17T07:40:10Z,"  Many computer vision applications require robust and efficient estimation of
camera geometry. The robust estimation is usually based on solving camera
geometry problems from a minimal number of input data measurements, i.e.,
solving minimal problems, in a RANSAC-style framework. Minimal problems often
result in complex systems of polynomial equations. The existing
state-of-the-art methods for solving such systems are either based on Gr\""obner
bases and the action matrix method, which have been extensively studied and
optimized in the recent years or recently proposed approach based on a sparse
resultant computation using an extra variable.
  In this paper, we study an interesting alternative sparse resultant-based
method for solving sparse systems of polynomial equations by hiding one
variable. This approach results in a larger eigenvalue problem than the action
matrix and extra variable sparse resultant-based methods; however, it does not
need to compute an inverse or elimination of large matrices that may be
numerically unstable. The proposed approach includes several improvements to
the standard sparse resultant algorithms, which significantly improves the
efficiency and stability of the hidden variable resultant-based solvers as we
demonstrate on several interesting computer vision problems. We show that for
the studied problems, our sparse resultant based approach leads to more stable
solvers than the state-of-the-art Gr\""obner bases-based solvers as well as
existing sparse resultant-based solvers, especially in close to critical
configurations. Our new method can be fully automated and incorporated into
existing tools for the automatic generation of efficient minimal solvers.
","['\nSnehal Bhayani\n', '\nZuzana Kukelova\n', '\nJanne Heikkilä\n']",arXiv admin note: text overlap with arXiv:1912.10268,,http://arxiv.org/abs/2007.10100v1,cs.CV,"['cs.CV', 'cs.SC', 'I.4; I.1']",,,[]
"Explicit isomorphisms of quaternion algebras over quadratic global
  fields",http://arxiv.org/abs/2007.06981v5,2020-07-14T11:53:20Z,2022-03-29T18:12:41Z,"  Let $L$ be a separable quadratic extension of either $\mathbb{Q}$ or
$\mathbb{F}_q(t)$. We propose efficient algorithms for finding isomorphisms
between quaternion algebras over $L$. Our techniques are based on computing
maximal one-sided ideals of the corestriction of a central simple $L$-algebra.
","['\nTímea Csahók\n', '\nPéter Kutas\n', '\nMickaël Montessinos\n', '\nGergely Zábrádi\n']","22 pages, includes implementation, characteristic 2 results put in a
  separate paper arXiv:2203.04068",,http://arxiv.org/abs/2007.06981v5,math.NT,"['math.NT', 'cs.SC', 'math.RA']",,,[]
"Bit-Slicing the Hilbert Space: Scaling Up Accurate Quantum Circuit
  Simulation to a New Level",http://arxiv.org/abs/2007.09304v1,2020-07-18T01:26:40Z,2020-07-18T01:26:40Z,"  Quantum computing is greatly advanced in recent years and is expected to
transform the computation paradigm in the near future. Quantum circuit
simulation plays a key role in the toolchain for the development of quantum
hardware and software systems. However, due to the enormous Hilbert space of
quantum states, simulating quantum circuits with classical computers is
extremely challenging despite notable efforts have been made. In this paper, we
enhance quantum circuit simulation in two dimensions: accuracy and scalability.
The former is achieved by using an algebraic representation of complex numbers;
the latter is achieved by bit-slicing the number representation and replacing
matrix-vector multiplication with symbolic Boolean function manipulation.
Experimental results demonstrate that our method can be superior to the
state-of-the-art for various quantum circuits and can simulate certain
benchmark families with up to tens of thousands of qubits.
","['\nYuan-Hung Tsai\n', '\nJie-Hong R. Jiang\n', '\nChiao-Shan Jhang\n']","9 pages, 2 figures",,http://arxiv.org/abs/2007.09304v1,cs.ET,"['cs.ET', 'cs.SC', 'quant-ph']",,,[]
Groebner basis structure of ideal interpolation,http://arxiv.org/abs/2007.11830v2,2020-07-23T07:31:04Z,2024-01-15T02:30:20Z,"  We study the relationship between certain Groebner bases for zero dimensional
ideals, and the interpolation condition functionals of ideal interpolation.
Ideal interpolation is defined by a linear idempotent projector whose kernel is
a polynomial ideal. In this paper, we propose the notion of ""reverse"" complete
reduced basis. Based on the notion, we present a fast algorithm to compute the
reduced Groebner basis for the kernel of ideal projector under an arbitrary
compatible ordering. As an application, we show that knowing the affine variety
makes available information concerning the reduced Groebner basis.
","['\nYihe Gong\n', '\nXue Jiang\n']","""Computing Groebner bases of ideal interpolation""(arXiv:2111.07340)
  is the new version of this one",,http://arxiv.org/abs/2007.11830v2,cs.SC,"['cs.SC', 'cs.NA', 'math.NA']",,,[]
Graphical Conditions for Rate Independence in Chemical Reaction Networks,http://arxiv.org/abs/2007.15642v1,2020-07-17T11:41:05Z,2020-07-17T11:41:05Z,"  Chemical Reaction Networks (CRNs) provide a useful abstraction of molecular
interaction networks in which molecular structures as well as mass conservation
principles are abstracted away to focus on the main dynamical properties of the
network structure. In their interpretation by ordinary differential equations,
we say that a CRN with distinguished input and output species computes a
positive real function $f : R+ $\rightarrow$ R+$, if for any initial
concentration x of the input species, the concentration of the output molecular
species stabilizes at concentration f (x). The Turing-completeness of that
notion of chemical analog computation has been established by proving that any
computable real function can be computed by a CRN over a finite set of
molecular species. Rate-independent CRNs form a restricted class of CRNs of
high practical value since they enjoy a form of absolute robustness in the
sense that the result is completely independent of the reaction rates and
depends solely on the input concentrations. The functions computed by
rate-independent CRNs have been characterized mathematically as the set of
piecewise linear functions from input species. However, this does not provide a
mean to decide whether a given CRN is rate-independent. In this paper, we
provide graphical conditions on the Petri Net structure of a CRN which entail
the rate-independence property either for all species or for some output
species. We show that in the curated part of the Biomodels repository, among
the 590 reaction models tested, 2 reaction graphs were found to satisfy our
rate-independence conditions for all species, 94 for some output species, among
which 29 for some non-trivial output species. Our graphical conditions are
based on a non-standard use of the Petri net notions of place-invariants and
siphons which are computed by constraint programming techniques for efficiency
reasons.
","['\nElisabeth Degrand\nLifeware\n', '\nFrançois Fages\nLifeware\n', '\nSylvain Soliman\nLifeware\n']",,"Proceedings CMSB 2020: The 18th International Conference on
  Computational Methods in Systems Biology, Sep 2020, Konstanz, Germany",http://arxiv.org/abs/2007.15642v1,q-bio.MN,"['q-bio.MN', 'cs.SC', 'q-bio.QM']",,,"['Lifeware', 'Lifeware', 'Lifeware']"
Interpretable Control by Reinforcement Learning,http://arxiv.org/abs/2007.09964v1,2020-07-20T09:35:04Z,2020-07-20T09:35:04Z,"  In this paper, three recently introduced reinforcement learning (RL) methods
are used to generate human-interpretable policies for the cart-pole balancing
benchmark. The novel RL methods learn human-interpretable policies in the form
of compact fuzzy controllers and simple algebraic equations. The
representations as well as the achieved control performances are compared with
two classical controller design methods and three non-interpretable RL methods.
All eight methods utilize the same previously generated data batch and produce
their controller offline - without interaction with the real benchmark
dynamics. The experiments show that the novel RL methods are able to
automatically generate well-performing policies which are at the same time
human-interpretable. Furthermore, one of the methods is applied to
automatically learn an equation-based policy for a hardware cart-pole
demonstrator by using only human-player-generated batch data. The solution
generated in the first attempt already represents a successful balancing
policy, which demonstrates the methods applicability to real-world problems.
","['\nDaniel Hein\n', '\nSteffen Limmer\n', '\nThomas A. Runkler\n']",,,http://arxiv.org/abs/2007.09964v1,cs.LG,"['cs.LG', 'cs.AI', 'cs.RO', 'cs.SC', 'cs.SY', 'eess.SY']",,,[]
A Family of Denominator Bounds for First Order Linear Recurrence Systems,http://arxiv.org/abs/2007.02926v1,2020-07-06T17:56:05Z,2020-07-06T17:56:05Z,"  For linear recurrence systems, the problem of finding rational solutions is
reduced to the problem of computing polynomial solutions by computing a content
bound or a denominator bound. There are several bounds in the literature. The
sharpest bound leads to polynomial solutions of lower degrees, but this
advantage need not compensate for the time spent on computing that bound.
  To strike the best balance between sharpness of the bound versus CPU time
spent obtaining it, we will give a family of bounds. The $J$'th member of this
family is similar to (Abramov, Barkatou, 1998) when $J=1$, similar to (van
Hoeij, 1998) when $J$ is large, and novel for intermediate values of $J$, which
give the best balance between sharpness and CPU time.
  The setting for our content bounds are systems $\tau(Y) = MY$ where $\tau$ is
an automorphism of a UFD, and $M$ is an invertible matrix with entries in its
field of fractions. This setting includes the shift case, the $q$-shift case,
the multi-basic case and others. We give two versions, a global version, and a
version that bounds each entry separately.
","['\nMark van Hoeij\n', '\nMoulay Barkatou\n', '\nJohannes Middeke\n']",13 pages,,http://arxiv.org/abs/2007.02926v1,cs.SC,['cs.SC'],,,[]
ACORNS: An Easy-To-Use Code Generator for Gradients and Hessians,http://arxiv.org/abs/2007.05094v1,2020-07-09T22:11:48Z,2020-07-09T22:11:48Z,"  The computation of first and second-order derivatives is a staple in many
computing applications, ranging from machine learning to scientific computing.
We propose an algorithm to automatically differentiate algorithms written in a
subset of C99 code and its efficient implementation as a Python script. We
demonstrate that our algorithm enables automatic, reliable, and efficient
differentiation of common algorithms used in physical simulation and geometry
processing.
","['\nDeshana Desai\n', '\nEtai Shuchatowitz\n', '\nZhongshi Jiang\n', '\nTeseo Schneider\n', '\nDaniele Panozzo\n']",,"SoftwareX, Volume 17, 2022",http://dx.doi.org/10.1016/j.softx.2021.100901,cs.MS,"['cs.MS', 'cs.SC']",10.1016/j.softx.2021.100901,,[]
"Error Correcting Codes, finding polynomials of bounded degree agreeing
  on a dense fraction of a set of points",http://arxiv.org/abs/2007.00445v1,2020-06-29T21:40:11Z,2020-06-29T21:40:11Z,"  Here we present some revised arguments to a randomized algorithm proposed by
Sudan to find the polynomials of bounded degree agreeing on a dense fraction of
a set of points in $\mathbb{F}^{2}$ for some field $\mathbb{F}$.
",['\nPriyank Deshpande\n'],,,http://arxiv.org/abs/2007.00445v1,cs.SC,"['cs.SC', 'cs.DS']",,,[]
"Improvement on Extrapolation of Species Abundance Distribution Across
  Scales from Moments Across Scales",http://arxiv.org/abs/2007.00451v3,2020-07-01T12:55:11Z,2021-07-03T08:15:14Z,"  Raw moments are used as a way to estimate species abundance distribution. The
almost linear pattern of the log transformation of raw moments across scales
allow us to extrapolate species abundance distribution for larger areas.
However, results may produce errors. Some of these errors are due to
computational complexity, fittings of patterns, binning methods, and so on. We
provide some methods to reduce some of the errors. The main result is
introducing new techniques for evaluating a more accurate species abundance
distributions across scales through moments across scales.
","['\nSaeid Alirezazadeh\n', '\nKhadijeh Alibabaei\n']","This work is done within 2017-2019. On the date of publishing on
  ArXiv, Saeid Alirezazadeh is with C4 - Cloud Computing Competence Centre
  (C4-UBI), Universidade da Beira Interior, Covilh\~{a}, Portugal, and Khadijeh
  Alibabaei is with C-MAST Center for Mechanical and Aerospace Science and
  Technologies, University of Beira Interior, Covilh\~{a}, Portugal",,http://arxiv.org/abs/2007.00451v3,q-bio.PE,"['q-bio.PE', 'cs.SC']",,,[]
Noetherian operators and primary decomposition,http://arxiv.org/abs/2006.13881v1,2020-06-24T17:06:07Z,2020-06-24T17:06:07Z,"  Noetherian operators are differential operators that encode primary
components of a polynomial ideal. We develop a framework, as well as
algorithms, for computing Noetherian operators with local dual spaces, both
symbolically and numerically. For a primary ideal, such operators provide an
alternative representation to one given by a set of generators. This
description fits well with numerical algebraic geometry, taking a step toward
the goal of numerical primary decomposition.
","['\nJustin Chen\n', '\nMarc Härkönen\n', '\nRobert Krone\n', '\nAnton Leykin\n']","17 pages, codebase available at
  https://github.com/haerski/NoetherianOperators",,http://arxiv.org/abs/2006.13881v1,math.AG,"['math.AG', 'cs.SC', 'math.AC', '14Q15, 14-04, 13N05, 65L80, 65D05']",,,[]
Ideal Membership Problem for Boolean Minority,http://arxiv.org/abs/2006.16422v1,2020-06-29T22:41:11Z,2020-06-29T22:41:11Z,"  The Ideal Membership Problem (IMP) tests if an input polynomial $f\in
\mathbb{F}[x_1,\dots,x_n]$ with coefficients from a field $\mathbb{F}$ belongs
to a given ideal $I \subseteq \mathbb{F}[x_1,\dots,x_n]$. It is a well-known
fundamental problem with many important applications, though notoriously
intractable in the general case. In this paper we consider the IMP for
polynomial ideals encoding combinatorial problems and where the input
polynomial $f$ has degree at most $d=O(1)$ (we call this problem IMP$_d$). A
dichotomy result between ``hard'' (NP-hard) and ``easy'' (polynomial time) IMPs
was recently achieved for Constraint Satisfaction Problems over finite domains
[Bulatov FOCS'17, Zhuk FOCS'17] (this is equivalent to IMP$_0$) and IMP$_d$ for
the Boolean domain [Mastrolilli SODA'19], both based on the classification of
the IMP through functions called polymorphisms. The complexity of the IMP$_d$
for five polymorphisms has been solved in [Mastrolilli SODA'19] whereas for the
ternary minority polymorphism it was incorrectly declared to have been resolved
by a previous result. As a matter of fact the complexity of the IMP$_d$ for the
ternary minority polymorphism is open. In this paper we provide the missing
link by proving that the IMP$_d$ for Boolean combinatorial ideals whose
constraints are closed under the minority polymorphism can be solved in
polynomial time. This result, along with the results in [Mastrolilli SODA'19],
completes the identification of the precise borderline of tractability for the
IMP$_d$ for constrained problems over the Boolean domain. This paper is
motivated by the pursuit of understanding the issue of bit complexity of
Sum-of-Squares proofs raised by O'Donnell [ITCS'17]. Raghavendra and Weitz
[ICALP'17] show how the IMP$_d$ tractability for combinatorial ideals implies
bounded coefficients in Sum-of-Squares proofs.
","['\nArpitha P. Bharathi\n', '\nMonaldo Mastrolilli\n']",21 pages,,http://arxiv.org/abs/2006.16422v1,cs.CC,"['cs.CC', 'cs.SC', 'math.CO']",,,[]
Learning an arbitrary mixture of two multinomial logits,http://arxiv.org/abs/2007.00204v2,2020-07-01T03:33:52Z,2020-09-28T09:28:56Z,"  In this paper, we consider mixtures of multinomial logistic models (MNL),
which are known to $\epsilon$-approximate any random utility model. Despite its
long history and broad use, rigorous results are only available for learning a
uniform mixture of two MNLs. Continuing this line of research, we study the
problem of learning an arbitrary mixture of two MNLs. We show that the
identifiability of the mixture models may only fail on an algebraic variety of
a negligible measure. This is done by reducing the problem of learning a
mixture of two MNLs to the problem of solving a system of univariate quartic
equations. We also devise an algorithm to learn any mixture of two MNLs using a
polynomial number of samples and a linear number of queries, provided that a
mixture of two MNLs over some finite universe is identifiable. Several
numerical experiments and conjectures are also presented.
",['\nWenpin Tang\n'],14 pages,,http://arxiv.org/abs/2007.00204v2,stat.ML,"['stat.ML', 'cs.CC', 'cs.LG', 'cs.SC']",,,[]
"Computer Algebra in Physics: The hidden SO(4) symmetry of the hydrogen
  atom",http://arxiv.org/abs/2006.12498v2,2020-06-22T14:37:42Z,2021-01-29T23:22:29Z,"  Pauli first noticed the hidden SO(4) symmetry for the Hydrogen atom in the
early stages of quantum mechanics [1]. Departing from that symmetry, one can
recover the spectrum of a spinless hydrogen atom and the degeneracy of its
states without explicitly solving Schr\""odinger's equation [2]. In this paper,
we derive that SO(4) symmetry and spectrum using a computer algebra system
(CAS). While this problem is well known [3, 4], its solution involves several
steps of manipulating expressions with tensorial quantum operators, simplifying
them by taking into account a combination of commutator rules and Einstein's
sum rule for repeated indices. Therefore, it is an excellent model to test the
current status of CAS concerning this kind of quantum-and-tensor-algebra
computations. Generally speaking, when capable, CAS can significantly help with
manipulations that, like non-commutative tensor calculus subject to algebra
rules, are tedious, time-consuming and error-prone. The presentation also shows
a pattern of computer algebra operations that can be useful for systematically
tackling more complicated symbolic problems of this kind.
","['\nPascal Szriftgiser\n', '\nEdgardo S. Cheb-Terrab\n']","28 pages, submitted for publication in Computer Physics
  Communications",,http://dx.doi.org/10.1016/j.cpc.2021.108076,cs.SC,"['cs.SC', 'math-ph', 'math.MP', 'physics.comp-ph', 'quant-ph', '81-08', 'I.1.1; G.4; J.2']",10.1016/j.cpc.2021.108076,,[]
Machine learning the real discriminant locus,http://arxiv.org/abs/2006.14078v2,2020-06-24T22:18:08Z,2022-08-08T13:25:42Z,"  Parameterized systems of polynomial equations arise in many applications in
science and engineering with the real solutions describing, for example,
equilibria of a dynamical system, linkages satisfying design constraints, and
scene reconstruction in computer vision. Since different parameter values can
have a different number of real solutions, the parameter space is decomposed
into regions whose boundary forms the real discriminant locus. This article
views locating the real discriminant locus as a supervised classification
problem in machine learning where the goal is to determine classification
boundaries over the parameter space, with the classes being the number of real
solutions. For multidimensional parameter spaces, this article presents a novel
sampling method which carefully samples the parameter space. At each sample
point, homotopy continuation is used to obtain the number of real solutions to
the corresponding polynomial system. Machine learning techniques including
nearest neighbor and deep learning are used to efficiently approximate the real
discriminant locus. One application of having learned the real discriminant
locus is to develop a real homotopy method that only tracks the real solution
paths unlike traditional methods which track all~complex~solution~paths.
Examples show that the proposed approach can efficiently approximate
complicated solution boundaries such as those arising from the equilibria of
the Kuramoto model.
","['\nEdgar A. Bernal\n', '\nJonathan D. Hauenstein\n', '\nDhagash Mehta\n', '\nMargaret H. Regan\n', '\nTingting Tang\n']","22 pages, 14 figures",,http://arxiv.org/abs/2006.14078v2,stat.ML,"['stat.ML', 'cs.LG', 'cs.SC', 'math.AG', 'stat.AP']",,,[]
On the Complexity of Solving Generic Over-determined Bilinear Systems,http://arxiv.org/abs/2006.09442v1,2020-06-16T18:39:09Z,2020-06-16T18:39:09Z,"  In this paper, we study the complexity of solving generic over-determined
bilinear systems over a finite field $\mathbb{F}$. Given a generic bilinear
sequence $B \in \mathbb{F}[\mathbf{x},\mathbf{y}]$, with respect to a partition
of variables $\mathbf{x}$, $\mathbf{y}$, we show that, the solutions of the
system $B= \mathbf{0}$ can be efficiently found on the
$\mathbb{F}[\mathbf{y}]$-module generated by $B$. Following this observation,
we propose three variations of Gr\""obner basis algorithms, that only involve
multiplication by monomials in they-variables, namely, $\mathbf{y}$-XL, based
on the XL algorithm, $\mathbf{y}$-MLX, based on the mutant XL algorithm, and
$\mathbf{y}$-HXL, basedon a hybrid approach. We define notions of regularity
for over-determined bilinear systems,that capture the idea of genericity, and
we develop the necessary theoretical tools to estimate the complexity of the
algorithms for such sequences. We also present extensive experimental results,
testing our conjecture, verifying our results, and comparing the complexity of
the various methods.
","['\nJohn B. Baena\n', '\nDaniel Cabarcas\n', '\nJavier Verbel\n']",,,http://arxiv.org/abs/2006.09442v1,cs.SC,['cs.SC'],,,[]
"Pointer Data Structure Synthesis from Answer Set Programming
  Specifications",http://arxiv.org/abs/2006.07440v2,2020-06-12T19:45:35Z,2020-08-13T03:23:33Z,"  We develop an inductive proof-technique to generate imperative programs for
pointer data structures from behavioural specifications expressed in the Answer
Set Programming (ASP) formalism. ASP is a non-monotonic logic based formalism
that employs negation-as-failure which helps emulate the human thought process,
allowing domain experts to model desired system behaviour succinctly. We argue
in this paper that ASP's reliance on negation-as-failure makes it a better
formalism than those based on first-order logic for writing formal
specifications. We assume the a domain expert provides the representation of
inductively defined data structures along with a specification of its
operations. Our procedures combined with our novel proof-technique reason over
the specifications and automatically generate an imperative program. Our
proof-technique leverages the idea of partial deduction to simplify logical
specifications. By algebraically simplifying logical specifications we arrive
at a residual specification which can be interpreted as an appropriate
imperative program. This work is in the realm of constructing programs that are
correct according to a given specification.
","['\nSarat Chandra Varanasi\n', '\nNeeraj Mittal\n', '\nGopal Gupta\n']",,,http://arxiv.org/abs/2006.07440v2,cs.LO,"['cs.LO', 'cs.SC']",,,[]
Toric Eigenvalue Methods for Solving Sparse Polynomial Systems,http://arxiv.org/abs/2006.10654v3,2020-06-18T16:24:36Z,2022-03-11T08:23:50Z,"  We consider the problem of computing homogeneous coordinates of points in a
zero-dimensional subscheme of a compact, complex toric variety $X$. Our
starting point is a homogeneous ideal $I$ in the Cox ring of $X$, which in
practice might arise from homogenizing a sparse polynomial system. We prove a
new eigenvalue theorem in the toric compact setting, which leads to a novel,
robust numerical approach for solving this problem. Our method works in
particular for systems having isolated solutions with arbitrary multiplicities.
It depends on the multigraded regularity properties of $I$. We study these
properties and provide bounds on the size of the matrices appearing in our
approach when $I$ is a complete intersection.
","['\nMatías R. Bender\n', '\nSimon Telen\n']","28 pages, to appear in Mathematics of Computations, AMS",,http://arxiv.org/abs/2006.10654v3,math.AG,"['math.AG', 'cs.SC', '14M25 (Primary), 65H04 (Secondary), 65H10, 65H17']",,,[]
"Walsh functions, scrambled $(0,m,s)$-nets, and negative covariance:
  applying symbolic computation to quasi-Monte Carlo integration",http://arxiv.org/abs/2006.06225v1,2020-06-11T06:51:17Z,2020-06-11T06:51:17Z,"  We investigate base $b$ Walsh functions for which the variance of the
integral estimator based on a scrambled $(0,m,s)$-net in base $b$ is less than
or equal to that of the Monte-Carlo estimator based on the same number of
points. First we compute the Walsh decomposition for the joint probability
density function of two distinct points randomly chosen from a scrambled
$(t,m,s)$-net in base $b$ in terms of certain counting numbers and simplify it
in the special case $t$ is zero. Using this, we obtain an expression for the
covariance of the integral estimator in terms of the Walsh coefficients of the
function. Finally, we prove that the covariance of the integral estimator is
negative when the Walsh coefficients of the function satisfy a certain decay
condition. To do this, we use creative telescoping and recurrence solving
algorithms from symbolic computation to find a sign equivalent closed form
expression for the covariance term.
","['\nJaspar Wiart\n', '\nElaine Wong\n']","27 pages; Supplementary material at
  https://wongey.github.io/digital-nets-walsh/","Mathematics and Computers in Simulation, Volume 182, April 2021,
  Pages 277-295",http://dx.doi.org/10.1016/j.matcom.2020.10.026,math.NA,"['math.NA', 'cs.NA', 'cs.SC']",10.1016/j.matcom.2020.10.026,,[]
"Computing Igusa's local zeta function of univariates in deterministic
  polynomial-time",http://arxiv.org/abs/2006.08926v1,2020-06-16T04:59:41Z,2020-06-16T04:59:41Z,"  Igusa's local zeta function $Z_{f,p}(s)$ is the generating function that
counts the number of integral roots, $N_{k}(f)$, of $f(\mathbf x) \bmod p^k$,
for all $k$. It is a famous result, in analytic number theory, that $Z_{f,p}$
is a rational function in $\mathbb{Q}(p^s)$. We give an elementary proof of
this fact for a univariate polynomial $f$. Our proof is constructive as it
gives a closed-form expression for the number of roots $N_{k}(f)$.
  Our proof, when combined with the recent root-counting algorithm of (Dwivedi,
Mittal, Saxena, CCC, 2019), yields the first deterministic poly($|f|, \log p$)
time algorithm to compute $Z_{f,p}(s)$. Previously, an algorithm was known only
in the case when $f$ completely splits over $\mathbb{Q}_p$; it required the
rational roots to use the concept of generating function of a tree
(Z\'u\~niga-Galindo, J.Int.Seq., 2003).
","['\nAshish Dwivedi\n', '\nNitin Saxena\n']","15 pages, ANTS 2020",,http://arxiv.org/abs/2006.08926v1,math.NT,"['math.NT', 'cs.CC', 'cs.DS', 'cs.SC', '11S40, 68Q01, 68W30 (Primary) 11Y16, 14G50 (Secondary)']",,,[]
"Logic, Probability and Action: A Situation Calculus Perspective",http://arxiv.org/abs/2006.09868v1,2020-06-17T13:49:53Z,2020-06-17T13:49:53Z,"  The unification of logic and probability is a long-standing concern in AI,
and more generally, in the philosophy of science. In essence, logic provides an
easy way to specify properties that must hold in every possible world, and
probability allows us to further quantify the weight and ratio of the worlds
that must satisfy a property. To that end, numerous developments have been
undertaken, culminating in proposals such as probabilistic relational models.
While this progress has been notable, a general-purpose first-order knowledge
representation language to reason about probabilities and dynamics, including
in continuous settings, is still to emerge. In this paper, we survey recent
results pertaining to the integration of logic, probability and actions in the
situation calculus, which is arguably one of the oldest and most well-known
formalisms. We then explore reduction theorems and programming interfaces for
the language. These results are motivated in the context of cognitive robotics
(as envisioned by Reiter and his colleagues) for the sake of concreteness.
Overall, the advantage of proving results for such a general language is that
it becomes possible to adapt them to any special-purpose fragment, including
but not limited to popular probabilistic relational models.
",['\nVaishak Belle\n'],,,http://arxiv.org/abs/2006.09868v1,cs.AI,"['cs.AI', 'cs.LG', 'cs.LO', 'cs.SC']",,,[]
A unified framework for equivalences in social networks,http://arxiv.org/abs/2006.10733v1,2020-06-18T17:57:38Z,2020-06-18T17:57:38Z,"  A key concern in network analysis is the study of social positions and roles
of actors in a network. The notion of ""position"" refers to an equivalence class
of nodes that have similar ties to other nodes, whereas a ""role"" is an
equivalence class of compound relations that connect the same pairs of nodes.
An open question in network science is whether it is possible to simultaneously
perform role and positional analysis. Motivated by the principle of
functoriality in category theory we propose a new method that allows to tie
role and positional analysis together. We illustrate our methods on two
well-studied data sets in network science.
","['\nNina Otter\n', '\nMason A. Porter\n']",working paper,,http://arxiv.org/abs/2006.10733v1,cs.SI,"['cs.SI', 'cs.DM', 'cs.SC', 'math.CT']",,,[]
"Neuro-Symbolic Visual Reasoning: Disentangling ""Visual"" from ""Reasoning""",http://arxiv.org/abs/2006.11524v3,2020-06-20T08:48:29Z,2020-08-25T23:30:57Z,"  Visual reasoning tasks such as visual question answering (VQA) require an
interplay of visual perception with reasoning about the question semantics
grounded in perception. However, recent advances in this area are still
primarily driven by perception improvements (e.g. scene graph generation)
rather than reasoning. Neuro-symbolic models such as Neural Module Networks
bring the benefits of compositional reasoning to VQA, but they are still
entangled with visual representation learning, and thus neural reasoning is
hard to improve and assess on its own. To address this, we propose (1) a
framework to isolate and evaluate the reasoning aspect of VQA separately from
its perception, and (2) a novel top-down calibration technique that allows the
model to answer reasoning questions even with imperfect perception. To this
end, we introduce a differentiable first-order logic formalism for VQA that
explicitly decouples question answering from visual perception. On the
challenging GQA dataset, this framework is used to perform in-depth,
disentangled comparisons between well-known VQA models leading to informative
insights regarding the participating models as well as the task.
","['\nSaeed Amizadeh\n', '\nHamid Palangi\n', '\nOleksandr Polozov\n', '\nYichen Huang\n', '\nKazuhito Koishida\n']","Published in Proceedings of the 37th International Conference on
  Machine Learning (ICML), Online, PMLR 119, 2020",,http://arxiv.org/abs/2006.11524v3,cs.LG,"['cs.LG', 'cs.AI', 'cs.CV', 'cs.NE', 'cs.SC', 'stat.ML']",,,[]
Good pivots for small sparse matrices,http://arxiv.org/abs/2006.01623v2,2020-06-02T14:07:56Z,2020-07-29T11:23:44Z,"  For sparse matrices up to size $8 \times 8$, we determine optimal choices for
pivot selection in Gaussian elimination. It turns out that they are slightly
better than the pivots chosen by a popular pivot selection strategy, so there
is some room for improvement. We then create a pivot selection strategy using
machine learning and find that it indeed leads to a small improvement compared
to the classical strategy.
","['\nManuel Kauers\n', '\nJakob Moosbauer\n']",11 pages,,http://dx.doi.org/10.1007/978-3-030-60026-6_20,cs.SC,"['cs.SC', 'cs.LG', '68W30']",10.1007/978-3-030-60026-6_20,,[]
"Nonlinear observability algorithms with known and unknown inputs:
  analysis and implementation",http://arxiv.org/abs/2006.00859v1,2020-06-01T11:40:47Z,2020-06-01T11:40:47Z,"  The observability of a dynamical system is affected by the presence of
external inputs, either known (such as control actions) or unknown
(disturbances). Inputs of unknown magnitude are especially detrimental for
observability, and they also complicate its analysis. Hence the availability of
computational tools capable of analysing the observability of nonlinear systems
with unknown inputs has been limited until lately. Two symbolic algorithms
based on differential geometry, ORC-DF and FISPO, have been recently proposed
for this task, but their critical analysis and comparison is still lacking.
Here we perform an analytical comparison of both algorithms and evaluate their
performance on a set of problems, discussing their strengths and limitations.
Additionally, we use these analyses to provide insights about certain aspects
of the relationship between inputs and observability. We find that, while
ORC-DF and FISPO follow a similar approach, they differ in key aspects that can
have a substantial influence on their applicability and computational cost. The
FISPO algorithm is more generally applicable, since it can analyse any
nonlinear ODE model. The ORC-DF algorithm analyses models that are affine in
the inputs, and if those models have known inputs it is sometimes more
efficient. Thus, the optimal choice of a method depends on the characteristics
of the problem under consideration. To facilitate the use of both algorithms we
implement the ORC-DF algorithm in a new version of STRIKE-GOLDD, a MATLAB
toolbox for structural identifiability and observability analysis. Since this
software tool already had an implementation of the FISPO algorithm, the new
release allows modellers and model users the convenience of choosing between
different algorithms in a single tool, without changing the coding of their
model.
","['\nNerea Martínez\n', '\nAlejandro F. Villaverde\n']","23 pages, 5 figures",,http://arxiv.org/abs/2006.00859v1,eess.SY,"['eess.SY', 'cs.SC', 'cs.SY', 'math.DG']",,,[]
"Characterizing an Analogical Concept Memory for Architectures
  Implementing the Common Model of Cognition",http://arxiv.org/abs/2006.01962v3,2020-06-02T21:54:03Z,2020-07-29T18:02:17Z,"  Architectures that implement the Common Model of Cognition - Soar, ACT-R, and
Sigma - have a prominent place in research on cognitive modeling as well as on
designing complex intelligent agents. In this paper, we explore how
computational models of analogical processing can be brought into these
architectures to enable concept acquisition from examples obtained
interactively. We propose a new analogical concept memory for Soar that
augments its current system of declarative long-term memories. We frame the
problem of concept learning as embedded within the larger context of
interactive task learning (ITL) and embodied language processing (ELP). We
demonstrate that the analogical learning methods implemented in the proposed
memory can quickly learn a diverse types of novel concepts that are useful not
only in recognition of a concept in the environment but also in action
selection. Our approach has been instantiated in an implemented cognitive
system \textsc{Aileen} and evaluated on a simulated robotic domain.
","['\nShiwali Mohan\n', '\nMatt Klenk\n', '\nMatthew Shreve\n', '\nKent Evans\n', '\nAaron Ang\n', '\nJohn Maxwell\n']","To be presented the Eighth Annual Conference on Advances in Cognitive
  Systems (ACS 2020) (https://advancesincognitivesystems.github.io/acs/)",,http://arxiv.org/abs/2006.01962v3,cs.AI,"['cs.AI', 'cs.HC', 'cs.RO', 'cs.SC']",,,[]
Analogical proportions,http://arxiv.org/abs/2006.02854v15,2020-06-04T13:44:36Z,2023-12-29T11:01:45Z,"  Analogy-making is at the core of human and artificial intelligence and
creativity with applications to such diverse tasks as proving mathematical
theorems and building mathematical theories, common sense reasoning, learning,
language acquisition, and story telling. This paper introduces from first
principles an abstract algebraic framework of analogical proportions of the
form `$a$ is to $b$ what $c$ is to $d$' in the general setting of universal
algebra. This enables us to compare mathematical objects possibly across
different domains in a uniform way which is crucial for AI-systems. It turns
out that our notion of analogical proportions has appealing mathematical
properties. As we construct our model from first principles using only
elementary concepts of universal algebra, and since our model questions some
basic properties of analogical proportions presupposed in the literature, to
convince the reader of the plausibility of our model we show that it can be
naturally embedded into first-order logic via model-theoretic types and prove
from that perspective that analogical proportions are compatible with
structure-preserving mappings. This provides conceptual evidence for its
applicability. In a broader sense, this paper is a first step towards a theory
of analogical reasoning and learning systems with potential applications to
fundamental AI-problems like common sense reasoning and computational learning
and creativity.
",['\nChristian Antić\n'],,,http://arxiv.org/abs/2006.02854v15,cs.LO,"['cs.LO', 'cs.AI', 'cs.LG', 'cs.SC']",,,[]
Decomposable sparse polynomial systems,http://arxiv.org/abs/2006.03154v1,2020-06-04T22:18:32Z,2020-06-04T22:18:32Z,"  The Macaulay2 package DecomposableSparseSystems implements methods for
studying and numerically solving decomposable sparse polynomial systems. We
describe the structure of decomposable sparse systems and explain how the
methods in this package may be used to exploit this structure, with examples.
","['\nTaylor Brysiewicz\n', '\nJose Israel Rodriguez\n', '\nFrank Sottile\n', '\nThomas Yahl\n']","7 pages, software available at
  https://www.math.tamu.edu/~thomasjyahl/research/DSS/DSSsite.html",J. Softw. Alg. Geom. 11 (2021) 53-59,http://dx.doi.org/10.2140/jsag.2021.11.53,math.AG,"['math.AG', 'cs.NA', 'cs.SC', 'math.NA', '14M25, 65H10, 65H20']",10.2140/jsag.2021.11.53,,[]
"A machine learning based software pipeline to pick the variable ordering
  for algorithms with polynomial inputs",http://arxiv.org/abs/2005.11251v1,2020-05-22T16:00:04Z,2020-05-22T16:00:04Z,"  We are interested in the application of Machine Learning (ML) technology to
improve mathematical software. It may seem that the probabilistic nature of ML
tools would invalidate the exact results prized by such software, however, the
algorithms which underpin the software often come with a range of choices which
are good candidates for ML application. We refer to choices which have no
effect on the mathematical correctness of the software, but do impact its
performance.
  In the past we experimented with one such choice: the variable ordering to
use when building a Cylindrical Algebraic Decomposition (CAD). We used the
Python library Scikit-Learn (sklearn) to experiment with different ML models,
and developed new techniques for feature generation and hyper-parameter
selection.
  These techniques could easily be adapted for making decisions other than our
immediate application of CAD variable ordering. Hence in this paper we present
a software pipeline to use sklearn to pick the variable ordering for an
algorithm that acts on a polynomial system. The code described is freely
available online.
","['\nDorian Florescu\n', '\nMatthew England\n']",Accepted into Proc ICMS 2020,"Mathematical Software (Proc. ICMS '20), pp. 302-322, (Lecture
  Notes in Computer Science, 12097). Springer International Publishing, 2020",http://dx.doi.org/10.1007/978-3-030-52200-1_30,cs.SC,"['cs.SC', 'cs.LG', '68W30, 68T05, 03C10', 'I.2.6; I.1.0']",10.1007/978-3-030-52200-1_30,,[]
miniKanren as a Tool for Symbolic Computation in Python,http://arxiv.org/abs/2005.11644v3,2020-05-24T03:09:08Z,2020-05-28T20:41:06Z,"  In this article, we give a brief overview of the current state and future
potential of symbolic computation within the Python statistical modeling and
machine learning community. We detail the use of miniKanren as an underlying
framework for term rewriting and symbolic mathematics, as well as its ability
to orchestrate the use of existing Python libraries. We also discuss the
relevance and potential of relational programming for implementing more robust,
portable, domain-specific ""math-level"" optimizations--with a slight focus on
Bayesian modeling. Finally, we describe the work going forward and raise some
questions regarding potential cross-overs between statistical modeling and
programming language theory.
",['\nBrandon T. Willard\n'],,,http://arxiv.org/abs/2005.11644v3,cs.PL,"['cs.PL', 'cs.SC']",,,[]
"Fast Decoding of Codes in the Rank, Subspace, and Sum-Rank Metric",http://arxiv.org/abs/2005.09916v2,2020-05-20T08:53:58Z,2021-03-10T15:49:04Z,"  We speed up existing decoding algorithms for three code classes in different
metrics: interleaved Gabidulin codes in the rank metric, lifted interleaved
Gabidulin codes in the subspace metric, and linearized Reed-Solomon codes in
the sum-rank metric. The speed-ups are achieved by new algorithms that reduce
the cores of the underlying computational problems of the decoders to one
common tool: computing left and right approximant bases of matrices over skew
polynomial rings. To accomplish this, we describe a skew-analogue of the
existing PM-Basis algorithm for matrices over ordinary polynomials. This
captures the bulk of the work in multiplication of skew polynomials, and the
complexity benefit comes from existing algorithms performing this faster than
in classical quadratic complexity. The new algorithms for the various
decoding-related computational problems are interesting in their own and have
further applications, in particular parts of decoders of several other codes
and foundational problems related to the remainder-evaluation of skew
polynomials.
","['\nHannes Bartz\n', '\nThomas Jerkovits\n', '\nSven Puchinger\n', '\nJohan Rosenkilde\n']","26 pages, accepted for publication in IEEE Transactions on
  Information Theory",,http://arxiv.org/abs/2005.09916v2,cs.IT,"['cs.IT', 'cs.SC', 'math.IT']",,,[]
First Neural Conjecturing Datasets and Experiments,http://arxiv.org/abs/2005.14664v1,2020-05-29T16:46:25Z,2020-05-29T16:46:25Z,"  We describe several datasets and first experiments with creating conjectures
by neural methods. The datasets are based on the Mizar Mathematical Library
processed in several forms and the problems extracted from it by the MPTP
system and proved by the E prover using the ENIGMA guidance. The conjecturing
experiments use the Transformer architecture and in particular its GPT-2
implementation.
","['\nJosef Urban\n', '\nJan Jakubův\n']",Accepted to CICM 2020,,http://arxiv.org/abs/2005.14664v1,cs.AI,"['cs.AI', 'cs.LG', 'cs.LO', 'cs.NE', 'cs.SC']",,,[]
"On Rational and Hypergeometric Solutions of Linear Ordinary Difference
  Equations in $Π\mathbfΣ^*$-field extensions",http://arxiv.org/abs/2005.04944v2,2020-05-11T09:15:31Z,2021-01-25T19:36:53Z,"  We present a complete algorithm that computes all hypergeometric solutions of
homogeneous linear difference equations and rational solutions of parameterized
linear difference equations in the setting of $\Pi\Sigma^*$-fields. More
generally, we provide a flexible framework for a big class of difference fields
that is built by a tower of $\Pi\Sigma^*$-field extensions over a difference
field that satisfies certain algorithmic properties. As a consequence one can
compute all solutions in terms of indefinite nested sums and products that
arise within the components of a parameterized linear difference equation, and
one can find all hypergeometric solutions that are defined over the arising
sums and products of a homogeneous linear difference equation.
","['\nSergei A. Abramov\n', '\nManuel Bronstein\n', '\nMarko Petkovšek\n', '\nCarsten Schneider\n']","Various typos have been removed and the presentation has been
  improved",,http://arxiv.org/abs/2005.04944v2,cs.SC,['cs.SC'],,,[]
Generalizing The Davenport-Mahler-Mignotte Bound -- The Weighted Case,http://arxiv.org/abs/2005.07843v1,2020-05-16T02:16:16Z,2020-05-16T02:16:16Z,"  Root separation bounds play an important role as a complexity measure in
understanding the behaviour of various algorithms in computational algebra,
e.g., root isolation algorithms. A classic result in the univariate setting is
the Davenport-Mahler-Mignotte (DMM) bound. One way to state the bound is to
consider a directed acyclic graph $(V,E)$ on a subset of roots of a degree $d$
polynomial $f(z) \in \mathbb{C}[z]$, where the edges point from a root of
smaller absolute value to one of larger absolute, and the in-degrees of all
vertices is at most one. Then the DMM bound is an amortized lower bound on the
following product: $\prod_{(\alpha,\beta) \in E}|\alpha-\beta|$. However, the
lower bound involves the discriminant of the polynomial $f$, and becomes
trivial if the polynomial is not square-free. This was resolved by Eigenwillig,
(2008), by using a suitable subdiscriminant instead of the discriminant.
Escorcielo-Perrucci, 2016, further dropped the in-degree constraint on the
graph by using the theory of finite differences. Emiris et al., 2019, have
generalized their result to handle the case where the exponent of the term
$|\alpha-\beta|$ in the product is at most the multiplicity of either of the
roots. In this paper, we generalize these results by allowing arbitrary
positive integer weights on the edges of the graph, i.e., for a weight function
$w: E \rightarrow \mathbb{Z}_{>0}$, we derive an amortized lower bound on
$\prod_{(\alpha,\beta) \in E}|\alpha-\beta|^{w(\alpha,\beta)}$. Such a product
occurs in the complexity estimates of some recent algorithms for root
clustering (e.g., Becker et al., 2016), where the weights are usually some
function of the multiplicity of the roots. Because of its amortized nature, our
bound is arguably better than the bounds obtained by manipulating existing
results to accommodate the weights.
",['\nVikram Sharma\n'],,,http://arxiv.org/abs/2005.07843v1,cs.SC,['cs.SC'],,,[]
Towards Efficient Normalizers of Primitive Groups,http://arxiv.org/abs/2005.04979v1,2020-05-11T10:14:31Z,2020-05-11T10:14:31Z,"  We present the ideas behind an algorithm to compute normalizers of primitive
groups with non-regular socle in polynomial time. We highlight a concept we
developed called permutation morphisms and present timings for a partial
implementation of our algorithm. This article is a collection of results from
the author's PhD thesis.
",['\nSergio Siccha\n'],,,http://arxiv.org/abs/2005.04979v1,math.GR,"['math.GR', 'cs.SC', '20B15 (Primary) 20B40, 68W30 (Secondary)', 'F.2.2; G.2.1']",,,[]
A modular extension for a computer algebra system,http://arxiv.org/abs/2005.05261v1,2020-05-11T17:05:30Z,2020-05-11T17:05:30Z,"  Computer algebra systems are complex software systems that cover a wide range
of scientific and practical problems. However, the absolute coverage cannot be
achieved. Often, it is required to create a user extension for an existing
computer algebra system. In this case, the extensibility of the system should
be taken into account. In this paper, we consider a technology for extending
the SymPy computer algebra system with a low-level module that implements a
random number generator.
","['\nMigran N. Gevorkyan\n', '\nAnna V. Korolkova\n', '\nDmitry S. Kulyabov\n', '\nLeonid A. Sevastianov\n']",in English; in Russian,,http://dx.doi.org/10.1134/S036176882002005X,cs.MS,"['cs.MS', 'cs.SC']",10.1134/S036176882002005X,,[]
The Extended Theory of Trees and Algebraic (Co)datatypes,http://arxiv.org/abs/2005.06659v2,2020-05-13T23:12:42Z,2020-08-07T01:24:42Z,"  The first-order theory of finite and infinite trees has been studied since
the eighties, especially by the logic programming community. Following
Djelloul, Dao and Fr\""uhwirth, we consider an extension of this theory with an
additional predicate for finiteness of trees, which is useful for expressing
properties about (not just datatypes but also) codatatypes. Based on their
work, we present a simplification procedure that determines whether any given
(not necessarily closed) formula is satisfiable, returning a simplified formula
which enables one to read off all possible models. Our extension makes the
algorithm usable for algebraic (co)datatypes, which was impossible in their
original work due to restrictive assumptions. We also provide a prototype
implementation of our simplification procedure and evaluate it on instances
from the SMT-LIB.
","['\nFabian Zaiser\nUniversity of Oxford\n', '\nC. -H. Luke Ong\nUniversity of Oxford\n']","In Proceedings VPT/HCVS 2020, arXiv:2008.02483","EPTCS 320, 2020, pp. 167-196",http://dx.doi.org/10.4204/EPTCS.320.14,cs.LO,"['cs.LO', 'cs.SC']",10.4204/EPTCS.320.14,,"['University of Oxford', 'University of Oxford']"
An Algebraic Model For Quorum Systems,http://arxiv.org/abs/2005.08536v2,2020-05-18T08:48:41Z,2020-06-02T08:00:40Z,"  Quorum systems are a key mathematical abstraction in distributed
fault-tolerant computing for capturing trust assumptions. A quorum system is a
collection of subsets of all processes, called quorums, with the property that
each pair of quorums have a non-empty intersection. They can be found at the
core of many reliable distributed systems, such as cloud computing platforms,
distributed storage systems and blockchains. In this paper we give a new
interpretation of quorum systems, starting with classical majority-based quorum
systems and extending this to Byzantine quorum systems. We propose an algebraic
representation of the theory underlying quorum systems making use of
multivariate polynomial ideals, incorporating properties of these systems, and
studying their algebraic varieties. To achieve this goal we will exploit
properties of Boolean Groebner bases. The nice nature of Boolean Groebner bases
allows us to avoid part of the combinatorial computations required to check
consistency and availability of quorum systems. Our results provide a novel
approach to test quorum systems properties from both algebraic and algorithmic
perspectives.
","['\nAlex Pellegrini\n', '\nLuca Zanolini\n']","15 pages, 3 algorithms",,http://arxiv.org/abs/2005.08536v2,cs.SC,"['cs.SC', 'cs.DC']",,,[]
Positional Games and QBF: A Polished Encoding,http://arxiv.org/abs/2005.05098v2,2020-05-11T13:32:03Z,2023-11-02T06:30:29Z,"  Positional games are a mathematical class of two-player games comprising
Tic-tac-toe and its generalizations. We propose a novel encoding of these games
into Quantified Boolean Formulas (QBFs) such that a game instance admits a
winning strategy for the first player if and only if the corresponding formula
is true. Our approach improves over previous QBF encodings of games in multiple
ways. First, it is generic and lets us encode other positional games, such as
Hex. Second, the structural properties of positional games, together with
careful treatment of illegal moves, let us generate more compact instances that
can be solved faster by state-of-the-art QBF solvers. We establish the latter
fact through extensive experiments. Finally, the compactness of our new
encoding makes it feasible to translate realistic game problems. We identify a
few such problems of historical significance and put them forward to the QBF
community as milestones of increasing difficulty.
","['\nValentin Mayer-Eichberger\n', '\nAbdallah Saffidine\n']","This technical report supersedes, extends and improves our SAT 2020
  paper ""Positional games and QBF: the corrective encoding""",,http://arxiv.org/abs/2005.05098v2,cs.LO,"['cs.LO', 'cs.AI', 'cs.GT', 'cs.SC']",,,[]
Neural Collaborative Reasoning,http://arxiv.org/abs/2005.08129v5,2020-05-16T23:29:31Z,2021-05-03T02:06:05Z,"  Existing Collaborative Filtering (CF) methods are mostly designed based on
the idea of matching, i.e., by learning user and item embeddings from data
using shallow or deep models, they try to capture the associative relevance
patterns in data, so that a user embedding can be matched with relevant item
embeddings using designed or learned similarity functions. However, as a
cognition rather than a perception intelligent task, recommendation requires
not only the ability of pattern recognition and matching from data, but also
the ability of cognitive reasoning in data. In this paper, we propose to
advance Collaborative Filtering (CF) to Collaborative Reasoning (CR), which
means that each user knows part of the reasoning space, and they collaborate
for reasoning in the space to estimate preferences for each other. Technically,
we propose a Neural Collaborative Reasoning (NCR) framework to bridge learning
and reasoning. Specifically, we integrate the power of representation learning
and logical reasoning, where representations capture similarity patterns in
data from perceptual perspectives, and logic facilitates cognitive reasoning
for informed decision making. An important challenge, however, is to bridge
differentiable neural networks and symbolic reasoning in a shared architecture
for optimization and inference. To solve the problem, we propose a modularized
reasoning architecture, which learns logical operations such as AND ($\wedge$),
OR ($\vee$) and NOT ($\neg$) as neural modules for implication reasoning
($\rightarrow$). In this way, logical expressions can be equivalently organized
as neural networks, so that logical reasoning and prediction can be conducted
in a continuous space. Experiments on real-world datasets verified the
advantages of our framework compared with both shallow, deep and reasoning
models.
","['\nHanxiong Chen\n', '\nShaoyun Shi\n', '\nYunqi Li\n', '\nYongfeng Zhang\n']",Accepted to the 30th Web Conference (WWW 2021),,http://dx.doi.org/10.1145/3442381.3449973,cs.IR,"['cs.IR', 'cs.AI', 'cs.LG', 'cs.SC']",10.1145/3442381.3449973,,[]
"Applying Genetic Programming to Improve Interpretability in Machine
  Learning Models",http://arxiv.org/abs/2005.09512v1,2020-05-18T16:09:49Z,2020-05-18T16:09:49Z,"  Explainable Artificial Intelligence (or xAI) has become an important research
topic in the fields of Machine Learning and Deep Learning. In this paper, we
propose a Genetic Programming (GP) based approach, named Genetic Programming
Explainer (GPX), to the problem of explaining decisions computed by AI systems.
The method generates a noise set located in the neighborhood of the point of
interest, whose prediction should be explained, and fits a local explanation
model for the analyzed sample. The tree structure generated by GPX provides a
comprehensible analytical, possibly non-linear, symbolic expression which
reflects the local behavior of the complex model. We considered three machine
learning techniques that can be recognized as complex black-box models: Random
Forest, Deep Neural Network and Support Vector Machine in twenty data sets for
regression and classifications problems. Our results indicate that the GPX is
able to produce more accurate understanding of complex models than the state of
the art. The results validate the proposed approach as a novel way to deploy GP
to improve interpretability.
","['\nLeonardo Augusto Ferreira\n', '\nFrederico Gadelha Guimarães\n', '\nRodrigo Silva\n']","8 pages, 8 figures, submitted and accepted to 2020 IEEE Congress on
  Evolutionary Computation (IEEE CEC 2020). Copyright 2020 IEEE. Personal use
  of this material is permitted. Permission from IEEE must be obtained for all
  other uses",,http://arxiv.org/abs/2005.09512v1,cs.LG,"['cs.LG', 'cs.AI', 'cs.NE', 'cs.SC', 'F.3.1; I.1.4; I.2.2; I.2.6']",,,[]
"Rational Solutions of First Order Algebraic Ordinary Differential
  Equations",http://arxiv.org/abs/2005.01289v1,2020-05-04T06:55:59Z,2020-05-04T06:55:59Z,"  Let $f(t, y,y')=\sum_{i=0}^d a_i(t, y)y'^i=0$ be a first order ordinary
differential equation with polynomial coefficients. Eremenko in 1999 proved
that there exists a constant $C$ such that every rational solution of $f(t,
y,y')=0$ is of degree not greater than $C$. Examples show that this degree
bound $C$ depends not only on the degrees of $f$ in $t,y,y'$ but also on the
coefficients of $f$ viewed as polynomial in $t,y,y'$. In this paper, we show
that if $$\max_{i=0}^d \{{\rm deg}(a_i,y)-2(d-i)\}>0 $$ then the degree bound
$C$ only depends on the degrees of $f$, and furthermore we present an explicit
expression for $C$ in terms of the degrees of $f$.
","['\nRuyong Feng\n', '\nShuang Feng\n']",40 pages,,http://arxiv.org/abs/2005.01289v1,cs.SC,"['cs.SC', '34A05, 68W30']",,,[]
"Characterizing Triviality of the Exponent Lattice of A Polynomial
  through Galois and Galois-Like Groups",http://arxiv.org/abs/2005.01963v1,2020-05-05T06:20:01Z,2020-05-05T06:20:01Z,"  The problem of computing \emph{the exponent lattice} which consists of all
the multiplicative relations between the roots of a univariate polynomial has
drawn much attention in the field of computer algebra. As is known, almost all
irreducible polynomials with integer coefficients have only trivial exponent
lattices. However, the algorithms in the literature have difficulty in proving
such triviality for a generic polynomial. In this paper, the relations between
the Galois group (respectively, \emph{the Galois-like groups}) and the
triviality of the exponent lattice of a polynomial are investigated. The
$\bbbq$\emph{-trivial} pairs, which are at the heart of the relations between
the Galois group and the triviality of the exponent lattice of a polynomial,
are characterized. An effective algorithm is developed to recognize these
pairs. Based on this, a new algorithm is designed to prove the triviality of
the exponent lattice of a generic irreducible polynomial, which considerably
improves a state-of-the-art algorithm of the same type when the polynomial
degree becomes larger. In addition, the concept of the Galois-like groups of a
polynomial is introduced. Some properties of the Galois-like groups are proved
and, more importantly, a sufficient and necessary condition is given for a
polynomial (which is not necessarily irreducible) to have trivial exponent
lattice.
",['\nTao Zheng\n'],"19 pages,2 figures",,http://arxiv.org/abs/2005.01963v1,cs.SC,"['cs.SC', 'math.NT']",,,[]
"Algorithmic Averaging for Studying Periodic Orbits of Planar
  Differential Systems",http://arxiv.org/abs/2005.03487v2,2020-05-06T07:12:36Z,2023-04-30T16:23:01Z,"  One of the main open problems in the qualitative theory of real planar
differential systems is the study of limit cycles. In this article, we present
an algorithmic approach for detecting how many limit cycles can bifurcate from
the periodic orbits of a given polynomial differential center when it is
perturbed inside a class of polynomial differential systems via the averaging
method. We propose four symbolic algorithms to implement the averaging method.
The first algorithm is based on the change of polar coordinates that allows one
to transform a considered differential system to the normal form of averaging.
The second algorithm is used to derive the solutions of certain differential
systems associated to the unperturbed term of the normal of averaging. The
third algorithm exploits the partial Bell polynomials and allows one to compute
the integral formula of the averaged functions at any order. The last algorithm
is based on the aforementioned algorithms and determines the exact expressions
of the averaged functions for the considered differential systems. The
implementation of our algorithms is discussed and evaluated using several
examples. The experimental results have extended the existing relevant results
for certain classes of differential systems.
",['\nBo Huang\n'],arXiv admin note: text overlap with arXiv:1905.03315,,http://arxiv.org/abs/2005.03487v2,cs.SC,"['cs.SC', 'math.DS']",,,[]
Subquadratic-Time Algorithms for Normal Bases,http://arxiv.org/abs/2005.03497v2,2020-05-05T19:54:39Z,2020-12-23T18:12:53Z,"  For any finite Galois field extension $\mathsf{K}/\mathsf{F}$, with Galois
group $G = \mathrm{Gal}(\mathsf{K}/\mathsf{F})$, there exists an element
$\alpha \in \mathsf{K}$ whose orbit $G\cdot\alpha$ forms an $\mathsf{F}$-basis
of $\mathsf{K}$. Such a $\alpha$ is called a normal element and $G\cdot\alpha$
is a normal basis. We introduce a probabilistic algorithm for testing whether a
given $\alpha \in \mathsf{K}$ is normal, when $G$ is either a finite abelian or
a metacyclic group. The algorithm is based on the fact that deciding whether
$\alpha$ is normal can be reduced to deciding whether $\sum_{g \in G}
g(\alpha)g \in \mathsf{K}[G]$ is invertible; it requires a slightly
subquadratic number of operations. Once we know that $\alpha$ is normal, we
show how to perform conversions between the power basis of
$\mathsf{K}/\mathsf{F}$ and the normal basis with the same asymptotic cost.
","['\nMark Giesbrecht\n', '\nArmin Jamshidpey\n', '\nÉric Schost\n']",arXiv admin note: substantial text overlap with arXiv:1903.03278,,http://arxiv.org/abs/2005.03497v2,cs.SC,"['cs.SC', 'cs.CC', '12Y05, 12F10, 11y16, 68Q25']",,,[]
Iterative Variable Reordering: Taming Huge System Families,http://arxiv.org/abs/2004.13287v1,2020-04-28T04:23:14Z,2020-04-28T04:23:14Z,"  For the verification of systems using model-checking techniques, symbolic
representations based on binary decision diagrams (BDDs) often help to tackle
the well-known state-space explosion problem. Symbolic BDD-based
representations have been also shown to be successful for the analysis of
families of systems that arise, e.g., through configurable parameters or
following the feature-oriented modeling approach. The state space of such
system families face an additional exponential blowup in the number of
parameters or features. It is well known that the order of variables in ordered
BDDs is crucial for the size of the model representation. Especially for
automatically generated models from real-world systems, family models might
even be not constructible due to bad variable orders. In this paper we describe
a technique, called iterative variable reordering, that can enable the
construction of large-scale family models. We exemplify feasibility of our
approach by means of an aircraft velocity control system with redundancy
mechanisms modeled in the input language of the probabilistic model checker
PRISM. We show that standard reordering and dynamic reordering techniques fail
to construct the family model due to memory and time constraints, respectively,
while the new iterative approach succeeds to generate a symbolic family model.
","['\nClemens Dubslaff\n', '\nAndrey Morozov\n', '\nChristel Baier\n', '\nKlaus Janschek\n']","In Proceedings MARS 2020, arXiv:2004.12403","EPTCS 316, 2020, pp. 121-133",http://dx.doi.org/10.4204/EPTCS.316.5,cs.LO,"['cs.LO', 'cs.PF', 'cs.SC']",10.4204/EPTCS.316.5,,[]
It is Time for New Perspectives on How to Fight Bloat in GP,http://arxiv.org/abs/2005.00603v1,2020-05-01T20:59:24Z,2020-05-01T20:59:24Z,"  The present and future of evolutionary algorithms depends on the proper use
of modern parallel and distributed computing infrastructures. Although still
sequential approaches dominate the landscape, available multi-core, many-core
and distributed systems will make users and researchers to more frequently
deploy parallel version of the algorithms. In such a scenario, new
possibilities arise regarding the time saved when parallel evaluation of
individuals are performed. And this time saving is particularly relevant in
Genetic Programming. This paper studies how evaluation time influences not only
time to solution in parallel/distributed systems, but may also affect size
evolution of individuals in the population, and eventually will reduce the
bloat phenomenon GP features. This paper considers time and space as two sides
of a single coin when devising a more natural method for fighting bloat. This
new perspective allows us to understand that new methods for bloat control can
be derived, and the first of such a method is described and tested.
Experimental data confirms the strength of the approach: using computing time
as a measure of individuals' complexity allows to control the growth in size of
genetic programming individuals.
","['\nFrancisco Fernández de Vega\n', '\nGustavo Olague\n', '\nFrancisco Chávez\n', '\nDaniel Lanza\n', '\nWolfgang Banzhaf\n', '\nErik Goodman\n']",,"Genetic Programming Theory and Practice XVII, 8 May 2020",http://dx.doi.org/10.1007/978-3-030-39958-0_2,cs.NE,"['cs.NE', 'cs.AI', 'cs.SC']",10.1007/978-3-030-39958-0_2,,[]
"Probing the Natural Language Inference Task with Automated Reasoning
  Tools",http://arxiv.org/abs/2005.02573v1,2020-05-06T03:18:11Z,2020-05-06T03:18:11Z,"  The Natural Language Inference (NLI) task is an important task in modern NLP,
as it asks a broad question to which many other tasks may be reducible: Given a
pair of sentences, does the first entail the second? Although the
state-of-the-art on current benchmark datasets for NLI are deep learning-based,
it is worthwhile to use other techniques to examine the logical structure of
the NLI task. We do so by testing how well a machine-oriented controlled
natural language (Attempto Controlled English) can be used to parse NLI
sentences, and how well automated theorem provers can reason over the resulting
formulae. To improve performance, we develop a set of syntactic and semantic
transformation rules. We report their performance, and discuss implications for
NLI and logic-based NLP.
","['\nZaid Marji\n', '\nAnimesh Nighojkar\n', '\nJohn Licato\n']","Accepted to Proceedings of The 33rd International Florida Artificial
  Intelligence Research Society Conference (FLAIRS-33, 2020)",,http://arxiv.org/abs/2005.02573v1,cs.AI,"['cs.AI', 'cs.CL', 'cs.SC']",,,[]
"Towards Concise, Machine-discovered Proofs of Gödel's Two
  Incompleteness Theorems",http://arxiv.org/abs/2005.02576v1,2020-05-06T03:29:34Z,2020-05-06T03:29:34Z,"  There is an increasing interest in applying recent advances in AI to
automated reasoning, as it may provide useful heuristics in reasoning over
formalisms in first-order, second-order, or even meta-logics. To facilitate
this research, we present MATR, a new framework for automated theorem proving
explicitly designed to easily adapt to unusual logics or integrate new
reasoning processes. MATR is formalism-agnostic, highly modular, and
programmer-friendly. We explain the high-level design of MATR as well as some
details of its implementation. To demonstrate MATR's utility, we then describe
a formalized metalogic suitable for proofs of G\""odel's Incompleteness
Theorems, and report on our progress using our metalogic in MATR to
semi-autonomously generate proofs of both the First and Second Incompleteness
Theorems.
","['\nElijah Malaby\n', '\nBradley Dragun\n', '\nJohn Licato\n']",,"In Proceedings of The 2020 International Florida Artificial
  Intelligence Research Society Conference (FLAIRS-33)",http://arxiv.org/abs/2005.02576v1,cs.LO,"['cs.LO', 'cs.AI', 'cs.SC']",,,[]
Learning selection strategies in Buchberger's algorithm,http://arxiv.org/abs/2005.01917v3,2020-05-05T02:27:00Z,2020-08-17T22:01:54Z,"  Studying the set of exact solutions of a system of polynomial equations
largely depends on a single iterative algorithm, known as Buchberger's
algorithm. Optimized versions of this algorithm are crucial for many computer
algebra systems (e.g., Mathematica, Maple, Sage). We introduce a new approach
to Buchberger's algorithm that uses reinforcement learning agents to perform
S-pair selection, a key step in the algorithm. We then study how the difficulty
of the problem depends on the choices of domain and distribution of
polynomials, about which little is known. Finally, we train a policy model
using proximal policy optimization (PPO) to learn S-pair selection strategies
for random systems of binomial equations. In certain domains, the trained model
outperforms state-of-the-art selection heuristics in total number of polynomial
additions performed, which provides a proof-of-concept that recent developments
in machine learning have the potential to improve performance of algorithms in
symbolic computation.
","['\nDylan Peifer\n', '\nMichael Stillman\n', '\nDaniel Halpern-Leistner\n']","14 pages, minor typo and format fixes, to appear in Proceedings of
  the 37th International Conference on Machine Learning (ICML 2020)",,http://arxiv.org/abs/2005.01917v3,cs.LG,"['cs.LG', 'cs.SC', 'math.AC', 'math.AG', 'stat.ML']",,,[]
Efficient Exact Verification of Binarized Neural Networks,http://arxiv.org/abs/2005.03597v2,2020-05-07T16:34:30Z,2020-10-27T04:00:16Z,"  Concerned with the reliability of neural networks, researchers have developed
verification techniques to prove their robustness. Most verifiers work with
real-valued networks. Unfortunately, the exact (complete and sound) verifiers
face scalability challenges and provide no correctness guarantees due to
floating point errors. We argue that Binarized Neural Networks (BNNs) provide
comparable robustness and allow exact and significantly more efficient
verification. We present a new system, EEV, for efficient and exact
verification of BNNs. EEV consists of two parts: (i) a novel SAT solver that
speeds up BNN verification by natively handling the reified cardinality
constraints arising in BNN encodings; and (ii) strategies to train
solver-friendly robust BNNs by inducing balanced layer-wise sparsity and low
cardinality bounds, and adaptively cancelling the gradients. We demonstrate the
effectiveness of EEV by presenting the first exact verification results for
L-inf-bounded adversarial robustness of nontrivial convolutional BNNs on the
MNIST and CIFAR10 datasets. Compared to exact verification of real-valued
networks of the same architectures on the same tasks, EEV verifies BNNs
hundreds to thousands of times faster, while delivering comparable verifiable
accuracy in most cases.
","['\nKai Jia\n', '\nMartin Rinard\n']",To be published in NeurIPS 2020,,http://arxiv.org/abs/2005.03597v2,cs.AI,"['cs.AI', 'cs.CV', 'cs.LG', 'cs.LO', 'cs.SC']",,,[]
"A practical approach to testing random number generators in computer
  algebra systems",http://arxiv.org/abs/2004.08913v1,2020-04-19T17:19:19Z,2020-04-19T17:19:19Z,"  This paper has a practical aim. For a long time, implementations of
pseudorandom number generators in standard libraries of programming languages
had poor quality. The situation started to improve only recently. Up to now, a
large number of libraries and weakly supported mathematical packages use
outdated algorithms for random number generation. Four modern sets of
statistical tests that can be used for verifying random number generators are
described. It is proposed to use command line utilities, which makes it
possible to avoid low-level programming in such languages as C or C++. Only
free open source systems are considered.
","['\nMigran N. Gevorkyan\n', '\nDmitry S. Kulyabov\n', '\nAnastasia V. Demidova\n', '\nAnna V. Korolkova\n']","in English, in Russian",,http://dx.doi.org/10.1134/S096554252001008X,cs.MS,"['cs.MS', 'cs.SC']",10.1134/S096554252001008X,,[]
High performance SIMD modular arithmetic for polynomial evaluation,http://arxiv.org/abs/2004.11571v1,2020-04-24T07:28:45Z,2020-04-24T07:28:45Z,"  Two essential problems in Computer Algebra, namely polynomial factorization
and polynomial greatest common divisor computation, can be efficiently solved
thanks to multiple polynomial evaluations in two variables using modular
arithmetic. In this article, we focus on the efficient computation of such
polynomial evaluations on one single CPU core. We first show how to leverage
SIMD computing for modular arithmetic on AVX2 and AVX-512 units, using both
intrinsics and OpenMP compiler directives. Then we manage to increase the
operational intensity and to exploit instruction-level parallelism in order to
increase the compute efficiency of these polynomial evaluations. All this
results in the end to performance gains up to about 5x on AVX2 and 10x on
AVX-512.
","['\nPierre Fortin\nLIP6\n', '\nAmbroise Fleury\nLIFL\n', '\nFrançois Lemaire\nLIFL\n', '\nMichael Monagan\n']",,,http://arxiv.org/abs/2004.11571v1,cs.DC,"['cs.DC', 'cs.SC']",,,"['LIP6', 'LIFL', 'LIFL']"
A case study for $ζ(4)$,http://arxiv.org/abs/2004.08158v2,2020-04-17T10:29:49Z,2020-09-23T15:19:32Z,"  Using symbolic summation tools in the setting of difference rings, we prove a
two-parametric identity that relates rational approximations to $\zeta(4)$.
","['\nCarsten Schneider\n', '\nWadim Zudilin\n']",13 pages,"in: Transcendence in Algebra, Combinatorics, Geometry and Number
  Theory, A. Bostan and K. Raschel (eds.), Springer Proceedings in Mathematics
  & Statistics 373 (2021), 421--435",http://dx.doi.org/10.1007/978-3-030-84304-5_17,math.NT,"['math.NT', 'cs.SC', 'math.CO']",10.1007/978-3-030-84304-5_17,,[]
"An Efficient Method for Computing Liouvillian First Integrals of Planar
  Polynomial Vector Fields",http://arxiv.org/abs/2004.09298v1,2020-04-20T13:56:45Z,2020-04-20T13:56:45Z,"  Here we present an efficient method to compute Darboux polynomials for
polynomial vector fields in the plane. This approach is restricetd to
polynomial vector fields presenting a Liouvillian first integral (or,
equivalently, to rational first order differential equations (rational 1ODEs)
presenting a Liouvillian general solution). The key to obtaining this method
was to separate the procedure of solving the (nonlinear) algebraic systems
resulting from the equation that translates the condition of existence of a
Darboux polynomial into feasible steos (procedures that requires less memory
consumption). We also present a brief performance analysis of the algorithms
developed.
","['\nL. G. S. Duarte\n', '\nL. A. C. P. da Mota\n']",,"Journal of Differential Equations Volume 300, 5 November 2021,
  Pages 356-385",http://dx.doi.org/10.1016/j.jde.2021.07.045,math-ph,"['math-ph', 'cs.SC', 'math.MP']",10.1016/j.jde.2021.07.045,,[]
The Imandra Automated Reasoning System (system description),http://arxiv.org/abs/2004.10263v1,2020-04-21T19:57:34Z,2020-04-21T19:57:34Z,"  We describe Imandra, a modern computational logic theorem prover designed to
bridge the gap between decision procedures such as SMT, semi-automatic
inductive provers of the Boyer-Moore family like ACL2, and interactive proof
assistants for typed higher-order logics. Imandra's logic is computational,
based on a pure subset of OCaml in which all functions are terminating, with
restrictions on types and higher-order functions that allow conjectures to be
translated into multi-sorted first-order logic with theories, including
arithmetic and datatypes. Imandra has novel features supporting large-scale
industrial applications, including a seamless integration of bounded and
unbounded verification, first-class computable counterexamples, efficiently
executable models and a cloud-native architecture supporting live multiuser
collaboration.
  The core reasoning mechanisms of Imandra are (i) a semi-complete procedure
for finding models of formulas in the logic mentioned above, centered around
the lazy expansion of recursive functions, and (ii) an inductive waterfall and
simplifier which ""lifts"" many Boyer-Moore ideas to our typed higher-order
setting.
  These mechanisms are tightly integrated and subject to many forms of user
control. Imandra's user interfaces include an interactive toplevel, Jupyter
notebooks and asynchronous document-based verification (in the spirit of
Isabelle's Prover IDE) with VS Code.
","['\nGrant Olney Passmore\n', '\nSimon Cruanes\n', '\nDenis Ignatovich\n', '\nDave Aitken\n', '\nMatt Bray\n', '\nElijah Kagan\n', '\nKostya Kanishev\n', '\nEwen Maclean\n', '\nNicola Mometto\n']","To appear in Proceedings of The International Joint Conference on
  Automated Reasoning (IJCAR) 2020, Lecture Notes in Artificial Intelligence,
  Springer-Verlag",,http://arxiv.org/abs/2004.10263v1,cs.LO,"['cs.LO', 'cs.AI', 'cs.PL', 'cs.SC', 'I.2.3; F.3.1; I.2.5; F.4.1']",,,[]
GAPS: Generator for Automatic Polynomial Solvers,http://arxiv.org/abs/2004.11765v1,2020-04-24T14:11:28Z,2020-04-24T14:11:28Z,"  Minimal problems in computer vision raise the demand of generating efficient
automatic solvers for polynomial equation systems. Given a polynomial system
repeated with different coefficient instances, the traditional Gr\""obner basis
or normal form based solution is very inefficient. Fortunately the Gr\""obner
basis of a same polynomial system with different coefficients is found to share
consistent inner structure. By precomputing such structures offline, Gr\""obner
basis as well as the polynomial system solutions can be solved automatically
and efficiently online. In the past decade, several tools have been released to
generate automatic solvers for a general minimal problems. The most recent tool
autogen from Larsson et al. is a representative of these tools with
state-of-the-art performance in solver efficiency. GAPS wraps and improves
autogen with more user-friendly interface, more functionality and better
stability. We demonstrate in this report the main approach and enhancement
features of GAPS. A short tutorial of the software is also included.
","['\nBo Li\n', '\nViktor Larsson\n']",,,http://arxiv.org/abs/2004.11765v1,cs.CV,"['cs.CV', 'cs.MS', 'cs.RO', 'cs.SC']",,,[]
"An Abstraction-guided Approach to Scalable and Rigorous Floating-Point
  Error Analysis",http://arxiv.org/abs/2004.11960v3,2020-04-24T19:42:33Z,2020-07-02T01:45:35Z,"  Automated techniques for rigorous floating-point round-off error analysis are
important in areas including formal verification of correctness and precision
tuning. Existing tools and techniques, while providing tight bounds, fail to
analyze expressions with more than a few hundred operators, thus unable to
cover important practical problems. In this work, we present Satire, a new tool
that sheds light on how scalability and bound-tightness can be attained through
a combination of incremental analysis, abstraction, and judicious use of
concrete and symbolic evaluation. Satire has handled problems exceeding 200K
operators. We present Satire's underlying error analysis approach,
information-theoretic abstraction heuristics, and a wide range of case studies,
with evaluation covering FFT, Lorenz system of equations, and various PDE
stencil types. Our results demonstrate the tightness of Satire's bounds, its
acceptable runtime, and valuable insights provided.
","['\nArnab Das\n', '\nIan Briggs\n', '\nGanesh Gopalakrishnan\n', '\nPavel Panchekha\n', '\nSriram Krishnamoorthy\n']","A more informative and updated version of this paper has been
  accepted for publication at SuperComputing 2020",,http://arxiv.org/abs/2004.11960v3,cs.PL,"['cs.PL', 'cs.NA', 'cs.SC', 'math.NA']",,,[]
"CLUE: Exact maximal reduction of kinetic models by constrained lumping
  of differential equations",http://arxiv.org/abs/2004.11961v2,2020-04-24T19:42:51Z,2020-12-15T02:09:43Z,"  Motivation: Detailed mechanistic models of biological processes can pose
significant challenges for analysis and parameter estimations due to the large
number of equations used to track the dynamics of all distinct configurations
in which each involved biochemical species can be found. Model reduction can
help tame such complexity by providing a lower-dimensional model in which each
macro-variable can be directly related to the original variables.
  Results: We present CLUE, an algorithm for exact model reduction of systems
of polynomial differential equations by constrained linear lumping. It computes
the smallest dimensional reduction as a linear mapping of the state space such
that the reduced model preserves the dynamics of user-specified linear
combinations of the original variables. Even though CLUE works with nonlinear
differential equations, it is based on linear algebra tools, which makes it
applicable to high-dimensional models. Using case studies from the literature,
we show how CLUE can substantially lower model dimensionality and help extract
biologically intelligible insights from the reduction.
  Availability: An implementation of the algorithm and relevant resources to
replicate the experiments herein reported are freely available for download at
https://github.com/pogudingleb/CLUE.
  Supplementary information: enclosed.
","['\nAlexey Ovchinnikov\n', '\nIsabel Cristina Pérez Verona\n', '\nGleb Pogudin\n', '\nMirco Tribastone\n']",,,http://arxiv.org/abs/2004.11961v2,q-bio.MN,"['q-bio.MN', 'cs.SC', 'cs.SY', 'eess.SY']",,,[]
Algorithms yield upper bounds in differential algebra,http://arxiv.org/abs/2005.01608v2,2020-04-22T02:26:06Z,2021-08-28T19:53:14Z,"  Consider an algorithm computing in a differential field with several
commuting derivations such that the only operations it performs with the
elements of the field are arithmetic operations, differentiation, and zero
testing. We show that, if the algorithm is guaranteed to terminate on every
input, then there is a computable upper bound for the size of the output of the
algorithm in terms of the size of the input. We also generalize this to
algorithms working with models of good enough theories (including for example,
difference fields).
  We then apply this to differential algebraic geometry to show that there
exists a computable uniform upper bound for the number of components of any
variety defined by a system of polynomial PDEs. We then use this bound to show
the existence of a computable uniform upper bound for the elimination problem
in systems of polynomial PDEs with delays.
","['\nWei Li\n', '\nAlexey Ovchinnikov\n', '\nGleb Pogudin\n', '\nThomas Scanlon\n']",,,http://arxiv.org/abs/2005.01608v2,math.AC,"['math.AC', 'cs.SC', 'math.AG', 'math.LO', '12H05, 12H10, 03C10, 03C60, 03D15']",,,[]
Resultants over principal Artinian rings,http://arxiv.org/abs/2004.03341v1,2020-04-07T13:17:38Z,2020-04-07T13:17:38Z,"  The resultant of two univariate polynomials is an invariant of great
importance in commutative algebra and vastly used in computer algebra systems.
Here we present an algorithm to compute it over Artinian principal rings with a
modified version of the Euclidean algorithm. Using the same strategy, we show
how the reduced resultant and a pair of B\'ezout coefficient can be computed.
Particular attention is devoted to the special case of
$\mathbf{Z}/n\mathbf{Z}$, where we perform a detailed analysis of the
asymptotic cost of the algorithm. Finally, we illustrate how the algorithms can
be exploited to improve ideal arithmetic in number fields and polynomial
arithmetic over $p$-adic fields.
","['\nClaus Fieker\n', '\nTommy Hofmann\n', '\nCarlo Sircana\n']",,,http://arxiv.org/abs/2004.03341v1,cs.SC,"['cs.SC', 'math.AC']",,,[]
New Opportunities for the Formal Proof of Computational Real Geometry?,http://arxiv.org/abs/2004.04034v1,2020-04-08T15:04:18Z,2020-04-08T15:04:18Z,"  The purpose of this paper is to explore the question ""to what extent could we
produce formal, machine-verifiable, proofs in real algebraic geometry?"" The
question has been asked before but as yet the leading algorithms for answering
such questions have not been formalised. We present a thesis that a new
algorithm for ascertaining satisfiability of formulae over the reals via
Cylindrical Algebraic Coverings [\'{A}brah\'{a}m, Davenport, England, Kremer,
\emph{Deciding the Consistency of Non-Linear Real Arithmetic Constraints with a
Conflict Driver Search Using Cylindrical Algebraic Coverings}, 2020] might
provide trace and outputs that allow the results to be more susceptible to
machine verification than those of competing algorithms.
","['\nErika {Á}brahám\n', '\nJames Davenport\n', '\nMatthew England\n', '\nGereon Kremer\n', '\nZak Tonks\n']",,"Proceedings of the 5th Workshop on Satisfiability Checking and
  Symbolic Computation (SC2 '20), CEUR Workshop Proceedings 2752, pp. 178-188,
  2020",http://arxiv.org/abs/2004.04034v1,cs.SC,"['cs.SC', 'cs.LO']",,,[]
Neural Analogical Matching,http://arxiv.org/abs/2004.03573v5,2020-04-07T17:50:52Z,2020-12-15T16:59:33Z,"  Analogy is core to human cognition. It allows us to solve problems based on
prior experience, it governs the way we conceptualize new information, and it
even influences our visual perception. The importance of analogy to humans has
made it an active area of research in the broader field of artificial
intelligence, resulting in data-efficient models that learn and reason in
human-like ways. While cognitive perspectives of analogy and deep learning have
generally been studied independently of one another, the integration of the two
lines of research is a promising step towards more robust and efficient
learning techniques. As part of a growing body of research on such an
integration, we introduce the Analogical Matching Network: a neural
architecture that learns to produce analogies between structured, symbolic
representations that are largely consistent with the principles of
Structure-Mapping Theory.
","['\nMaxwell Crouse\n', '\nConstantine Nakos\n', '\nIbrahim Abdelaziz\n', '\nKenneth Forbus\n']",AAAI version,,http://arxiv.org/abs/2004.03573v5,cs.AI,"['cs.AI', 'cs.NE', 'cs.SC']",,,[]
"A Simple Method for Computing Some Pseudo-Elliptic Integrals in Terms of
  Elementary Functions",http://arxiv.org/abs/2004.04910v3,2020-04-10T05:04:15Z,2020-09-24T12:12:03Z,"  We introduce a method for computing some pseudo-elliptic integrals in terms
of elementary functions. The method is simple and fast in comparison to the
algebraic case of the Risch-Trager-Bronstein algorithm. This method can quickly
solve many pseudo-elliptic integrals, which other well-known computer algebra
systems either fail, return an answer in terms of special functions, or require
more than 20 seconds of computing time. Randomised tests showed our method
solved 73.4% of the integrals that could be solved with the best implementation
of the Risch-Trager-Bronstein algorithm. Unlike the symbolic integration
algorithms of Risch, Davenport, Trager, Bronstein and Miller; our method is not
a decision process. The implementation of this method is less than 200 lines of
Mathematica code and can be easily ported to other CAS that can solve systems
of polynomial equations.
",['\nSam Blake\n'],,,http://arxiv.org/abs/2004.04910v3,cs.SC,"['cs.SC', 'cs.IT', 'math.IT']",,,[]
"Generic bivariate multi-point evaluation, interpolation and modular
  composition with precomputation",http://arxiv.org/abs/2003.12468v2,2020-03-27T15:26:25Z,2020-06-04T10:09:24Z,"  Suppose $\mathbb{K}$ is a large enough field and $\mathcal{P} \subset
\mathbb{K}^2$ is a fixed, generic set of points which is available for
precomputation. We introduce a technique called \emph{reshaping} which allows
us to design quasi-linear algorithms for both: computing the evaluations of an
input polynomial $f \in \mathbb{K}[x,y]$ at all points of $\mathcal{P}$; and
computing an interpolant $f \in \mathbb{K}[x,y]$ which takes prescribed values
on $\mathcal{P}$ and satisfies an input $y$-degree bound. Our genericity
assumption is explicit and we prove that it holds for most point sets over a
large enough field. If $\mathcal{P}$ violates the assumption, our algorithms
still work and the performance degrades smoothly according to a distance from
being generic. To show that the reshaping technique may have an impact on other
related problems, we apply it to modular composition: suppose generic
polynomials $M \in \mathbb{K}[x]$ and $A \in \mathbb{K}[x]$ are available for
precomputation, then given an input $f \in \mathbb{K}[x,y]$ we show how to
compute $f(x, A(x)) \operatorname{rem} M(x)$ in quasi-linear time.
","['\nVincent Neiger\n', '\nJohan Rosenkilde\n', '\nGrigory Solomatov\n']","ISSAC 2020. 8 pages, 7 algorithms",,http://dx.doi.org/10.1145/3373207.3404032,cs.SC,['cs.SC'],10.1145/3373207.3404032,,[]
Fast Encoding of AG Codes over $C_{ab}$ Curves,http://arxiv.org/abs/2003.13333v2,2020-03-30T10:56:36Z,2020-08-18T08:34:35Z,"  We investigate algorithms for encoding of one-point algebraic geometry (AG)
codes over certain plane curves called $C_{ab}$ curves, as well as algorithms
for inverting the encoding map, which we call ""unencoding"". Some $C_{ab}$
curves have many points or are even maximal, e.g. the Hermitian curve. Our
encoding resp. unencoding algorithms have complexity $\tilde{O}(n^{3/2})$ resp.
$\tilde{O}(qn)$ for AG codes over any $C_{ab}$ curve satisfying very mild
assumptions, where $n$ is the code length and $q$ the base field size, and
$\tilde{O}$ ignores constants and logarithmic factors in the estimate. For
codes over curves whose evaluation points lie on a grid-like structure, notably
the Hermitian curve and norm-trace curves, we show that our algorithms have
quasi-linear time complexity $\tilde{O}(n)$ for both operations. For infinite
families of curves whose number of points is a constant factor away from the
Hasse--Weil bound, our encoding algorithm has complexity $\tilde{O}(n^{5/4})$
while unencoding has $\tilde{O}(n^{3/2})$.
","['\nPeter Beelen\n', '\nJohan Rosenkilde\n', '\nGrigory Solomatov\n']",,,http://arxiv.org/abs/2003.13333v2,math.AG,"['math.AG', 'cs.IT', 'cs.SC', 'math.IT']",,,[]
"Parallel Computation of tropical varieties, their positive part, and
  tropical Grassmannians",http://arxiv.org/abs/2003.13752v1,2020-03-30T19:01:17Z,2020-03-30T19:01:17Z,"  In this article, we present a massively parallel framework for computing
tropicalizations of algebraic varieties which can make use of finite
symmetries. We compute the tropical Grassmannian TGr$_0(3,8)$, and show that it
refines the $15$-dimensional skeleton of the Dressian Dr$(3,8)$ with the
exception of $23$ special cones for which we construct explicit obstructions to
the realizability of their tropical linear spaces. Moreover, we propose
algorithms for identifying maximal-dimensional tropical cones which belong to
the positive tropicalization. These algorithms exploit symmetries of the
tropical variety even though the positive tropicalization need not be
symmetric. We compute the maximal-dimensional cones of the positive
Grassmannian TGr$^+(3,8)$ and compare them to the cluster complex of the
classical Grassmannian Gr$(3,8)$.
","['\nDominik Bendle\n', '\nJanko Boehm\n', '\nYue Ren\n', '\nBenjamin Schröter\n']","32 pages, 9 figures",,http://arxiv.org/abs/2003.13752v1,math.AG,"['math.AG', 'cs.SC', 'math.CO', '14T15, 68W10, 68W30, 14Q15, 14M15, 52B15']",,,[]
"Stream/block ciphers, difference equations and algebraic attacks",http://arxiv.org/abs/2003.14215v2,2020-03-28T18:40:13Z,2021-08-23T10:00:16Z,"  In this paper we model a class of stream and block ciphers as systems of
(ordinary) explicit difference equations over a finite field. We call this
class ""difference ciphers"" and we show that ciphers of application interest, as
for example systems of LFSRs with a combiner, Trivium and Keeloq, belong to the
class. By using Difference Algebra, that is, the formal theory of difference
equations, we can properly define and study important properties of these
ciphers, such as their invertibility and periodicity. We describe then general
cryptanalytic methods for difference ciphers that follow from these properties
and are useful to assess the security. We illustrate such algebraic attacks in
practice by means of the ciphers Bivium and Keeloq.
","['\nRoberto La Scala\n', '\nSharwan K. Tiwari\n']","26 pages, to appear in Journal of Symbolic Computation",,http://arxiv.org/abs/2003.14215v2,cs.CR,"['cs.CR', 'cs.SC', 'math.AC', 'math.RA']",,,[]
"Interpolation of Dense and Sparse Rational Functions and other
  Improvements in $\texttt{FireFly}$",http://arxiv.org/abs/2004.01463v2,2020-04-03T10:40:08Z,2021-05-03T18:02:27Z,"  We present the main improvements and new features in version $\texttt{2.0}$
of the open-source $\texttt{C++}$ library $\texttt{FireFly}$ for the
interpolation of rational functions. This includes algorithmic improvements,
e.g. a hybrid algorithm for dense and sparse rational functions and an
algorithm to identify and remove univariate factors. The new version is applied
to a Feynman-integral reduction to showcase the runtime improvements achieved.
Moreover, $\texttt{FireFly}$ now supports parallelization with $\texttt{MPI}$
and offers new tools like a parser for expressions or an executable for the
insertion of replacement tables.
","['\nJonas Klappert\n', '\nSven Yannick Klein\n', '\nFabian Lange\n']","28 pages, 10 tables, 1 figure",Comput. Phys. Commun. 264 (2021) 107968,http://dx.doi.org/10.1016/j.cpc.2021.107968,cs.MS,"['cs.MS', 'cs.SC', 'hep-ph']",10.1016/j.cpc.2021.107968,,[]
Epistemic Phase Transitions in Mathematical Proofs,http://arxiv.org/abs/2004.00055v2,2020-03-31T18:39:56Z,2022-04-12T15:25:22Z,"  Mathematical proofs are both paradigms of certainty and some of the most
explicitly-justified arguments that we have in the cultural record. Their very
explicitness, however, leads to a paradox, because the probability of error
grows exponentially as the argument expands. When a mathematician encounters a
proof, how does she come to believe it? Here we show that, under a
cognitively-plausible belief formation mechanism combining deductive and
abductive reasoning, belief in mathematical arguments can undergo what we call
an epistemic phase transition: a dramatic and rapidly-propagating jump from
uncertainty to near-complete confidence at reasonable levels of claim-to-claim
error rates. To show this, we analyze an unusual dataset of forty-eight
machine-aided proofs from the formalized reasoning system Coq, including major
theorems ranging from ancient to 21st Century mathematics, along with five
hand-constructed cases including Euclid, Apollonius, Hernstein's Topics in
Algebra, and Andrew Wiles's proof of Fermat's Last Theorem. Our results bear
both on recent work in the history and philosophy of mathematics on how we
understand proofs, and on a question, basic to cognitive science, of how we
justify complex beliefs.
","['\nScott Viteri\n', '\nSimon DeDeo\n']","22 pages, 5 figures. Matches published version. Supplementary
  information available at
  https://www.sciencedirect.com/science/article/pii/S0010027722001081","Cognition, 225, 105120 (2022)",http://dx.doi.org/10.1016/j.cognition.2022.105120,cs.SC,"['cs.SC', 'cs.AI', 'math.HO', 'physics.soc-ph', 'q-bio.NC']",10.1016/j.cognition.2022.105120,,[]
"An Algorithm for Computing a Minimal Comprehensive Gröbner\, Basis of
  a Parametric Polynomial System",http://arxiv.org/abs/2003.07957v1,2020-03-17T21:43:18Z,2020-03-17T21:43:18Z,"  An algorithm to generate a minimal comprehensive Gr\""obner\, basis of a
parametric polynomial system from an arbitrary faithful comprehensive
Gr\""obner\, system is presented. A basis of a parametric polynomial ideal is a
comprehensive Gr\""obner\, basis if and only if for every specialization of
parameters in a given field, the specialization of the basis is a Gr\""obner\,
basis of the associated specialized polynomial ideal. The key idea used in
ensuring minimality is that of a polynomial being essential with respect to a
comprehensive Gr\""obner\, basis. The essentiality check is performed by
determining whether a polynomial can be covered for various specializations by
other polynomials in the associated branches in a comprehensive Gr\""obner\,
system. The algorithm has been implemented and successfully tried on many
examples from the literature.
","['\nDeepak Kapur\n', '\nYiming Yang\n']",8 pages,,http://arxiv.org/abs/2003.07957v1,cs.SC,['cs.SC'],,,[]
"Moment State Dynamical Systems for Nonlinear Chance-Constrained Motion
  Planning",http://arxiv.org/abs/2003.10379v2,2020-03-23T16:48:55Z,2020-03-28T19:02:59Z,"  Chance-constrained motion planning requires uncertainty in dynamics to be
propagated into uncertainty in state. When nonlinear models are used, Gaussian
assumptions on the state distribution do not necessarily apply since almost all
random variables propagated through nonlinear dynamics results in non-Gaussian
state distributions. To address this, recent works have developed moment-based
approaches for enforcing chance-constraints on non-Gaussian state
distributions. However, there still lacks fast and accurate moment propagation
methods to determine the necessary statistical moments of these state
distributions. To address this gap, we present a framework that, given a
stochastic dynamical system, can algorithmically search for a new dynamical
system in terms of moment state that can be used to propagate moments of
disturbance random variables into moments of the state distribution. The key
algorithm, TreeRing, can be applied to a large class of nonlinear systems which
we refer to as trigonometric polynomial systems. As an example application, we
present a distributionally robust RRT (DR-RRT) algorithm that propagates
uncertainty through the nonlinear Dubin's car model without linearization.
","['\nAllen Wang\n', '\nAshkan Jasour\n', '\nBrian Williams\n']",,,http://arxiv.org/abs/2003.10379v2,eess.SY,"['eess.SY', 'cs.RO', 'cs.SC', 'cs.SY']",,,[]
"The Absent-Minded Passengers Problem: A Motivating Challenge Solved by
  Computer Algebra",http://arxiv.org/abs/2003.01921v2,2020-03-04T07:32:33Z,2020-06-24T09:48:42Z,"  In (S.B. Ekhad and D. Zeilberger, 2020) an exciting case study has been
initiated in which experimental mathematics and symbolic computation are
utilized to discover new properties concerning the so-called Absent-Minded
Passengers Problem. Based on these results, Doron Zeilberger raised some
challenging tasks to gain further probabilistic insight. In this note we report
on this enterprise. In particular, we demonstrate how the computer algebra
packages of RISC can be used to carry out the underlying heavy calculations.
",['\nCarsten Schneider\n'],"Removed various typos and inserted an extra link for a Mathematica
  notebook to repeat the (not-so-costly) calculations",,http://arxiv.org/abs/2003.01921v2,math.CO,"['math.CO', 'cs.SC']",,,[]
Entropy of tropical holonomic sequences,http://arxiv.org/abs/2003.05466v2,2020-03-11T18:08:11Z,2020-04-24T16:57:37Z,"  We introduce tropical holonomic sequences of a given order and calculate
their entropy in case of the second order.
",['\nDima Grigoriev\n'],,,http://arxiv.org/abs/2003.05466v2,math.AG,"['math.AG', 'cs.SC', '14T05']",,,[]
"Deciding the Consistency of Non-Linear Real Arithmetic Constraints with
  a Conflict Driven Search Using Cylindrical Algebraic Coverings",http://arxiv.org/abs/2003.05633v2,2020-03-12T06:02:48Z,2020-11-23T11:06:24Z,"  We present a new algorithm for determining the satisfiability of conjunctions
of non-linear polynomial constraints over the reals, which can be used as a
theory solver for satisfiability modulo theory (SMT) solving for non-linear
real arithmetic. The algorithm is a variant of Cylindrical Algebraic
Decomposition (CAD) adapted for satisfiability, where solution candidates
(sample points) are constructed incrementally, either until a satisfying sample
is found or sufficient samples have been sampled to conclude unsatisfiability.
The choice of samples is guided by the input constraints and previous
conflicts.
  The key idea behind our new approach is to start with a partial sample;
demonstrate that it cannot be extended to a full sample; and from the reasons
for that rule out a larger space around the partial sample, which build up
incrementally into a cylindrical algebraic covering of the space. There are
similarities with the incremental variant of CAD, the NLSAT method of Jovanovic
and de Moura, and the NuCAD algorithm of Brown; but we present worked examples
and experimental results on a preliminary implementation to demonstrate the
differences to these, and the benefits of the new approach.
","['\nErika Ábrahám\n', '\nJames H. Davenport\n', '\nMatthew England\n', '\nGereon Kremer\n']",,"Journal of Logical and Algebraic Methods in Programming, 119,
  Article Number 100633, Elsvier, 2021",http://dx.doi.org/10.1016/j.jlamp.2020.100633,cs.SC,"['cs.SC', 'cs.LO']",10.1016/j.jlamp.2020.100633,,[]
FunGrim: a symbolic library for special functions,http://arxiv.org/abs/2003.06181v1,2020-03-13T10:07:21Z,2020-03-13T10:07:21Z,"  We present the Mathematical Functions Grimoire (FunGrim), a website and
database of formulas and theorems for special functions. We also discuss the
symbolic computation library used as the backend and main development tool for
FunGrim, and the Grim formula language used in these projects to represent
mathematical content semantically.
",['\nFredrik Johansson\nLFANT\n'],,,http://arxiv.org/abs/2003.06181v1,cs.MS,"['cs.MS', 'cs.SC']",,,['LFANT']
Experimental Evaluation of a Method to Simplify Expressions,http://arxiv.org/abs/2003.06203v1,2020-03-13T11:12:19Z,2020-03-13T11:12:19Z,"  We present a method to simplify expressions in the context of an equational
theory. The basic ideas and concepts of the method have been presented
previously elsewhere but here we tackle the difficult task of making it
efficient in practice, in spite of its great generality. We first recall the
notion of a collection of structures, which allows us to manipulate very large
(possibly infinite) sets of terms as a whole, i.e., without enumerating their
elements. Then we use this tool to construct algorithms to simplify
expressions. We give various reasons why it is difficult to make these
algorithms precise and efficient. We then propose a number of approches to
solve the raised issues. Finally, and importantly, we provide a detailed
experimental evaluation of the method and a comparison of several variants of
it. Although the method is completely generic, we use (arbitrary, not only
two-level) boolean expressions as the application field for these experiments
because impressive simplifications can be obtained in spite of the hardness of
the problem.
",['\nBaudouin Le Charlier\n'],Paper rejected at IJCAR 2020,,http://arxiv.org/abs/2003.06203v1,cs.LO,"['cs.LO', 'cs.SC']",,,[]
Algorithm to enumerate superspecial Howe curves of genus $4$,http://arxiv.org/abs/2003.04153v1,2020-03-09T13:53:14Z,2020-03-09T13:53:14Z,"  A Howe curve is a curve of genus $4$ obtained as the fiber product over
$\mathbf{P}^1$ of two elliptic curves. Any Howe curve is canonical. This paper
provides an efficient algorithm to find superspecial Howe curves and that to
enumerate their isomorphism classes. We discuss not only an algorithm to test
the superspeciality but also an algorithm to test isomorphisms for Howe curves.
Our algorithms are much more efficient than conventional ones proposed by the
authors so far for general canonical curves. We show the existence of a
superspecial Howe curve in characteristic $7<p\le 331$ and enumerate the
isomorphism classes of superspecial Howe curves in characteristic $p\le 53$, by
executing our algorithms over the computer algebra system Magma.
","['\nMomonari Kudo\n', '\nShushi Harashita\n']","18 pages. Magma codes used to obtain the main results will be appear
  at the website of the first author","Proceedings of the Fourteenth Algorithmic Number Theory Symposium
  (ANTS-XIV), edited by Steven Galbraith, Open Book Series 4, Mathematical
  Sciences Publishers, Berkeley, 2020",http://dx.doi.org/10.2140/obs.2020.4.301,math.NT,"['math.NT', 'cs.SC', 'math.AG']",10.2140/obs.2020.4.301,,[]
Neuro-symbolic Architectures for Context Understanding,http://arxiv.org/abs/2003.04707v1,2020-03-09T15:04:07Z,2020-03-09T15:04:07Z,"  Computational context understanding refers to an agent's ability to fuse
disparate sources of information for decision-making and is, therefore,
generally regarded as a prerequisite for sophisticated machine reasoning
capabilities, such as in artificial intelligence (AI). Data-driven and
knowledge-driven methods are two classical techniques in the pursuit of such
machine sense-making capability. However, while data-driven methods seek to
model the statistical regularities of events by making observations in the
real-world, they remain difficult to interpret and they lack mechanisms for
naturally incorporating external knowledge. Conversely, knowledge-driven
methods, combine structured knowledge bases, perform symbolic reasoning based
on axiomatic principles, and are more interpretable in their inferential
processing; however, they often lack the ability to estimate the statistical
salience of an inference. To combat these issues, we propose the use of hybrid
AI methodology as a general framework for combining the strengths of both
approaches. Specifically, we inherit the concept of neuro-symbolism as a way of
using knowledge-bases to guide the learning progress of deep neural networks.
We further ground our discussion in two applications of neuro-symbolism and, in
both cases, show that our systems maintain interpretability while achieving
comparable performance, relative to the state-of-the-art.
","['\nAlessandro Oltramari\n', '\nJonathan Francis\n', '\nCory Henson\n', '\nKaixin Ma\n', '\nRuwan Wickramarachchi\n']","In: Ilaria Tiddi, Freddy Lecue, Pascal Hitzler (eds.), Knowledge
  Graphs for eXplainable AI -- Foundations, Applications and Challenges.
  Studies on the Semantic Web, IOS Press, Amsterdam, 2020. arXiv admin note:
  text overlap with arXiv:1910.14087",,http://arxiv.org/abs/2003.04707v1,cs.AI,"['cs.AI', 'cs.CL', 'cs.SC']",,,[]
"Transforming ODEs and PDEs with radical coefficients into rational
  coefficients",http://arxiv.org/abs/2003.06301v1,2020-03-13T13:53:51Z,2020-03-13T13:53:51Z,"  We present an algorithm that transforms, if possible, a given ODE or PDE with
radical function coefficients into one with rational coefficients by means of a
rational change of variables. It also applies to systems of linear ODEs. It is
based on previous work on reparametrization of radical algebraic varieties.
","['\nJorge Caravantes\n', '\nJ. Rafael Sendra\n', '\nDavid Sevilla\n', '\nCarlos Villarino\n']","14 pages, submitted to a journal",,http://arxiv.org/abs/2003.06301v1,math.CA,"['math.CA', 'cs.SC', 'math.AG']",,,[]
"Fast In-place Algorithms for Polynomial Operations: Division,
  Evaluation, Interpolation",http://arxiv.org/abs/2002.10304v3,2020-02-24T15:27:58Z,2020-06-09T13:04:27Z,"  We consider space-saving versions of several important operations on
univariate polynomials, namely power series inversion and division, division
with remainder, multi-point evaluation, and interpolation. Now-classical
results show that such problems can be solved in (nearly) the same asymptotic
time as fast polynomial multiplication. However, these reductions, even when
applied to an in-place variant of fast polynomial multiplication, yield
algorithms which require at least a linear amount of extra space for
intermediate results. We demonstrate new in-place algorithms for the
aforementioned polynomial computations which require only constant extra space
and achieve the same asymptotic running time as their out-of-place
counterparts. We also provide a precise complexity analysis so that all
constants are made explicit, parameterized by the space usage of the underlying
multiplication algorithms.
","['\nPascal Giorgi\n', '\nBruno Grenet\n', '\nDaniel S. Roche\n']",,"Proc. ISSAC'20, pp 210-217, ACM, 2020",http://dx.doi.org/10.1145/3373207.3404061,cs.SC,"['cs.SC', 'cs.CC']",10.1145/3373207.3404061,,[]
Space Efficient Representations of Finite Groups,http://arxiv.org/abs/2002.11391v1,2020-02-26T10:16:44Z,2020-02-26T10:16:44Z,"  The Cayley table representation of a group uses $\mathcal{O}(n^2)$ words for
a group of order $n$ and answers multiplication queries in time
$\mathcal{O}(1)$. It is interesting to ask if there is a $o(n^2)$ space
representation of groups that still has $\mathcal{O}(1)$ query-time. We show
that for any $\delta$, $\frac{1}{\log n} \le \delta \le 1$, there is an
$\mathcal{O}(\frac{n^{1 +\delta}}{\delta})$ space representation for groups of
order $n$ with $\mathcal{O}(\frac{1}{\delta})$ query-time.
  We also show that for Z-groups, simple groups and several group classes
defined in terms of semidirect product, there are linear space representations
with at most logarithmic query-time.
  Farzan and Munro (ISSAC'06) defined a model for group representation and gave
a succinct data structure for abelian groups with constant query-time. They
asked if their result can be extended to categorically larger group classes. We
construct data structures in their model for Hamiltonian groups and some other
classes of groups with constant query-time.
","['\nBireswar Das\n', '\nShivdutt Sharma\n', '\nP. R. Vaidyanathan\n']",,,http://arxiv.org/abs/2002.11391v1,cs.DS,"['cs.DS', 'cs.SC']",,,[]
"A Linear Algebra Approach for Detecting Binomiality of Steady State
  Ideals of Reversible Chemical Reaction Networks",http://arxiv.org/abs/2002.12693v2,2020-02-28T13:07:53Z,2020-06-15T17:03:58Z,"  Motivated by problems from Chemical Reaction Network Theory, we investigate
whether steady state ideals of reversible reaction networks are generated by
binomials. We take an algebraic approach considering, besides concentrations of
species, also rate constants as indeterminates. This leads us to the concept of
unconditional binomiality, meaning binomiality for all values of the rate
constants. This concept is different from conditional binomiality that applies
when rate constant values or relations among rate constants are given. We start
by representing the generators of a steady state ideal as sums of binomials,
which yields a corresponding coefficient matrix. On these grounds we propose an
efficient algorithm for detecting unconditional binomiality. That algorithm
uses exclusively elementary column and row operations on the coefficient
matrix. We prove asymptotic worst case upper bounds on the time complexity of
our algorithm. Furthermore, we experimentally compare its performance with
other existing methods.
","['\nHamid Rahkooy\n', '\nOvidiu Radulescu\n', '\nThomas Sturm\n']",,"Proc. CASC 2020, LNCS 12291, pp.492-509, Springer 2020",http://dx.doi.org/10.1007/978-3-030-60026-6_29,cs.SC,"['cs.SC', 'q-bio.MN']",10.1007/978-3-030-60026-6_29,,[]
"Effective Localization Using Double Ideal Quotient and Its
  Implementation",http://arxiv.org/abs/2003.00220v1,2020-02-29T09:40:12Z,2020-02-29T09:40:12Z,"  In this paper, we propose a new method for localization of polynomial ideal,
which we call ""Local Primary Algorithm"". For an ideal $I$ and a prime ideal
$P$, our method computes a $P$-primary component of $I$ after checking if $P$
is associated with $I$ by using ""double ideal quotient"" $(I:(I:P))$ and its
variants which give us a lot of information about localization of $I$.
","['\nYuki Ishihara\n', '\nKazuhiro Yokoyama\n']",,"Proc. CASC 2018, LNCS 11077, pp.272-287, Springer 2018",http://dx.doi.org/10.1007/978-3-319-99639-4_19,math.AC,"['math.AC', 'cs.SC']",10.1007/978-3-319-99639-4_19,,[]
Modular Techniques for Effective Localization and Double Ideal Quotient,http://arxiv.org/abs/2003.00496v2,2020-03-01T14:39:54Z,2022-02-14T09:21:13Z,"  By double ideal quotient, we mean $(I:(I:J))$ where ideals $I$ and $J$. In
our previous work [11], double ideal quotient and its variants are shown to be
very useful for checking prime divisor and generating primary component.
Combining those properties, we can compute ""direct localization"" effectively,
comparing with full primary decomposition. In this paper, we apply modular
techniques effectively to computation of such double ideal quotient and its
variants, where first we compute them modulo several prime numbers and then
lift them up over rational numbers by Chinese Remainder Theorem and rational
reconstruction. As a new modular technique for double ideal quotient and its
variants, we devise criteria for output from modular computations. Also, we
apply modular techniques to intermediate primary decomposition. We examine the
effectiveness of our modular techniques for several examples by preliminary
computational experiences on Singular.
",['\nYuki Ishihara\n'],,"In Proceedings of ISSAC '20, ACM, 265-272 (2020)",http://dx.doi.org/10.1145/3373207.3404017,math.AC,"['math.AC', 'cs.SC']",10.1145/3373207.3404017,,[]
Criteria for the numerical constant recognition,http://arxiv.org/abs/2002.12690v2,2020-02-28T13:01:21Z,2021-06-08T09:12:09Z,"  The need for recognition/approximation of functions in terms of elementary
functions/operations emerges in many areas of experimental mathematics,
numerical analysis, computer algebra systems, model building, machine learning,
approximation and data compression. One of the most underestimated methods is
the symbolic regression. In the article, reductionist approach is applied,
reducing full problem to constant functions, i.e, pure numbers (decimal,
floating-point). However, existing solutions are plagued by lack of solid
criteria distinguishing between random formula, matching approximately or
literally decimal expansion and probable ''exact'' (the best) expression match
in the sense of Occam's razor. In particular, convincing STOP criteria for
search were never developed. In the article, such a criteria, working in
statistical sense, are provided. Recognition process can be viewed as (1)
enumeration of all formulas in order of increasing Kolmogorov complexity K (2)
random process with appropriate statistical distribution (3) compression of a
decimal string. All three approaches are remarkably consistent, and provide
essentially the same limit for practical depth of search. Tested unique
formulas count must not exceed 1/sigma, where sigma is relative numerical error
of the target constant. Beyond that, further search is pointless, because, in
the view of approach (1), number of equivalent expressions within error bounds
grows exponentially; in view of (2), probability of random match approaches 1;
in view of (3) compression ratio much smaller than 1.
",['\nAndrzej Odrzywolek\n'],20 pages + Supplemental Material,,http://arxiv.org/abs/2002.12690v2,cs.DM,"['cs.DM', 'cs.SC', 'stat.OT', 'G.2.3; G.3; G.4; I.1.1; I.2.m; F.2.3; G.1.m; F.1.m']",,,[]
"A complexity chasm for solving univariate sparse polynomial equations
  over $p$-adic fields",http://arxiv.org/abs/2003.00314v4,2020-02-29T17:30:26Z,2021-06-06T18:43:56Z,"  We reveal a complexity chasm, separating the trinomial and tetranomial cases,
for solving univariate sparse polynomial equations over certain local fields.
First, for any fixed field
$K\in\{\mathbb{Q}_2,\mathbb{Q}_3,\mathbb{Q}_5,\ldots\}$, we prove that any
polynomial $f\in\mathbb{Z}[x]$ with exactly $3$ monomial terms, degree $d$, and
all coefficients having absolute value at most $H$, can be solved over $K$ in
deterministic time $O(\log^{O(1)}(dH))$ in the classical Turing model. (The
best previous algorithms were of complexity exponential in $\log d$, even for
just counting roots in $\mathbb{Q}_p$.) In particular, our algorithm generates
approximations in $\mathbb{Q}$ with bit-length $O(\log^{O(1)}(dH))$ to all the
roots of $f$ in $K$, and these approximations converge quadratically under
Newton iteration. On the other hand, we give a unified family of tetranomials
requiring $\Omega(d\log H)$ digits to distinguish the base-$p$ expansions of
their roots in $K$.
","['\nJ. Maurice Rojas\n', '\nYuyu Zhu\n']","19 pages, 3 figures. This version contains an Appendix missing from
  the ISSAC 2021 conference version, as well as some corrections and
  improvements",,http://arxiv.org/abs/2003.00314v4,math.NT,"['math.NT', 'cs.CC', 'cs.SC']",,,[]
Solving Satisfiability of Polynomial Formulas By Sample-Cell Projection,http://arxiv.org/abs/2003.00409v2,2020-03-01T05:36:09Z,2020-03-04T03:01:35Z,"  A new algorithm for deciding the satisfiability of polynomial formulas over
the reals is proposed. The key point of the algorithm is a new projection
operator, called sample-cell projection operator, custom-made for
Conflict-Driven Clause Learning (CDCL)-style search. Although the new operator
is also a CAD (Cylindrical Algebraic Decomposition)-like projection operator
which computes the cell (not necessarily cylindrical) containing a given sample
such that each polynomial from the problem is sign-invariant on the cell, it is
of singly exponential time complexity. The sample-cell projection operator can
efficiently guide CDCL-style search away from conflicting states. Experiments
show the effectiveness of the new algorithm.
","['\nHaokun Li\n', '\nBican Xia\n']",,,http://arxiv.org/abs/2003.00409v2,cs.LO,"['cs.LO', 'cs.AI', 'cs.SC']",,,[]
Maximum Absolute Determinants of Upper Hessenberg Bohemian Matrices,http://arxiv.org/abs/2003.00454v2,2020-03-01T10:03:17Z,2020-05-09T02:30:00Z,"  A matrix is called Bohemian if its entries are sampled from a finite set of
integers. We determine the maximum absolute determinant of upper Hessenberg
Bohemian Matrices for which the subdiagonal entries are fixed to be $1$ and
upper triangular entries are sampled from $\{0,1,\cdots,n\}$, extending
previous results for $n=1$ and $n=2$ and proving a recent conjecture of Fasi &
Negri Porzio [8]. Furthermore, we generalize the problem to non-integer-valued
entries.
","['\nJonathan P. Keating\n', '\nAhmet Abdullah Keleş\n']",,,http://arxiv.org/abs/2003.00454v2,cs.SC,"['cs.SC', 'cs.NA', 'math.CO', 'math.NA']",,,[]
Signature-based algorithms for Gr{ö}bner bases over Tate algebras,http://arxiv.org/abs/2002.04491v2,2020-02-11T15:47:40Z,2021-05-10T09:43:01Z,"  Introduced by Tate in [Ta71], Tate algebras play a major role in the context
of analytic geometry over the-adics, where they act as a counterpart to the use
of polynomial algebras in classical algebraic geometry. In [CVV19] the
formalism of Gr{\""o}bner bases over Tate algebras has been introduced and
effectively implemented. One of the bottleneck in the algorithms was the time
spent on reduction , which are significantly costlier than over polynomials. In
the present article, we introduce two signature-based Gr{\""o}bner bases
algorithms for Tate algebras, in order to avoid many reductions. They have been
implemented in SageMath. We discuss their superiority based on numerical
evidences.
","['\nXavier Caruso\nLFANT\n', '\nTristan Vaccon\nXLIM-MATHIS\n', '\nThibaut Verron\nJKU\n']","ISSAC 2021 - International Symposium on Symbolic and Algebraic
  Computation, Jul 2020, Kalamata / Virtual, Greece",,http://dx.doi.org/10.1145/3373207.3404035,cs.SC,['cs.SC'],10.1145/3373207.3404035,,"['LFANT', 'XLIM-MATHIS', 'JKU']"
"A divide-and-conquer algorithm for computing Gröbner bases of syzygies
  in finite dimension",http://arxiv.org/abs/2002.06404v2,2020-02-15T16:15:54Z,2020-06-04T10:24:15Z,"  Let $f_1,\ldots,f_m$ be elements in a quotient $R^n / N$ which has finite
dimension as a $K$-vector space, where $R = K[X_1,\ldots,X_r]$ and $N$ is an
$R$-submodule of $R^n$. We address the problem of computing a Gr\""obner basis
of the module of syzygies of $(f_1,\ldots,f_m)$, that is, of vectors
$(p_1,\ldots,p_m) \in R^m$ such that $p_1 f_1 + \cdots + p_m f_m = 0$.
  An iterative algorithm for this problem was given by Marinari, M\""oller, and
Mora (1993) using a dual representation of $R^n / N$ as the kernel of a
collection of linear functionals. Following this viewpoint, we design a
divide-and-conquer algorithm, which can be interpreted as a generalization to
several variables of Beckermann and Labahn's recursive approach for matrix
Pad\'e and rational interpolation problems. To highlight the interest of this
method, we focus on the specific case of bivariate Pad\'e approximation and
show that it improves upon the best known complexity bounds.
","['\nSimone Naldi\n', '\nVincent Neiger\n']","ISSAC 2020. 8 pages, 4 algorithms",,http://dx.doi.org/10.1145/3373207.3404059,cs.SC,['cs.SC'],10.1145/3373207.3404059,,[]
On the Uniqueness of Simultaneous Rational Function Reconstruction,http://arxiv.org/abs/2002.08748v1,2020-02-20T14:22:31Z,2020-02-20T14:22:31Z,"  This paper focuses on the problem of reconstructing a vector of rational
functions given some evaluations, or more generally given their remainders
modulo different polynomials. The special case of rational functions sharing
the same denominator, a.k.a.Simultaneous Rational Function Reconstruction
(SRFR), has many applications from linear system solving to coding theory,
provided that SRFR has a unique solution. The number of unknowns in SRFR is
smaller than for a general vector of rational function. This allows to reduce
the number of evaluation points needed to guarantee the existence of a
solution, but we may lose its uniqueness. In this work, we prove that
uniqueness is guaranteed for a generic instance.
","['\nEleonora Guerrini\n', '\nRomain Lebreton\n', '\nIlaria Zappatore\n']",,,http://arxiv.org/abs/2002.08748v1,cs.SC,['cs.SC'],,,[]
Smooth Points on Semi-algebraic Sets,http://arxiv.org/abs/2002.04707v3,2020-02-11T21:52:41Z,2023-05-19T20:32:52Z,"  Many algorithms for determining properties of real algebraic or
semi-algebraic sets rely upon the ability to compute smooth points. Existing
methods to compute smooth points on semi-algebraic sets use symbolic quantifier
elimination tools. In this paper, we present a simple algorithm based on
computing the critical points of some well-chosen function that guarantees the
computation of smooth points in each connected compact component of a real
(semi)-algebraic set. Our technique is intuitive in principal, performs well on
previously difficult examples, and is straightforward to implement using
existing numerical algebraic geometry software. The practical efficiency of our
approach is demonstrated by solving a conjecture on the number of equilibria of
the Kuramoto model for the $n=4$ case. We also apply our method to design an
efficient algorithm to compute the real dimension of (semi)-algebraic sets, the
original motivation for this research.
","['\nKatherine Harris\n', '\nJonathan D. Hauenstein\n', '\nAgnes Szanto\n']",,Journal of Symbolic Computation 116 (2023) 183-212,http://dx.doi.org/10.1016/j.jsc.2022.09.003,cs.SC,"['cs.SC', 'cs.NA', 'math.AG', 'math.NA']",10.1016/j.jsc.2022.09.003,,[]
"ENIGMA Anonymous: Symbol-Independent Inference Guiding Machine (system
  description)",http://arxiv.org/abs/2002.05406v2,2020-02-13T09:44:38Z,2020-04-28T13:59:26Z,"  We describe an implementation of gradient boosting and neural guidance of
saturation-style automated theorem provers that does not depend on consistent
symbol names across problems. For the gradient-boosting guidance, we manually
create abstracted features by considering arity-based encodings of formulas.
For the neural guidance, we use symbol-independent graph neural networks (GNNs)
and their embedding of the terms and clauses. The two methods are efficiently
implemented in the E prover and its ENIGMA learning-guided framework.
  To provide competitive real-time performance of the GNNs, we have developed a
new context-based approach to evaluation of generated clauses in E. Clauses are
evaluated jointly in larger batches and with respect to a large number of
already selected clauses (context) by the GNN that estimates their collectively
most useful subset in several rounds of message passing. This means that
approximative inference rounds done by the GNN are efficiently interleaved with
precise symbolic inference rounds done inside E. The methods are evaluated on
the MPTP large-theory benchmark and shown to achieve comparable real-time
performance to state-of-the-art symbol-based methods. The methods also show
high complementarity, solving a large number of hard Mizar problems.
","['\nJan Jakubův\n', '\nKarel Chvalovský\n', '\nMiroslav Olšák\n', '\nBartosz Piotrowski\n', '\nMartin Suda\n', '\nJosef Urban\n']",,,http://arxiv.org/abs/2002.05406v2,cs.AI,"['cs.AI', 'cs.LG', 'cs.LO', 'cs.NE', 'cs.SC']",,,[]
An Additive Decomposition in S-Primitive Towers,http://arxiv.org/abs/2002.02355v1,2020-02-06T17:00:47Z,2020-02-06T17:00:47Z,"  We consider the additive decomposition problem in primitive towers and
present an algorithm to decompose a function in an S-primitive tower as a sum
of a derivative in the tower and a remainder which is minimal in some sense.
Special instances of S-primitive towers include differential fields generated
by finitely many logarithmic functions and logarithmic integrals. A function in
an S-primitive tower is integrable in the tower if and only if the remainder is
equal to zero. The additive decomposition is achieved by viewing our towers not
as a traditional chain of extension fields, but rather as a direct sum of
certain subrings. Furthermore, we can determine whether or not a function in an
S-primitive tower has an elementary integral without solving any differential
equations. We also show that a kind of S-primitive towers, known as logarithmic
towers, can be embedded into a particular extension where we can obtain a finer
remainder.
","['\nHao Du\n', '\nJing Guo\n', '\nZiming Li\n', '\nElaine Wong\n']","This article has been submitted to ISSAC2020 for review.
  Supplementary material at https://wongey.github.io/add-decomp-sprimitive/","ISSAC 2020: Proceedings of the 45th International Symposium on
  Symbolic and Algebraic Computation, July 2020, Pages 146-153",http://dx.doi.org/10.1145/3373207.3404025,cs.SC,['cs.SC'],10.1145/3373207.3404025,,[]
Efficient q-Integer Linear Decomposition of Multivariate Polynomials,http://arxiv.org/abs/2002.00124v2,2020-02-01T02:05:19Z,2021-02-12T15:49:48Z,"  We present two new algorithms for the computation of the q-integer linear
decomposition of a multivariate polynomial. Such a decomposition is essential
for the treatment of q-hypergeometric symbolic summation via creative
telescoping and for describing the q-counterpart of Ore-Sato theory. Both of
our algorithms require only basic integer and polynomial arithmetic and work
for any unique factorization domain containing the ring of integers. Complete
complexity analyses are conducted for both our algorithms and two previous
algorithms in the case of multivariate integer polynomials, showing that our
algorithms have better theoretical performances. A Maple implementation is also
included which suggests that our algorithms are also much faster in practice
than previous algorithms.
","['\nMark Giesbrecht\n', '\nHui Huang\n', '\nGeorge Labahn\n', '\nEugene Zima\n']",,,http://arxiv.org/abs/2002.00124v2,cs.SC,"['cs.SC', 'math.RA']",,,[]
Separating Variables in Bivariate Polynomial Ideals,http://arxiv.org/abs/2002.01541v2,2020-02-04T21:18:00Z,2020-06-05T14:50:38Z,"  We present an algorithm which for any given ideal $I\subseteq\mathbb{K}
[x,y]$ finds all elements of $I$ that have the form $f(x) - g(y)$, i.e., all
elements in which no monomial is a multiple of $xy$.
","['\nManfred Buchacher\n', '\nManuel Kauers\n', '\nGleb Pogudin\n']",,,http://arxiv.org/abs/2002.01541v2,cs.SC,"['cs.SC', 'math.AC']",,,[]
Integral P-Recursive Sequences,http://arxiv.org/abs/2002.02783v1,2020-02-07T13:49:00Z,2020-02-07T13:49:00Z,"  In an earlier paper, the notion of integrality known from algebraic number
fields and fields of algebraic functions has been extended to D-finite
functions. The aim of the present paper is to extend the notion to the case of
P-recursive sequences. In order to do so, we formulate a general algorithm for
finding all integral elements for valued vector spaces and then show that this
algorithm includes not only the algebraic and the D-finite cases but also
covers the case of P-recursive sequences.
","['\nShaoshi Chen\n', '\nLixin Du\n', '\nManuel Kauers\n', '\nThibaut Verron\n']",20 pages,,http://arxiv.org/abs/2002.02783v1,cs.SC,"['cs.SC', 'math.NT']",,,[]
"The Fundamental Theorem of Tropical Partial Differential Algebraic
  Geometry",http://arxiv.org/abs/2002.03041v1,2020-02-07T23:08:18Z,2020-02-07T23:08:18Z,"  Tropical Differential Algebraic Geometry considers difficult or even
intractable problems in Differential Equations and tries to extract information
on their solutions from a restricted structure of the input. The Fundamental
Theorem of Tropical Differential Algebraic Geometry states that the support of
solutions of systems of ordinary differential equations with formal power
series coefficients over an uncountable algebraically closed field of
characteristic zero can be obtained by solving a so-called tropicalized
differential system. Tropicalized differential equations work on a completely
different algebraic structure which may help in theoretical and computational
questions. We show that the Fundamental Theorem can be extended to the case of
systems of partial differential equations by introducing vertex sets of Newton
polygons.
","['\nSebastian Falkensteiner\n', '\nCristhian Garay-López\n', '\nMercedes Haiech\n', '\nMarc Paul Noordman\n', '\nZeinab Toghani\n', '\nFrançois Boulier\n']","20 pages, 2 figures. Submitted to ISSAC 2020",,http://arxiv.org/abs/2002.03041v1,math.AG,"['math.AG', 'cs.SC', '13P15, 13N99, 14T99, 52B20']",,,[]
Essentially Optimal Sparse Polynomial Multiplication,http://arxiv.org/abs/2001.11959v2,2020-01-31T17:23:45Z,2020-06-05T09:15:22Z,"  We present a probabilistic algorithm to compute the product of two univariate
sparse polynomials over a field with a number of bit operations that is
quasi-linear in the size of the input and the output. Our algorithm works for
any field of characteristic zero or larger than the degree. We mainly rely on
sparse interpolation and on a new algorithm for verifying a sparse product that
has also a quasi-linear time complexity. Using Kronecker substitution
techniques we extend our result to the multivariate case.
","['\nPascal Giorgi\n', '\nBruno Grenet\n', '\nArmelle Perret du Cray\n']",12 pages,"Proc. ISSAC'20, pp 202-209, ACM, 2020",http://dx.doi.org/10.1145/3373207.3404026,cs.SC,"['cs.SC', 'cs.CC', 'cs.DS']",10.1145/3373207.3404026,,[]
Unsatisfiability Proofs for Weight 16 Codewords in Lam's Problem,http://arxiv.org/abs/2001.11973v2,2020-01-31T17:43:22Z,2020-05-01T14:55:36Z,"  In the 1970s and 1980s, searches performed by L. Carter, C. Lam, L. Thiel,
and S. Swiercz showed that projective planes of order ten with weight 16
codewords do not exist. These searches required highly specialized and
optimized computer programs and required about 2,000 hours of computing time on
mainframe and supermini computers. In 2011, these searches were verified by D.
Roy using an optimized C program and 16,000 hours on a cluster of desktop
machines. We performed a verification of these searches by reducing the problem
to the Boolean satisfiability problem (SAT). Our verification uses the
cube-and-conquer SAT solving paradigm, symmetry breaking techniques using the
computer algebra system Maple, and a result of Carter that there are ten
nonisomorphic cases to check. Our searches completed in about 30 hours on a
desktop machine and produced nonexistence proofs of about 1 terabyte in the
DRAT (deletion resolution asymmetric tautology) format.
","['\nCurtis Bright\n', '\nKevin K. H. Cheung\n', '\nBrett Stevens\n', '\nIlias Kotsireas\n', '\nVijay Ganesh\n']","To appear in Proceedings of the 29th International Joint Conference
  on Artificial Intelligence (IJCAI 2020)",,http://dx.doi.org/10.24963/ijcai.2020/203,cs.DM,"['cs.DM', 'cs.LO', 'cs.SC', 'math.CO']",10.24963/ijcai.2020/203,,[]
Nonexistence Certificates for Ovals in a Projective Plane of Order Ten,http://arxiv.org/abs/2001.11974v3,2020-01-31T17:43:24Z,2020-05-30T21:12:16Z,"  In 1983, a computer search was performed for ovals in a projective plane of
order ten. The search was exhaustive and negative, implying that such ovals do
not exist. However, no nonexistence certificates were produced by this search,
and to the best of our knowledge the search has never been independently
verified. In this paper, we rerun the search for ovals in a projective plane of
order ten and produce a collection of nonexistence certificates that, when
taken together, imply that such ovals do not exist. Our search program uses the
cube-and-conquer paradigm from the field of satisfiability (SAT) checking,
coupled with a programmatic SAT solver and the nauty symbolic computation
library for removing symmetries from the search.
","['\nCurtis Bright\n', '\nKevin K. H. Cheung\n', '\nBrett Stevens\n', '\nIlias Kotsireas\n', '\nVijay Ganesh\n']","Appears in the Proceedings of the 31st International Workshop on
  Combinatorial Algorithms (IWOCA 2020)",Lecture Notes in Computer Science 12126 (2020) 97-111,http://dx.doi.org/10.1007/978-3-030-48966-3_8,cs.DM,"['cs.DM', 'cs.LO', 'cs.SC', 'math.CO']",10.1007/978-3-030-48966-3_8,,[]
Linearly Constrained Gaussian Processes with Boundary Conditions,http://arxiv.org/abs/2002.00818v3,2020-02-03T15:19:03Z,2021-02-15T11:34:34Z,"  One goal in Bayesian machine learning is to encode prior knowledge into prior
distributions, to model data efficiently. We consider prior knowledge from
systems of linear partial differential equations together with their boundary
conditions. We construct multi-output Gaussian process priors with realizations
in the solution set of such systems, in particular only such solutions can be
represented by Gaussian process regression. The construction is fully
algorithmic via Gr\""obner bases and it does not employ any approximation. It
builds these priors combining two parametrizations via a pullback: the first
parametrizes the solutions for the system of differential equations and the
second parametrizes all functions adhering to the boundary conditions.
",['\nMarkus Lange-Hegermann\n'],,,http://arxiv.org/abs/2002.00818v3,cs.LG,"['cs.LG', 'cs.SC', 'math.AC', 'stat.ML', '13P10, 13P20, 18-04, 47F05, 60G15, 60-08, 62G05, 68T01', 'G.3; I.1.2; I.1.4; I.2.6; I.5.1']",,,[]
"Convergence analysis of particle swarm optimization using stochastic
  Lyapunov functions and quantifier elimination",http://arxiv.org/abs/2002.01673v1,2020-02-05T07:47:07Z,2020-02-05T07:47:07Z,"  This paper adds to the discussion about theoretical aspects of particle swarm
stability by proposing to employ stochastic Lyapunov functions and to determine
the convergence set by quantifier elimination. We present a computational
procedure and show that this approach leads to reevaluation and extension of
previously know stability regions for PSO using a Lyapunov approach under
stagnation assumptions.
","['\nMaximilian Gerwien\n', '\nRick Voßwinkel\n', '\nHendrik Richter\n']",,,http://arxiv.org/abs/2002.01673v1,cs.NE,"['cs.NE', 'cs.SC', 'math.DS']",,,[]
On mu-Symmetric Polynomials,http://arxiv.org/abs/2001.07403v1,2020-01-21T09:26:49Z,2020-01-21T09:26:49Z,"  In this paper, we study functions of the roots of a univariate polynomial in
which the roots have a given multiplicity structure $\mu$. Traditionally, root
functions are studied via the theory of symmetric polynomials; we extend this
theory to $\mu$-symmetric polynomials. We were motivated by a conjecture from
Becker et al.~(ISSAC 2016) about the $\mu$-symmetry of a particular root
function $D^+(\mu)$, called D-plus. To investigate this conjecture, it was
desirable to have fast algorithms for checking if a given root function is
$\mu$-symmetric. We designed three such algorithms: one based on Gr\""{o}bner
bases, another based on preprocessing and reduction, and the third based on
solving linear equations. We implemented them in Maple and experiments show
that the latter two algorithms are significantly faster than the first.
","['\nJing Yang\n', '\nChee K. Yap\n']",,,http://arxiv.org/abs/2001.07403v1,cs.SC,['cs.SC'],,,[]
Sparse Polynomial Interpolation Based on Diversification,http://arxiv.org/abs/2002.03706v1,2020-01-21T16:39:11Z,2020-01-21T16:39:11Z,"  We consider the problem of interpolating a sparse multivariate polynomial
over a finite field, represented with a black box. Building on the algorithm of
Ben-Or and Tiwari for interpolating polynomials over rings with characteristic
zero, we develop a new Monte Carlo algorithm over the finite field by doing
additional probes. To interpolate a polynomial $f\in F_q[x_1,\dots,x_n]$ with a
partial degree bound $D$ and a term bound $T$, our new algorithm costs
$O^\thicksim(nT\log ^2q+nT\sqrt{D}\log q)$ bit operations and uses $2(n+1)T$
probes to the black box. If $q\geq O(nT^2D)$, it has constant success rate to
return the correct polynomial. Compared with previous algorithms over general
finite field, our algorithm has better complexity in the parameters $n,T,D$ and
is the first one to achieve the complexity of fractional power about $D$, while
keeping linear in $n,T$. A key technique is a randomization which makes all
coefficients of the unknown polynomial distinguishable, producing a diverse
polynomial. This approach, called diversification, was proposed by Giesbrecht
and Roche in 2011. Our algorithm interpolates each variable independently using
$O(T)$ probes, and then uses the diversification to correlate terms in
different images. At last, we get the exponents by solving the discrete
logarithms and obtain coefficients by solving a linear system. We have
implemented our algorithm in Maple. Experimental results shows that our
algorithm can applied to sparse polynomials with large degree. We also analyze
the success rate of the algorithm.
",['\nQiao-Long Huang\n'],"17 pages, 3 figures, 2 tables",,http://arxiv.org/abs/2002.03706v1,cs.SC,"['cs.SC', 'math.RA', 'I.1.2']",,,[]
Sparse Polynomial Interpolation Based on Derivative,http://arxiv.org/abs/2002.03708v1,2020-01-21T16:34:15Z,2020-01-21T16:34:15Z,"  In this paper, we propose two new interpolation algorithms for sparse
multivariate polynomials represented by a straight-line program(SLP). Both of
our algorithms work over any finite fields $F_q$ with large characteristic. The
first one is a Monte Carlo randomized algorithm. Its arithmetic complexity is
linear in the number $T$ of non-zero terms of $f$, in the number $n$ of
variables. If $q$ is $O((nTD)^{(1)})$, where $D$ is the partial degree bound,
then our algorithm has better complexity than other existing algorithms. The
second one is a deterministic algorithm. It has better complexity than existing
deterministic algorithms over a field with large characteristic. Its arithmetic
complexity is quadratic in $n,T,\log D$, i.e., quadratic in the size of the
sparse representation. And we also show that the complexity of our
deterministic algorithm is the same as the one of deterministic zero-testing of
Bl\""{a}ser et al. for the polynomial given by an SLP over finite field (for
large characteristic).
",['\nQiao-Long Huang\n'],15 pages,,http://arxiv.org/abs/2002.03708v1,cs.SC,"['cs.SC', 'math.RA', 'I.1.2']",,,[]
Bisimilar Conversion of Multi-valued Networks to Boolean Networks,http://arxiv.org/abs/2001.07371v1,2020-01-21T07:58:53Z,2020-01-21T07:58:53Z,"  Discrete modelling frameworks of Biological networks can be divided in two
distinct categories: Boolean and Multi-valued. Although Multi-valued networks
are more expressive for qualifying the regulatory behaviours modelled by more
than two values, the ability to automatically convert them to Boolean network
with an equivalent behaviour breaks down the fundamental borders between the
two approaches. Theoretically investigating the conversion process provides
relevant insights into bridging the gap between them. Basically, the conversion
aims at finding a Boolean network bisimulating a Multi-valued one. In this
article, we investigate the bisimilar conversion where the Boolean integer
coding is a parameter that can be freely modified. Based on this analysis, we
define a computational method automatically inferring a bisimilar Boolean
network from a given Multi-valued one.
","['\nFranck Delaplace\nIBISC\n', '\nSergiu Ivanov\nLACL\n']",,,http://arxiv.org/abs/2001.07371v1,cs.DM,"['cs.DM', 'cs.SC', 'q-bio.QM']",,,"['IBISC', 'LACL']"
Sparse Interpolation in Terms of Multivariate Chebyshev Polynomials,http://arxiv.org/abs/2001.09144v1,2020-01-24T18:52:21Z,2020-01-24T18:52:21Z,"  Sparse interpolation} refers to the exact recovery of a function as a short
linear combination of basis functions from a limited number of evaluations. For
multivariate functions, the case of the monomial basis is well studied, as is
now the basis of exponential functions. Beyond the multivariate Chebyshev
polynomial obtained as tensor products of univariate Chebyshev polynomials, the
theory of root systems allows to define a variety of generalized multivariate
Chebyshev polynomials that have connections to topics such as Fourier analysis
and representations of Lie algebras. We present a deterministic algorithm to
recover a function that is the linear combination of at most r such polynomials
from the knowledge of r and an explicitly bounded number of evaluations of this
function.
","['\nEvelyne Hubert\n', '\nMichael F. Singer\n']",,,http://arxiv.org/abs/2001.09144v1,cs.SC,"['cs.SC', 'math.CA', 'math.RA', 'math.RT', '13A50, 17B10, 17B22, 30E05, 33C52, 33F10, 68W30']",,,[]
On the Uniqueness Problem for Quadrature Domains,http://arxiv.org/abs/2001.09431v3,2020-01-26T10:23:39Z,2021-05-11T00:54:26Z,"  We study questions of existence and uniqueness of quadrature domains using
computational tools from real algebraic geometry. These problems are
transformed into questions about the number of solutions to an associated real
semi-algebraic system, which is analyzed using the method of real comprehensive
triangular decomposition.
","['\nYacin Ameur\n', '\nMartin Helmer\n', '\nFelix Tellander\n']",Corrected a minor and isolated typo in (4),Computational Methods and Function Theory (2021),http://dx.doi.org/10.1007/s40315-021-00373-w,math.CV,"['math.CV', 'cs.SC', 'math.AG', '30C20, 31A25, 14P10, 68W30']",10.1007/s40315-021-00373-w,,[]
Smart Induction for Isabelle/HOL (System Description),http://arxiv.org/abs/2001.10834v1,2020-01-27T15:29:34Z,2020-01-27T15:29:34Z,"  Proof assistants offer tactics to facilitate inductive proofs. However, it
still requires human ingenuity to decide what arguments to pass to those
induction tactics. To automate this process, we present smart_induct for
Isabelle/HOL. Given an inductive problem in any problem domain, smart_induct
lists promising arguments for the induct tactic without relying on a search.
Our evaluation demonstrated smart_induct produces valuable recommendations
across problem domains.
",['\nYutaka Nagashima\n'],Under submission at IJCAR2020 as a System Description,,http://arxiv.org/abs/2001.10834v1,cs.AI,"['cs.AI', 'cs.LO', 'cs.PL', 'cs.SC']",,,[]
On fast multiplication of a matrix by its transpose,http://arxiv.org/abs/2001.04109v4,2020-01-13T09:16:43Z,2020-06-19T08:06:40Z,"  We present a non-commutative algorithm for the multiplication of a
2x2-block-matrix by its transpose using 5 block products (3 recursive calls and
2 general products) over C or any finite field.We use geometric considerations
on the space of bilinear forms describing 2x2 matrix products to obtain this
algorithm and we show how to reduce the number of involved additions.The
resulting algorithm for arbitrary dimensions is a reduction of multiplication
of a matrix by its transpose to general matrix product, improving by a constant
factor previously known reductions.Finally we propose schedules with low memory
footprint that support a fast and memory efficient practical implementation
over a finite field.To conclude, we show how to use our result in LDLT
factorization.
","['\nJean-Guillaume Dumas\nCASC\n', '\nClement Pernet\nCASC\n', '\nAlexandre Sedoglavic\nCRIStAL\n']","ISSAC 2020, Jul 2020, Kalamata, Greece",,http://dx.doi.org/10.1145/3373207.3404021,cs.SC,['cs.SC'],10.1145/3373207.3404021,,"['CASC', 'CASC', 'CRIStAL']"
"Proof of the tree module property for exceptional representations of
  tame quivers",http://arxiv.org/abs/2001.00016v3,2019-12-31T18:54:12Z,2021-06-27T22:28:50Z,"  This document serves as an arXiv entry point for the appendix to the paper
[13] (the ancillary file e6_proof.pdf -- ``Proof of the tree module property
for exceptional representations of the quiver $\widetilde{\mathbb{E}}_6$'') and
the appendix to the paper [12] (the ancillary file d6_proof.pdf -- ``Proof of
the tree module property for exceptional representations of the quiver
$\widetilde{\mathbb{D}}_6$''). The ancillary files contain the computer
generated part of the proofs of the main results in [13] respectively [12],
giving a complete and general list of tree representations corresponding to
exceptional modules over the path algebra of the canonically oriented Euclidean
quiver $\widetilde{\mathbb{E}}_6$, respectively $\widetilde{\mathbb{D}}_6$. The
proofs (involving induction and symbolic computation with block matrices) were
partially generated by a purposefully developed computer software, outputting
in a detailed step-by-step fashion as if written ``by hand''.
  We also give here a short theoretical introduction and an overview of the
computational method used to prove the formulas given in the papers [13] and
[12].
","['\nSzabolcs Lénárt\n', '\nÁbel Lőrinczi\n', '\nCsaba Szántó\n', '\nIstván Szöllősi\n']","8 pages, with ancillary documents (e6_proof.pdf and d6_proof.pdf)",,http://dx.doi.org/10.4064/cm7931-1-2020,math.RT,"['math.RT', 'cs.SC', '16G20, 16G70']",10.4064/cm7931-1-2020 10.4064/cm8270-11-2020,,[]
Stieltjes moment sequences for pattern-avoiding permutations,http://arxiv.org/abs/2001.00393v3,2020-01-02T11:16:07Z,2020-10-17T09:08:24Z,"  A small set of combinatorial sequences have coefficients that can be
represented as moments of a nonnegative measure on $[0, \infty)$. Such
sequences are known as Stieltjes moment sequences. This article focuses on some
classical sequences in enumerative combinatorics, denoted $Av(\mathcal{P})$,
and counting permutations of $\{1, 2, \ldots, n \}$ that avoid some given
pattern $\mathcal{P}$. For increasing patterns $\mathcal{P}=(12\ldots k)$, we
recall that the corresponding sequences, $Av(123\ldots k)$, are Stieltjes
moment sequences, and we explicitly find the underlying density function,
either exactly or numerically, by using the Stieltjes inversion formula as a
fundamental tool. We show that the generating functions of the sequences $\,
Av(1234)$ and $\, Av(12345)$ correspond, up to simple rational functions, to an
order-one linear differential operator acting on a classical modular form given
as a pullback of a Gaussian $\, _2F_1$ hypergeometric function, respectively to
an order-two linear differential operator acting on the square of a classical
modular form given as a pullback of a $\, _2F_1$ hypergeometric function. We
demonstrate that the density function for the Stieltjes moment sequence
$Av(123\ldots k)$ is closely, but non-trivially, related to the density
attached to the distance traveled by a walk in the plane with $k-1$ unit steps
in random directions. Finally, we study the challenging case of the $Av(1324)$
sequence and give compelling numerical evidence that this too is a Stieltjes
moment sequence. Accepting this, we show how rigorous lower bounds on the
growth constant of this sequence can be constructed, which are stronger than
existing bounds. A further unproven assumption leads to even better bounds,
which can be extrapolated to give an estimate of the (unknown) growth constant.
","['\nAlin Bostan\n', '\nAndrew Elvey Price\n', '\nAnthony John Guttmann\n', '\nJean-Marie Maillard\n']","59 pages, 11 figures","The Electronic Journal of Combinatorics, 2020",http://arxiv.org/abs/2001.00393v3,math.CO,"['math.CO', 'cs.SC', 'Primary 44A60, 68W30, 33F10, 15B52, Secondary 05A15, 05A10, 11B65,\n  60B20, 11F03, 11F12, 33A30, 33C05, 34A05']",,,[]
Intuitionistic Linear Temporal Logics,http://arxiv.org/abs/1912.12893v1,2019-12-30T11:49:31Z,2019-12-30T11:49:31Z,"  We consider intuitionistic variants of linear temporal logic with `next',
`until' and `release' based on expanding posets: partial orders equipped with
an order-preserving transition function. This class of structures gives rise to
a logic which we denote $\iltl$, and by imposing additional constraints we
obtain the logics $\itlb$ of persistent posets and $\itlht$ of here-and-there
temporal logic, both of which have been considered in the literature. We prove
that $\iltl$ has the effective finite model property and hence is decidable,
while $\itlb$ does not have the finite model property. We also introduce
notions of bounded bisimulations for these logics and use them to show that the
`until' and `release' operators are not definable in terms of each other, even
over the class of persistent posets.
","['\nPhilippe Balbiani\n', '\nJoseph Boudou\n', '\nMartín Diéguez\n', '\nDavid Fernández-Duque\n']","arXiv admin note: text overlap with arXiv:1704.02847,
  arXiv:1803.05078",,http://arxiv.org/abs/1912.12893v1,cs.LO,"['cs.LO', 'cs.AI', 'cs.SC']",,,[]
Linear Programming using Limited-Precision Oracles,http://arxiv.org/abs/1912.12820v1,2019-12-30T05:37:16Z,2019-12-30T05:37:16Z,"  Since the elimination algorithm of Fourier and Motzkin, many different
methods have been developed for solving linear programs. When analyzing the
time complexity of LP algorithms, it is typically either assumed that
calculations are performed exactly and bounds are derived on the number of
elementary arithmetic operations necessary, or the cost of all arithmetic
operations is considered through a bit-complexity analysis. Yet in practice,
implementations typically use limited-precision arithmetic. In this paper we
introduce the idea of a limited-precision LP oracle and study how such an
oracle could be used within a larger framework to compute exact precision
solutions to LPs. Under mild assumptions, it is shown that a polynomial number
of calls to such an oracle and a polynomial number of bit operations, is
sufficient to compute an exact solution to an LP. This work provides a
foundation for understanding and analyzing the behavior of the methods that are
currently most effective in practice for solving LPs exactly.
","['\nAmbros Gleixner\n', '\nDaniel E. Steffy\n']",,"Mathematical Programming, 2019",http://dx.doi.org/10.1007/s10107-019-01444-6,math.OC,"['math.OC', 'cs.CC', 'cs.NA', 'cs.SC', 'math.NA', '90C05, 68Q25, 11K60, 68W30, 65G30, 65F99']",10.1007/s10107-019-01444-6,,[]
Differentiable Set Operations for Algebraic Expressions,http://arxiv.org/abs/1912.12181v1,2019-12-21T19:01:30Z,2019-12-21T19:01:30Z,"  Basic principles of set theory have been applied in the context of
probability and binary computation. Applying the same principles on
inequalities is less common but can be extremely beneficial in a variety of
fields. This paper formulates a novel approach to directly apply set operations
on inequalities to produce resultant inequalities with differentiable
boundaries. The suggested approach uses inequalities of the form Ei:
fi(x1,x2,..,xn) and an expression of set operations in terms of Ei like, (E1
and E2) or E3, or can be in any standard form like the Conjunctive Normal Form
(CNF) to produce an inequality F(x1,x2,..,xn)<=1 which represents the resulting
bounded region from the expressions and has a differentiable boundary. To
ensure differentiability of the solution, a trade-off between representation
accuracy and curvature at borders (especially corners) is made. A set of
parameters is introduced which can be fine-tuned to improve the accuracy of
this approach. The various applications of the suggested approach have also
been discussed which range from computer graphics to modern machine learning
systems to fascinating demonstrations for educational purposes (current use). A
python script to parse such expressions is also provided.
",['\nJasdeep Singh Grover\n'],"11 pages, 12 figures",,http://arxiv.org/abs/1912.12181v1,cs.SC,"['cs.SC', '03E20, 00A06, 00A66']",,,[]
A sparse resultant based method for efficient minimal solvers,http://arxiv.org/abs/1912.10268v1,2019-12-21T14:29:44Z,2019-12-21T14:29:44Z,"  Many computer vision applications require robust and efficient estimation of
camera geometry. The robust estimation is usually based on solving camera
geometry problems from a minimal number of input data measurements, i.e.
solving minimal problems in a RANSAC framework. Minimal problems often result
in complex systems of polynomial equations. Many state-of-the-art efficient
polynomial solvers to these problems are based on Gr\""obner bases and the
action-matrix method that has been automatized and highly optimized in recent
years. In this paper we study an alternative algebraic method for solving
systems of polynomial equations, i.e., the sparse resultant-based method and
propose a novel approach to convert the resultant constraint to an eigenvalue
problem. This technique can significantly improve the efficiency and stability
of existing resultant-based solvers. We applied our new resultant-based method
to a large variety of computer vision problems and show that for most of the
considered problems, the new method leads to solvers that are the same size as
the the best available Gr\""obner basis solvers and of similar accuracy. For
some problems the new sparse-resultant based method leads to even smaller and
more stable solvers than the state-of-the-art Gr\""obner basis solvers. Our new
method can be fully automatized and incorporated into existing tools for
automatic generation of efficient polynomial solvers and as such it represents
a competitive alternative to popular Gr\""obner basis methods for minimal
problems in computer vision.
","['\nSnehal Bhayani\n', '\nZuzana Kukelova\n', '\nJanne Heikkilä\n']",,,http://arxiv.org/abs/1912.10268v1,cs.CV,"['cs.CV', 'cs.SC']",,,[]
Towards Symbolic Factual Change in DEL,http://arxiv.org/abs/1912.10717v1,2019-12-23T10:26:34Z,2019-12-23T10:26:34Z,"  We extend symbolic model checking for Dynamic Epistemic Logic (DEL) with
factual change. Our transformers provide a compact representation of action
models with pre- and postconditions, for both S5 and the general case. The
method can be implemented using binary decision diagrams and we expect it to
improve model checking performance. As an example we give a symbolic
representation of the Sally-Anne false belief task.
",['\nMalvin Gattinger\n'],"11 pages, original proceedings available at
  http://www2.sfs.uni-tuebingen.de/esslli-stus-2017/#proceedings","Karoliina Lohiniva, Johannes Wahle (Eds.): Proceedings of the
  ESSLLI 2017 Student Session, pp. 14-24, 2017",http://arxiv.org/abs/1912.10717v1,cs.LO,"['cs.LO', 'cs.SC', '03B70 (Primary) 68Q60 (Secondary)', 'F.4.1; I.2.4']",,,[]
"Efficient Algorithm for the Linear Complexity of Sequences and Some
  Related Consequences",http://arxiv.org/abs/1912.11617v1,2019-12-25T08:07:55Z,2019-12-25T08:07:55Z,"  The linear complexity of a sequence $s$ is one of the measures of its
predictability. It represents the smallest degree of a linear recursion which
the sequence satisfies. There are several algorithms to find the linear
complexity of a periodic sequence $s$ of length $N$ (where $N$ is of some given
form) over a finite field $F_q$ in $O(N)$ symbol field operations. The first
such algorithm is The Games-Chan Algorithm which considers binary sequences of
period $2^n$, and is known for its extreme simplicity. We generalize this
algorithm and apply it efficiently for several families of binary sequences.
Our algorithm is very simple, it requires $\beta N$ bit operations for a small
constant $\beta$, where $N$ is the period of the sequence. We make an analysis
on the number of bit operations required by the algorithm and compare it with
previous algorithms. In the process, the algorithm also finds the recursion for
the shortest linear feedback shift-register which generates the sequence. Some
other interesting properties related to shift-register sequences, which might
not be too surprising but generally unnoted, are also consequences of our
exposition.
","['\nYeow Meng Chee\n', '\nJohan Chrisnata\n', '\nTuvi Etzion\n', '\nHan Mao Kiah\n']",,,http://arxiv.org/abs/1912.11617v1,cs.CR,"['cs.CR', 'cs.SC']",,,[]
Deeply Integrating C11 Code Support into Isabelle/PIDE,http://arxiv.org/abs/1912.10630v1,2019-12-23T05:40:20Z,2019-12-23T05:40:20Z,"  We present a framework for C code in C11 syntax deeply integrated into the
Isabelle/PIDE development environment. Our framework provides an abstract
interface for verification back-ends to be plugged-in independently. Thus,
various techniques such as deductive program verification or white-box testing
can be applied to the same source, which is part of an integrated PIDE document
model. Semantic back-ends are free to choose the supported C fragment and its
semantics. In particular, they can differ on the chosen memory model or the
specification mechanism for framing conditions.
  Our framework supports semantic annotations of C sources in the form of
comments. Annotations serve to locally control back-end settings, and can
express the term focus to which an annotation refers. Both the logical and the
syntactic context are available when semantic annotations are evaluated. As a
consequence, a formula in an annotation can refer both to HOL or C variables.
  Our approach demonstrates the degree of maturity and expressive power the
Isabelle/PIDE subsystem has achieved in recent years. Our integration technique
employs Lex and Yacc style grammars to ensure efficient deterministic parsing.
We present two case studies for the integration of (known) semantic back-ends
in order to validate the design decisions for our back-end interface.
","['\nFrédéric Tuong\nLRI, Université Paris-Saclay\n', '\nBurkhart Wolff\nLRI, Université Paris-Saclay\n']","In Proceedings F-IDE 2019, arXiv:1912.09611","EPTCS 310, 2019, pp. 13-28",http://dx.doi.org/10.4204/EPTCS.310.3,cs.PL,"['cs.PL', 'cs.LO', 'cs.SC', 'cs.SE']",10.4204/EPTCS.310.3,,"['LRI, Université Paris-Saclay', 'LRI, Université Paris-Saclay']"
"Sparse Interpolation With Errors in Chebyshev Basis Beyond
  Redundant-Block Decoding",http://arxiv.org/abs/1912.05719v5,2019-12-12T01:19:33Z,2020-11-04T13:05:27Z,"  We present sparse interpolation algorithms for recovering a polynomial with
$\le B$ terms from $N$ evaluations at distinct values for the variable when
$\le E$ of the evaluations can be erroneous. Our algorithms perform exact
arithmetic in the field of scalars $\mathsf{K}$ and the terms can be standard
powers of the variable or Chebyshev polynomials, in which case the
characteristic of $\mathsf{K}$ is $\ne 2$. Our algorithms return a list of
valid sparse interpolants for the $N$ support points and run in
polynomial-time. For standard power basis our algorithms sample at $N = \lfloor
\frac{4}{3} E + 2 \rfloor B$ points, which are fewer points than $N = 2(E+1)B -
1$ given by Kaltofen and Pernet in 2014. For Chebyshev basis our algorithms
sample at $N = \lfloor \frac{3}{2} E + 2 \rfloor B$ points, which are also
fewer than the number of points required by the algorithm given by Arnold and
Kaltofen in 2015, which has $N = 74 \lfloor \frac{E}{13} + 1 \rfloor$ for $B =
3$ and $E \ge 222$. Our method shows how to correct $2$ errors in a block of
$4B$ points for standard basis and how to correct $1$ error in a block of $3B$
points for Chebyshev Basis.
","['\nErich L. Kaltofen\n', '\nZhi-Hong Yang\n']",in IEEE Transactions on Information Theory,,http://dx.doi.org/10.1109/TIT.2020.3027036,cs.SC,['cs.SC'],10.1109/TIT.2020.3027036,,[]
"Evaluation of Chebyshev polynomials on intervals and application to root
  finding",http://arxiv.org/abs/1912.05843v1,2019-12-12T09:17:56Z,2019-12-12T09:17:56Z,"  In approximation theory, it is standard to approximate functions by
polynomials expressed in the Chebyshev basis. Evaluating a polynomial $f$ of
degree n given in the Chebyshev basis can be done in $O(n)$ arithmetic
operations using the Clenshaw algorithm. Unfortunately, the evaluation of $f$
on an interval $I$ using the Clenshaw algorithm with interval arithmetic
returns an interval of width exponential in $n$. We describe a variant of the
Clenshaw algorithm based on ball arithmetic that returns an interval of width
quadratic in $n$ for an interval of small enough width. As an application, our
variant of the Clenshaw algorithm can be used to design an efficient root
finding algorithm.
","['\nViviane Ledoux\nGAMBLE, ENS Paris\n', '\nGuillaume Moroz\nGAMBLE\n']",,"Mathematical Aspects of Computer and Information Sciences 2019,
  Nov 2019, Gebze, Turkey",http://arxiv.org/abs/1912.05843v1,cs.SC,['cs.SC'],,,"['GAMBLE, ENS Paris', 'GAMBLE']"
"Modular Termination for Second-Order Computation Rules and Application
  to Algebraic Effect Handlers",http://arxiv.org/abs/1912.03434v11,2019-12-07T04:06:36Z,2022-06-13T11:34:05Z,"  We present a new modular proof method of termination for second-order
computation, and report its implementation SOL. The proof method is useful for
proving termination of higher-order foundational calculi. To establish the
method, we use a variation of semantic labelling translation and Blanqui's
General Schema: a syntactic criterion of strong normalisation. As an
application, we apply this method to show termination of a variant of
call-by-push-value calculus with algebraic effects and effect handlers. We also
show that our tool SOL is effective to solve higher-order termination problems.
",['\nMakoto Hamana\n'],,"Logical Methods in Computer Science, Volume 18, Issue 2 (June 14,
  2022) lmcs:6600",http://dx.doi.org/10.46298/lmcs-18(2:18)2022,cs.SC,"['cs.SC', 'cs.LO']",10.46298/lmcs-18(2:18)2022,,[]
"Building Executable Secure Design Models for Smart Contracts with Formal
  Methods",http://arxiv.org/abs/1912.04051v1,2019-12-09T14:03:21Z,2019-12-09T14:03:21Z,"  Smart contracts are appealing because they are self-executing business
agreements between parties with the predefined and immutable obligations and
rights. However, as with all software, smart contracts may contain
vulnerabilities because of design flaws, which may be exploited by one of the
parties to defraud the others. In this paper, we demonstrate a systematic
approach to building secure design models for smart contracts using formal
methods. To build the secure models, we first model the behaviors of
participating parties as state machines, and then, we model the predefined
obligations and rights of contracts, which specify the interactions among state
machines for achieving the business goal. After that, we illustrate executable
secure model design patterns in TLA+ (Temporal Logic of Actions) to against
well-known smart contract vulnerabilities in terms of state machines and
obligations and rights at the design level. These vulnerabilities are found in
Ethereum contracts, including Call to the unknown, Gasless send, Reentrancy,
Lost in the transfer, and Unpredictable state. The resultant TLA+
specifications are called secure models. We illustrate our approach to detect
the vulnerabilities using a real-estate contract example at the design level.
","['\nWeifeng Xu\n', '\nGlenn A. Fink\n']",6 pages,"The 3rd Workshop on Trusted Smart Contracts In Association with
  Financial Cryptography 2019, St. Kitts, Feb. 2019",http://arxiv.org/abs/1912.04051v1,cs.SE,"['cs.SE', 'cs.SC']",,,[]
A Fast Self-correcting $π$ Algorithm,http://arxiv.org/abs/1912.05319v4,2019-12-10T15:53:14Z,2019-12-21T09:30:12Z,"  We have rediscovered a simple algorithm to compute the mathematical constant
\[ \pi=3.14159265\cdots. \] The algorithm had been known for a long time but it
might not be recognized as a fast, practical algorithm. The time complexity of
it can be proved to be \[ O(M(n)\log^2 n) \] bit operations for computing $\pi$
with error $O(2^{-n})$, where $M(n)$ is the time complexity to multiply two
$n$-bit integers. We conjecture that the algorithm actually runs in \[
O(M(n)\log n). \] The algorithm is \emph{self-correcting} in the sense that,
given an approximated value of $\pi$ as an input, it can compute a more
accurate approximation of $\pi$ with cubic convergence.
",['\nTsz-Wo Sze\n'],9 pages,,http://arxiv.org/abs/1912.05319v4,math.NT,"['math.NT', 'cs.SC', '11Y60, 11Y16 (Primary) 68Q25 (Secondary)']",,,[]
"Analysis of the Conradi-Kahle Algorithm for Detecting Binomiality on
  Biological Models",http://arxiv.org/abs/1912.06896v1,2019-12-14T18:11:26Z,2019-12-14T18:11:26Z,"  We analyze the Conradi-Kahle Algorithm for detecting binomiality. We present
experiments using two implementations of the algorithm in Macaulay2 and Maple
on biological models and assess the performance of the algorithm on these
models. We compare the two implementations with each other and with Gr\""obner
bases computations up to their performance on these biological models.
","['\nAlexandru Iosif\n', '\nHamid Rahkooy\n']",,,http://arxiv.org/abs/1912.06896v1,q-bio.MN,"['q-bio.MN', 'cs.SC']",,,[]
"Visualizing Planar and Space Implicit Real Algebraic Curves with
  Singularities",http://arxiv.org/abs/1912.07507v1,2019-12-16T17:12:09Z,2019-12-16T17:12:09Z,"  We present a new method for visualizing implicit real algebraic curves inside
a bounding box in the $2$-D or $3$-D ambient space based on numerical
continuation and critical point methods. The underlying techniques work also
for tracing space curve in higher-dimensional space. Since the topology of a
curve near a singular point of it is not numerically stable, we trace only the
curve outside neighborhoods of singular points and replace each neighborhood
simply by a point, which produces a polygonal approximation that is
$\epsilon$-close to the curve. Such an approximation is more stable for
defining the numerical connectedness of the complement of the projection of the
curve in $\mathbb{R}^2$, which is important for applications such as solving
bi-parametric polynomial systems. The algorithm starts by computing three types
of key points of the curve, namely the intersection of the curve with small
spheres centered at singular points, regular critical points of every connected
components of the curve, as well as intersection points of the curve with the
given bounding box. It then traces the curve starting with and in the order of
the above three types of points. This basic scheme is further enhanced by
several optimizations, such as grouping singular points in natural clusters,
tracing the curve by a try-and-resume strategy and handling ""pseudo singular
points"". The effectiveness of the algorithm is illustrated by numerous
examples. This manuscript extends our preliminary results that appeared in CASC
2018.
","['\nChangbo Chen\n', '\nWenyuan Wu\n', '\nYong Feng\n']",,,http://arxiv.org/abs/1912.07507v1,cs.SC,"['cs.SC', 'cs.CG']",,,[]
"A refined machinery to calculate large moments from coupled systems of
  linear differential equations",http://arxiv.org/abs/1912.04390v1,2019-12-09T21:53:26Z,2019-12-09T21:53:26Z,"  The large moment method can be used to compute a large number of moments of
physical quantities that are described by coupled systems of linear
differential equations. Besides these systems the algorithm requires a certain
number of initial values as input, that are often hard to derive in a
preprocessing step.Thus a major challenge is to keep the number of initial
values as small as possible. We present the basic ideas of the underlying large
moment method and present refined versions that reduce significantly the number
of required initial values.
","['\nJohannes Blümlein\n', '\nPeter Marquard\n', '\nCarsten Schneider\n']",,,http://arxiv.org/abs/1912.04390v1,cs.SC,"['cs.SC', 'hep-ph', 'physics.comp-ph']",,,[]
"Chain Rules for Hessian and Higher Derivatives Made Easy by Tensor
  Calculus",http://arxiv.org/abs/1911.13292v1,2019-11-29T18:49:17Z,2019-11-29T18:49:17Z,"  Computing multivariate derivatives of matrix-like expressions in the compact,
coordinate free fashion is very important for both theory and applied
computations (e.g. optimization and machine learning).
  The critical components of such computations are \emph{chain and product
rules} for derivatives. Although they are taught early in simple scenarios,
practical applications involve high-dimensional arrays; in this context it is
very hard to find easy accessible and compact explanation.
  This paper discusses how to relatively simply carry such derivations based on
the (simplified as adapted in applied computer science) concept of tensors.
Numerical examples in modern Python libraries are provided. This discussion
simplifies and illustrates an earlier exposition by Manton (2012).
",['\nMaciej Skorski\n'],,,http://arxiv.org/abs/1911.13292v1,cs.SC,['cs.SC'],,,[]
Counting invariant subspaces and decompositions of additive polynomials,http://arxiv.org/abs/1912.00212v2,2019-11-30T14:40:50Z,2019-12-31T13:27:20Z,"  The functional (de)composition of polynomials is a topic in pure and computer
algebra with many applications. The structure of decompositions of (suitably
normalized) polynomials f(x) = g(h(x)) in F[x] over a field F is well
understood in many cases, but less well when the degree of f is divisible by
the positive characteristic p of F. This work investigates the decompositions
of r-additive polynomials, where every exponent and also the field size is a
power of r, which itself is a power of p.
  The decompositions of an r-additive polynomial f are intimately linked to the
Frobenius-invariant subspaces of its root space V in the algebraic closure of
F. We present an efficient algorithm to compute the rational Jordan form of the
Frobenius automorphism on V. A formula of Fripertinger (2011) then counts the
number of Frobenius-invariant subspaces of a given dimension and we derive the
number of decompositions with prescribed degrees.
","['\nJoachim von zur Gathen\n', '\nMark Giesbrecht\n', '\nKonstantin Ziegler\n']",,,http://arxiv.org/abs/1912.00212v2,cs.SC,['cs.SC'],,,[]
Computing syzygies in finite dimension using fast linear algebra,http://arxiv.org/abs/1912.01848v2,2019-12-04T08:36:40Z,2020-06-19T10:08:49Z,"  We consider the computation of syzygies of multivariate polynomials in a
finite-dimensional setting: for a $\mathbb{K}[X_1,\dots,X_r]$-module
$\mathcal{M}$ of finite dimension $D$ as a $\mathbb{K}$-vector space, and given
elements $f_1,\dots,f_m$ in $\mathcal{M}$, the problem is to compute syzygies
between the $f_i$'s, that is, polynomials $(p_1,\dots,p_m)$ in
$\mathbb{K}[X_1,\dots,X_r]^m$ such that $p_1 f_1 + \dots + p_m f_m = 0$ in
$\mathcal{M}$. Assuming that the multiplication matrices of the $r$ variables
with respect to some basis of $\mathcal{M}$ are known, we give an algorithm
which computes the reduced Gr\""obner basis of the module of these syzygies, for
any monomial order, using $O(m D^{\omega-1} + r D^\omega \log(D))$ operations
in the base field $\mathbb{K}$, where $\omega$ is the exponent of matrix
multiplication. Furthermore, assuming that $\mathcal{M}$ is itself given as
$\mathcal{M} = \mathbb{K}[X_1,\dots,X_r]^n/\mathcal{N}$, under some assumptions
on $\mathcal{N}$ we show that these multiplication matrices can be computed
from a Gr\""obner basis of $\mathcal{N}$ within the same complexity bound. In
particular, taking $n=1$, $m=1$ and $f_1=1$ in $\mathcal{M}$, this yields a
change of monomial order algorithm along the lines of the FGLM algorithm with a
complexity bound which is sub-cubic in $D$.
","['\nVincent Neiger\n', '\nÉric Schost\n']","34 pages, 7 algorithms, Journal of Complexity",,http://dx.doi.org/10.1016/j.jco.2020.101502,cs.SC,['cs.SC'],10.1016/j.jco.2020.101502,,[]
"Towards identification of explicit solutions to overdetermined systems
  of differential equations",http://arxiv.org/abs/1912.12126v1,2019-12-05T12:26:41Z,2019-12-05T12:26:41Z,"  The authors proposed a general way to find particular solutions for
overdetermined systems of PDEs previously, where the number of equations is
greater than the number of unknown functions. In this paper, we propose an
algorithm for finding solutions for overdetermined PDE systems, where we use a
method for finding an explicit solution for overdetermined algebraic
(polynomial) equations. Using this algorithm, the solution of some
overdetermined PDE systems can be obtained in explicit form. The main
difficulty of this algorithm is the huge number of polynomial equations that
arise, which need to be investigated and solved numerically or explicitly. For
example, the overdetermined hydrodynamic equations obtained earlier by the
authors give a minimum of 10 million such equations. However, if they are
solved explicitly, then we can write out the solution of the hydrodynamic
equations in a general form, which is of great scientific interest.
","['\nMaxim Zaytsev\n', ""\nV'yacheslav Akkerman\n""]",,,http://arxiv.org/abs/1912.12126v1,cs.SC,['cs.SC'],,,[]
"Improved cross-validation for classifiers that make algorithmic choices
  to minimise runtime without compromising output correctness",http://arxiv.org/abs/1911.12672v1,2019-11-28T12:35:55Z,2019-11-28T12:35:55Z,"  Our topic is the use of machine learning to improve software by making
choices which do not compromise the correctness of the output, but do affect
the time taken to produce such output. We are particularly concerned with
computer algebra systems (CASs), and in particular, our experiments are for
selecting the variable ordering to use when performing a cylindrical algebraic
decomposition of $n$-dimensional real space with respect to the signs of a set
of polynomials.
  In our prior work we explored the different ML models that could be used, and
how to identify suitable features of the input polynomials. In the present
paper we both repeat our prior experiments on problems which have more
variables (and thus exponentially more possible orderings), and examine the
metric which our ML classifiers targets. The natural metric is computational
runtime, with classifiers trained to pick the ordering which minimises this.
However, this leads to the situation were models do not distinguish between any
of the non-optimal orderings, whose runtimes may still vary dramatically. In
this paper we investigate a modification to the cross-validation algorithms of
the classifiers so that they do distinguish these cases, leading to improved
results.
","['\nDorian Florescu\n', '\nMatthew England\n']","16 pages. Accepted into the Proceedings of MACIS 2019. arXiv admin
  note: text overlap with arXiv:1906.01455","Mathematical Aspects of Computer and Information Sciences (Proc.
  MACIS '19), LNCS vol 11989, pages 169-184, Springer International, 2020",http://dx.doi.org/10.1007/978-3-030-43120-4_27,cs.SC,"['cs.SC', 'cs.LG', '68W30, 68T05, 03C10', 'I.2.6; I.1.0']",10.1007/978-3-030-43120-4_27,,[]
Deep Learning for Symbolic Mathematics,http://arxiv.org/abs/1912.01412v1,2019-12-02T15:05:24Z,2019-12-02T15:05:24Z,"  Neural networks have a reputation for being better at solving statistical or
approximate problems than at performing calculations or working with symbolic
data. In this paper, we show that they can be surprisingly good at more
elaborated tasks in mathematics, such as symbolic integration and solving
differential equations. We propose a syntax for representing mathematical
problems, and methods for generating large datasets that can be used to train
sequence-to-sequence models. We achieve results that outperform commercial
Computer Algebra Systems such as Matlab or Mathematica.
","['\nGuillaume Lample\n', '\nFrançois Charton\n']",,,http://arxiv.org/abs/1912.01412v1,cs.SC,"['cs.SC', 'cs.LG']",,,[]
Schur Polynomials do not have small formulas if the Determinant doesn't!,http://arxiv.org/abs/1911.12520v1,2019-11-28T04:26:29Z,2019-11-28T04:26:29Z,"  Schur Polynomials are families of symmetric polynomials that have been
classically studied in Combinatorics and Algebra alike. They play a central
role in the study of Symmetric functions, in Representation theory [Sta99], in
Schubert calculus [LM10] as well as in Enumerative combinatorics [Gas96, Sta84,
Sta99]. In recent years, they have also shown up in various incarnations in
Computer Science, e.g, Quantum computation [HRTS00, OW15] and Geometric
complexity theory [IP17].
  However, unlike some other families of symmetric polynomials like the
Elementary Symmetric polynomials, the Power Symmetric polynomials and the
Complete Homogeneous Symmetric polynomials, the computational complexity of
syntactically computing Schur polynomials has not been studied much. In
particular, it is not known whether Schur polynomials can be computed
efficiently by algebraic formulas. In this work, we address this question, and
show that unless \emph{every} polynomial with a small algebraic branching
program (ABP) has a small algebraic formula, there are Schur polynomials that
cannot be computed by algebraic formula of polynomial size. In other words,
unless the algebraic complexity class $\mathrm{VBP}$ is equal to the complexity
class $\mathrm{VF}$, there exist Schur polynomials which do not have polynomial
size algebraic formulas.
  As a consequence of our proof, we also show that computing the determinant of
certain \emph{generalized} Vandermonde matrices is essentially as hard as
computing the general symbolic determinant. To the best of our knowledge, these
are one of the first hardness results of this kind for families of polynomials
which are not \emph{multilinear}. A key ingredient of our proof is the study of
composition of \emph{well behaved} algebraically independent polynomials with a
homogeneous polynomial, and might be of independent interest.
","['\nPrasad Chaugule\n', '\nMrinal Kumar\n', '\nNutan Limaye\n', '\nChandra Kanta Mohapatra\n', '\nAdrian She\n', '\nSrikanth Srinivasan\n']",,,http://arxiv.org/abs/1911.12520v1,cs.CC,"['cs.CC', 'cs.DM', 'cs.SC']",,,[]
Efficient Recognition of Graph Languages,http://arxiv.org/abs/1911.12884v3,2019-11-28T22:32:41Z,2021-01-01T12:34:22Z,"  Graph transformation is the rule-based modification of graphs, and is a
discipline dating back to the 1970s. In general, to match the left-hand graph
of a fixed rule within a host graph requires polynomial time, but to improve
matching performance, D\""orr proposed to equip rules and host graphs with
distinguished root nodes. This model was implemented by Plump and Bak, but
unfortunately, such rules are not invertible. We address this problem by
defining rootedness using a partial function into a two-point set rather than
pointing graphs with root nodes, meaning derivations are natural double
pushouts. Moreover, we give a sufficient condition on rules to give constant
time rule application on graphs of bounded degree, and that, the graph class of
trees can be recognised in linear time, given an input graph of bounded degree.
Finally, we define a new notion of confluence up to garbage and non-garbage
critical pairs, showing it is sufficient to require strong joinability of only
the non-garbage critical pairs to establish confluence up to garbage. Finally,
this new result, presented for conventional graph transformation systems, can
be lifted to our rooted setting by encoding node labels and rootedness as
looped edges.
","['\nGraham Campbell\n', '\nDetlef Plump\n']","Project Report, Department of Computer Science, University of York,
  83 pages, 2019. arXiv admin note: substantial text overlap with
  arXiv:1906.05170",,http://arxiv.org/abs/1911.12884v3,cs.LO,"['cs.LO', 'cs.CC', 'cs.SC', '68Q42']",,,[]
On Computational Poisson Geometry I: Symbolic Foundations,http://arxiv.org/abs/1912.01746v1,2019-12-04T00:10:04Z,2019-12-04T00:10:04Z,"  We present a computational toolkit for (local) Poisson-Nijenhuis calculus on
manifolds. Our python module $\textsf{PoissonGeometry}$ implements our
algorithms, and accompanies this paper. We include two examples of how our
methods can be used, one for gauge transformations of Poisson bivectors in
dimension 3, and a second one that determines parametric Poisson bivector
fields in dimension 4.
","['\nM. A. Evangelista-Alvarado\n', '\nJ. C. Ruíz-Pantaleón\n', '\nP. Suárez-Serrato\n']","21 pages, 19 Algorithms; Our code repository is found at
  https://github.com/appliedgeometry/poissongeometry","Jour. Geometric Mechanics 13 (2021), no. 4, 607--628",http://dx.doi.org/10.3934/jgm.2021018,math.DG,"['math.DG', 'cs.SC', 'math.DS', 'math.SG']",10.3934/jgm.2021018,,[]
Complexity of a Root Clustering Algorithm,http://arxiv.org/abs/1912.02820v1,2019-12-05T08:59:16Z,2019-12-05T08:59:16Z,"  Approximating the roots of a holomorphic function in an input box is a
fundamental problem in many domains. Most algorithms in the literature for
solving this problem are conditional, i.e., they make some simplifying
assumptions, such as, all the roots are simple or there are no roots on the
boundary of the input box, or the underlying machine model is Real RAM. Root
clustering is a generalization of the root approximation problem that allows
for errors in the computation and makes no assumption on the multiplicity of
the roots. An unconditional algorithm for computing a root clustering of a
holomorphic function was given by Yap, Sagraloff and Sharma in 2013. They
proposed a subdivision based algorithm using effective predicates based on
Pellet's test while avoiding any comparison with zeros (using soft zero
comparisons instead). In this paper, we analyze the running time of their
algorithm. We use the continuous amortization framework to derive an upper
bound on the size of the subdivision tree. We specialize this bound to the case
of polynomials and some simple transcendental functions such as exponential and
trigonometric sine. We show that the algorithm takes exponential time even for
these simple functions, unlike the case of polynomials. We also derive a bound
on the bit-precision used by the algorithm. To the best of our knowledge, this
is the first such result for holomorphic functions. We introduce new geometric
parameters, such as the relative growth of the function on the input box, for
analyzing the algorithm. Thus, our estimates naturally generalize the known
results, i.e., for the case of polynomials.
","['\nPrashant Batra\n', '\nVikram Sharma\n']","52 pages, 1 figure",,http://arxiv.org/abs/1912.02820v1,cs.DS,"['cs.DS', 'cs.CC', 'cs.SC']",,,[]
Algebraic Analysis of Rotation Data,http://arxiv.org/abs/1912.00396v1,2019-12-01T12:45:20Z,2019-12-01T12:45:20Z,"  We develop algebraic tools for statistical inference from samples of rotation
matrices. This rests on the theory of D-modules in algebraic analysis.
Noncommutative Gr\""obner bases are used to design numerical algorithms for
maximum likelihood estimation, building on the holonomic gradient method of
Sei, Shibata, Takemura, Ohara, and Takayama. We study the Fisher model for
sampling from rotation matrices, and we apply our algorithms for data from the
applied sciences. On the theoretical side, we generalize the underlying
equivariant D-modules from SO(3) to arbitrary Lie groups. For compact groups,
our D-ideals encode the normalizing constant of the Fisher model.
","['\nMichael F. Adamer\n', '\nAndrás C. Lőrincz\n', '\nAnna-Laura Sattelberger\n', '\nBernd Sturmfels\n']",21 pages,Alg. Stat. 11 (2020) 189-211,http://dx.doi.org/10.2140/astat.2020.11.189,math.ST,"['math.ST', 'cs.SC', 'math.AG', 'math.OC', 'stat.TH']",10.2140/astat.2020.11.189,,[]
"On sequences associated to the invariant theory of rank two simple Lie
  algebras",http://arxiv.org/abs/1911.10288v1,2019-11-23T00:11:51Z,2019-11-23T00:11:51Z,"  We study two families of sequences, listed in the On-Line Encyclopedia of
Integer Sequences (OEIS), which are associated to invariant theory of Lie
algebras. For the first family, we prove combinatorially that the sequences
A059710 and A108307 are related by a binomial transform. Based on this, we
present two independent proofs of a recurrence equation for A059710, which was
conjectured by Mihailovs. Besides, we also give a direct proof of Mihailovs'
conjecture by the method of algebraic residues. As a consequence, closed
formulae for the generating function of sequence A059710 are obtained in terms
of classical Gaussian hypergeometric functions. Moreover, we show that
sequences in the second family are also related by binomial transforms.
","['\nAlin Bostan\n', '\nJordan Tirrell\n', '\nBruce W. Westbury\n', '\nYi Zhang\n']",12 pages,,http://arxiv.org/abs/1911.10288v1,math.CO,"['math.CO', 'cs.SC']",,,[]
New practical advances in polynomial root clustering,http://arxiv.org/abs/1911.06706v1,2019-11-15T15:32:56Z,2019-11-15T15:32:56Z,"  We report an ongoing work on clustering algorithms for complex roots of a
univariate polynomial $p$ of degree $d$ with real or complex coefficients. As
in their previous best subdivision algorithms our root-finders are robust even
for multiple roots of a polynomial given by a black box for the approximation
of its coefficients, and their complexity decreases at least proportionally to
the number of roots in a region of interest (ROI) on the complex plane, such as
a disc or a square, but we greatly strengthen the main ingredient of the
previous algorithms. Namely our new counting test essentially amounts to the
evaluation of a polynomial $p$ and its derivative $p'$, which is a major
benefit, e.g., for sparse polynomials $p$. Moreover with evaluation at about
$\log(d)$ points (versus the previous record of order $d$) we output correct
number of roots in a disc whose contour has no roots of $p$ nearby. Moreover we
greatly soften the latter requirement versus the known subdivision algorithms.
Our second and less significant contribution concerns subdivision algorithms
for polynomials with real coefficients. Our tests demonstrate the power of the
proposed algorithms.
","['\nRémi Imbach\n', '\nVictor Y. Pan\n']",Version submitted and accepted to MACIS 2019,,http://arxiv.org/abs/1911.06706v1,cs.SC,"['cs.SC', 'cs.NA', 'math.NA']",,,[]
"Improving Graph Neural Network Representations of Logical Formulae with
  Subgraph Pooling",http://arxiv.org/abs/1911.06904v3,2019-11-15T23:12:30Z,2020-06-05T17:24:50Z,"  Recent advances in the integration of deep learning with automated theorem
proving have centered around the representation of logical formulae as inputs
to deep learning systems. In particular, there has been a growing interest in
adapting structure-aware neural methods to work with the underlying graph
representations of logical expressions. While more effective than character and
token-level approaches, graph-based methods have often made representational
trade-offs that limited their ability to capture key structural properties of
their inputs. In this work we propose a novel approach for embedding logical
formulae that is designed to overcome the representational limitations of prior
approaches. Our architecture works for logics of different expressivity; e.g.,
first-order and higher-order logic. We evaluate our approach on two standard
datasets and show that the proposed architecture achieves state-of-the-art
performance on both premise selection and proof step classification.
","['\nMaxwell Crouse\n', '\nIbrahim Abdelaziz\n', '\nCristina Cornelio\n', '\nVeronika Thost\n', '\nLingfei Wu\n', '\nKenneth Forbus\n', '\nAchille Fokoue\n']",,,http://arxiv.org/abs/1911.06904v3,cs.AI,"['cs.AI', 'cs.LG', 'cs.LO', 'cs.SC']",,,[]
Fast Derivatives for Multilinear Polynomials,http://arxiv.org/abs/1911.02235v2,2019-11-06T07:31:41Z,2020-09-24T13:06:47Z,"  The article considers linear functions of many (n) variables - multilinear
polynomials (MP). The three-steps evaluation is presented that uses the minimal
possible number of floating point operations for non-sparse MP at each step.
The minimal number of additions is achieved in the algorithm for fast MP
derivatives (FMPD) calculation. The cost of evaluating all first derivatives
approaches to only 1/8 of MP evaluation with a growing number of variables. The
FMPD algorithm structure exhibits similarity to the Fast Fourier Transformation
(FFT) algorithm.
",['\nValeri Aronov\n'],"7 pages, 3 figures",,http://arxiv.org/abs/1911.02235v2,cs.SC,['cs.SC'],,,[]
"Entropy supplementary conservation law for non-linear systems of PDEs
  with non-conservative terms: application to the modelling and analysis of
  complex fluid flows using computer algebra",http://arxiv.org/abs/1911.02313v1,2019-11-06T11:00:40Z,2019-11-06T11:00:40Z,"  In the present contribution, we investigate first-order nonlinear systems of
partial differential equations which are constituted of two parts: a system of
conservation laws and non-conservative first order terms. Whereas the theory of
first-order systems of conservation laws is well established and the conditions
for the existence of supplementary conservation laws, and more specifically of
an entropy supplementary conservation law for smooth solutions, well known,
there exists so far no general extension to obtain such supplementary
conservation laws when non-conservative terms are present. We propose a
framework in order to extend the existing theory and show that the presence of
non-conservative terms somewhat complexifies the problem since numerous
combinations of the conservative and non-conservative terms can lead to a
supplementary conservation law. We then identify a restricted framework in
order to design and analyze physical models of complex fluid flows by means of
computer algebra and thus obtain the entire ensemble of possible combination of
conservative and non-conservative terms with the objective of obtaining
specifically an entropy supplementary conservation law. The theory as well as
developed computer algebra tool are then applied to a Baer-Nunziato two-phase
flow model and to a multicomponent plasma fluid model. The first one is a
first-order fluid model, with non-conservative terms impacting on the linearly
degenerate field and requires a closure since there is no way to derive
interfacial quantities from averaging principles and we need guidance in order
to close the pressure and velocity of the interface and the thermodynamics of
the mixture. The second one involves first order terms for the heavy species
coupled to second order terms for the electrons, the non-conservative terms
impact the genuinely nonlinear fields and the model can be rigorously derived
from kinetic theory. We show how the theory allows to recover the whole
spectrum of closures obtained so far in the literature for the two-phase flow
system as well as conditions when one aims at extending the thermodynamics and
also applies to the plasma case, where we recover the usual entropy
supplementary equation, thus assessing the effectiveness and scope of the
proposed theory.
","['\nPierre Cordesse\nCMAP\n', '\nMarc Massot\nCMAP\n']",,"Communications in Mathematical Sciences, Vol 18 (2), (2020)
  515-534",http://dx.doi.org/10.4310/CMS.2020.v18.n2.a10,cs.SC,['cs.SC'],10.4310/CMS.2020.v18.n2.a10,,"['CMAP', 'CMAP']"
"Minimal representations and algebraic relations for single nested
  products",http://arxiv.org/abs/1911.04837v1,2019-11-12T13:27:14Z,2019-11-12T13:27:14Z,"  Recently, it has been shown constructively how a finite set of hypergeometric
products, multibasic hypergeometric products or their mixed versions can be
modeled properly in the setting of formal difference rings. Here special
emphasis is put on robust constructions: whenever further products have to be
considered, one can reuse --up to some mild modifications-- the already
existing difference ring. In this article we relax this robustness criteria and
seek for another form of optimality. We will elaborate a general framework to
represent a finite set of products in a formal difference ring where the number
of transcendental product generators is minimal. As a bonus we are able to
describe explicitly all relations among the given input products.
",['\nCarsten Schneider\n'],,,http://arxiv.org/abs/1911.04837v1,cs.SC,['cs.SC'],,,[]
"A Nonexistence Certificate for Projective Planes of Order Ten with
  Weight 15 Codewords",http://arxiv.org/abs/1911.04032v2,2019-11-11T01:48:58Z,2020-03-25T22:54:52Z,"  Using techniques from the fields of symbolic computation and satisfiability
checking we verify one of the cases used in the landmark result that projective
planes of order ten do not exist. In particular, we show that there exist no
projective planes of order ten that generate codewords of weight fifteen, a
result first shown in 1973 via an exhaustive computer search. We provide a
simple satisfiability (SAT) instance and a certificate of unsatisfiability that
can be used to automatically verify this result for the first time. All
previous demonstrations of this result have relied on search programs that are
difficult or impossible to verify---in fact, our search found partial
projective planes that were missed by previous searches due to previously
undiscovered bugs. Furthermore, we show how the performance of the SAT solver
can be dramatically increased by employing functionality from a computer
algebra system (CAS). Our SAT+CAS search runs significantly faster than all
other published searches verifying this result.
","['\nCurtis Bright\n', '\nKevin Cheung\n', '\nBrett Stevens\n', '\nDominique Roy\n', '\nIlias Kotsireas\n', '\nVijay Ganesh\n']","To appear in Applicable Algebra in Engineering, Communication and
  Computing",,http://dx.doi.org/10.1007/s00200-020-00426-y,cs.DM,"['cs.DM', 'cs.LO', 'cs.SC', 'math.CO']",10.1007/s00200-020-00426-y,,[]
A Root-Free Splitting-Lemma for Systems of Linear Differential Equations,http://arxiv.org/abs/1911.05837v1,2019-11-02T16:29:48Z,2019-11-02T16:29:48Z,"  We consider the formal reduction of a system of linear differential equations
and show that, if the system can be block-diagonalised through transformation
with a ramified Shearing-transformation and following application of the
Splitting Lemma, and if the spectra of the leading block matrices of the
ramified system satisfy a symmetry condition, this block-diagonalisation can
also be achieved through an unramified transformation. Combined with classical
results by Turritin and Wasow as well as work by Balser, this yields a
constructive and simple proof of the existence of an unramified block-diagonal
form from which formal invariants such as the Newton polygon can be read
directly. Our result is particularly useful for designing efficient algorithms
for the formal reduction of the system.
",['\nEckhard Pflügel\n'],,,http://arxiv.org/abs/1911.05837v1,cs.SC,['cs.SC'],,,[]
A sufficient condition for local nonnegativity,http://arxiv.org/abs/1910.13815v1,2019-10-29T02:21:26Z,2019-10-29T02:21:26Z,"  A real polynomial $f$ is called local nonnegative at a point $p$, if it is
nonnegative in a neighbourhood of $p$. In this paper, a sufficient condition
for determining this property is constructed. Newton's principal part of $f$
(denoted as $f_N$) plays a key role in this process. We proved that if every
$F$-face, $(f_N)_F$, of $f_N$ is strictly positive over $(\mathbb{R}\setminus
0)^n$, then $f$ is local nonnegative at the origin $O$.
","['\nJia Xu\n', '\nYong Yao\n']",,,http://arxiv.org/abs/1910.13815v1,math.AG,"['math.AG', 'cs.SC']",,,[]
"qFunctions -- A Mathematica package for $q$-series and partition theory
  applications",http://arxiv.org/abs/1910.12410v1,2019-10-28T02:43:03Z,2019-10-28T02:43:03Z,"  We describe the qFunctions Mathematica package for $q$-series and partition
theory applications. This package includes both experimental and symbolic
tools. The experimental set of elements includes guessers for $q$-shift
equations and recurrences for given $q$-series and fitting/finding explicit
expressions for sequences of polynomials. This package can symbolically handle
formal manipulations on $q$-differential, $q$-shift equations and recurrences,
such as switching between these forms, finding the greatest common divisor of
recurrences, and formal substitutions. Here, we also extend the classical
method of the weighted words approach. Moreover, qFunctions has implementations
that automate the recurrence system creation of the weighted words approach as
well as a scheme on cylindric partitions.
","['\nJakob Ablinger\n', '\nAli K. Uncu\n']",17 pages,,http://arxiv.org/abs/1910.12410v1,cs.SC,"['cs.SC', 'math.CO', 'math.NT', '05A15, 05A17, 05A19, 11P81, 33D15, 68-01, 68R05']",,,[]
Multi-Agent Safety Verification using Symmetry Transformations,http://arxiv.org/abs/1911.00608v1,2019-11-01T22:51:34Z,2019-11-01T22:51:34Z,"  We show that symmetry transformations and caching can enable scalable, and
possibly unbounded, verification of multi-agent systems. Symmetry
transformations map solutions and to other solutions. We show that this
property can be used to transform cached reachsets to compute new reachsets,
for hybrid and multi-agent models. We develop a notion of virtual system which
define symmetry transformations for a broad class of agent models that visit
waypoint sequences. Using this notion of virtual system, we present a prototype
tool CacheReach that builds a cache of reachtubes for this system, in a way
that is agnostic of the representation of the reachsets and the reachability
analysis subroutine used. Our experimental evaluation of CacheReach shows up to
66% savings in safety verification computation time on multi-agent systems with
3-dimensional linear and 4-dimensional nonlinear fixed-wing aircraft models
following sequences of waypoints. These savings and our theoretical results
illustrate the potential benefits of using symmetry-based caching in the safety
verification of multi-agent systems.
","['\nHussein Sibai\n', '\nNavid Mokhlesi\n', '\nChuchu Fan\n', '\nSayan Mitra\n']",,,http://arxiv.org/abs/1911.00608v1,cs.FL,"['cs.FL', 'cs.LO', 'cs.SC', 'cs.SY', 'eess.SY']",,,[]
Review of Recent Heap Specification and Verification Techniques,http://arxiv.org/abs/1910.10176v2,2019-10-22T18:01:33Z,2022-03-24T11:43:05Z,"  The article provides an overview of the existing methods of dynamic memory
verification; a comparative analysis is carried out; the applicability for
solving problems of control, monitoring, and verification of dynamic memory is
evaluated. This article is divided into eight sections. The first section
introduces formal verification, followed by a section that discusses dynamic
memory management problems. The third section discusses Hoare's calculus
resumed by heap transformations to the stack. The fifth and sixth sections
introduce the concept of dynamic memory shape analysis and the rotation of
pointers. The seventh is on separation logic. The last section discusses
possible areas of further research, particularly the recognition at recording
level of various instances of objects; automation of proofs; ""hot"" code, that
is, software code that updates itself when the program runs; expanding
intuitiveness, for instance, on proof explanations.
",['\nRené Haberland\n'],"fully translated preprint (English); final journal paper 26 pages (in
  Russian)","Computer Tools in Education Journal, ISSN 2071-2359, ISSN
  2071-2340",http://dx.doi.org/10.32603/2071-2340-2019-2-5-30,cs.LO,"['cs.LO', 'cs.SC']",10.32603/2071-2340-2019-2-5-30,,[]
Programming and Symbolic Computation in Maude,http://arxiv.org/abs/1910.08416v1,2019-10-18T13:38:57Z,2019-10-18T13:38:57Z,"  Rewriting logic is both a flexible semantic framework within which widely
different concurrent systems can be naturally specified and a logical framework
in which widely different logics can be specified. Maude programs are exactly
rewrite theories. Maude has also a formal environment of verification tools.
Symbolic computation is a powerful technique for reasoning about the
correctness of concurrent systems and for increasing the power of formal tools.
We present several new symbolic features of Maude that enhance formal reasoning
about Maude programs and the effectiveness of formal tools. They include: (i)
very general unification modulo user-definable equational theories, and (ii)
symbolic reachability analysis of concurrent systems using narrowing. The paper
does not focus just on symbolic features: it also describes several other new
Maude features, including: (iii) Maude's strategy language for controlling
rewriting, and (iv) external objects that allow flexible interaction of Maude
object-based concurrent systems with the external world. In particular,
meta-interpreters are external objects encapsulating Maude interpreters that
can interact with many other objects. To make the paper self-contained and give
a reasonably complete language overview, we also review the basic Maude
features for equational rewriting and rewriting with rules, Maude programming
of concurrent object systems, and reflection. Furthermore, we include many
examples illustrating all the Maude notions and features described in the
paper.
","['\nFrancisco Durán\n', '\nSteven Eker\n', '\nSantiago Escobar\n', '\nNarciso Martí-Oliet\n', '\nJosé Meseguer\n', '\nRubén Rubio\n', '\nCarolyn Talcott\n']","Journal of Logical and Algebraic Methods in Programming, 2019",,http://dx.doi.org/10.1016/j.jlamp.2019.100497,cs.LO,"['cs.LO', 'cs.PL', 'cs.SC']",10.1016/j.jlamp.2019.100497,,[]
D-Modules and Holonomic Functions,http://arxiv.org/abs/1910.01395v2,2019-10-03T10:50:17Z,2019-12-17T12:51:34Z,"  In algebraic geometry, one studies the solutions to polynomial equations, or,
equivalently, to linear partial differential equations with constant
coefficients. These lecture notes address the more general case when the
coefficients are polynomials. The letter D stands for the Weyl algebra, and a
D-module is a left module over D. We focus on left ideals, or D-ideals. We
represent holonomic functions in several variables by the linear differential
equations they satisfy. This encoding by a D-ideal is useful for many problems,
e.g., in geometry, physics and statistics. We explain how to work with
holonomic functions. Applications include volume computations and likelihood
inference.
","['\nAnna-Laura Sattelberger\n', '\nBernd Sturmfels\n']","36 pages; smaller corrections and improvements; adapted to the
  stylefile of the proceedings of the Thematic Einstein Semester on Algebraic
  Geometry; hints and solutions for solving the problems added",,http://arxiv.org/abs/1910.01395v2,math.AG,"['math.AG', 'cs.SC']",,,[]
The Multivariate Schwartz-Zippel Lemma,http://arxiv.org/abs/1910.01095v5,2019-10-02T17:20:41Z,2021-11-22T04:20:16Z,"  Motivated by applications in combinatorial geometry, we consider the
following question: Let $\lambda=(\lambda_1,\lambda_2,\ldots,\lambda_m)$ be an
$m$-partition of a positive integer $n$, $S_i \subseteq \mathbb{C}^{\lambda_i}$
be finite sets, and let $S:=S_1 \times S_2 \times \ldots \times S_m \subset
\mathbb{C}^n$ be the multi-grid defined by $S_i$. Suppose $p$ is an $n$-variate
degree $d$ polynomial. How many zeros does $p$ have on $S$?
  We first develop a multivariate generalization of Combinatorial
Nullstellensatz that certifies existence of a point $t \in S$ so that $p(t)
\neq 0$. Then we show that a natural multivariate generalization of the
DeMillo-Lipton-Schwartz-Zippel lemma holds, except for a special family of
polynomials that we call $\lambda$-reducible. This yields a simultaneous
generalization of Szemer\'edi-Trotter theorem and Schwartz-Zippel lemma into
higher dimensions, and has applications in incidence geometry. Finally, we
develop a symbolic algorithm that identifies certain $\lambda$-reducible
polynomials. More precisely, our symbolic algorithm detects polynomials that
include a cartesian product of hypersurfaces in their zero set. It is likely
that using Chow forms the algorithm can be generalized to handle arbitrary
$\lambda$-reducible polynomials, which we leave as an open problem.
","['\nM. Levent Doğan\n', '\nAlperen A. Ergür\n', '\nJake D. Mundo\n', '\nElias Tsigaridas\n']","Added a few elementary lemmas to improve readability, and fixed a
  mistake in a proof in the previous version. We spotted the mistake after a
  question of Joshua Zahl, and very thankful for his question. The paper is to
  appear in SIAM Journal of Discrete Mathematics","SIAM Journal of Discrete Mathematics, Vol 36, Issue 2, 2022",http://dx.doi.org/10.1137/20M1333869,math.CO,"['math.CO', 'cs.CG', 'cs.SC', 'math.AG', 'P52C10, 12D10']",10.1137/20M1333869,,[]
Approximate GCD in a Bernstein basis,http://arxiv.org/abs/1910.01998v1,2019-10-04T15:42:02Z,2019-10-04T15:42:02Z,"  We adapt Victor Y. Pan's root-based algorithm for finding approximate GCD to
the case where the polynomials are expressed in Bernstein bases. We use the
numerically stable companion pencil of Gudbj\""orn F. J\'onsson to compute the
roots, and the Hopcroft-Karp bipartite matching method to find the degree of
the approximate GCD. We offer some refinements to improve the process.
","['\nRobert M. Corless\n', '\nLeili Rafiee Sevyeri\n']",,,http://arxiv.org/abs/1910.01998v1,math.NA,"['math.NA', 'cs.NA', 'cs.SC']",,,[]
"Efficiently and Effectively Recognizing Toricity of Steady State
  Varieties",http://arxiv.org/abs/1910.04100v2,2019-10-09T16:25:58Z,2020-04-15T07:40:47Z,"  We consider the problem of testing whether the points in a complex or real
variety with non-zero coordinates form a multiplicative group or, more
generally, a coset of a multiplicative group. For the coset case, we study the
notion of shifted toric varieties which generalizes the notion of toric
varieties. This requires a geometric view on the varieties rather than an
algebraic view on the ideals. We present algorithms and computations on 129
models from the BioModels repository testing for group and coset structures
over both the complex numbers and the real numbers. Our methods over the
complex numbers are based on Gr\""obner basis techniques and binomiality tests.
Over the real numbers we use first-order characterizations and employ real
quantifier elimination. In combination with suitable prime decompositions and
restrictions to subspaces it turns out that almost all models show coset
structure. Beyond our practical computations, we give upper bounds on the
asymptotic worst-case complexity of the corresponding problems by proposing
single exponential algorithms that test complex or real varieties for toricity
or shifted toricity. In the positive case, these algorithms produce generating
binomials. In addition, we propose an asymptotically fast algorithm for testing
membership in a binomial variety over the algebraic closure of the rational
numbers.
","['\nDima Grigoriev\n', '\nAlexandru Iosif\n', '\nHamid Rahkooy\n', '\nThomas Sturm\n', '\nAndreas Weber\n']","We made the presentation clearer and fixed many small flaws and
  typos. A database with our computations is now available as ancillary file","Math. Comput. Sci., 15(2):199-232, Jun 2021",http://dx.doi.org/10.1007/s11786-020-00479-9,q-bio.MN,"['q-bio.MN', 'cs.SC', 'math.AG', '14Q20 (Primary), 14P05, 92C42 (Secondary)']",10.1007/s11786-020-00479-9,,[]
Input-output equations and identifiability of linear ODE models,http://arxiv.org/abs/1910.03960v5,2019-10-09T12:53:23Z,2022-01-27T17:02:05Z,"  Structural identifiability is a property of a differential model with
parameters that allows for the parameters to be determined from the model
equations in the absence of noise. The method of input-output equations is one
method for verifying structural identifiability. This method stands out in its
importance because the additional insights it provides can be used to analyze
and improve models. However, its complete theoretical grounds and applicability
are still to be established. A subtlety and key for this method to work
correctly is knowing whether the coefficients of these equations are
identifiable.
  In this paper, to address this, we prove identifiability of the coefficients
of input-output equations for types of differential models that often appear in
practice, such as linear models with one output and linear compartment models
in which, from each compartment, one can reach either a leak or an input. This
shows that checking identifiability via input-output equations for these models
is legitimate and, as we prove, that the field of identifiable functions is
generated by the coefficients of the input-output equations. Finally, we
exploit a connection between input-output equations and the transfer function
matrix to show that, for a linear compartment model with an input and strongly
connected graph, the field of all identifiable functions is generated by the
coefficients of the transfer function matrix even if the initial conditions are
generic.
","['\nAlexey Ovchinnikov\n', '\nGleb Pogudin\n', '\nPeter Thompson\n']",,,http://dx.doi.org/10.1109/TAC.2022.3145571,math.DS,"['math.DS', 'cs.SC', 'cs.SY', 'eess.SY', 'math.AC', '12H05, 34A55, 92B05, 93C15, 93B25, 93B30']",10.1109/TAC.2022.3145571,,[]
Equivariant Hilbert series for hierarchical models,http://arxiv.org/abs/1909.13026v2,2019-09-28T05:15:24Z,2019-10-03T19:16:41Z,"  Toric ideals to hierarchical models are invariant under the action of a
product of symmetric groups. Taking the number of factors, say m, into account,
we introduce and study invariant filtrations and their equivariant Hilbert
series. We present a condition that guarantees that the equivariant Hilbert
series is a rational function in m+1 variables with rational coefficients.
Furthermore we give explicit formulas for the rational functions with
coefficients in a number field and an algorithm for determining the rational
functions with rational coefficients. A key is to construct finite automata
that recognize languages corresponding to invariant filtrations.
","['\nAida Maraj\n', '\nUwe Nagel\n']",Some fixed typos,Alg. Stat. 12 (2021) 21-42,http://dx.doi.org/10.2140/astat.2021.12.21,math.AC,"['math.AC', 'cs.SC']",10.2140/astat.2021.12.21,,[]
Graph Neural Reasoning May Fail in Certifying Boolean Unsatisfiability,http://arxiv.org/abs/1909.11588v2,2019-09-25T16:24:50Z,2019-09-27T16:57:26Z,"  It is feasible and practically-valuable to bridge the characteristics between
graph neural networks (GNNs) and logical reasoning. Despite considerable
efforts and successes witnessed to solve Boolean satisfiability (SAT), it
remains a mystery of GNN-based solvers for more complex predicate logic
formulae. In this work, we conjectures with some evidences, that
generally-defined GNNs present several limitations to certify the
unsatisfiability (UNSAT) in Boolean formulae. It implies that GNNs may probably
fail in learning the logical reasoning tasks if they contain proving UNSAT as
the sub-problem included by most predicate logic formulae.
","['\nZiliang Chen\n', '\nZhanfu Yang\n']",6 pages,,http://arxiv.org/abs/1909.11588v2,cs.LG,"['cs.LG', 'cs.LO', 'cs.SC', 'stat.ML']",,,[]
"Elimination-based certificates for triangular equivalence and rank
  profiles",http://arxiv.org/abs/1909.05692v1,2019-09-11T07:49:03Z,2019-09-11T07:49:03Z,"  In this paper, we give novel certificates for triangular equivalence and rank
profiles. These certificates enable somebody to verify the row or column rank
profiles or the whole rank profile matrix faster than recomputing them, with a
negligible overall overhead. We first provide quadratic time and space
non-interactive certificates saving the logarithmic factors of previously known
ones. Then we propose interactive certificates for the same problems whose
Monte Carlo verification complexity requires a small constant number of
matrix-vector multiplications, a linear space, and a linear number of extra
field operations , with a linear number of interactions. As an application we
also give an interactive protocol, certifying the determinant or the signature
of dense matrices, faster for the Prover than the best previously known one.
Finally we give linear space and constant round certificates for the row or
column rank profiles.
","['\nJean-Guillaume Dumas\nCASC\n', '\nErich Kaltofen\nNCSU\n', '\nDavid Lucas\nCASC\n', '\nClément Pernet\nCASC\n']","Journal of Symbolic Computation, Elsevier, In press. arXiv admin
  note: substantial text overlap with arXiv:1702.03755",,http://dx.doi.org/10.1016/j.jsc.2019.07.013,cs.SC,['cs.SC'],10.1016/j.jsc.2019.07.013,,"['CASC', 'NCSU', 'CASC', 'CASC']"
Imperative Program Synthesis from Answer Set Programs,http://arxiv.org/abs/1909.09058v1,2019-09-19T15:56:47Z,2019-09-19T15:56:47Z,"  Our research concerns generating imperative programs from Answer Set
Programming Specifications. ASP is highly declarative and is ideal for writing
specifications. Further with negation-as-failure it is easy to succinctly
represent combinatorial search problems. We are currently working on
synthesizing imperative programs from ASP programs by turning the negation into
useful computations. This opens up a novel way to synthesize programs from
executable specifications.
",['\nSarat Chandra Varanasi\n'],"In Proceedings ICLP 2019, arXiv:1909.07646","EPTCS 306, 2019, pp. 413-417",http://dx.doi.org/10.4204/EPTCS.306.55,cs.SC,"['cs.SC', 'cs.LO']",10.4204/EPTCS.306.55,,[]
Efficient Rational Creative Telescoping,http://arxiv.org/abs/1909.06898v2,2019-09-15T22:10:00Z,2021-08-09T07:02:29Z,"  We present a new algorithm to compute minimal telescopers for rational
functions in two discrete variables. As with recent reduction-based approaches,
our algorithm has the important feature that the computation of a telescoper is
independent of its certificate. In addition, our algorithm uses a compact
representation of the certificate, which allows it to be easily manipulated and
analyzed without knowing the precise expanded form. This representation hides
potential expression swell until the final (and optional) expansion, which can
be accomplished in time polynomial in the size of the expanded certificate. A
complexity analysis, along with a Maple implementation, indicates that our
algorithm has better theoretical and practical performance than the
reduction-based approach in the rational case.
","['\nMark Giesbrecht\n', '\nHui Huang\n', '\nGeorge Labahn\n', '\nEugene Zima\n']",,,http://arxiv.org/abs/1909.06898v2,cs.SC,"['cs.SC', 'math.CO', 'math.RA']",,,[]
Gröbner Bases with Reduction Machines,http://arxiv.org/abs/1909.01746v1,2019-09-04T12:49:08Z,2019-09-04T12:49:08Z,"  In this paper, we make a contribution to the computation of Gr\""obner bases.
For polynomial reduction, instead of choosing the leading monomial of a
polynomial as the monomial with respect to which the reduction process is
carried out, we investigate what happens if we make that choice arbitrarily. It
turns out not only this is possible (the fact that this produces a normal form
being already known in the literature), but, for a fixed choice of reductors,
the obtained normal form is the same no matter the order in which we reduce the
monomials. To prove this, we introduce reduction machines, which work by
reducing each monomial independently and then collecting the result. We show
that such a machine can simulate any such reduction. We then discuss different
implementations of these machines. Some of these implementations address
inherent inefficiencies in reduction machines (repeating the same
computations). We describe a first implementation and look at some experimental
results.
","['\nGeorgiana Şurlea\nDepartment of Computer Science, West University. Timişoara, Romania\n', '\nAdrian Crăciun\nDepartment of Computer Science, West University. Timişoara, Romania\n']","In Proceedings FROM 2019, arXiv:1909.00584","EPTCS 303, 2019, pp. 61-75",http://dx.doi.org/10.4204/EPTCS.303.5,cs.SC,['cs.SC'],10.4204/EPTCS.303.5,,"['Department of Computer Science, West University. Timişoara, Romania', 'Department of Computer Science, West University. Timişoara, Romania']"
Proceedings Third Symposium on Working Formal Methods,http://arxiv.org/abs/1909.00584v1,2019-09-02T07:49:49Z,2019-09-02T07:49:49Z,"  This volume contains the proceedings of FROM 2019: the Third Symposium on
Working Formal Methods, held on September 3-5, 2019 in Timi\c{s}oara (Romania).
FROM aims to bring together researchers and practitioners who work on formal
methods by contributing new theoretical results, methods, techniques, and
frameworks, and/or make the formal methods to work by creating or using
software tools that apply theoretical contributions.
","['\nMircea Marin\nWest University of Timişoara\n', '\nAdrian Crăciun\nWest University of Timişoara\n']",,"EPTCS 303, 2019",http://dx.doi.org/10.4204/EPTCS.303,cs.LO,"['cs.LO', 'cs.SC']",10.4204/EPTCS.303,,"['West University of Timişoara', 'West University of Timişoara']"
SATURN -- Software Deobfuscation Framework Based on LLVM,http://arxiv.org/abs/1909.01752v2,2019-09-04T12:50:21Z,2019-09-05T08:25:32Z,"  The strength of obfuscated software has increased over the recent years.
Compiler based obfuscation has become the de facto standard in the industry and
recent papers also show that injection of obfuscation techniques is done at the
compiler level. In this paper we discuss a generic approach for deobfuscation
and recompilation of obfuscated code based on the compiler framework LLVM. We
show how binary code can be lifted back into the compiler intermediate language
LLVM-IR and explain how we recover the control flow graph of an obfuscated
binary function with an iterative control flow graph construction algorithm
based on compiler optimizations and SMT solving. Our approach does not make any
assumptions about the obfuscated code, but instead uses strong compiler
optimizations available in LLVM and Souper Optimizer to simplify away the
obfuscation. Our experimental results show that this approach can be effective
to weaken or even remove the applied obfuscation techniques like constant
unfolding, certain arithmetic-based opaque expressions, dead code insertions,
bogus control flow or integer encoding found in public and commercial
obfuscators. The recovered LLVM-IR can be further processed by custom
deobfuscation passes that are now applied at the same level as the injected
obfuscation techniques or recompiled with one of the available LLVM backends.
The presented work is implemented in a deobfuscation tool called SATURN.
","['\nPeter Garba\n', '\nMatteo Favaro\n']","reverse engineering, llvm, code lifting, obfuscation, deobfuscation,
  static software analysis, binary recompilation, binary rewriting","3rd International Workshop on Software PROtection, Nov 2019,
  London, United Kingdom",http://arxiv.org/abs/1909.01752v2,cs.CR,"['cs.CR', 'cs.SC']",,,[]
On the k-synchronizability of systems,http://arxiv.org/abs/1909.01627v2,2019-09-04T08:58:53Z,2020-01-21T14:24:45Z,"  In this paper, we work on the notion of k-synchronizability: a system is
k-synchronizable if any of its executions, up to reordering causally
independent actions, can be divided into a succession of k-bounded interaction
phases. We show two results (both for mailbox and peer-to-peer automata):
first, the reachability problem is decidable for k-synchronizable systems;
second, the membership problem (whether a given system is k-synchronizable) is
decidable as well. Our proofs fix several important issues in previous attempts
to prove these two results for mailbox automata.
","['\nCinzia Di Giusto\nC&A\n', '\nCinzia Giusto\nSARDES\n', '\nLaetitia Laversa\nC&A\n', '\nEtienne Lozes\n']",,,http://arxiv.org/abs/1909.01627v2,cs.FL,"['cs.FL', 'cs.CL', 'cs.SC', 'cs.SE']",,,"['C&A', 'SARDES', 'C&A']"
"Defeating Opaque Predicates Statically through Machine Learning and
  Binary Analysis",http://arxiv.org/abs/1909.01640v1,2019-09-04T09:19:14Z,2019-09-04T09:19:14Z,"  We present a new approach that bridges binary analysis techniques with
machine learning classification for the purpose of providing a static and
generic evaluation technique for opaque predicates, regardless of their
constructions. We use this technique as a static automated deobfuscation tool
to remove the opaque predicates introduced by obfuscation mechanisms. According
to our experimental results, our models have up to 98% accuracy at detecting
and deob-fuscating state-of-the-art opaque predicates patterns. By contrast,
the leading edge deobfuscation methods based on symbolic execution show less
accuracy mostly due to the SMT solvers constraints and the lack of scalability
of dynamic symbolic analyses. Our approach underlines the efficiency of hybrid
symbolic analysis and machine learning techniques for a static and generic
deobfuscation methodology.
","['\nRamtine Tofighi-Shirazi\nTL, IF\n', '\nIrina Asăvoae\nTL\n', '\nPhilippe Elbaz-Vincent\nIF\n', '\nThanh-Ha Le\nTL\n']",,"3rd International Workshop on Software PROtection, Nov 2019,
  London, United Kingdom",http://arxiv.org/abs/1909.01640v1,cs.CR,"['cs.CR', 'cs.LG', 'cs.PL', 'cs.SC']",,,"['TL, IF', 'TL', 'IF', 'TL']"
Verifying the DPLL Algorithm in Dafny,http://arxiv.org/abs/1909.01743v1,2019-09-04T12:48:13Z,2019-09-04T12:48:13Z,"  Modern high-performance SAT solvers quickly solve large satisfiability
instances that occur in practice. If the instance is satisfiable, then the SAT
solver can provide a witness which can be checked independently in the form of
a satisfying truth assignment. However, if the instance is unsatisfiable, the
certificates could be exponentially large or the SAT solver might not be able
to output certificates. The implementation of the SAT solver should then be
trusted not to contain bugs. However, the data structures and algorithms
implemented by a typical high-performance SAT solver are complex enough to
allow for subtle programming errors. To counter this issue, we build a verified
SAT solver using the Dafny system. We discuss its implementation in the present
article.
","['\nCezar-Constantin Andrici\nAlexandru Ioan Cuza University\n', '\nŞtefan Ciobâcă\nAlexandru Ioan Cuza University\n']","In Proceedings FROM 2019, arXiv:1909.00584","EPTCS 303, 2019, pp. 3-15",http://dx.doi.org/10.4204/EPTCS.303.1,cs.LO,"['cs.LO', 'cs.PL', 'cs.SC']",10.4204/EPTCS.303.1,,"['Alexandru Ioan Cuza University', 'Alexandru Ioan Cuza University']"
"Proving two conjectural series for $ζ(7)$ and discovering more
  series for $ζ(7)$",http://arxiv.org/abs/1908.06631v1,2019-08-19T07:58:11Z,2019-08-19T07:58:11Z,"  We give a proof of two identities involving binomial sums at infinity
conjectured by Z-W Sun. In order to prove these identities, we use a recently
presented method i.e. we view the series as specializations of generating
series and derive integral representations. Using substitutions, we express
these integral representations in terms of cyclotomic harmonic polylogarithms.
Finally, by applying known relations among the cyclotomic harmonic
polylogarithms, we derive the results. These methods are implemented in the
computer algebra package HarmonicSums.
",['\nJakob Ablinger\n'],5 pages,,http://arxiv.org/abs/1908.06631v1,math.CO,"['math.CO', 'cs.SC']",,,[]
Residues of skew rational functions,http://arxiv.org/abs/1908.08430v2,2019-08-22T15:04:36Z,2021-06-17T10:10:51Z,"  This paper constitutes a first attempt to do analysis with skew polynomials.
Precisely, our main objective is to develop a theory of residues for skew
rational functions (which are, by definition, the quotients of two skew
polynomials). We prove in particular a skew analogue of the residue formula and
a skew analogue of the classical formula of change of variables for residues.
","['\nXavier Caruso\nIMB, LFANT\n']",,,http://arxiv.org/abs/1908.08430v2,math.RA,"['math.RA', 'cs.SC']",,,"['IMB, LFANT']"
Converting ALC Connection Proofs into ALC Sequents,http://arxiv.org/abs/1908.09477v1,2019-08-26T05:38:33Z,2019-08-26T05:38:33Z,"  The connection method has earned good reputation in the area of automated
theorem proving, due to its simplicity, efficiency and rational use of memory.
This method has been applied recently in automatic provers that reason over
ontologies written in the description logic ALC. However, proofs generated by
connection calculi are difficult to understand. Proof readability is largely
lost by the transformations to disjunctive normal form applied over the
formulae to be proven. Such a proof model, albeit efficient, prevents inference
systems based on it from effectively providing justifications and/or
descriptions of the steps used in inferences. To address this problem, in this
paper we propose a method for converting matricial proofs generated by the ALC
connection method to ALC sequent proofs, which are much easier to understand,
and whose translation to natural language is more straightforward. We also
describe a calculus that accepts the input formula in a non-clausal ALC format,
what simplifies the translation.
","['\nEunice Palmeira\nFederal Institute of Alagoas\n', '\nFred Freitas\nFederal University of Pernambuco\n', '\nJens Otten\nUniversity of Oslo\n']","In Proceedings PxTP 2019, arXiv:1908.08639. Thanks to CAPES:
  Coordination for the Improvement of Higher Level Personnel","EPTCS 301, 2019, pp. 3-17",http://dx.doi.org/10.4204/EPTCS.301.3,cs.SC,"['cs.SC', 'cs.LO']",10.4204/EPTCS.301.3,,"['Federal Institute of Alagoas', 'Federal University of Pernambuco', 'University of Oslo']"
Lemma Generation for Horn Clause Satisfiability: A Preliminary Study,http://arxiv.org/abs/1908.07188v1,2019-08-20T06:35:19Z,2019-08-20T06:35:19Z,"  It is known that the verification of imperative, functional, and logic
programs can be reduced to the satisfiability of constrained Horn clauses
(CHCs), and this satisfiability check can be performed by using CHC solvers,
such as Eldarica and Z3. These solvers perform well when they act on simple
constraint theories, such as Linear Integer Arithmetic and the theory of
Booleans, but their efficacy is very much reduced when the clauses refer to
constraints on inductively defined structures, such as lists or trees.
Recently, we have presented a transformation technique for eliminating those
inductively defined data structures, and hence avoiding the need for
incorporating induction principles into CHC solvers. However, this technique
may fail when the transformation requires the use of lemmata whose generation
needs ingenuity. In this paper we show, through an example, how during the
process of transforming CHCs for eliminating inductively defined structures one
can introduce suitable predicates, called difference predicates, whose
definitions correspond to the lemmata to be introduced. Through a second
example, we show that, whenever difference predicates cannot be introduced, we
can introduce, instead, auxiliary queries which also correspond to lemmata, and
the proof of these lemmata can be done by showing the satisfiability of those
queries.
","['\nEmanuele De Angelis\nDEC, University ""G. d\'Annunzio"" of Chieti-Pescara, Italy\n', '\nFabio Fioravanti\nDEC, University ""G. d\'Annunzio"" of Chieti-Pescara, Italy\n', '\nAlberto Pettorossi\nDICII, University of Roma Tor Vergata, Italy\n', '\nMaurizio Proietti\nCNR-IASI, Rome, Italy\n']","In Proceedings VPT 2019, arXiv:1908.06723","EPTCS 299, 2019, pp. 4-18",http://dx.doi.org/10.4204/EPTCS.299.4,cs.LO,"['cs.LO', 'cs.PL', 'cs.SC']",10.4204/EPTCS.299.4,,"['DEC, University ""G. d\'Annunzio"" of Chieti-Pescara, Italy', 'DEC, University ""G. d\'Annunzio"" of Chieti-Pescara, Italy', 'DICII, University of Roma Tor Vergata, Italy', 'CNR-IASI, Rome, Italy']"
"PPT: New Low Complexity Deterministic Primality Tests Leveraging
  Explicit and Implicit Non-Residues. A Set of Three Companion Manuscripts",http://arxiv.org/abs/1908.06964v1,2019-08-20T16:48:15Z,2019-08-20T16:48:15Z,"  In this set of three companion manuscripts/articles, we unveil our new
results on primality testing and reveal new primality testing algorithms
enabled by those results. The results have been classified (and referred to) as
lemmas/corollaries/claims whenever we have complete analytic proof(s);
otherwise the results are introduced as conjectures.
  In Part/Article 1, we start with the Baseline Primality Conjecture~(PBPC)
which enables deterministic primality detection with a low complexity = O((log
N)^2) ; when an explicit value of a Quadratic Non Residue (QNR) modulo-N is
available (which happens to be the case for an overwhelming majority = 11/12 =
91.67% of all odd integers). We then demonstrate Primality Lemma PL-1, which
reveals close connections between the state-of-the-art Miller-Rabin method and
the renowned Euler-Criterion. This Lemma, together with the Baseline Primality
Conjecture enables a synergistic fusion of Miller-Rabin iterations and our
method(s), resulting in hybrid algorithms that are substantially better than
their components. Next, we illustrate how the requirement of an explicit value
of a QNR can be circumvented by using relations of the form: Polynomial(x) mod
N = 0 ; whose solutions implicitly specify Non Residues modulo-N. We then
develop a method to derive low-degree canonical polynomials that together
guarantee implicit Non Residues modulo-N ; which along with the Generalized
Primality Conjectures enable algorithms that achieve a worst case deterministic
polynomial complexity = O( (log N)^3 polylog(log N)) ; unconditionally ; for
any/all values of N.
  In Part/Article 2 , we present substantial experimental data that corroborate
all the conjectures. No counter example has been found.
  Finally in Part/Article 3, we present analytic proof(s) of the Baseline
Primality Conjecture that we have been able to complete for some special cases.
","['\nDhananjay Phatak\n', '\nAlan T. Sherman\n', '\nSteven D. Houston\n', '\nAndrew Henry\n']","a set of 3 companion articles.217 (two hundred and seventeen) pages
  including everything = table of contents, list of figures, list of tables and
  an acknowledgment at the end. There is no watermark or highlighted text. Only
  color is in hyper-links and figures",,http://arxiv.org/abs/1908.06964v1,cs.CR,"['cs.CR', 'cs.CC', 'cs.DS', 'cs.SC', 'math.NT']",,,[]
On Symbolic Approaches for Computing the Matrix Permanent,http://arxiv.org/abs/1908.03252v1,2019-08-08T20:01:59Z,2019-08-08T20:01:59Z,"  Counting the number of perfect matchings in bipartite graphs, or equivalently
computing the permanent of 0-1 matrices, is an important combinatorial problem
that has been extensively studied by theoreticians and practitioners alike. The
permanent is #P-Complete; hence it is unlikely that a polynomial-time algorithm
exists for the problem. Researchers have therefore focused on finding tractable
subclasses of matrices for permanent computation. One such subclass that has
received much attention is that of sparse matrices i.e. matrices with few
entries set to 1, the rest being 0. For this subclass, improved theoretical
upper bounds and practically efficient algorithms have been developed. In this
paper, we ask whether it is possible to go beyond sparse matrices in our quest
for developing scalable techniques for the permanent, and answer this question
affirmatively. Our key insight is to represent permanent computation
symbolically using Algebraic Decision Diagrams (ADDs). ADD-based techniques
naturally use dynamic programming, and hence avoid redundant computation
through memoization. This permits exploiting the hidden structure in a large
class of matrices that have so far remained beyond the reach of permanent
computation techniques. The availability of sophisticated libraries
implementing ADDs also makes the task of engineering practical solutions
relatively straightforward. While a complete characterization of matrices
admitting a compact ADD representation remains open, we provide strong
experimental evidence of the effectiveness of our approach for computing the
permanent, not just for sparse matrices, but also for dense matrices and for
matrices with ""similar"" rows.
","['\nSupratik Chakraborty\n', '\nAditya A. Shrotri\n', '\nMoshe Y. Vardi\n']","To appear in proceedings of the 25th International Conference on
  Principles and Practice of Constraint Programming (2019)",,http://arxiv.org/abs/1908.03252v1,cs.DS,"['cs.DS', 'cs.SC']",,,[]
Computing zero-dimensional tropical varieties via projections,http://arxiv.org/abs/1908.03486v1,2019-08-09T15:01:20Z,2019-08-09T15:01:20Z,"  We present an algorithm for computing zero-dimensional tropical varieties
using projections. Our main tools are fast unimodular transforms of
lexicographical Gr\""obner bases. We prove that our algorithm requires only a
polynomial number of arithmetic operations if given a Gr\""obner basis, and we
demonstrate that our implementation compares favourably to other existing
implementations. Applying it to the computation of general positive-dimensional
tropical varieties, we argue that the complexity for calculating tropical links
is dominated by the complexity of the Gr\""obner walk.
","['\nPaul Görlach\n', '\nYue Ren\n', '\nLeon Zhang\n']",,,http://arxiv.org/abs/1908.03486v1,math.AG,"['math.AG', 'cs.SC', '14T05, 13P10, 13P15, 68W30']",,,[]
Probabilistic Saturations and Alt's Problem,http://arxiv.org/abs/1908.06020v1,2019-08-15T03:12:22Z,2019-08-15T03:12:22Z,"  Alt's problem, formulated in 1923, is to count the number of four-bar
linkages whose coupler curve interpolates nine general points in the plane.
This problem can be phrased as counting the number of solutions to a system of
polynomial equations which was first solved numerically using homotopy
continuation by Wampler, Morgan, and Sommese in 1992. Since there is still not
a proof that all solutions were obtained, we consider upper bounds for Alt's
problem by counting the number of solutions outside of the base locus to a
system arising as the general linear combination of polynomials. In particular,
we derive effective symbolic and numeric methods for studying such systems
using probabilistic saturations that can be employed using both finite fields
and floating-point computations. We give bounds on the size of finite field
required to achieve a desired level of certainty. These methods can also be
applied to many other problems where similar systems arise such as computing
the volumes of Newton-Okounkov bodies and computing intersection theoretic
invariants including Euler characteristics, Chern classes, and Segre classes.
","['\nJonathan D. Hauenstein\n', '\nMartin Helmer\n']",,"Experimental Mathematics, 2020",http://dx.doi.org/10.1080/10586458.2020.1740835,math.AG,"['math.AG', 'cs.SC', '14Qxx, 13Pxx, 70B15, 68T40, 70Q05, 93C85, 68W30']",10.1080/10586458.2020.1740835,,[]
A localized version of the basic triangle theorem,http://arxiv.org/abs/1908.03327v3,2019-08-09T06:34:40Z,2020-08-06T11:38:20Z,"  In this short note, we give a localized version of the basic triangle
theorem, first published in 2011 (see [4]) in order to prove the independence
of hyperlogarithms over various function fields. This version provides direct
access to rings of scalars and avoids the recourse to fraction fields as that
of meromorphic functions for instance.
","['\nGérard Duchamp\nLIPN\n', '\nNihar Gargava\nEPFL\n', '\nHoang Ngoc Minh\nLIPN\n', '\nPierre Simonnet\nSPE\n']",,,http://arxiv.org/abs/1908.03327v3,cs.SC,"['cs.SC', 'math.AC', 'math.CO']",,,"['LIPN', 'EPFL', 'LIPN', 'SPE']"
"Computing the Characteristic Polynomial of a Finite Rank Two Drinfeld
  Module",http://arxiv.org/abs/1907.12731v1,2019-07-30T04:11:48Z,2019-07-30T04:11:48Z,"  Motivated by finding analogues of elliptic curve point counting techniques,
we introduce one deterministic and two new Monte Carlo randomized algorithms to
compute the characteristic polynomial of a finite rank-two Drinfeld module. We
compare their asymptotic complexity to that of previous algorithms given by
Gekeler, Narayanan and Garai-Papikian and discuss their practical behavior. In
particular, we find that all three approaches represent either an improvement
in complexity or an expansion of the parameter space over which the algorithm
may be applied. Some experimental results are also presented.
","['\nYossef Musleh\n', '\nÉric Schost\n']",,,http://dx.doi.org/10.1145/3326229.3326256,cs.SC,['cs.SC'],10.1145/3326229.3326256,,[]
Computing strong regular characteristic pairs with Groebner bases,http://arxiv.org/abs/1907.13537v2,2019-07-31T14:51:25Z,2020-07-01T08:49:09Z,"  The W-characteristic set of a polynomial ideal is the minimal triangular set
contained in the reduced lexicographical Groebner basis of the ideal. A pair
(G,C) of polynomial sets is a strong regular characteristic pair if G is a
reduced lexicographical Groebner basis, C is the W-characteristic set of the
ideal <G>, the saturated ideal sat(C) of C is equal to <G>, and C is regular.
In this paper, we show that for any polynomial ideal I with given generators
one can either detect that I is unit, or construct a strong regular
characteristic pair (G,C) by computing Groebner bases such that
I$\subseteq$sat(C)=<G> and sat(C) divides I, so the ideal I can be split into
the saturated ideal sat(C) and the quotient ideal I:sat(C). Based on this
strategy of splitting by means of quotient and with Groebner basis and ideal
computations, we devise a simple algorithm to decompose an arbitrary polynomial
set F into finitely many strong regular characteristic pairs, from which two
representations for the zeros of F are obtained: one in terms of strong regular
Groebner bases and the other in terms of regular triangular sets. We present
some properties about strong regular characteristic pairs and characteristic
decomposition and illustrate the proposed algorithm and its performance by
examples and experimental results.
","['\nRina Dong\n', '\nDongming Wang\n']",18 pages,,http://arxiv.org/abs/1907.13537v2,cs.SC,['cs.SC'],,,[]
CREST: Hardware Formal Verification with ANSI-C Reference Specifications,http://arxiv.org/abs/1908.01324v1,2019-08-04T12:18:50Z,2019-08-04T12:18:50Z,"  This paper presents CREST, a prototype front-end tool intended as an add-on
to commercial EDA formal verifcation environments. CREST is an adaptation of
the CBMC bounded model checker for C, an academic tool widely used in industry
for software analysis and property verification. It leverages the capabilities
of CBMC to process hardware datapath specifications written in arbitrary
ANSI-C, without limiting restrictions to a synthesizable subset. We briefly
sketch the architecture of our tool and show its use in a range of verification
case studies.
","['\nAndreas Tiemeyer\n', '\nTom Melham\n', '\nDaniel Kroening\n', ""\nJohn O'Leary\n""]",4 pages,,http://arxiv.org/abs/1908.01324v1,cs.PL,"['cs.PL', 'cs.AR', 'cs.SC']",,,[]
A Unified Algebraic Framework for Non-Monotonicity,http://arxiv.org/abs/1907.09103v1,2019-07-22T03:15:28Z,2019-07-22T03:15:28Z,"  Tremendous research effort has been dedicated over the years to thoroughly
investigate non-monotonic reasoning. With the abundance of non-monotonic
logical formalisms, a unified theory that enables comparing the different
approaches is much called for. In this paper, we present an algebraic graded
logic we refer to as LogAG capable of encompassing a wide variety of
non-monotonic formalisms. We build on Lin and Shoham's argument systems first
developed to formalize non-monotonic commonsense reasoning. We show how to
encode argument systems as LogAG theories, and prove that LogAG captures the
notion of belief spaces in argument systems. Since argument systems capture
default logic, autoepistemic logic, the principle of negation as failure, and
circumscription, our results show that LogAG captures the before-mentioned
non-monotonic logical formalisms as well. Previous results show that LogAG
subsumes possibilistic logic and any non-monotonic inference relation
satisfying Makinson's rationality postulates. In this way, LogAG provides a
powerful unified framework for non-monotonicity.
","['\nNourhan Ehab\n', '\nHaythem O. Ismail\n']","In Proceedings TARK 2019, arXiv:1907.08335","EPTCS 297, 2019, pp. 155-174",http://dx.doi.org/10.4204/EPTCS.297.11,cs.LO,"['cs.LO', 'cs.AI', 'cs.SC']",10.4204/EPTCS.297.11,,[]
Polynomial Reduction and Super Congruences,http://arxiv.org/abs/1907.09391v1,2019-07-17T00:21:45Z,2019-07-17T00:21:45Z,"  Based on a reduction processing, we rewrite a hypergeometric term as the sum
of the difference of a hypergeometric term and a reduced hypergeometric term
(the reduced part, in short). We show that when the initial hypergeometric term
has a certain kind of symmetry, the reduced part contains only odd or even
powers. As applications, we derived two infinite families of super-congruences.
","['\nQing-Hu Hou\n', '\nYan-Ping Mu\n', '\nDoron Zeilberger\n']",18 pages,,http://arxiv.org/abs/1907.09391v1,math.CO,"['math.CO', 'cs.SC', 'math.NT', '05A19, 11A07, 68W30']",,,[]
Upper Hessenberg and Toeplitz Bohemians,http://arxiv.org/abs/1907.10677v1,2019-07-23T16:39:11Z,2019-07-23T16:39:11Z,"  We look at Bohemians, specifically those with population $\{-1, 0, {+1}\}$
and sometimes $\{0,1,i,-1,-i\}$. More, we specialize the matrices to be upper
Hessenberg Bohemian. From there, focusing on only those matrices whose
characteristic polynomials have maximal height allows us to explicitly identify
these polynomials and give useful bounds on their height, and conjecture an
accurate asymptotic formula. The lower bound for the maximal characteristic
height is exponential in the order of the matrix; in contrast, the height of
the matrices remains constant. We give theorems about the numbers of normal
matrices and the numbers of stable matrices in these families.
","['\nEunice Y. S. Chan\n', '\nRobert M. Corless\n', '\nLaureano Gonzalez-Vega\n', '\nJ. Rafael Sendra\n', '\nJuana Sendra\n']","24 pages, 1 figure. arXiv admin note: substantial text overlap with
  arXiv:1809.10653, arXiv:1809.10664",,http://arxiv.org/abs/1907.10677v1,cs.SC,"['cs.SC', 'cs.NA', 'math.NA', '15B05, 15B36, 11C20']",,,[]
Extensional Higher-Order Paramodulation in Leo-III,http://arxiv.org/abs/1907.11501v2,2019-07-26T11:58:08Z,2021-02-28T06:05:39Z,"  Leo-III is an automated theorem prover for extensional type theory with
Henkin semantics and choice. Reasoning with primitive equality is enabled by
adapting paramodulation-based proof search to higher-order logic. The prover
may cooperate with multiple external specialist reasoning systems such as
first-order provers and SMT solvers. Leo-III is compatible with the TPTP/TSTP
framework for input formats, reporting results and proofs, and standardized
communication between reasoning systems, enabling e.g. proof reconstruction
from within proof assistants such as Isabelle/HOL. Leo-III supports reasoning
in polymorphic first-order and higher-order logic, in all normal quantified
modal logics, as well as in different deontic logics. Its development had
initiated the ongoing extension of the TPTP infrastructure to reasoning within
non-classical logics.
","['\nAlexander Steen\n', '\nChristoph Benzmüller\n']","34 pages, 7 Figures, 1 Table; submitted article",,http://dx.doi.org/10.1007/s10817-021-09588-x,cs.AI,"['cs.AI', 'cs.LO', 'cs.SC', 'math.LO', '03B35, 03B15, 03B45, 68T15, 68T27, 68T30, 03Bxx', 'I.2.3; F.4.1; I.2.4']",10.1007/s10817-021-09588-x,,[]
"ART: Abstraction Refinement-Guided Training for Provably Correct Neural
  Networks",http://arxiv.org/abs/1907.10662v3,2019-07-17T16:58:33Z,2020-10-01T21:49:51Z,"  Artificial Neural Networks (ANNs) have demonstrated remarkable utility in
various challenging machine learning applications. While formally verified
properties of their behaviors are highly desired, they have proven notoriously
difficult to derive and enforce. Existing approaches typically formulate this
problem as a post facto analysis process. In this paper, we present a novel
learning framework that ensures such formal guarantees are enforced by
construction. Our technique enables training provably correct networks with
respect to a broad class of safety properties, a capability that goes
well-beyond existing approaches, without compromising much accuracy. Our key
insight is that we can integrate an optimization-based abstraction refinement
loop into the learning process and operate over dynamically constructed
partitions of the input space that considers accuracy and safety objectives
synergistically. The refinement procedure iteratively splits the input space
from which training data is drawn, guided by the efficacy with which such
partitions enable safety verification. We have implemented our approach in a
tool (ART) and applied it to enforce general safety properties on unmanned
aviator collision avoidance system ACAS Xu dataset and the Collision Detection
dataset. Importantly, we empirically demonstrate that realizing safety does not
come at the price of much accuracy. Our methodology demonstrates that an
abstraction refinement methodology provides a meaningful pathway for building
both accurate and correct machine learning networks.
","['\nXuankang Lin\n', '\nHe Zhu\n', '\nRoopsha Samanta\n', '\nSuresh Jagannathan\n']",FMCAD'20,,http://arxiv.org/abs/1907.10662v3,cs.LG,"['cs.LG', 'cs.AI', 'cs.PL', 'cs.SC', 'stat.ML']",,,[]
Formal verification of trading in financial markets,http://arxiv.org/abs/1907.07885v1,2019-07-18T05:50:29Z,2019-07-18T05:50:29Z,"  We introduce a formal framework for analyzing trades in financial markets. An
exchange is where multiple buyers and sellers participate to trade. These days,
all big exchanges use computer algorithms that implement double sided auctions
to match buy and sell requests and these algorithms must abide by certain
regulatory guidelines. For example, market regulators enforce that a matching
produced by exchanges should be \emph{fair}, \emph{uniform} and
\emph{individual rational}. To verify these properties of trades, we first
formally define these notions in a theorem prover and then give formal proofs
of relevant results on matchings. Finally, we use this framework to verify
properties of two important classes of double sided auctions. All the
definitions and results presented in this paper are completely formalised in
the Coq proof assistant without adding any additional axioms to it.
","['\nSuneel Sarswat\n', '\nAbhishek Kr Singh\n']",Preprint of 12 pages in lipicsv2016,,http://arxiv.org/abs/1907.07885v1,cs.LO,"['cs.LO', 'cs.DS', 'cs.FL', 'cs.GT', 'cs.SC', 'q-fin.TR']",,,[]
"Annotary: A Concolic Execution System for Developing Secure Smart
  Contracts",http://arxiv.org/abs/1907.03868v1,2019-07-08T20:56:27Z,2019-07-08T20:56:27Z,"  Ethereum smart contracts are executable programs, deployed on a peer-to-peer
network and executed in a consensus-based fashion. Their bytecode is public,
immutable and once deployed to the blockchain, cannot be patched anymore. As
smart contracts may hold Ether worth of several million dollars, they are
attractive targets for attackers and indeed some contracts have successfully
been exploited in the recent past, resulting in tremendous financial losses.
The correctness of smart contracts is thus of utmost importance. While first
approaches on formal verification exist, they demand users to be well-versed in
formal methods which are alien to many developers and are only able to analyze
individual contracts, without considering their execution environment, i.e.,
calls to external contracts, sequences of transaction, and values from the
actual blockchain storage. In this paper, we present Annotary, a concolic
execution framework to analyze smart contracts for vulnerabilities, supported
by annotations which developers write directly in the Solidity source code. In
contrast to existing work, Annotary supports analysis of inter-transactional,
inter-contract control flows and combines symbolic execution of EVM bytecode
with a resolution of concrete values from the public Ethereum blockchain. While
the analysis of Annotary tends to weight precision higher than soundness, we
analyze inter-transactional call chains to eliminate false positives from
unreachable states that traditional symbolic execution would not be able to
handle. We present the annotation and analysis concepts of Annotary, explain
its implementation on top of the Laser symbolic virtual machine, and
demonstrate its usage as a plugin for the Sublime Text editor.
","['\nKonrad Weiss\n', '\nJulian Schütte\n']",,,http://arxiv.org/abs/1907.03868v1,cs.CR,"['cs.CR', 'cs.SC']",,,[]
Computing Valuations of the Dieudonné Determinants,http://arxiv.org/abs/1907.04512v2,2019-07-10T05:27:04Z,2021-02-16T09:29:54Z,"  This paper addresses the problem of computing valuations of the Dieudonn\'e
determinants of matrices over discrete valuation skew fields (DVSFs). Under a
reasonable computational model, we propose two algorithms for a class of DVSFs,
called split. Our algorithms are extensions of the combinatorial relaxation of
Murota (1995) and the matrix expansion by Moriyama--Murota (2013), both of
which are based on combinatorial optimization. While our algorithms require an
upper bound on the output, we give an estimation of the bound for skew
polynomial matrices and show that the estimation is valid only for skew
polynomial matrices.
  We consider two applications of this problem. The first one is the
noncommutative weighted Edmonds' problem (nc-WEP), which is to compute the
degree of the Dieudonn\'e determinants of matrices having noncommutative
symbols. We show that the presented algorithms reduce the nc-WEP to the
unweighted problem in polynomial time. In particular, we show that the nc-WEP
over the rational field is solvable in time polynomial in the input bit-length.
We also present an application to analyses of degrees of freedom of linear
time-varying systems by establishing formulas on the solution spaces of linear
differential/difference equations.
",['\nTaihei Oki\n'],"A preliminary version of the part of this paper about Edmonds'
  problem has been appeared at the 47th International Colloquium on Automata,
  Languages and Programming (ICALP '20), July 2020, under the title of ""On
  solving (non)commutative weighted Edmonds' problem"". The previous version of
  this paper was titled ""Computing the maximum degree of minors in skew
  polynomial matrices""",,http://arxiv.org/abs/1907.04512v2,cs.DS,"['cs.DS', 'cs.SC']",,,[]
"Topological rewriting systems applied to standard bases and syntactic
  algebras",http://arxiv.org/abs/1907.06394v4,2019-07-15T09:44:25Z,2019-11-29T08:42:37Z,"  We propose a functional description of rewriting systems on topological
vector spaces. We introduce the topological confluence property as an
approximation of the confluence property. Using a representation of linear
topological rewriting systems with continuous reduction operators, we show that
the topological confluence is characterised by lattice operations. We relate
these operations to standard bases and show that the latter induce
topologically confluent rewriting systems on formal power series. Finally, we
investigate duality for reduction operators that we relate to series
representations and syntactic algebras. In particular, we use duality for
proving that an algebra is syntactic or not.
",['\nCyrille Chenavier\n'],,,http://arxiv.org/abs/1907.06394v4,math.RA,"['math.RA', 'cs.SC']",,,[]
Solving p-adic polynomial systems via iterative eigenvector algorithms,http://arxiv.org/abs/1907.03740v1,2019-07-08T17:54:18Z,2019-07-08T17:54:18Z,"  In this article, we describe an implementation of a polynomial system solver
to compute the approximate solutions of a 0-dimensional polynomial system with
finite precision p-adic arithmetic. We also describe an improvement to an
algorithm of Caruso, Roe, and Vaccon for calculating the eigenvalues and
eigenvectors of a p-adic matrix.
",['\nAvinash Kulkarni\n'],13 pages. Comments welcome,,http://arxiv.org/abs/1907.03740v1,math.NA,"['math.NA', 'cs.NA', 'cs.SC', 'math.NT', '15A18 (primary), 11S05 (secondary)']",,,[]
"Proving Properties of Sorting Programs: A Case Study in Horn Clause
  Verification",http://arxiv.org/abs/1907.03999v1,2019-07-09T06:02:42Z,2019-07-09T06:02:42Z,"  The proof of a program property can be reduced to the proof of satisfiability
of a set of constrained Horn clauses (CHCs) which can be automatically
generated from the program and the property. In this paper we have conducted a
case study in Horn clause verification by considering several sorting programs
with the aim of exploring the effectiveness of a transformation technique which
allows us to eliminate inductive data structures such as lists or trees. If
this technique is successful, we derive a set of CHCs with constraints over the
integers and booleans only, and the satisfiability check can often be performed
in an effective way by using state-of-the-art CHC solvers, such as Eldarica or
Z3. In this case study we have also illustrated the usefulness of a companion
technique based on the introduction of the so-called difference predicates,
whose definitions correspond to lemmata required during the verification. We
have considered functional programs which implement the following kinds of
sorting algorithms acting on lists of integers: (i) linearly recursive sorting
algorithms, such as insertion sort and selection sort, and (ii) non-linearly
recursive sorting algorithms, such as quicksort and mergesort, and we have
considered the following properties: (i) the partial correctness properties,
that is, the orderedness of the output lists, and the equality of the input and
output lists when viewed as multisets, and (ii) some arithmetic properties,
such as the equality of the sum of the elements before and after sorting.
","['\nEmanuele De Angelis\nDEC, University ""G. d\'Annunzio"" of Chieti-Pescara, Italy\n', '\nFabio Fioravanti\nDEC, University ""G. d\'Annunzio"" of Chieti-Pescara, Italy\n', '\nAlberto Pettorossi\nDICII, University of Roma Tor Vergata, Italy\n', '\nMaurizio Proietti\nCNR-IASI, Rome, Italy\n']","In Proceedings HCVS/PERR 2019, arXiv:1907.03523","EPTCS 296, 2019, pp. 48-75",http://dx.doi.org/10.4204/EPTCS.296.8,cs.LO,"['cs.LO', 'cs.PL', 'cs.SC']",10.4204/EPTCS.296.8,,"['DEC, University ""G. d\'Annunzio"" of Chieti-Pescara, Italy', 'DEC, University ""G. d\'Annunzio"" of Chieti-Pescara, Italy', 'DICII, University of Roma Tor Vergata, Italy', 'CNR-IASI, Rome, Italy']"
"SAT Solvers and Computer Algebra Systems: A Powerful Combination for
  Mathematics",http://arxiv.org/abs/1907.04408v2,2019-07-09T20:49:14Z,2019-09-16T17:24:42Z,"  Over the last few decades, many distinct lines of research aimed at
automating mathematics have been developed, including computer algebra systems
(CASs) for mathematical modelling, automated theorem provers for first-order
logic, SAT/SMT solvers aimed at program verification, and higher-order proof
assistants for checking mathematical proofs. More recently, some of these lines
of research have started to converge in complementary ways. One success story
is the combination of SAT solvers and CASs (SAT+CAS) aimed at resolving
mathematical conjectures.
  Many conjectures in pure and applied mathematics are not amenable to
traditional proof methods. Instead, they are best addressed via computational
methods that involve very large combinatorial search spaces. SAT solvers are
powerful methods to search through such large combinatorial
spaces---consequently, many problems from a variety of mathematical domains
have been reduced to SAT in an attempt to resolve them. However, solvers
traditionally lack deep repositories of mathematical domain knowledge that can
be crucial to pruning such large search spaces. By contrast, CASs are deep
repositories of mathematical knowledge but lack efficient general search
capabilities. By combining the search power of SAT with the deep mathematical
knowledge in CASs we can solve many problems in mathematics that no other known
methods seem capable of solving.
  We demonstrate the success of the SAT+CAS paradigm by highlighting many
conjectures that have been disproven, verified, or partially verified using our
tool MathCheck. These successes indicate that the paradigm is positioned to
become a standard method for solving problems requiring both a significant
amount of search and deep mathematical reasoning. For example, the SAT+CAS
paradigm has recently been used by Heule, Kauers, and Seidl to find many new
algorithms for $3\times3$ matrix multiplication.
","['\nCurtis Bright\n', '\nIlias Kotsireas\n', '\nVijay Ganesh\n']","To appear in Proceedings of the 29th International Conference on
  Computer Science and Software Engineering",,http://arxiv.org/abs/1907.04408v2,cs.LO,"['cs.LO', 'cs.AI', 'cs.SC', 'math.CO']",,,[]
"Improved Structural Methods for Nonlinear Differential-Algebraic
  Equations via Combinatorial Relaxation",http://arxiv.org/abs/1907.04511v1,2019-07-10T05:18:32Z,2019-07-10T05:18:32Z,"  Differential-algebraic equations (DAEs) are widely used for modeling of
dynamical systems. In numerical analysis of DAEs, consistent initialization and
index reduction are important preprocessing prior to numerical integration.
Existing DAE solvers commonly adopt structural preprocessing methods based on
combinatorial optimization. Unfortunately, the structural methods fail if the
DAE has numerical or symbolic cancellations. For such DAEs, methods have been
proposed to modify them to other DAEs to which the structural methods are
applicable, based on the combinatorial relaxation technique. Existing
modification methods, however, work only for a class of DAEs that are linear or
close to linear.
  This paper presents two new modification methods for nonlinear DAEs: the
substitution method and the augmentation method. Both methods are based on the
combinatorial relaxation approach and are applicable to a large class of
nonlinear DAEs. The substitution method symbolically solves equations for some
derivatives based on the implicit function theorem and substitutes the solution
back into the system. Instead of solving equations, the augmentation method
modifies DAEs by appending new variables and equations. The augmentation method
has advantages that the equation solving is not needed and the sparsity of DAEs
is retained. It is shown in numerical experiments that both methods, especially
the augmentation method, successfully modify high-index DAEs that the DAE
solver in MATLAB cannot handle.
",['\nTaihei Oki\n'],"A preliminary version of this paper is to appear in Proceedings of
  the 44th International Symposium on Symbolic and Algebraic Computation (ISSAC
  2019), Beijing, China, July 2019",,http://arxiv.org/abs/1907.04511v1,cs.SC,"['cs.SC', 'cs.NA', 'math.NA', 'math.OC']",,,[]
"The SAT+CAS Method for Combinatorial Search with Applications to Best
  Matrices",http://arxiv.org/abs/1907.04987v2,2019-07-11T03:46:54Z,2019-11-18T04:12:30Z,"  In this paper, we provide an overview of the SAT+CAS method that combines
satisfiability checkers (SAT solvers) and computer algebra systems (CAS) to
resolve combinatorial conjectures, and present new results vis-\`a-vis best
matrices. The SAT+CAS method is a variant of the
Davis$\unicode{8211}$Putnam$\unicode{8211}$Logemann$\unicode{8211}$Loveland
$\operatorname{DPLL}(T)$ architecture, where the $T$ solver is replaced by a
CAS. We describe how the SAT+CAS method has been previously used to resolve
many open problems from graph theory, combinatorial design theory, and number
theory, showing that the method has broad applications across a variety of
fields. Additionally, we apply the method to construct the largest best
matrices yet known and present new skew Hadamard matrices constructed from best
matrices. We show the best matrix conjecture (that best matrices exist in all
orders of the form $r^2+r+1$) which was previously known to hold for $r\leq6$
also holds for $r=7$. We also confirmed the results of the exhaustive searches
that have been previously completed for $r\leq6$.
","['\nCurtis Bright\n', '\nDragomir Ž. Đoković\n', '\nIlias Kotsireas\n', '\nVijay Ganesh\n']",To appear in Annals of Mathematics and Artificial Intelligence,,http://dx.doi.org/10.1007/s10472-019-09681-3,cs.LO,"['cs.LO', 'cs.SC', 'math.CO']",10.1007/s10472-019-09681-3,,[]
"Logic Conditionals, Supervenience, and Selection Tasks",http://arxiv.org/abs/1907.06773v2,2019-07-15T22:16:00Z,2019-08-11T13:17:01Z,"  Principles of cognitive economy would require that concepts about objects,
properties and relations should be introduced only if they simplify the
conceptualisation of a domain. Unexpectedly, classic logic conditionals,
specifying structures holding within elements of a formal conceptualisation, do
not always satisfy this crucial principle. The paper argues that this
requirement is captured by supervenience, hereby further identified as a
property necessary for compression. The resulting theory suggests an
alternative explanation of the empirical experiences observable in Wason's
selection tasks, associating human performance with conditionals on the ability
of dealing with compression, rather than with logic necessity.
",['\nGiovanni Sileno\n'],,,http://arxiv.org/abs/1907.06773v2,cs.AI,"['cs.AI', 'cs.CL', 'cs.SC']",,,[]
"Semantic Preserving Bijective Mappings for Expressions involving Special
  Functions in Computer Algebra Systems and Document Preparation Systems",http://arxiv.org/abs/1906.11485v1,2019-06-27T08:08:09Z,2019-06-27T08:08:09Z,"  Purpose: Modern mathematicians and scientists of math-related disciplines
often use Document Preparation Systems (DPS) to write and Computer Algebra
Systems (CAS) to calculate mathematical expressions. Usually, they translate
the expressions manually between DPS and CAS. This process is time-consuming
and error-prone. Our goal is to automate this translation. This paper uses
Maple and Mathematica as the CAS, and LaTeX as our DPS.
  Design/methodology/approach: Bruce Miller at the National Institute of
Standards and Technology (NIST) developed a collection of special LaTeX macros
that create links from mathematical symbols to their definitions in the NIST
Digital Library of Mathematical Functions (DLMF). We are using these macros to
perform rule-based translations between the formulae in the DLMF and CAS.
Moreover, we develop software to ease the creation of new rules and to discover
inconsistencies.
  Findings: We created 396 mappings and translated 58.8% of DLMF formulae
(2,405 expressions) successfully between Maple and DLMF. For a significant
percentage, the special function definitions in Maple and the DLMF were
different. Therefore, an atomic symbol in one system maps to a composite
expression in the other system. The translator was also successfully used for
automatic verification of mathematical online compendia and CAS. Our evaluation
techniques discovered two errors in the DLMF and one defect in Maple.
  Originality: This paper introduces the first translation tool for special
functions between LaTeX and CAS. The approach improves error-prone manual
translations and can be used to verify mathematical online compendia and CAS.
","['\nAndre Greiner-Petter\n', '\nMoritz Schubotz\n', '\nHoward S. Cohl\n', '\nBela Gipp\n']","This work was supported by the German Research Foundation (DFG, grant
  GI-1259-1)",,http://dx.doi.org/10.1108/AJIM-08-2018-0185,cs.DL,"['cs.DL', 'cs.SC']",10.1108/AJIM-08-2018-0185,,[]
Solving Polynomial Systems with phcpy,http://arxiv.org/abs/1907.00096v1,2019-06-28T22:19:04Z,2019-06-28T22:19:04Z,"  The solutions of a system of polynomials in several variables are often
needed, e.g.: in the design of mechanical systems, and in phase-space analyses
of nonlinear biological dynamics. Reliable, accurate, and comprehensive
numerical solutions are available through PHCpack, a FOSS package for solving
polynomial systems with homotopy continuation. This paper explores new
developments in phcpy, a scripting interface for PHCpack, over the past five
years. For instance, phcpy is now available online through a JupyterHub server
featuring Python2, Python3, and SageMath kernels. As small systems are solved
in real-time by phcpy, they are suitable for interactive exploration through
the notebook interface. Meanwhile, phcpy supports GPU parallelization,
improving the speed and quality of solutions to much larger polynomial systems.
From various model design and analysis problems in STEM, certain classes of
polynomial system frequently arise, to which phcpy is well-suited.
","['\nJasmine Otto\n', '\nAngus Forbes\n', '\nJan Verschelde\n']",Accepted for publication in the SciPy 2019 proceedings,,http://arxiv.org/abs/1907.00096v1,cs.MS,"['cs.MS', 'cs.DC', 'cs.SC', 'math.AG']",,,[]
Absolute root separation,http://arxiv.org/abs/1907.01232v2,2019-07-02T08:33:01Z,2019-11-29T07:00:32Z,"  The absolute separation of a polynomial is the minimum nonzero difference
between the absolute values of its roots. In the case of polynomials with
integer coefficients, it can be bounded from below in terms of the degree and
the height (the maximum absolute value of the coefficients) of the polynomial.
We improve the known bounds for this problem and related ones. Then we report
on extensive experiments in low degrees, suggesting that the current bounds are
still very pessimistic.
","['\nYann Bugeaud\n', '\nAndrej Dujella\n', '\nWenjie Fang\n', '\nTomislav Pejković\n', '\nBruno Salvy\n']",,Experimental Mathematics. On-line 2019,http://dx.doi.org/10.1080/10586458.2019.1699480,math.CA,"['math.CA', 'cs.SC', 'math.NT']",10.1080/10586458.2019.1699480,,[]
Effective problem solving using SAT solvers,http://arxiv.org/abs/1906.06251v2,2019-06-14T15:32:17Z,2019-09-16T17:20:51Z,"  In this article we demonstrate how to solve a variety of problems and puzzles
using the built-in SAT solver of the computer algebra system Maple. Once the
problems have been encoded into Boolean logic, solutions can be found (or shown
to not exist) automatically, without the need to implement any search
algorithm. In particular, we describe how to solve the $n$-queens problem, how
to generate and solve Sudoku puzzles, how to solve logic puzzles like the
Einstein riddle, how to solve the 15-puzzle, how to solve the maximum clique
problem, and finding Graeco-Latin squares.
","['\nCurtis Bright\n', '\nJürgen Gerhard\n', '\nIlias Kotsireas\n', '\nVijay Ganesh\n']",To appear in Proceedings of the Maple Conference 2019,,http://dx.doi.org/10.1007/978-3-030-41258-6_15,cs.AI,"['cs.AI', 'cs.LO', 'cs.SC']",10.1007/978-3-030-41258-6_15,,[]
Neurally-Guided Structure Inference,http://arxiv.org/abs/1906.07304v2,2019-06-17T23:22:28Z,2019-08-15T08:38:29Z,"  Most structure inference methods either rely on exhaustive search or are
purely data-driven. Exhaustive search robustly infers the structure of
arbitrarily complex data, but it is slow. Data-driven methods allow efficient
inference, but do not generalize when test data have more complex structures
than training data. In this paper, we propose a hybrid inference algorithm, the
Neurally-Guided Structure Inference (NG-SI), keeping the advantages of both
search-based and data-driven methods. The key idea of NG-SI is to use a neural
network to guide the hierarchical, layer-wise search over the compositional
space of structures. We evaluate our algorithm on two representative structure
inference tasks: probabilistic matrix decomposition and symbolic program
parsing. It outperforms data-driven and search-based alternatives on both
tasks.
","['\nSidi Lu\n', '\nJiayuan Mao\n', '\nJoshua B. Tenenbaum\n', '\nJiajun Wu\n']","Proceedings of the 36th International Conference on Machine Learning
  (ICML 2019). First two authors contributed equally. Project page:
  http://ngsi.csail.mit.edu",PMLR(2019)97: 4144--4153,http://arxiv.org/abs/1906.07304v2,cs.LG,"['cs.LG', 'cs.AI', 'cs.SC', 'stat.ML']",,,[]
"New Features in the Second Version of the Cadabra Computer Algebra
  System",http://arxiv.org/abs/1906.02599v1,2019-06-06T14:12:05Z,2019-06-06T14:12:05Z,"  In certain scientific domains, there is a need for tensor operations. To
facilitate tensor computations,computer algebra systems are employed. In our
research, we have been using Cadabra as the main computer algebra system for
several years. Recently, an operable second version of this software was
released. In this version, a number of improvements were made that can be
regarded as revolutionary ones. The most significant improvements are the
implementation of component computations and the change in the ideology of the
Cadabra's software mechanism as compared to the first version. This paper
provides a brief overview of the key improvements in the Cadabra system.
","['\nD. S. Kulyabov\n', '\nA. V. Korolkova\n', '\nL. A. Sevastianov\n']",in English; in Russian,,http://dx.doi.org/10.1134/S0361768819020063,cs.SC,['cs.SC'],10.1134/S0361768819020063,,[]
Standard Lattices of Compatibly Embedded Finite Fields,http://arxiv.org/abs/1906.00870v1,2019-06-03T15:27:44Z,2019-06-03T15:27:44Z,"  Lattices of compatibly embedded finite fields are useful in computer algebra
systems for managing many extensions of a finite field $\mathbb{F}_p$ at once.
They can also be used to represent the algebraic closure $\bar{\mathbb{F}}_p$,
and to represent all finite fields in a standard manner. The most well known
constructions are Conway polynomials, and the Bosma-Cannon-Steel framework used
in Magma. In this work, leveraging the theory of the Lenstra-Allombert
isomorphism algorithm, we generalize both at the same time. Compared to Conway
polynomials, our construction defines a much larger set of field extensions
from a small pre-computed table; however it is provably as inefficient as
Conway polynomials if one wants to represent all field extensions, and thus
yields no asymptotic improvement for representing $\bar{\mathbb{F}}_p$.
Compared to Bosma-Cannon-Steel lattices, it is considerably more efficient both
in computation time and storage: all algorithms have at worst quadratic
complexity, and storage is linear in the number of represented field extensions
and their degrees. Our implementation written in C/Flint/Julia/Nemo shows that
our construction in indeed practical.
","['\nLuca De Feo\n', '\nHugues Randriam\n', '\nÉdouard Rousseau\n']","9 pages, 1 figure, double column. Comments welcome!",,http://dx.doi.org/10.1145/3326229.3326251,math.NT,"['math.NT', 'cs.SC']",10.1145/3326229.3326251,,[]
"Algorithmically generating new algebraic features of polynomial systems
  for machine learning",http://arxiv.org/abs/1906.01455v1,2019-06-03T09:31:28Z,2019-06-03T09:31:28Z,"  There are a variety of choices to be made in both computer algebra systems
(CASs) and satisfiability modulo theory (SMT) solvers which can impact
performance without affecting mathematical correctness. Such choices are
candidates for machine learning (ML) approaches, however, there are
difficulties in applying standard ML techniques, such as the efficient
identification of ML features from input data which is typically a polynomial
system. Our focus is selecting the variable ordering for cylindrical algebraic
decomposition (CAD), an important algorithm implemented in several CASs, and
now also SMT-solvers. We created a framework to describe all the previously
identified ML features for the problem and then enumerated all options in this
framework to automatically generation many more features. We validate the
usefulness of these with an experiment which shows that an ML choice for CAD
variable ordering is superior to those made by human created heuristics, and
further improved with these additional features. We expect that this technique
of feature generation could be useful for other choices related to CAD, or even
choices for other algorithms with polynomial systems for input.
","['\nDorian Florescu\n', '\nMatthew England\n']","To appear in Proc SC-Square Workshop 2019. arXiv admin note:
  substantial text overlap with arXiv:1904.11061","Proceedings of the 4th Workshop on Satisfiability Checking and
  Symbolic Computation (SC2 '19), 12 pages. CEUR Workshop Proceedings 2460,
  2019",http://arxiv.org/abs/1906.01455v1,cs.SC,"['cs.SC', 'cs.LG', '68W30, 68T05, 03C10', 'I.2.6; I.1.0']",,,[]
Polynomial root clustering and explicit deflation,http://arxiv.org/abs/1906.04920v2,2019-06-11T12:56:04Z,2019-11-15T16:24:55Z,"  We seek complex roots of a univariate polynomial $P$ with real or complex
coefficients. We address this problem based on recent algorithms that use
subdivision and have a nearly optimal complexity. They are particularly
efficient when only roots in a given Region Of Interest (ROI) are sought. We
propose two improvements for root finders. The first one is applied to
polynomials having only real coefficients; their roots are either real or
appear in complex conjugate pairs. We show how to adapt the subdivision scheme
to focus the computational effort on the imaginary positive part of the ROI. In
our second improvement we deflate $P$ to decrease its degree and the arithmetic
cost of the subdivision.
","['\nRémi Imbach\n', '\nVictor Y. Pan\n']","Part on clustering roots of polynomials with real coefficients has
  been removed, since it is now in ""New practical advances in polynomial root
  clustering""",,http://arxiv.org/abs/1906.04920v2,cs.SC,"['cs.SC', 'cs.NA', 'math.NA']",,,[]
"Neural Variational Inference For Estimating Uncertainty in Knowledge
  Graph Embeddings",http://arxiv.org/abs/1906.04985v2,2019-06-12T07:52:02Z,2019-08-19T03:01:30Z,"  Recent advances in Neural Variational Inference allowed for a renaissance in
latent variable models in a variety of domains involving high-dimensional data.
While traditional variational methods derive an analytical approximation for
the intractable distribution over the latent variables, here we construct an
inference network conditioned on the symbolic representation of entities and
relation types in the Knowledge Graph, to provide the variational
distributions. The new framework results in a highly-scalable method. Under a
Bernoulli sampling framework, we provide an alternative justification for
commonly used techniques in large-scale stochastic variational inference, which
drastically reduce training time at a cost of an additional approximation to
the variational lower bound. We introduce two models from this highly scalable
probabilistic framework, namely the Latent Information and Latent Fact models,
for reasoning over knowledge graph-based representations. Our Latent
Information and Latent Fact models improve upon baseline performance under
certain conditions. We use the learnt embedding variance to estimate predictive
uncertainty during link prediction, and discuss the quality of these learnt
uncertainty estimates. Our source code and datasets are publicly available
online at
https://github.com/alexanderimanicowenrivers/Neural-Variational-Knowledge-Graphs.
","['\nAlexander I. Cowen-Rivers\n', '\nPasquale Minervini\n', '\nTim Rocktaschel\n', '\nMatko Bosnjak\n', '\nSebastian Riedel\n', '\nJun Wang\n']",Accepted at IJCAI 19 Neural-Symbolic Learning and Reasoning Workshop,,http://arxiv.org/abs/1906.04985v2,cs.LG,"['cs.LG', 'cs.AI', 'cs.SC', 'stat.ML']",,,[]
Efficient Graph Rewriting,http://arxiv.org/abs/1906.05170v2,2019-06-11T15:50:57Z,2021-01-01T11:46:20Z,"  Graph transformation is the rule-based modification of graphs, and is a
discipline dating back to the 1970s. The declarative nature of graph rewriting
rules comes at a cost. In general, to match the left-hand graph of a fixed rule
within a host graph requires polynomial time. To improve matching performance,
D\""orr proposed to equip rules and host graphs with distinguished root nodes.
This model was implemented by Plump and Bak, but unfortunately, is not
invertible. We address this problem by defining rootedness using a partial
function onto a two-point set rather than pointing graphs with root nodes. We
show a new result that the graph class of trees can be recognised by a rooted
GT system in linear time, given an input graph of bounded degree. Finally, we
define a new notion of confluence modulo garbage and non-garbage critical
pairs, showing it is sufficient to require strong joinability of only the
non-garbage critical pairs to establish confluence modulo garbage.
",['\nGraham Campbell\n'],"BSc Thesis, Department of Computer Science, University of York, 54
  pages, 2019",,http://arxiv.org/abs/1906.05170v2,cs.LO,"['cs.LO', 'cs.CC', 'cs.PL', 'cs.SC', '68Q42, 68Q25', 'F.4.2; F.2.2']",,,[]
New ways to multiply 3 x 3-matrices,http://arxiv.org/abs/1905.10192v1,2019-05-24T12:36:00Z,2019-05-24T12:36:00Z,"  It is known since the 1970s that no more than 23 multiplications are required
for computing the product of two 3 x 3-matrices. It is not known whether this
can also be done with fewer multiplications. However, there are several
mutually inequivalent ways of doing the job with 23 multiplications. In this
article, we extend this list considerably by providing more than 13 000 new and
mutually inequivalent schemes for multiplying 3 x 3-matrices using 23
multiplications. Moreover, we show that the set of all these schemes is a
manifold of dimension at least 17.
","['\nMarijn J. H. Heule\n', '\nManuel Kauers\n', '\nMartina Seidl\n']",,,http://arxiv.org/abs/1905.10192v1,cs.SC,['cs.SC'],,,[]
Factorizations for a Class of Multivariate Polynomial Matrices,http://arxiv.org/abs/1905.11872v1,2019-05-28T15:10:59Z,2019-05-28T15:10:59Z,"  Following the works by Lin et al. (Circuits Syst. Signal Process. 20(6):
601-618, 2001) and Liu et al. (Circuits Syst. Signal Process. 30(3): 553-566,
2011), we investigate how to factorize a class of multivariate polynomial
matrices. The main theorem in this paper shows that an $l\times m$ polynomial
matrix admits a factorization with respect to a polynomial if the polynomial
and all the $(l-1)\times (l-1)$ reduced minors of the matrix generate the unit
ideal. This result is a further generalization of previous works, and based on
this, we give an algorithm which can be used to factorize more polonomial
matrices. In addition, an illustrate example is given to show that our main
theorem is non-trivial and valuable.
","['\nDong Lu\n', '\nDingkang Wang\n', '\nFanghui Xiao\n']","12 pages, Submitted to Multidimensional Systems and Signal Processing",,http://arxiv.org/abs/1905.11872v1,cs.SC,['cs.SC'],,,[]
Confluence by Critical Pair Analysis Revisited (Extended Version),http://arxiv.org/abs/1905.11733v2,2019-05-28T10:46:05Z,2019-06-03T04:47:15Z,"  We present two methods for proving confluence of left-linear term rewrite
systems. One is hot-decreasingness, combining the parallel/development
closedness theorems with rule labelling based on a terminating subsystem. The
other is critical-pair-closing system, allowing to boil down the confluence
problem to confluence of a special subsystem whose duplicating rules are
relatively terminating.
","['\nNao Hirokawa\n', '\nJulian Nagele\n', '\nVincent van Oostrom\n', '\nMichio Oyamaguchi\n']",Added a reference to the conference version,,http://arxiv.org/abs/1905.11733v2,cs.LO,"['cs.LO', 'cs.SC']",,,[]
Reconstruction of rational ruled surfaces from their silhouettes,http://arxiv.org/abs/1905.11853v2,2019-05-28T14:33:24Z,2021-04-27T18:14:30Z,"  We provide algorithms to reconstruct rational ruled surfaces in
three-dimensional projective space from the `apparent contour' of a single
projection to the projective plane. We deal with the case of tangent
developables and of general projections to $\mathbb{p}^3$ of rational normal
scrolls. In the first case, we use the fact that every such surface is the
projection of the tangent developable of a rational normal curve, while in the
second we start by reconstructing the rational normal scroll. In both instances
we then reconstruct the correct projection to $\mathbb{p}^3$ of these surfaces
by exploiting the information contained in the singularities of the apparent
contour.
","['\nMatteo Gallet\n', '\nNiels Lubbes\n', '\nJosef Schicho\n', '\nJan Vršek\n']",17 pages,"Journal of Symbolic Computation, Volume 104, May-June 2021, Pages
  366-380",http://dx.doi.org/10.1016/j.jsc.2020.08.002,cs.SC,"['cs.SC', 'math.AG']",10.1016/j.jsc.2020.08.002,,[]
"A closed-form formula for the Kullback-Leibler divergence between Cauchy
  distributions",http://arxiv.org/abs/1905.10965v2,2019-05-27T04:05:17Z,2019-05-28T10:03:25Z,"  We report a closed-form expression for the Kullback-Leibler divergence
between Cauchy distributions which involves the calculation of a novel definite
integral. The formula shows that the Kullback-Leibler divergence between Cauchy
densities is always finite and symmetric.
","['\nFrédéric Chyzak\n', '\nFrank Nielsen\n']",8 pages,,http://arxiv.org/abs/1905.10965v2,cs.IT,"['cs.IT', 'cs.SC', 'math.IT']",,,[]
On the Parallelization of Triangular Decomposition of Polynomial Systems,http://arxiv.org/abs/1906.00039v1,2019-05-31T19:16:47Z,2019-05-31T19:16:47Z,"  We discuss the parallelization of algorithms for solving polynomial systems
symbolically by way of triangular decomposition. Algorithms for solving
polynomial systems combine low-level routines for performing arithmetic
operations on polynomials and high-level procedures which produce the different
components (points, curves, surfaces) of the solution set. The latter
""component-level"" parallelization of triangular decompositions, our focus here,
belongs to the class of dynamic irregular parallel applications. Possible
speedup factors depend on geometrical properties of the solution set (number of
components, their dimensions and degrees); these algorithms do not scale with
the number of processors. In this paper we combine two different concurrency
schemes, the fork-join model and producer-consumer patterns, to better capture
opportunities for component-level parallelization. We report on our
implementation with the publicly available BPAS library. Our experimentation
with 340 systems yields promising results.
","['\nMohammadali Asadi\n', '\nAlexander Brandt\n', '\nRobert H. C. Moir\n', '\nMarc Moreno Maza\n', '\nYuzhen Xie\n']",,,http://arxiv.org/abs/1906.00039v1,cs.SC,"['cs.SC', 'cs.DC', 'cs.MS']",,,[]
"Abstract Predicate Entailment over Points-To Heaplets is Syntax
  Recognition",http://arxiv.org/abs/1906.00217v1,2019-06-01T12:36:37Z,2019-06-01T12:36:37Z,"  Abstract predicates are considered in this paper as abstraction technique for
heap-separated configurations, and as genuine Prolog predicates which are
translated straight into a corresponding formal language grammar used as
validation scheme for intermediate heap states. The approach presented is
rule-based because the abstract predicates are rule-based, the parsing
technique can be interpreted as an automated fold/unfold of the corresponding
heap graph.
","['\nRené Haberland\n', '\nKirill Krinkin\n', '\nSergey Ivanovskiy\n']","9 pages, 3 figures","IEEE Xplore, 18th Conf. of Open Innovations (FRUCT), 2016,
  pp.66-74",http://dx.doi.org/10.1109/FRUCT-ISPIT.2016.7561510,cs.LO,"['cs.LO', 'cs.FL', 'cs.SC']",10.1109/FRUCT-ISPIT.2016.7561510,,[]
"An Algorithm for Computing Invariant Projectors in Representations of
  Wreath Products",http://arxiv.org/abs/1906.00858v2,2019-05-29T12:21:42Z,2020-06-18T01:00:53Z,"  We describe an algorithm for computing the complete set of primitive
orthogonal idempotents in the centralizer ring of the permutation
representation of a wreath product. This set of idempotents determines the
decomposition of the representation into irreducible components. In the
formalism of quantum mechanics, these idempotents are projection operators into
irreducible invariant subspaces of the Hilbert space of a multipartite quantum
system. The C implementation of the algorithm constructs irreducible
decompositions of high-dimensional representations of wreath products. Examples
of computations are given.
",['\nVladimir V. Kornyak\n'],"13 pages, version 2: computer outputs corrected","CASC 2019, LNCS 11661, pp. 300-314, 2019",http://arxiv.org/abs/1906.00858v2,math.RT,"['math.RT', 'cs.SC', 'math-ph', 'math.MP']",,,[]
Change of basis for m-primary ideals in one and two variables,http://arxiv.org/abs/1905.04614v1,2019-05-12T00:44:16Z,2019-05-12T00:44:16Z,"  Following recent work by van der Hoeven and Lecerf (ISSAC 2017), we discuss
the complexity of linear mappings, called untangling and tangling by those
authors, that arise in the context of computations with univariate polynomials.
We give a slightly faster tangling algorithm and discuss new applications of
these techniques. We show how to extend these ideas to bivariate settings, and
use them to give bounds on the arithmetic complexity of certain algebras.
","['\nSeung Gyu Hyun\n', '\nStephen Melczer\n', '\nÉric Schost\n', '\nCatherine St-Pierre\n']","In Proceedings ISSAC'19, ACM, New York, USA. See proceedings version
  for final formatting",,http://dx.doi.org/10.1145/3326229.3326268,cs.SC,['cs.SC'],10.1145/3326229.3326268,,[]
Lonely Points in Simplices,http://arxiv.org/abs/1905.08747v1,2019-05-21T16:50:17Z,2019-05-21T16:50:17Z,"  Given a lattice L in Z^m and a subset A of R^m, we say that a point in A is
lonely if it is not equivalent modulo L to another point of A. We are
interested in identifying lonely points for specific choices of L when A is a
dilated standard simplex, and in conditions on L which ensure that the number
of lonely points is unbounded as the simplex dilation goes to infinity.
","['\nMaximilian Jaroschek\n', '\nManuel Kauers\n', '\nLaura Kovacs\n']",,,http://arxiv.org/abs/1905.08747v1,cs.SC,['cs.SC'],,,[]
Computing symmetric determinantal representations,http://arxiv.org/abs/1905.07035v1,2019-05-16T21:10:50Z,2019-05-16T21:10:50Z,"  We introduce the DeterminantalRepresentations package for Macaulay2, which
computes definite symmetric determinantal representations of real polynomials.
We focus on quadrics and plane curves of low degree (i.e. cubics and quartics).
Our algorithms are geared towards speed and robustness, employing linear
algebra and numerical algebraic geometry, without genericity assumptions on the
polynomials.
","['\nJustin Chen\n', '\nPapri Dey\n']","Code available at
  https://github.com/papridey/DeterminantalRepresentations",J. Softw. Alg. Geom. 10 (2020) 9-15,http://dx.doi.org/10.2140/jsag.2020.10.9,math.AG,"['math.AG', 'cs.SC', '11C20, 15A15, 65F40, 15B99']",10.2140/jsag.2020.10.9,,[]
A polynomial approach to the Collatz conjecture,http://arxiv.org/abs/1905.08462v1,2019-05-21T06:53:34Z,2019-05-21T06:53:34Z,"  The Collatz conjecture is explored using polynomials based on a binary
numeral system. It is shown that the degree of the polynomials, on average,
decreases after a finite number of steps of the Collatz operation, which
provides a weak proof of the conjecture by using induction with respect to the
degree of the polynomials.
","['\nFeng Pan\n', '\nJerry P. Draayer\n']",7.5 pages and 1 Figure,,http://arxiv.org/abs/1905.08462v1,math.NT,"['math.NT', 'cs.SC']",,,[]
Stationary points at infinity for analytic combinatorics,http://arxiv.org/abs/1905.05250v3,2019-05-13T19:15:27Z,2021-02-19T17:40:41Z,"  On complex algebraic varieties, height functions arising in combinatorial
applications fail to be proper. This complicates the description and
computation via Morse theory of key topological invariants. Here we establish
checkable conditions under which the behavior at infinity may be ignored, and
the usual theorems of classical and stratified Morse theory may be applied.
This allows for simplified arguments in the field of analytic combinatorics in
several variables, and forms the basis for new methods applying to problems
beyond the reach of previous techniques.
","['\nYuliy Baryshnikov\n', '\nStephen Melczer\n', '\nRobin Pemantle\n']",Updated and simplified presentation and statements of main results,,http://arxiv.org/abs/1905.05250v3,math.CO,"['math.CO', 'cs.SC', 'math.AG']",,,[]
Effects Without Monads: Non-determinism -- Back to the Meta Language,http://arxiv.org/abs/1905.06544v1,2019-05-16T06:11:37Z,2019-05-16T06:11:37Z,"  We reflect on programming with complicated effects, recalling an
undeservingly forgotten alternative to monadic programming and checking to see
how well it can actually work in modern functional languages. We adopt and
argue the position of factoring an effectful program into a first-order
effectful DSL with a rich, higher-order 'macro' system. Not all programs can be
thus factored. Although the approach is not general-purpose, it does admit
interesting programs. The effectful DSL is likewise rather problem-specific and
lacks general-purpose monadic composition, or even functions. On the upside, it
expresses the problem elegantly, is simple to implement and reason about, and
lends itself to non-standard interpretations such as code generation
(compilation) and abstract interpretation. A specialized DSL is liable to be
frequently extended; the experience with the tagless-final style of DSL
embedding shown that the DSL evolution can be made painless, with the maximum
code reuse. We illustrate the argument on a simple but representative example
of a rather complicated effect -- non-determinism, including committed choice.
Unexpectedly, it turns out we can write interesting non-deterministic programs
in an ML-like language just as naturally and elegantly as in the
functional-logic language Curry -- and not only run them but also statically
analyze, optimize and compile. The richness of the Meta Language does, in
reality, compensate for the simplicity of the effectful DSL. The key idea goes
back to the origins of ML as the Meta Language for the Edinburgh LCF theorem
prover. Instead of using ML to build theorems, we now build (DSL) programs.
","['\nOleg Kiselyov\nTohoku University, Japan\n']","In Proceedings ML 2017, arXiv:1905.05909","EPTCS 294, 2019, pp. 15-40",http://dx.doi.org/10.4204/EPTCS.294.2,cs.PL,"['cs.PL', 'cs.LO', 'cs.SC']",10.4204/EPTCS.294.2,,"['Tohoku University, Japan']"
Fairness in Machine Learning with Tractable Models,http://arxiv.org/abs/1905.07026v2,2019-05-16T20:31:26Z,2020-01-13T13:25:33Z,"  Machine Learning techniques have become pervasive across a range of different
applications, and are now widely used in areas as disparate as recidivism
prediction, consumer credit-risk analysis and insurance pricing. The prevalence
of machine learning techniques has raised concerns about the potential for
learned algorithms to become biased against certain groups. Many definitions
have been proposed in the literature, but the fundamental task of reasoning
about probabilistic events is a challenging one, owing to the intractability of
inference.
  The focus of this paper is taking steps towards the application of tractable
models to fairness. Tractable probabilistic models have emerged that guarantee
that conditional marginal can be computed in time linear in the size of the
model. In particular, we show that sum product networks (SPNs) enable an
effective technique for determining the statistical relationships between
protected attributes and other training variables. If a subset of these
training variables are found by the SPN to be independent of the training
attribute then they can be considered `safe' variables, from which we can train
a classification model without concern that the resulting classifier will
result in disparate outcomes for different demographic groups.
  Our initial experiments on the `German Credit' data set indicate that this
processing technique significantly reduces disparate treatment of male and
female credit applicants, with a small reduction in classification accuracy
compared to state of the art. We will also motivate the concept of ""fairness
through percentile equivalence"", a new definition predicated on the notion that
individuals at the same percentile of their respective distributions should be
treated equivalently, and this prevents unfair penalisation of those
individuals who lie at the extremities of their respective distributions.
","['\nMichael Varley\n', '\nVaishak Belle\n']","In AAAI Workshop: Statistical Relational Artificial Intelligence
  (StarAI), 2020. (This is the extended version.)",,http://arxiv.org/abs/1905.07026v2,cs.LG,"['cs.LG', 'cs.AI', 'cs.SC', 'stat.ML']",,,[]
"A Correctness Result for Synthesizing Plans With Loops in Stochastic
  Domains",http://arxiv.org/abs/1905.07028v1,2019-05-16T20:41:57Z,2019-05-16T20:41:57Z,"  Finite-state controllers (FSCs), such as plans with loops, are powerful and
compact representations of action selection widely used in robotics, video
games and logistics. There has been steady progress on synthesizing FSCs in
deterministic environments, but the algorithmic machinery needed for lifting
such techniques to stochastic environments is not yet fully understood. While
the derivation of FSCs has received some attention in the context of discounted
expected reward measures, they are often solved approximately and/or without
correctness guarantees. In essence, that makes it difficult to analyze
fundamental concerns such as: do all paths terminate, and do the majority of
paths reach a goal state?
  In this paper, we present new theoretical results on a generic technique for
synthesizing FSCs in stochastic environments, allowing for highly granular
specifications on termination and goal satisfaction.
","['\nLaszlo Treszkai\n', '\nVaishak Belle\n']",,,http://arxiv.org/abs/1905.07028v1,cs.AI,"['cs.AI', 'cs.RO', 'cs.SC']",,,[]
"FiniteFlow: multivariate functional reconstruction using finite fields
  and dataflow graphs",http://arxiv.org/abs/1905.08019v2,2019-05-20T12:07:06Z,2019-07-17T10:17:15Z,"  Complex algebraic calculations can be performed by reconstructing analytic
results from numerical evaluations over finite fields. We describe FiniteFlow,
a framework for defining and executing numerical algorithms over finite fields
and reconstructing multivariate rational functions. The framework employs
computational graphs, known as dataflow graphs, to combine basic building
blocks into complex algorithms. This allows to easily implement a wide range of
methods over finite fields in high-level languages and computer algebra
systems, without being concerned with the low-level details of the numerical
implementation. This approach sidesteps the appearance of large intermediate
expressions and can be massively parallelized. We present applications to the
calculation of multi-loop scattering amplitudes, including the reduction via
integration-by-parts identities to master integrals or special functions, the
computation of differential equations for Feynman integrals, multi-loop
integrand reduction, the decomposition of amplitudes into form factors, and the
derivation of integrable symbols from a known alphabet. We also release a
proof-of-concept C++ implementation of this framework, with a high-level
interface in Mathematica.
",['\nTiziano Peraro\n'],"54 pages, 7 figures, published version",JHEP 1907 (2019) 031,http://dx.doi.org/10.1007/JHEP07(2019)031,hep-ph,"['hep-ph', 'cs.SC', 'hep-th']",10.1007/JHEP07(2019)031,,[]
Automatic Generation of Moment-Based Invariants for Prob-Solvable Loops,http://arxiv.org/abs/1905.02835v3,2019-05-07T22:57:05Z,2019-05-29T16:21:20Z,"  One of the main challenges in the analysis of probabilistic programs is to
compute invariant properties that summarise loop behaviours. Automation of
invariant generation is still at its infancy and most of the times targets only
expected values of the program variables, which is insufficient to recover the
full probabilistic program behaviour. We present a method to automatically
generate moment-based invariants of a subclass of probabilistic programs,
called Prob-Solvable loops, with polynomial assignments over random variables
and parametrised distributions. We combine methods from symbolic summation and
statistics to derive invariants as valid properties over higher-order moments,
such as expected values or variances, of program variables. We successfully
evaluated our work on several examples where full automation for computing
higher-order moments and invariants over program variables was not yet
possible.
","['\nEzio Bartocci\n', '\nLaura Kovács\n', '\nMiroslav Stankovič\n']",,,http://arxiv.org/abs/1905.02835v3,cs.SC,['cs.SC'],,,[]
"An Algorithmic Approach to Limit Cycles of Nonlinear Differential
  Systems: the Averaging Method Revisited",http://arxiv.org/abs/1905.03315v1,2019-05-08T20:04:44Z,2019-05-08T20:04:44Z,"  This paper introduces an algorithmic approach to the analysis of bifurcation
of limit cycles from the centers of nonlinear continuous differential systems
via the averaging method. We develop three algorithms to implement the
averaging method. The first algorithm allows to transform the considered
differential systems to the normal formal of averaging. Here, we restricted the
unperturbed term of the normal form of averaging to be identically zero. The
second algorithm is used to derive the computational formulae of the averaged
functions at any order. The third algorithm is based on the first two
algorithms that determines the exact expressions of the averaged functions for
the considered differential systems. The proposed approach is implemented in
Maple and its effectiveness is shown by several examples. Moreover, we report
some incorrect results in published papers on the averaging method.
","['\nBo Huang\n', '\nChee Yap\n']","Proc. 44th ISSAC, July 15--18, 2019, Beijing, China",,http://dx.doi.org/10.1145/3326229.3326234,cs.SC,"['cs.SC', 'K.1.1']",10.1145/3326229.3326234,,[]
"Implementations of efficient univariate polynomial matrix algorithms and
  application to bivariate resultants",http://arxiv.org/abs/1905.04356v1,2019-05-10T19:44:41Z,2019-05-10T19:44:41Z,"  Complexity bounds for many problems on matrices with univariate polynomial
entries have been improved in the last few years. Still, for most related
algorithms, efficient implementations are not available, which leaves open the
question of the practical impact of these algorithms, e.g. on applications such
as decoding some error-correcting codes and solving polynomial systems or
structured linear systems. In this paper, we discuss implementation aspects for
most fundamental operations: multiplication, truncated inversion, approximants,
interpolants, kernels, linear system solving, determinant, and basis reduction.
We focus on prime fields with a word-size modulus, relying on Shoup's C++
library NTL. Combining these new tools to implement variants of Villard's
algorithm for the resultant of generic bivariate polynomials (ISSAC 2018), we
get better performance than the state of the art for large parameters.
","['\nSeung Gyu Hyun\n', '\nVincent Neiger\n', '\nÉric Schost\n']","ISSAC 2019, 8 pages on 2 columns",,http://arxiv.org/abs/1905.04356v1,cs.SC,['cs.SC'],,,[]
The complexity of MinRank,http://arxiv.org/abs/1905.02682v3,2019-05-06T16:34:00Z,2022-03-10T16:24:38Z,"  In this note, we leverage some of our results from arXiv:1706.06319 to
produce a concise and rigorous proof for the complexity of the generalized
MinRank Problem in the under-defined and well-defined case. Our main theorem
recovers and extends previous results by Faug\`ere, Safey El Din, Spaenlehauer
(arXiv:1112.4411).
","['\nAlessio Caminata\n', '\nElisa Gorla\n']","Final version. Theorem numbering adjusted to match the published
  version","Women in Numbers Europe III. Association for Women in Mathematics
  Series, vol 24, pp. 163-169, Springer, Cham, 2021",http://arxiv.org/abs/1905.02682v3,cs.SC,"['cs.SC', 'cs.CR', '94A60, 13P10, 13P15, 13C40, 13P25']",,,[]
The strong approximation theorem and computing with linear groups,http://arxiv.org/abs/1905.02683v1,2019-05-07T16:36:07Z,2019-05-07T16:36:07Z,"  We obtain a computational realization of the strong approximation theorem.
That is, we develop algorithms to compute all congruence quotients modulo
rational primes of a finitely generated Zariski dense group $H \leq
\mathrm{SL}(n, \mathbb{Z})$ for $n \geq 2$. More generally, we are able to
compute all congruence quotients of a finitely generated Zariski dense subgroup
of $\mathrm{SL}(n, \mathbb{Q})$ for $n > 2$.
","['\nAlla Detinko\n', '\nDane Flannery\n', '\nAlexander Hulpke\n']",,,http://dx.doi.org/10.1016/j.jalgebra.2019.04.011,math.GR,"['math.GR', 'cs.SC', '20-04, 20G15, 20H25, 68W30']",10.1016/j.jalgebra.2019.04.011,,[]
"Effective Coefficient Asymptotics of Multivariate Rational Functions via
  Semi-Numerical Algorithms for Polynomial Systems",http://arxiv.org/abs/1905.04187v2,2019-05-10T14:15:32Z,2020-01-15T01:04:27Z,"  The coefficient sequences of multivariate rational functions appear in many
areas of combinatorics. Their diagonal coefficient sequences enjoy nice
arithmetic and asymptotic properties, and the field of analytic combinatorics
in several variables (ACSV) makes it possible to compute asymptotic expansions.
We consider these methods from the point of view of effectivity. In particular,
given a rational function, ACSV requires one to determine a (generically)
finite collection of points that are called critical and minimal. Criticality
is an algebraic condition, meaning it is well treated by classical methods in
computer algebra, while minimality is a semi-algebraic condition describing
points on the boundary of the domain of convergence of a multivariate power
series. We show how to obtain dominant asymptotics for the diagonal coefficient
sequence of multivariate rational functions under some genericity assumptions
using symbolic-numeric techniques. To our knowledge, this is the first
completely automatic treatment and complexity analysis for the asymptotic
enumeration of rational functions in an arbitrary number of variables.
","['\nStephen Melczer\n', '\nBruno Salvy\n']",46 pages. Final version accepted to Journal of Symbolic Computation,"Journal of Symbolic Computation 103 (2021), 234--279",http://dx.doi.org/10.1016/j.jsc.2020.01.001,cs.SC,"['cs.SC', 'math.CO']",10.1016/j.jsc.2020.01.001,,[]
Integrality and arithmeticity of solvable linear groups,http://arxiv.org/abs/1905.04287v1,2019-05-10T17:58:19Z,2019-05-10T17:58:19Z,"  We develop a practical algorithm to decide whether a finitely generated
subgroup of a solvable algebraic group $G$ is arithmetic. This incorporates a
procedure to compute a generating set of an arithmetic subgroup of $G$. We also
provide a simple new algorithm for integrality testing of finitely generated
solvable-by-finite linear groups over the rational field. The algorithms have
been implemented in {\sc Magma}.
","['\nW. A. de Graaf\n', '\nA. S. Detinko\n', '\nD. L. Flannery\n']",,"Journal of Symbolic Computation, 68 (2015) 138-145",http://arxiv.org/abs/1905.04287v1,math.GR,"['math.GR', 'cs.SC', '20-04, 20G15, 20H25, 68W30']",,,[]
Asymptotics of multivariate sequences in the presence of a lacuna,http://arxiv.org/abs/1905.04174v2,2019-05-10T13:56:42Z,2022-04-07T22:10:57Z,"  We explain a discontinuous drop in the exponential growth rate for certain
multivariate generating functions at a critical parameter value, in even
dimensions d at least 4. This result depends on computations in the homology of
the algebraic variety where the generating function has a pole. These
computations are similar to, and inspired by, a thread of research in
applications of complex algebraic geometry to hyperbolic PDEs, going back to
Leray, Petrowski, Atiyah, Bott and Garding. As a consequence, we give a
topological explanation for certain asymptotic phenomenon appearing in the
combinatorics and number theory literature. Furthermore, we show how to combine
topological methods with symbolic algebraic computation to determine explicitly
the dominant asymptotics for such multivariate generating functions, giving a
significant new tool to attack the so-called connection problem for asymptotics
of P-recursive sequences. This in turn enables the rigorous determination of
integer coefficients in the Morse-Smale complex, which are difficult to
determine using direct geometric methods.
","['\nYuliy Baryshnikov\n', '\nStephen Melczer\n', '\nRobin Pemantle\n']",,,http://arxiv.org/abs/1905.04174v2,math.CO,"['math.CO', 'cs.SC', 'math.GT', '05A16, 57Q99']",,,[]
Computing the volume of compact semi-algebraic sets,http://arxiv.org/abs/1904.11705v1,2019-04-26T08:04:18Z,2019-04-26T08:04:18Z,"  Let $S\subset R^n$ be a compact basic semi-algebraic set defined as the real
solution set of multivariate polynomial inequalities with rational
coefficients. We design an algorithm which takes as input a polynomial system
defining $S$ and an integer $p\geq 0$ and returns the $n$-dimensional volume of
$S$ at absolute precision $2^{-p}$.Our algorithm relies on the relationship
between volumes of semi-algebraic sets and periods of rational integrals. It
makes use of algorithms computing the Picard-Fuchs differential equation of
appropriate periods, properties of critical points, and high-precision
numerical integration of differential equations.The algorithm runs in
essentially linear time with respect to~$p$. This improves upon the previous
exponential bounds obtained by Monte-Carlo or moment-based methods. Assuming a
conjecture of Dimca, the arithmetic cost of the algebraic subroutines for
computing Picard-Fuchs equations and critical points is singly exponential in
$n$ and polynomial in the maximum degree of the input.
","['\nPierre Lairez\nSPECFUN\n', '\nMarc Mezzarobba\nPEQUAN\n', '\nMohab Safey El Din\nPolSys\n']",,"International Symposium on Symbolic and Algebraic Computation, Jul
  2019, Beijing, China",http://dx.doi.org/10.1145/3326229.3326262,cs.SC,['cs.SC'],10.1145/3326229.3326262,,"['SPECFUN', 'PEQUAN', 'PolSys']"
"Comparing machine learning models to choose the variable ordering for
  cylindrical algebraic decomposition",http://arxiv.org/abs/1904.11061v2,2019-04-24T20:42:59Z,2019-06-05T09:48:25Z,"  There has been recent interest in the use of machine learning (ML) approaches
within mathematical software to make choices that impact on the computing
performance without affecting the mathematical correctness of the result. We
address the problem of selecting the variable ordering for cylindrical
algebraic decomposition (CAD), an important algorithm in Symbolic Computation.
Prior work to apply ML on this problem implemented a Support Vector Machine
(SVM) to select between three existing human-made heuristics, which did better
than anyone heuristic alone. The present work extends to have ML select the
variable ordering directly, and to try a wider variety of ML techniques.
  We experimented with the NLSAT dataset and the Regular Chains Library CAD
function for Maple 2018. For each problem, the variable ordering leading to the
shortest computing time was selected as the target class for ML. Features were
generated from the polynomial input and used to train the following ML models:
k-nearest neighbours (KNN) classifier, multi-layer perceptron (MLP), decision
tree (DT) and SVM, as implemented in the Python scikit-learn package. We also
compared these with the two leading human constructed heuristics for the
problem: Brown's heuristic and sotd. On this dataset all of the ML approaches
outperformed the human made heuristics, some by a large margin.
","['\nMatthew England\n', '\nDorian Florescu\n']",Accepted into CICM 2019,"In: C. Kaliszyk, E. Brady, A. Kohlhase and C.C. Sacerdoti eds.,
  Intelligent Computer Mathematics (Proceedings of CICM 2019), pp. 93-108,
  (Lecture Notes in Computer Science, 11617). Springer International
  Publishing, 2019",http://dx.doi.org/10.1007/978-3-030-23250-4_7,cs.SC,"['cs.SC', 'cs.LG', '68W30, 68T05, 03C10', 'I.2.6; I.1.0']",10.1007/978-3-030-23250-4_7,,[]
"$\mathtt{bimEX}$: A Mathematica package for exact computations in $3+1$
  bimetric relativity",http://arxiv.org/abs/1904.10464v2,2019-04-23T18:00:01Z,2020-01-15T12:29:37Z,"  We present $\mathtt{bimEX}$, a Mathematica package for exact computations in
3$+$1 bimetric relativity. It is based on the $\mathtt{xAct}$ bundle, which can
handle computations involving both abstract tensors and their components. In
this communication, we refer to the latter case as concrete computations. The
package consists of two main parts. The first part involves the abstract
tensors, and focuses on how to deal with multiple metrics in $\mathtt{xAct}$.
The second part takes an ansatz for the primary variables in a chart as the
input, and returns the covariant BSSN bimetric equations in components in that
chart. Several functions are implemented to make this process as fast and
user-friendly as possible. The package has been used and tested extensively in
spherical symmetry and was the workhorse in obtaining the bimetric covariant
BSSN equations and reproducing the bimetric 3$+$1 equations in the spherical
polar chart.
",['\nFrancesco Torsello\n'],"9 pages. It matches with the published version. GitHub repository at
  https://github.com/nubirel/bimEX. Program files doi:
  http://dx.doi.org/10.17632/2s5d7csc9w.1",Computer Physics Communications 247 (2020) 106948,http://dx.doi.org/10.1016/j.cpc.2019.106948,cs.SC,"['cs.SC', 'astro-ph.CO', 'cs.MS', 'gr-qc']",10.1016/j.cpc.2019.106948,,[]
"Constructing minimal telescopers for rational functions in three
  discrete variables",http://arxiv.org/abs/1904.11614v3,2019-04-25T22:48:14Z,2022-07-07T02:36:55Z,"  We present a new algorithm for constructing minimal telescopers for rational
functions in three discrete variables. This is the first discrete
reduction-based algorithm that goes beyond the bivariate case. The termination
of the algorithm is guaranteed by a known existence criterion of telescopers.
Our approach has the important feature that it avoids the potentially costly
computation of certificates. Computational experiments are also provided so as
to illustrate the efficiency of our approach.
","['\nShaoshi Chen\n', '\nQing-Hu Hou\n', '\nHui Huang\n', '\nGeorge Labahn\n', '\nRong-Hua Wang\n']",,,http://dx.doi.org/10.1016/j.aam.2022.102389,cs.SC,"['cs.SC', 'math.CO', 'math.RA']",10.1016/j.aam.2022.102389,,[]
"Algorithmic approach to strong consistency analysis of finite difference
  approximations to PDE systems",http://arxiv.org/abs/1904.12912v1,2019-04-29T19:16:47Z,2019-04-29T19:16:47Z,"  For a wide class of polynomially nonlinear systems of partial differential
equations we suggest an algorithmic approach to the s(trong)-consistency
analysis of their finite difference approximations on Cartesian grids. First we
apply the differential Thomas decomposition to the input system, resulting in a
partition of the solution set. We consider the output simple subsystem that
contains a solution of interest. Then, for this subsystem, we suggest an
algorithm for verification of s-consistency for its finite difference
approximation. For this purpose we develop a difference analogue of the
differential Thomas decomposition, both of which jointly allow to verify the
s-consistency of the approximation. As an application of our approach, we show
how to produce s-consistent difference approximations to the incompressible
Navier-Stokes equations including the pressure Poisson equation.
","['\nVladimir P. Gerdt\n', '\nDaniel Robertz\n']",,,http://arxiv.org/abs/1904.12912v1,cs.SC,"['cs.SC', 'math.AP', 'math.NA', 'math.RA']",,,[]
Asymptotic Solutions of Polynomial Equations with Exp-Log Coefficients,http://arxiv.org/abs/1904.06796v1,2019-04-15T01:00:01Z,2019-04-15T01:00:01Z,"  We present an algorithm for computing asymptotic approximations of roots of
polynomials with exp-log function coefficients. The real and imaginary parts of
the approximations are given as explicit exp-log expressions. We provide a
method for deciding which approximations correspond to real roots. We report on
implementation of the algorithm and present empirical data.
",['\nAdam Strzeboński\n'],,,http://arxiv.org/abs/1904.06796v1,cs.SC,['cs.SC'],,,[]
"Unification and combination of a class of traversal strategies made with
  pattern matching and fixed-points",http://arxiv.org/abs/1904.07668v2,2019-04-16T13:49:10Z,2021-12-14T17:28:47Z,"  Motivated by an ongoing project on computer aided derivation of asymptotic
models governed by partial differential equations, we introduce a class of term
transformations that consists of traversal strategies and insertion of
contexts. We define unification and combination operations on this class which
amount to merging transformations in order to obtain more complex ones. We show
that the unification and combination operations enjoy nice algebraic properties
like associativity, congruence and the existence of neutral elements. The main
part of this paper is devoted to proving that the unification and combination
operations are correct.
","['\nWalid Belkhir\n', '\nNicolas Ratier\n', '\nDuy Duc Nguyen\n', '\nMichel Lenczner\n']",67 pages,,http://arxiv.org/abs/1904.07668v2,cs.LO,"['cs.LO', 'cs.SC']",,,[]
"Unification and combination of iterative insertion strategies with
  rudimentary traversals and failure",http://arxiv.org/abs/1904.10901v1,2019-04-16T13:48:59Z,2019-04-16T13:48:59Z,"  We introduce a new class of extensions of terms that consists in navigation
strategies and insertion of contexts. We introduce an operation of combination
on this class which is associative, admits a neutral element and so that each
extension is idempotent. The class of extension is also shown to be closed by
combination, with a constructive proof. This new framework is general and
independent of any application semantics. However it has been introduced for
the kernel of a software tool which aims at aiding derivation of multiscale
partial differential equation models.
","['\nWalid Belkhir\n', '\nNicolas Ratier\n', '\nDuy Duc Nguyen Michel Lenczner\n']",arXiv admin note: substantial text overlap with arXiv:1904.07668,,http://arxiv.org/abs/1904.10901v1,cs.LO,"['cs.LO', 'cs.SC']",,,[]
"Proceedings Joint International Workshop on Linearity & Trends in Linear
  Logic and Applications",http://arxiv.org/abs/1904.06159v1,2019-04-12T11:29:34Z,2019-04-12T11:29:34Z,"  This volume contains a selection of papers presented at Linearity/TLLA 2018:
Joint Linearity and TLLA workshops (part of FLOC 2018) held on July 7-8, 2018
in Oxford. Linearity has been a key feature in several lines of research in
both theoretical and practical approaches to computer science. On the
theoretical side there is much work stemming from linear logic dealing with
proof technology, complexity classes and more recently quantum computation. On
the practical side there is work on program analysis, expressive operational
semantics for programming languages, linear programming languages, program
transformation, update analysis and efficient implementation techniques. Linear
logic is not only a theoretical tool to analyse the use of resources in logic
and computation. It is also a corpus of tools, approaches, and methodologies
(proof nets, exponential decomposition, geometry of interaction, coherent
spaces, relational models, etc.) that were originally developed for the study
of linear logic's syntax and semantics and are nowadays applied in several
other fields.
","['\nThomas Ehrhard\n', '\nMaribel Fernández\n', '\nValeria de Paiva\n', '\nLorenzo Tortora de Falco\n']",,"EPTCS 292, 2019",http://dx.doi.org/10.4204/EPTCS.292,cs.LO,"['cs.LO', 'cs.CC', 'cs.PL', 'cs.SC']",10.4204/EPTCS.292,,[]
Testing zero-dimensionality of varieties at a point,http://arxiv.org/abs/1903.12365v1,2019-03-29T06:28:16Z,2019-03-29T06:28:16Z,"  Effective methods are introduced for testing zero-dimensionality of varieties
at a point. The motivation of this paper is to compute and analyze deformations
of isolated hypersurface singularities. As an application, methods for
computing local dimensions are also described. For the case where a given ideal
contains parameters, the proposed algorithms can output in particular a
decomposition of a parameter space into strata according to the local dimension
at a point of the associated varieties. The key of the proposed algorithms is
the use of the notion of comprehensive Gr\""obner systems.
","['\nKatsusuke Nabeshima\n', '\nShinichi Tajima\n']",,,http://arxiv.org/abs/1903.12365v1,cs.SC,"['cs.SC', '13P10']",,,[]
Tropical Differential Groebner Basis,http://arxiv.org/abs/1904.02275v1,2019-04-03T23:56:32Z,2019-04-03T23:56:32Z,"  In this paper, the tropical differential Gr\""obner basis is studied, which is
a natural generalization of the tropical Gr\""obner basis to the recently
introduced tropical differential algebra. Like the differential Gr\""obner
basis, the tropical differential Gr\""obner basis generally contains an infinite
number of elements. We give a Buchberger style criterion for the tropical
differential Gr\""obner basis. For ideals generated by homogeneous linear
differential polynomials with constant coefficients, we give a complete
algorithm to compute the tropical differential Gr\""obner basis.
","['\nYouren Hu\n', '\nXiao-Shan Gao\n']",,,http://arxiv.org/abs/1904.02275v1,cs.SC,['cs.SC'],,,[]
Computing huge Groebner basis like cyclic10 over $\Q$ with Giac,http://arxiv.org/abs/1903.12427v1,2019-03-29T10:00:47Z,2019-03-29T10:00:47Z,"  We present a short description on how to fine-tune the modular algorithm
implemented in the Giac computer algebra system to reconstruct huge Groebner
basis over $\Q$.The classical cyclic10 benchmark will serve as example.
",['\nBernard Parisse\nIF\n'],,,http://arxiv.org/abs/1903.12427v1,cs.SC,"['cs.SC', 'cs.MS']",,,['IF']
Reconstructing Rational Functions with $\texttt{FireFly}$,http://arxiv.org/abs/1904.00009v2,2019-03-29T18:54:01Z,2019-10-11T13:23:22Z,"  We present the open-source $\texttt{C++}$ library $\texttt{FireFly}$ for the
reconstruction of multivariate rational functions over finite fields. We
discuss the involved algorithms and their implementation. As an application, we
use $\texttt{FireFly}$ in the context of integration-by-parts reductions and
compare runtime and memory consumption to a fully algebraic approach with the
program $\texttt{Kira}$.
","['\nJonas Klappert\n', '\nFabian Lange\n']","46 pages, 3 figures, 6 tables; v2: matches published version",Comput. Phys. Commun. 247 (2020) 106951,http://dx.doi.org/10.1016/j.cpc.2019.106951,cs.SC,"['cs.SC', 'hep-ph']",10.1016/j.cpc.2019.106951,,[]
Exact Lower Bounds for Monochromatic Schur Triples and Generalizations,http://arxiv.org/abs/1904.01925v2,2019-04-03T11:36:30Z,2020-10-12T10:08:41Z,"  We derive exact and sharp lower bounds for the number of monochromatic
generalized Schur triples $(x,y,x+ay)$ whose entries are from the set
$\{1,\dots,n\}$, subject to a coloring with two different colors. Previously,
only asymptotic formulas for such bounds were known, and only for
$a\in\mathbb{N}$. Using symbolic computation techniques, these results are
extended here to arbitrary $a\in\mathbb{R}$. Furthermore, we give exact
formulas for the minimum number of monochromatic Schur triples for $a=1,2,3,4$,
and briefly discuss the case $0<a<1$.
","['\nChristoph Koutschan\n', '\nElaine Wong\n']",24 Pages,"Algorithmic Combinatorics: Enumerative Combinatorics, Special
  Functions and Computer Algebra (Veronika Pillwein, Carsten Schneider, eds.),
  Texts & Monographs in Symbolic Computation, pages 223-248, Springer, 2020.
  ISBN 978-3-030-44558-4",http://dx.doi.org/10.1007/978-3-030-44559-1_13,math.CO,"['math.CO', 'cs.SC']",10.1007/978-3-030-44559-1_13,,[]
On the Equivalence of Automatic and Symbolic Differentiation,http://arxiv.org/abs/1904.02990v4,2019-04-05T11:05:55Z,2022-12-05T15:31:45Z,"  We show that reverse mode automatic differentiation and symbolic
differentiation are equivalent in the sense that they both perform the same
operations when computing derivatives. This is in stark contrast to the common
claim that they are substantially different. The difference is often
illustrated by claiming that symbolic differentiation suffers from ""expression
swell"" whereas automatic differentiation does not. Here, we show that this
statement is not true. ""Expression swell"" refers to the phenomenon of a much
larger representation of the derivative as opposed to the representation of the
original function.
",['\nSoeren Laue\n'],,,http://arxiv.org/abs/1904.02990v4,cs.SC,"['cs.SC', 'cs.LG']",,,[]
On some classes of irreducible polynomials,http://arxiv.org/abs/1903.08441v1,2019-03-20T11:06:16Z,2019-03-20T11:06:16Z,"  The aim of the paper is to produce new families of irreducible polynomials,
generalizing previous results in the area. One example of our general result is
that for a near-separated polynomial, i.e., polynomials of the form
$F(x,y)=f_1(x)f_2(y)-f_2(x)f_1(y)$, then $F(x,y)+r$ is always irreducible for
any constant $r$ different from zero. We also provide the biggest known family
of HIP polynomials in several variables. These are polynomials
$p(x_1,\ldots,x_n) \in K[x_1,\ldots,x_n]$ over a zero characteristic field $K$
such that $p(h_1(x_1),\ldots,h_n(x_n))$ is irreducible over $K$ for every
$n$-tuple $h_1(x_1),\ldots,h_n(x_n)$ of non constant one variable polynomials
over $K$. The results can also be applied to fields of positive characteristic,
with some modifications.
","['\nJaime Gutierrez\n', '\nJorge Jimenez Urroz\n']",,,http://arxiv.org/abs/1903.08441v1,cs.SC,['cs.SC'],,,[]
Cylindrical Algebraic Decomposition with Equational Constraints,http://arxiv.org/abs/1903.08999v1,2019-03-20T11:35:08Z,2019-03-20T11:35:08Z,"  Cylindrical Algebraic Decomposition (CAD) has long been one of the most
important algorithms within Symbolic Computation, as a tool to perform
quantifier elimination in first order logic over the reals. More recently it is
finding prominence in the Satisfiability Checking community as a tool to
identify satisfying solutions of problems in nonlinear real arithmetic.
  The original algorithm produces decompositions according to the signs of
polynomials, when what is usually required is a decomposition according to the
truth of a formula containing those polynomials. One approach to achieve that
coarser (but hopefully cheaper) decomposition is to reduce the polynomials
identified in the CAD to reflect a logical structure which reduces the solution
space dimension: the presence of Equational Constraints (ECs).
  This paper may act as a tutorial for the use of CAD with ECs: we describe all
necessary background and the current state of the art. In particular, we
present recent work on how McCallum's theory of reduced projection may be
leveraged to make further savings in the lifting phase: both to the polynomials
we lift with and the cells lifted over. We give a new complexity analysis to
demonstrate that the double exponent in the worst case complexity bound for CAD
reduces in line with the number of ECs. We show that the reduction can apply to
both the number of polynomials produced and their degree.
","['\nMatthew England\n', '\nRussell Bradford\n', '\nJames H. Davenport\n']","Accepted into the Journal of Symbolic Computation. arXiv admin note:
  text overlap with arXiv:1501.04466","Journal of Symbolic Computation, volume 100, pages 38-71,
  Elsevier, 2020",http://dx.doi.org/10.1016/j.jsc.2019.07.019,cs.SC,"['cs.SC', '68W30, 03C10', 'I.1.2']",10.1016/j.jsc.2019.07.019,,[]
Local Search for Fast Matrix Multiplication,http://arxiv.org/abs/1903.11391v2,2019-03-27T12:53:51Z,2019-08-19T07:18:38Z,"  Laderman discovered a scheme for computing the product of two 3x3 matrices
using only 23 multiplications in 1976. Since then, some more such schemes were
proposed, but it remains open how many there are and whether there exist
schemes with fewer than 23 multiplications. In this paper we present two
independent SAT-based methods for finding new schemes. Both methods allow
computing a few hundred new schemes individually, and many thousands when
combined. Local search SAT solvers outperform CDCL solvers consistently in this
application.
","['\nMarijn J. H. Heule\n', '\nManuel Kauers\n', '\nMartina Seidl\n']",,,http://arxiv.org/abs/1903.11391v2,cs.LO,"['cs.LO', 'cs.SC']",,,[]
"Computation of the Expected Euler Characteristic for the Largest
  Eigenvalue of a Real Non-central Wishart Matrix",http://arxiv.org/abs/1903.10099v2,2019-03-25T02:06:17Z,2020-05-22T02:07:36Z,"  We give an approximate formula for the distribution of the largest eigenvalue
of real Wishart matrices by the expected Euler characteristic method for the
general dimension. The formula is expressed in terms of a definite integral
with parameters. We derive a differential equation satisfied by the integral
for the $2 \times 2$ matrix case and perform a numerical analysis of it.
","['\nNobuki Takayama\n', '\nLin Jiu\n', '\nSatoshi Kuriki\n', '\nYi Zhang\n']",,,http://arxiv.org/abs/1903.10099v2,math.ST,"['math.ST', 'cs.SC', 'stat.TH']",,,[]
Minimizing polynomial functions on quantum computers,http://arxiv.org/abs/1903.08270v1,2019-03-19T22:03:35Z,2019-03-19T22:03:35Z,"  This expository paper reviews some of the recent uses of computational
algebraic geometry in classical and quantum optimization. The paper assumes an
elementary background in algebraic geometry and adiabatic quantum computing
(AQC), and concentrates on presenting concrete examples (with Python codes
tested on a quantum computer) of applying algebraic geometry constructs:
solving binary optimization, factoring, and compiling. Reversing the direction,
we also briefly describe a novel use of quantum computers to compute Groebner
bases for toric ideals. We also show how Groebner bases play a role in studying
AQC at a fundamental level within a Morse theory framework. We close by placing
our work in perspective, by situating this leg of the journey, as part of a
marvelous intellectual expedition that began with our ancients over 4000 years
ago.
","['\nRaouf Dridi\n', '\nHedayat Alghassi\n', '\nSridhar Tayur\n']","Commemorating 30 years since the publication of Ray, Chakrabarti &
  Chakrabarti, Phys. Rev. B 39 (1989) 11828","Special issue of Science & Culture, 2019",http://arxiv.org/abs/1903.08270v1,quant-ph,"['quant-ph', 'cs.SC', 'math-ph', 'math.AG', 'math.MP']",,,[]
Quadratic Probabilistic Algorithms for Normal Bases,http://arxiv.org/abs/1903.03278v1,2019-03-08T04:24:07Z,2019-03-08T04:24:07Z,"  It is well known that for any finite Galois extension field $K/F$, with
Galois group $G = \mathrm{Gal}(K/F)$, there exists an element $\alpha \in K$
whose orbit $G\cdot\alpha$ forms an $F$-basis of $K$. Such an element $\alpha$
is called \emph{normal} and $G\cdot\alpha$ is called a normal basis. In this
paper we introduce a probabilistic algorithm for finding a normal element when
$G$ is either a finite abelian or a metacyclic group. The algorithm is based on
the fact that deciding whether a random element $\alpha \in K$ is normal can be
reduced to deciding whether $\sum_{\sigma \in G} \sigma(\alpha)\sigma \in K[G]$
is invertible. In an algebraic model, the cost of our algorithm is quadratic in
the size of $G$ for metacyclic $G$ and slightly subquadratic for abelian $G$.
","['\nMark Giesbrecht\n', '\nArmin Jamshidpey\n', '\nÉric Schost\n']",,,http://arxiv.org/abs/1903.03278v1,cs.SC,"['cs.SC', '12Y05, 12F10, 11y16, 68Q25']",,,[]
"Recursive Matrix Algorithms in Commutative Domain for Cluster with
  Distributed Memory",http://arxiv.org/abs/1903.04394v1,2019-03-11T16:07:22Z,2019-03-11T16:07:22Z,"  We give an overview of the theoretical results for matrix block-recursive
algorithms in commutative domains and present the results of experiments that
we conducted with new parallel programs based on these algorithms on a
supercomputer MVS-10P at the Joint Supercomputer Center of the Russian Academy
of Science. To demonstrate a scalability of these programs we measure the
running time of the program for a different number of processors and plot the
graphs of efficiency factor. Also we present the main application areas in
which such parallel algorithms are used. It is concluded that this class of
algorithms allows to obtain efficient parallel programs on clusters with
distributed memory.
","['\nGennadi Malaschonok\n', '\nEvgeni Ilchenko\n']","8 pages, 9 figures","2018 Ivannikov Memorial Workshop (IVMEM), (Yerevan, Armenia, 3-4
  May 2018) Publisher: IEEE 2019, p.40-47",http://dx.doi.org/10.1109/IVMEM.2018.00015,cs.SC,"['cs.SC', 'cs.DC', 'math.RA']",10.1109/IVMEM.2018.00015,,[]
Discovering and Proving Infinite Pochhammer Sum Identities,http://arxiv.org/abs/1902.11001v2,2019-02-28T10:40:48Z,2019-04-10T13:50:12Z,"  We consider nested sums involving the Pochhammer symbol at infinity and
rewrite them in terms of a small set of constants, such as powers of $\pi,$
$\log(2)$ or zeta values. In order to perform these simplifications, we view
the series as specializations of generating series. For these generating
series, we derive integral representations in terms of root-valued iterated
integrals or directly in terms of cyclotomic harmonic polylogarithms. Using
substitutions, we express the root-valued iterated integrals as cyclotomic
harmonic polylogarithms. Finally, by applying known relations among the
cyclotomic harmonic polylogarithms, we derive expressions in terms of several
constants. The methods are implemented in the computer algebra package
HarmonicSums.
",['\nJakob Ablinger\n'],22 pages,,http://arxiv.org/abs/1902.11001v2,math.CO,"['math.CO', 'cs.SC', 'math.NT', '05A10, 68W30, 11M32']",,,[]
"Identifying the Parametric Occurrence of Multiple Steady States for some
  Biological Networks",http://arxiv.org/abs/1902.04882v1,2019-02-13T13:12:34Z,2019-02-13T13:12:34Z,"  We consider a problem from biological network analysis of determining regions
in a parameter space over which there are multiple steady states for positive
real values of variables and parameters. We describe multiple approaches to
address the problem using tools from Symbolic Computation. We describe how
progress was made to achieve semi-algebraic descriptions of the
multistationarity regions of parameter space, and compare symbolic results to
numerical methods. The biological networks studied are models of the
mitogen-activated protein kinases (MAPK) network which has already consumed
considerable effort using special insights into its structure of corresponding
models. Our main example is a model with 11 equations in 11 variables and 19
parameters, 3 of which are of interest for symbolic treatment. The model also
imposes positivity conditions on all variables and parameters.
  We apply combinations of symbolic computation methods designed for mixed
equality/inequality systems, specifically virtual substitution, lazy real
triangularization and cylindrical algebraic decomposition, as well as a
simplification technique adapted from Gaussian elimination and graph theory. We
are able to determine multistationarity of our main example over a
2-dimensional parameter space. We also study a second MAPK model and a symbolic
grid sampling technique which can locate such regions in 3-dimensional
parameter space.
","['\nR. Bradford\n', '\nJ. H. Davenport\n', '\nM. England\n', '\nH. Errami\n', '\nV. Gerdt\n', '\nD. Grigoriev\n', '\nC. Hoyt\n', '\nM. Kosta\n', '\nO. Radulescu\n', '\nT. Sturm\n', '\nA. Weber\n']","60 pages - author preprint. Accepted in the Journal of Symbolic
  Computation","Journal of Symbolic Computation, volume 98, pp. 84 - 119, Elsevier
  2020",http://dx.doi.org/10.1016/j.jsc.2019.07.008,cs.SC,"['cs.SC', 'I.1.4']",10.1016/j.jsc.2019.07.008,,[]
"Computing Minimal Presentations and Bigraded Betti Numbers of
  2-Parameter Persistent Homology",http://arxiv.org/abs/1902.05708v6,2019-02-15T07:19:16Z,2022-05-19T23:05:34Z,"  Motivated by applications to topological data analysis, we give an efficient
algorithm for computing a (minimal) presentation of a bigraded $K[x,y]$-module
$M$, where $K$ is a field. The algorithm takes as input a short chain complex
of free modules $X\xrightarrow{f} Y \xrightarrow{g} Z$ such that $M\cong
\ker{g}/\mathrm{im}{f}$. It runs in time $O(|X|^3+|Y|^3+|Z|^3)$ and requires
$O(|X|^2+|Y|^2+|Z|^2)$ memory, where $|\cdot |$ denotes the rank. Given the
presentation computed by our algorithm, the bigraded Betti numbers of $M$ are
readily computed. Our approach is based on a simple matrix reduction algorithm,
slight variants of which compute kernels of morphisms between free modules,
minimal generating sets, and Gr\""obner bases. Our algorithm for computing
minimal presentations has been implemented in RIVET, a software tool for the
visualization and analysis of two-parameter persistent homology. In experiments
on topological data analysis problems, our implementation outperforms the
standard computational commutative algebra packages Singular and Macaulay2 by a
wide margin.
","['\nMichael Lesnick\n', '\nMatthew Wright\n']",typo fixes,,http://arxiv.org/abs/1902.05708v6,math.AT,"['math.AT', 'cs.SC', 'math.AC', '55N31, 13D02']",,,[]
"Counting basic-irreducible factors mod $p^k$ in deterministic poly-time
  and $p$-adic applications",http://arxiv.org/abs/1902.07785v1,2019-02-20T21:33:43Z,2019-02-20T21:33:43Z,"  Finding an irreducible factor, of a polynomial $f(x)$ modulo a prime $p$, is
not known to be in deterministic polynomial time. Though there is such a
classical algorithm that {\em counts} the number of irreducible factors of
$f\bmod p$. We can ask the same question modulo prime-powers $p^k$. The
irreducible factors of $f\bmod p^k$ blow up exponentially in number; making it
hard to describe them. Can we count those irreducible factors $\bmod~p^k$ that
remain irreducible mod $p$? These are called {\em basic-irreducible}. A simple
example is in $f=x^2+px \bmod p^2$; it has $p$ many basic-irreducible factors.
Also note that, $x^2+p \bmod p^2$ is irreducible but not basic-irreducible!
  We give an algorithm to count the number of basic-irreducible factors of
$f\bmod p^k$ in deterministic poly(deg$(f),k\log p$)-time. This solves the open
questions posed in (Cheng et al, ANTS'18 \& Kopp et al, Math.Comp.'19). In
particular, we are counting roots $\bmod\ p^k$; which gives the first
deterministic poly-time algorithm to compute Igusa zeta function of $f$. Also,
our algorithm efficiently partitions the set of all basic-irreducible factors
(possibly exponential) into merely deg$(f)$-many disjoint sets, using a compact
tree data structure and {\em split} ideals.
","['\nAshish Dwivedi\n', '\nRajat Mittal\n', '\nNitin Saxena\n']",28 pages,,http://arxiv.org/abs/1902.07785v1,cs.SC,"['cs.SC', 'cs.CC', 'cs.DS', 'math.NT']",,,[]
Generic reductions for in-place polynomial multiplication,http://arxiv.org/abs/1902.02967v1,2019-02-08T08:06:40Z,2019-02-08T08:06:40Z,"  The polynomial multiplication problem has attracted considerable attention
since the early days of computer algebra, and several algorithms have been
designed to achieve the best possible time complexity. More recently, efforts
have been made to improve the space complexity, developing modified versions of
a few specific algorithms to use no extra space while keeping the same
asymptotic running time. In this work, we broaden the scope in two regards.
First, we ask whether an arbitrary multiplication algorithm can be performed
in-place generically. Second, we consider two important variants which produce
only part of the result (and hence have less space to work with), the so-called
middle and short products, and ask whether these operations can also be
performed in-place. To answer both questions in (mostly) the affirmative, we
provide a series of reductions starting with any linear-space multiplication
algorithm. For full and short product algorithms these reductions yield
in-place versions with the same asymptotic time complexity as the out-of-place
version. For the middle product, the reduction incurs an extra logarithmic
factor in the time complexity only when the algorithm is quasi-linear.
","['\nPascal Giorgi\nECO\n', '\nBruno Grenet\nECO\n', '\nDaniel Roche\n']",,,http://arxiv.org/abs/1902.02967v1,cs.SC,"['cs.SC', 'cs.CC']",,,"['ECO', 'ECO']"
On the Complexity of Toric Ideals,http://arxiv.org/abs/1902.01484v1,2019-02-04T22:31:16Z,2019-02-04T22:31:16Z,"  We investigate the computational complexity of problems on toric ideals such
as normal forms, Gr\""obner bases, and Graver bases. We show that all these
problems are strongly NP-hard in the general case. Nonetheless, we can derive
efficient algorithms by taking advantage of the sparsity pattern of the matrix.
We describe this sparsity pattern with a graph, and study the parameterized
complexity of toric ideals in terms of graph parameters such as treewidth and
treedepth. In particular, we show that the normal form problem can be solved in
parameter-tractable time in terms of the treedepth. An important application of
this result is in multiway ideals arising in algebraic statistics. We also give
a parameter-tractable membership test to the reduced Gr\""obner basis. This test
leads to an efficient procedure for computing the reduced Gr\""obner basis.
Similar results hold for Graver bases computation.
","['\nDiego Cifuentes\n', '\nShmuel Onn\n']",15 pages,,http://arxiv.org/abs/1902.01484v1,cs.SC,"['cs.SC', 'math.AC', 'math.AG', '68W30 (primary), 13P10, 90C10 (secondary)']",,,[]
Modeling Terms by Graphs with Structure Constraints (Two Illustrations),http://arxiv.org/abs/1902.02010v1,2019-02-06T03:22:34Z,2019-02-06T03:22:34Z,"  In the talk at the workshop my aim was to demonstrate the usefulness of graph
techniques for tackling problems that have been studied predominantly as
problems on the term level: increasing sharing in functional programs, and
addressing questions about Milner's process semantics for regular expressions.
For both situations an approach that is based on modeling terms by graphs with
structure constraints has turned out to be fruitful. In this extended abstract
I describe the underlying problems, give references, provide examples, indicate
the chosen approaches, and compare the initial situations as well as the
results that have been obtained, and some results that are being developed at
present.
",['\nClemens Grabmayer\nGran Sasso Science Institute\n'],"In Proceedings TERMGRAPH 2018, arXiv:1902.01510","EPTCS 288, 2019, pp. 1-13",http://dx.doi.org/10.4204/EPTCS.288.1,cs.LO,"['cs.LO', 'cs.DM', 'cs.FL', 'cs.SC']",10.4204/EPTCS.288.1,,['Gran Sasso Science Institute']
"Exact Optimization via Sums of Nonnegative Circuits and Sums of AM/GM
  Exponentials",http://arxiv.org/abs/1902.02123v2,2019-02-06T11:39:18Z,2021-08-20T10:07:14Z,"  We provide two hybrid numeric-symbolic optimization algorithms, computing
exact sums of nonnegative circuits (SONC) and sums of
arithmetic-geometric-exponentials (SAGE) decompositions. Moreover, we provide a
hybrid numeric-symbolic decision algorithm for polynomials lying in the
interior of the SAGE cone. Each framework, inspired by previous contributions
of Parrilo and Peyrl, is a rounding-projection procedure.
  For a polynomial lying in the interior of the SAGE cone, we prove that the
decision algorithm terminates within a number of arithmetic operations, which
is polynomial in the number of terms of the input, and linear in the distance
to the boundary of the cone. We also provide experimental comparisons regarding
the implementation of the two optimization algorithms.
","['\nVictor Magron\n', '\nHenning Seidler\n', '\nTimo de Wolff\n']","19 pages + 4 pages appendix, 1 figure, 3 tables",,http://arxiv.org/abs/1902.02123v2,cs.SC,"['cs.SC', 'cs.CC', 'math.OC', '14P10, 68W30, 90C25 (primary), 14Q20, 68R01 (secondary)']",,,[]
"When Causal Intervention Meets Adversarial Examples and Image Masking
  for Deep Neural Networks",http://arxiv.org/abs/1902.03380v3,2019-02-09T06:44:13Z,2019-06-25T15:07:42Z,"  Discovering and exploiting the causality in deep neural networks (DNNs) are
crucial challenges for understanding and reasoning causal effects (CE) on an
explainable visual model. ""Intervention"" has been widely used for recognizing a
causal relation ontologically. In this paper, we propose a causal inference
framework for visual reasoning via do-calculus. To study the intervention
effects on pixel-level features for causal reasoning, we introduce pixel-wise
masking and adversarial perturbation. In our framework, CE is calculated using
features in a latent space and perturbed prediction from a DNN-based model. We
further provide the first look into the characteristics of discovered CE of
adversarially perturbed images generated by gradient-based methods
\footnote{~~https://github.com/jjaacckkyy63/Causal-Intervention-AE-wAdvImg}.
Experimental results show that CE is a competitive and robust index for
understanding DNNs when compared with conventional methods such as
class-activation mappings (CAMs) on the Chest X-Ray-14 dataset for
human-interpretable feature(s) (e.g., symptom) reasoning. Moreover, CE holds
promises for detecting adversarial examples as it possesses distinct
characteristics in the presence of adversarial perturbations.
","['\nChao-Han Huck Yang\n', '\nYi-Chieh Liu\n', '\nPin-Yu Chen\n', '\nXiaoli Ma\n', '\nYi-Chang James Tsai\n']","Noted our camera-ready version has changed the title. ""When Causal
  Intervention Meets Adversarial Examples and Image Masking for Deep Neural
  Networks"" as the v3 official paper title in IEEE Proceeding. Please use it in
  your formal reference. Accepted at IEEE ICIP 2019. Pytorch code has released
  on https://github.com/jjaacckkyy63/Causal-Intervention-AE-wAdvImg","2019 26th IEEE International Conference on Image Processing
  (ICIP). IEEE",http://dx.doi.org/10.1109/ICIP.2019.8803554,cs.CV,"['cs.CV', 'cs.AI', 'cs.LG', 'cs.SC']",10.1109/ICIP.2019.8803554,,[]
"Signature-based Möller's algorithm for strong Gröbner bases over
  PIDs",http://arxiv.org/abs/1901.09586v1,2019-01-28T10:38:11Z,2019-01-28T10:38:11Z,"  Signature-based algorithms are the latest and most efficient approach as of
today to compute Gr\""obner bases for polynomial systems over fields. Recently,
possible extensions of these techniques to general rings have attracted the
attention of several authors.
  In this paper, we present a signature-based version of M\""oller's classical
variant of Buchberger's algorithm for computing strong Gr\""obner bases over
Principal Ideal Domains (or PIDs). It ensures that the signatures do not
decrease during the algorithm, which makes it possible to apply classical
signature criteria for further optimization. In particular, with the F5
criterion, the signature version of M\""oller's algorithm computes a Gr\""obner
basis without reductions to zero for a polynomial system given by a regular
sequence. We also show how Buchberger's chain criterion can be implemented so
as to be compatible with the signatures.
  We prove correctness and termination of the algorithm. Furthermore, we have
written a toy implementation in Magma, allowing us to quantitatively compare
the efficiency of the various criteria for eliminating S-pairs.
","['\nMaria Francis\n', '\nThibaut Verron\n']",,,http://arxiv.org/abs/1901.09586v1,cs.SC,['cs.SC'],,,[]
"Effective certification of approximate solutions to systems of equations
  involving analytic functions",http://arxiv.org/abs/1901.10384v2,2019-01-24T21:26:36Z,2019-07-19T14:50:10Z,"  We develop algorithms for certifying an approximation to a nonsingular
solution of a square system of equations built from univariate analytic
functions. These algorithms are based on the existence of oracles for
evaluating basic data about the input analytic functions. One approach for
certification is based on alpha-theory while the other is based on the Krawczyk
generalization of Newton's iteration. We show that the necessary oracles exist
for D-finite functions and compare the two algorithmic approaches for this case
using our software implementation in SageMath.
","['\nMichael Burr\n', '\nKisun Lee\n', '\nAnton Leykin\n']",16 pages,,http://arxiv.org/abs/1901.10384v2,cs.SC,"['cs.SC', '14Q99, 68W30']",,,[]
Symbolic integration of hyperexponential 1-forms,http://arxiv.org/abs/1901.09029v1,2019-01-25T18:56:12Z,2019-01-25T18:56:12Z,"  Let $H$ be a hyperexponential function in $n$ variables $x=(x_1,\dots,x_n)$
with coefficients in a field $\mathbb{K}$, $[\mathbb{K}:\mathbb{Q}] <\infty$,
and $\omega$ a rational differential $1$-form. Assume that $H\omega$ is closed
and $H$ transcendental. We prove using Schanuel conjecture that there exist a
univariate function $f$ and multivariate rational functions $F,R$ such that
$\int H\omega= f(F(x))+H(x)R(x)$. We present an algorithm to compute this
decomposition. This allows us to present an algorithm to construct a basis of
the cohomology of differential $1$-forms with coefficients in
$H\mathbb{K}[x,1/(SD)]$ for a given $H$, $D$ being the denominator of $dH/H$
and $S\in\mathbb{K}[x]$ a square free polynomial. As an application, we
generalize a result of Singer on differential equations on the plane: whenever
it admits a Liouvillian first integral $I$ but no Darbouxian first integral,
our algorithm gives a rational variable change linearising the system.
",['\nThierry Combot\n'],8 pages,,http://arxiv.org/abs/1901.09029v1,math.DG,"['math.DG', 'cs.SC', '68W30']",,,[]
Nearly Optimal Sparse Polynomial Multiplication,http://arxiv.org/abs/1901.09355v6,2019-01-27T11:43:59Z,2020-04-20T21:01:09Z,"  In the sparse polynomial multiplication problem, one is asked to multiply two
sparse polynomials f and g in time that is proportional to the size of the
input plus the size of the output. The polynomials are given via lists of their
coefficients F and G, respectively. Cole and Hariharan (STOC 02) have given a
nearly optimal algorithm when the coefficients are positive, and Arnold and
Roche (ISSAC 15) devised an algorithm running in time proportional to the
""structural sparsity"" of the product, i.e. the set supp(F)+supp(G). The latter
algorithm is particularly efficient when there not ""too many cancellations"" of
coefficients in the product. In this work we give a clean, nearly optimal
algorithm for the sparse polynomial multiplication problem.
",['\nVasileios Nakos\n'],Accepted to IEEE Transactions on Information Theory,,http://arxiv.org/abs/1901.09355v6,cs.SC,"['cs.SC', 'cs.DS']",,,[]
"On the Existence of Telescopers for Rational Functions in Three
  Variables",http://arxiv.org/abs/1901.09377v2,2019-01-27T14:00:58Z,2020-02-17T13:15:48Z,"  Zeilberger's method of creative telescoping is crucial for the
computer-generated proofs of combinatorial and special-function identities.
Telescopers are linear differential or ($q$-)recurrence operators computed by
algorithms for creative telescoping. For a given class of inputs, when
telescopers exist and how to construct telescopers efficiently if they exist
are two fundamental problems related to creative telescoping. In this paper, we
solve the existence problem of telescopers for rational functions in three
variables including 18 cases. We reduce the existence problem from the
trivariate case to the bivariate case and some related problems. The existence
criteria given in this paper enable us to determine the termination of
algorithms for creative telescoping with trivariate rational inputs.
","['\nShaoshi Chen\n', '\nLixin Du\n', '\nRong-Hua Wang\n', '\nChaochao Zhu\n']",30 pages,,http://arxiv.org/abs/1901.09377v2,cs.SC,"['cs.SC', 'math.CO', '33F10']",,,[]
"On the Complexity of Computing the Topology of Real Algebraic Space
  Curves",http://arxiv.org/abs/1901.10317v1,2019-01-28T10:38:46Z,2019-01-28T10:38:46Z,"  In this paper, we present a deterministic algorithm to find a strong generic
position for an algebraic space curve. We modify our existing algorithm for
computing the topology of an algebraic space curve and analyze the bit
complexity of the algorithm. It is $\tilde{\mathcal {O}} (N^{20})$, where
$N=\max\{d,\tau\}$, $d, \tau$ are the degree bound and the bit size bound of
the coefficients of the defining polynomials of the algebraic space curve. To
our knowledge, this is the best bound among the existing work. It gains the
existing results at least $N^2$.
","['\nKai Jin\n', '\nJin-San Cheng\n']",,,http://arxiv.org/abs/1901.10317v1,cs.SC,"['cs.SC', 'cs.DS']",,,[]
Gr{ö}bner bases over Tate algebras,http://arxiv.org/abs/1901.09574v1,2019-01-28T09:41:40Z,2019-01-28T09:41:40Z,"  Tate algebras are fundamental objects in the context of analytic geometry
over the p-adics. Roughly speaking, they play the same role as polynomial
algebras play in classical algebraic geometry. In the present article, we
develop the formalism of Gr{\""o}bner bases for Tate algebras. We prove an
analogue of the Buchberger criterion in our framework and design a
Buchberger-like and a F4-like algorithm for computing Gr{\""o}bner bases over
Tate algebras. An implementation in SM is also discussed.
","['\nXavier Caruso\nIRMAR\n', '\nTristan Vaccon\nJKU\n', '\nThibaut Verron\nJKU\n']",,,http://arxiv.org/abs/1901.09574v1,math.AG,"['math.AG', 'cs.SC', 'math.NT']",,,"['IRMAR', 'JKU', 'JKU']"
A Faster Solution to Smale's 17th Problem I: Real Binomial Systems,http://arxiv.org/abs/1901.09739v1,2019-01-28T15:33:48Z,2019-01-28T15:33:48Z,"  Suppose $F:=(f_1,\ldots,f_n)$ is a system of random $n$-variate polynomials
with $f_i$ having degree $\leq\!d_i$ and the coefficient of $x^{a_1}_1\cdots
x^{a_n}_n$ in $f_i$ being an independent complex Gaussian of mean $0$ and
variance $\frac{d_i!}{a_1!\cdots a_n!\left(d_i-\sum^n_{j=1}a_j \right)!}$.
Recent progress on Smale's 17th Problem by Lairez --- building upon seminal
work of Shub, Beltran, Pardo, B\""{u}rgisser, and Cucker --- has resulted in a
deterministic algorithm that finds a single (complex) approximate root of $F$
using just $N^{O(1)}$ arithmetic operations on average, where
$N\!:=\!\sum^n_{i=1}\frac{(n+d_i)!}{n!d_i!}$ ($=n(n+\max_i
d_i)^{O(\min\{n,\max_i d_i)\}}$) is the maximum possible total number of
monomial terms for such an $F$. However, can one go faster when the number of
terms is smaller, and we restrict to real coefficient and real roots? And can
one still maintain average-case polynomial-time with more general probability
measures?
  We show the answer is yes when $F$ is instead a binomial system --- a case
whose numerical solution is a key step in polyhedral homotopy algorithms for
solving arbitrary polynomial systems. We give a deterministic algorithm that
finds a real approximate root (or correctly decides there are none) using just
$O(n^2(\log(n)+\log\max_i d_i))$ arithmetic operations on average. Furthermore,
our approach allows Gaussians with arbitrary variance. We also discuss briefly
the obstructions to maintaining average-case time polynomial in $n\log \max_i
d_i$ when $F$ has more terms.
","['\nGrigoris Paouris\n', '\nKaitlyn Phillipson\n', '\nJ. Maurice Rojas\n']","8 pages, submitted to a conference. Minor typos corrected",,http://arxiv.org/abs/1901.09739v1,math.AG,"['math.AG', 'cs.CC', 'cs.SC', 'math.NA']",,,[]
LU factorization with errors *,http://arxiv.org/abs/1901.10730v1,2019-01-30T09:51:16Z,2019-01-30T09:51:16Z,"  We present new algorithms to detect and correct errors in the lower-upper
factorization of a matrix, or the triangular linear system solution, over an
arbitrary field. Our main algorithms do not require any additional information
or encoding other than the original inputs and the erroneous output. Their
running time is softly linear in the dimension times the number of errors when
there are few errors, smoothly growing to the cost of fast matrix
multiplication as the number of errors increases. We also present applications
to general linear system solving.
","['\nJean-Guillaume Dumas\nCASYS\n', '\nJoris Van Der Hoeven\nCNRS, LIX\n', '\nClément Pernet\nCASYS\n', '\nDaniel Roche\n']",,,http://arxiv.org/abs/1901.10730v1,cs.SC,"['cs.SC', 'cs.IT', 'math.IT']",,,"['CASYS', 'CNRS, LIX', 'CASYS']"
"Rational Solutions of First-Order Algebraic Ordinary Difference
  Equations",http://arxiv.org/abs/1901.11048v2,2019-01-30T19:04:50Z,2019-02-01T20:21:27Z,"  We propose an algebraic geometric approach for studying rational solutions of
first-order algebraic ordinary difference equations. For an autonomous
first-order algebraic ordinary difference equations, we give an upper bound for
the degrees of its rational solutions, and thus derive a complete algorithm for
computing corresponding rational solutions.
","['\nThieu N. Vo\n', '\nYi Zhang\n']",,,http://arxiv.org/abs/1901.11048v2,cs.SC,"['cs.SC', 'math.AC', 'math.AG']",,,[]
Algorithmic counting of nonequivalent compact Huffman codes,http://arxiv.org/abs/1901.11343v4,2019-01-31T13:11:24Z,2023-01-09T15:46:21Z,"  It is known that the following five counting problems lead to the same
integer sequence~$f_t(n)$: the number of nonequivalent compact Huffman codes of
length~$n$ over an alphabet of $t$ letters, the number of `nonequivalent'
canonical rooted $t$-ary trees (level-greedy trees) with $n$~leaves, the number
of `proper' words, the number of bounded degree sequences, and the number of
ways of writing $1= \frac{1}{t^{x_1}}+ \dots + \frac{1}{t^{x_n}}$ with integers
$0 \leq x_1 \leq x_2 \leq \dots \leq x_n$. In this work, we show that one can
compute this sequence for \textbf{all} $n<N$ with essentially one power series
division. In total we need at most $N^{1+\varepsilon}$ additions and
multiplications of integers of $cN$ bits, $c<1$, or $N^{2+\varepsilon}$ bit
operations, respectively. This improves an earlier bound by Even and Lempel who
needed $O(N^3)$ operations in the integer ring or $O(N^4)$ bit operations,
respectively.
","['\nChristian Elsholtz\n', '\nClemens Heuberger\n', '\nDaniel Krenn\n']",,,http://dx.doi.org/10.1007/s00200-022-00593-0,math.CO,"['math.CO', 'cs.CC', 'cs.DM', 'cs.SC', '05A15, 05C05, 05C30, 11D68, 68P30']",10.1007/s00200-022-00593-0,,[]
An Optimization-Based Sum-of-Squares Approach to Vizing's Conjecture,http://arxiv.org/abs/1901.10288v3,2019-01-29T13:52:23Z,2019-05-06T13:36:29Z,"  Vizing's conjecture (open since 1968) relates the sizes of dominating sets in
two graphs to the size of a dominating set in their Cartesian product graph. In
this paper, we formulate Vizing's conjecture itself as a Positivstellensatz
existence question. In particular, we encode the conjecture as an
ideal/polynomial pair such that the polynomial is nonnegative if and only if
the conjecture is true. We demonstrate how to use semidefinite optimization
techniques to computationally obtain numeric sum-of-squares certificates, and
then show how to transform these numeric certificates into symbolic
certificates approving nonnegativity of our polynomial.
  After outlining the theoretical structure of this computer-based proof of
Vizing's conjecture, we present computational and theoretical results. In
particular, we present exact low-degree sparse sum-of-squares certificates for
particular families of graphs.
","['\nElisabeth Gaar\n', '\nDaniel Krenn\n', '\nSusan Margulies\n', '\nAngelika Wiegele\n']",,"Proceedings of the International Symposium on Symbolic and
  Algebraic Computation (ISSAC '19), July 15--18, 2019, Beijing, China",http://dx.doi.org/10.1145/3326229.3326239,math.CO,"['math.CO', 'cs.SC', 'math.AC', 'math.AG', 'math.OC', '90C27, 05C99, 13P10, 68W30, 90C22']",10.1145/3326229.3326239,,[]
Efficiently factoring polynomials modulo $p^4$,http://arxiv.org/abs/1901.06628v1,2019-01-20T06:40:43Z,2019-01-20T06:40:43Z,"  Polynomial factoring has famous practical algorithms over fields-- finite,
rational \& $p$-adic. However, modulo prime powers it gets hard as there is
non-unique factorization and a combinatorial blowup ensues. For example, $x^2+p
\bmod p^2$ is irreducible, but $x^2+px \bmod p^2$ has exponentially many
factors! We present the first randomized poly(deg $f, \log p$) time algorithm
to factor a given univariate integral $f(x)$ modulo $p^k$, for a prime $p$ and
$k \leq 4$. Thus, we solve the open question of factoring modulo $p^3$ posed in
(Sircana, ISSAC'17).
  Our method reduces the general problem of factoring $f(x) \bmod p^k$ to that
of {\em root finding} in a related polynomial $E(y) \bmod\langle p^k,
\varphi(x)^\ell \rangle$ for some irreducible $\varphi \bmod p$. We could
efficiently solve the latter for $k\le4$, by incrementally transforming $E(y)$.
Moreover, we discover an efficient and strong generalization of Hensel lifting
to lift factors of $f(x) \bmod p$ to those $\bmod\ p^4$ (if possible). This was
previously unknown, as the case of repeated factors of $f(x) \bmod p$ forbids
classical Hensel lifting.
","['\nAshish Dwivedi\n', '\nRajat Mittal\n', '\nNitin Saxena\n']",22 pages,,http://arxiv.org/abs/1901.06628v1,cs.CC,"['cs.CC', 'cs.DS', 'cs.SC', 'math.NT']",,,[]
On Fast Matrix Inversion via Fast Matrix Multiplication,http://arxiv.org/abs/1901.00904v1,2019-01-03T20:20:13Z,2019-01-03T20:20:13Z,"  Volker Strassen first suggested an algorithm to multiply matrices with worst
case running time less than the conventional $\mathcal{O}(n^3)$ operations in
1969. He also presented a recursive algorithm with which to invert matrices,
and calculate determinants using matrix multiplication. James R. Bunch & John
E. Hopcroft improved upon this in 1974 by providing modifications to the
inversion algorithm in the case where principal submatrices were singular,
amongst other improvements. We cover the case of multivariate polynomial matrix
inversion, where it is noted that conventional methods that assume a field will
experience major setbacks. Initially, the author and others published a
presentation of a fraction free formulation of inversion via matrix
multiplication along with motivations, however analysis of this presentation
was rudimentary. We hence provide a discussion of the true complexities of this
fraction free method arising from matrix multiplication, and arrive at its
limitations.
",['\nZak Tonks\n'],,,http://arxiv.org/abs/1901.00904v1,cs.SC,"['cs.SC', '68W30']",,,[]
Spectral Approach to Verifying Non-linear Arithmetic Circuits,http://arxiv.org/abs/1901.02950v1,2019-01-09T22:13:57Z,2019-01-09T22:13:57Z,"  This paper presents a fast and effective computer algebraic method for
analyzing and verifying non-linear integer arithmetic circuits using a novel
algebraic spectral model. It introduces a concept of algebraic spectrum, a
numerical form of polynomial expression; it uses the distribution of
coefficients of the monomials to determine the type of arithmetic function
under verification. In contrast to previous works, the proof of functional
correctness is achieved by computing an algebraic spectrum combined with a
local rewriting of word-level polynomials. The speedup is achieved by
propagating coefficients through the circuit using And-Inverter Graph (AIG)
datastructure. The effectiveness of the method is demonstrated with experiments
including standard and Booth multipliers, and other synthesized non-linear
arithmetic circuits up to 1024 bits containing over 12 million gates.
","['\nCunxi Yu\n', '\nTiankai Su\n', '\nAtif Yasin\n', '\nMaciej Ciesielski\n']",To appear at ASP-DAC'19,"24th Asia and South Pacific Design Automation Conference. 2019
  ASP-DAC",http://arxiv.org/abs/1901.02950v1,cs.SC,['cs.SC'],,,[]
"Subresultants of $(x-α)^m$ and $(x-β)^n$, Jacobi polynomials
  and complexity",http://arxiv.org/abs/1812.11789v2,2018-12-31T13:27:17Z,2019-10-05T08:49:59Z,"  In an earlier article together with Carlos D'Andrea [BDKSV2017], we described
explicit expressions for the coefficients of the order-$d$ polynomial
subresultant of $(x-\alpha)^m$ and $(x-\beta)^n $ with respect to Bernstein's
set of polynomials $\{(x-\alpha)^j(x-\beta)^{d-j}, \, 0\le j\le d\}$, for $0\le
d<\min\{m, n\}$. The current paper further develops the study of these
structured polynomials and shows that the coefficients of the subresultants of
$(x-\alpha)^m$ and $(x-\beta)^n$ with respect to the monomial basis can be
computed in linear arithmetic complexity, which is faster than for arbitrary
polynomials. The result is obtained as a consequence of the amazing though
seemingly unnoticed fact that these subresultants are scalar multiples of
Jacobi polynomials up to an affine change of variables.
","['\nA. Bostan\n', '\nT. Krick\n', '\nA. Szanto\n', '\nM. Valdettaro\n']","34 pages, accepted for publication in Journal of Symbolic Computation",,http://arxiv.org/abs/1812.11789v2,cs.SC,"['cs.SC', 'math.CA', '13P15, 15B05, 33C05, 33C45, 33F10, 68W30']",,,[]
Sheaves: A Topological Approach to Big Data,http://arxiv.org/abs/1901.01341v1,2019-01-04T23:44:47Z,2019-01-04T23:44:47Z,"  This document develops general concepts useful for extracting knowledge
embedded in large graphs or datasets that have pair-wise relationships, such as
cause-effect-type relations. Almost no underlying assumptions are made, other
than that the data can be presented in terms of pair-wise relationships between
objects/events. This assumption is used to mine for patterns in the dataset,
defining a reduced graph or dataset that boils-down or concentrates information
into a more compact form. The resulting extracted structure or set of patterns
are manifestly symbolic in nature, as they capture and encode the graph
structure of the dataset in terms of a (generative) grammar. This structure is
identified as having the formal mathematical structure of a sheaf. In essence,
this paper introduces the basic concepts of sheaf theory into the domain of
graphical datasets.
",['\nLinas Vepstas\n'],"49 pages, 24 figures","Bulletin of Novosibirsk Computing Center,Computer Science, No. 41,
  2017",http://dx.doi.org/10.31144/bncc.cs.2542-1972.2017.n41.p55-89,cs.LG,"['cs.LG', 'cs.DS', 'cs.SC']",10.31144/bncc.cs.2542-1972.2017.n41.p55-89,,[]
SIAN: software for structural identifiability analysis of ODE models,http://arxiv.org/abs/1812.10180v1,2018-12-26T00:10:54Z,2018-12-26T00:10:54Z,"  Biological processes are often modeled by ordinary differential equations
with unknown parameters. The unknown parameters are usually estimated from
experimental data. In some cases, due to the structure of the model, this
estimation problem does not have a unique solution even in the case of
continuous noise-free data. It is therefore desirable to check the uniqueness a
priori before carrying out actual experiments. We present a new software SIAN
(Structural Identifiability ANalyser) that does this. Our software can tackle
problems that could not be tackled by previously developed packages.
","['\nHoon Hong\n', '\nAlexey Ovchinnikov\n', '\nGleb Pogudin\n', '\nChee Yap\n']","This article has been accepted for publication in Bioinformatics
  published by Oxford University Press",Bioinformatics 35 (2019) 2873-2874,http://dx.doi.org/10.1093/bioinformatics/bty1069,cs.SC,"['cs.SC', 'cs.SY', 'math.DS', 'q-bio.QM']",10.1093/bioinformatics/bty1069,,[]
"A new algorithm for irreducible decomposition of representations of
  finite groups",http://arxiv.org/abs/1901.05274v1,2018-12-23T16:14:51Z,2018-12-23T16:14:51Z,"  An algorithm for irreducible decomposition of representations of finite
groups over fields of characteristic zero is described. The algorithm uses the
fact that the decomposition induces a partition of the invariant inner product
into a complete set of mutually orthogonal projectors. By expressing the
projectors through the basis elements of the centralizer ring of the
representation, the problem is reduced to solving systems of quadratic
equations. The current implementation of the algorithm is able to split
representations of dimensions up to hundreds of thousands. Examples of
calculations are given.
",['\nVladimir V Kornyak\n'],"10 pages, based on talk at Group32, Prague, July 9-13, 2018","IOP Conf. Series: Journal of Physics: Conf. Series 1194 (2019)
  012060",http://dx.doi.org/10.1088/1742-6596/1194/1/012060,math.RT,"['math.RT', 'cs.SC', 'math.GR']",10.1088/1742-6596/1194/1/012060,,[]
Computing Nearby Non-trivial Smith Forms,http://arxiv.org/abs/1812.04590v2,2018-12-11T18:21:42Z,2019-09-08T01:39:09Z,"  We consider the problem of computing the nearest matrix polynomial with a
non-trivial Smith Normal Form. We show that computing the Smith form of a
matrix polynomial is amenable to numeric computation as an optimization
problem. Furthermore, we describe an effective optimization technique to find a
nearby matrix polynomial with a non-trivial Smith form. The results are then
generalized to include the computation of a matrix polynomial having a maximum
specified number of ones in the Smith Form (i.e., with a maximum specified
McCoy rank). We discuss the geometry and existence of solutions and how our
results can be used for an error analysis. We develop an optimization-based
approach and demonstrate an iterative numerical method for computing a nearby
matrix polynomial with the desired spectral properties. We also describe an
implementation of our algorithms and demonstrate the robustness with examples
in Maple.
","['\nMark Giesbrecht\n', '\nJoseph Haraldson\n', '\nGeorge Labahn\n']",,,http://arxiv.org/abs/1812.04590v2,cs.SC,['cs.SC'],,,[]
BOSPHORUS: Bridging ANF and CNF Solvers,http://arxiv.org/abs/1812.04580v1,2018-12-11T18:05:56Z,2018-12-11T18:05:56Z,"  Algebraic Normal Form (ANF) and Conjunctive Normal Form (CNF) are commonly
used to encode problems in Boolean algebra. ANFs are typically solved via
Gr""obner basis algorithms, often using more memory than is feasible; while CNFs
are solved using SAT solvers, which cannot exploit the algebra of polynomials
naturally. We propose a paradigm that bridges between ANF and CNF solving
techniques: the techniques are applied in an iterative manner to emph{learn
facts} to augment the original problems. Experiments on over 1,100 benchmarks
arising from four different applications domains demonstrate that learnt facts
can significantly improve runtime and enable more benchmarks to be solved.
","['\nDavin Choo\n', '\nMate Soos\n', '\nKian Ming A. Chai\n', '\nKuldeep S. Meel\n']",To Appear in Proceedings of DATE 2019,,http://arxiv.org/abs/1812.04580v1,cs.LO,"['cs.LO', 'cs.SC']",,,[]
The Complexity of Factors of Multivariate Polynomials,http://arxiv.org/abs/1812.06828v1,2018-12-17T15:16:33Z,2018-12-17T15:16:33Z,"  The existence of string functions, which are not polynomial time computable,
but whose graph is checkable in polynomial time, is a basic assumption in
cryptography. We prove that in the framework of algebraic complexity, there are
no such families of polynomial functions of polynomially bounded degree over
fields of characteristic zero. The proof relies on a polynomial upper bound on
the approximative complexity of a factor g of a polynomial f in terms of the
(approximative) complexity of f and the degree of the factor g. This extends a
result by Kaltofen (STOC 1986). The concept of approximative complexity allows
to cope with the case that a factor has an exponential multiplicity, by using a
perturbation argument. Our result extends to randomized (two-sided error)
decision complexity.
",['\nPeter Bürgisser\n'],"This is an updated version of a paper published in J. FoCM in 2004.
  Here we have corrected an error in the statement and proof of Theorem 5.7","Foundations of Computational Mathematics 4(4): 369-396, 2004",http://dx.doi.org/10.1007/s10208-002-0059-5,cs.CC,"['cs.CC', 'cs.SC', '68Q17, 13P05']",10.1007/s10208-002-0059-5,,[]
In Praise of Sequence (Co-)Algebra and its implementation in Haskell,http://arxiv.org/abs/1812.05878v2,2018-12-14T12:17:56Z,2019-02-28T12:09:31Z,"  What is Sequence Algebra? This is a question that any teacher or student of
mathematics or computer science can engage with. Sequences are in Calculus,
Combinatorics, Statistics and Computation. They are foundational, a step up
from number arithmetic. Sequence operations are easy to implement from scratch
(in Haskell) and afford a wide variety of testing and experimentation. When
bits and pieces of sequence algebra are pulled together from the literature,
there emerges a claim for status as a substantial pre-analysis topic. Here we
set the stage by bringing together a variety of sequence algebra concepts for
the first time in one paper. This provides a novel economical overview,
intended to invite a broad mathematical audience to cast an eye over the
subject. A complete, yet succinct, basic implementation of sequence operations
is presented, ready to play with. The implementation also serves as a benchmark
for introducing Haskell by mathematical example.
",['\nKieran Clenaghan\n'],"43 pages. Work commenced when the author was a visitor at the
  University of York",,http://arxiv.org/abs/1812.05878v2,math.CO,"['math.CO', 'cs.DM', 'cs.PL', 'cs.SC', 'math.HO', '0502, 4001, 4002, 9702, 9704, 6802, 6804, 68R05']",,,[]
"An efficient reduction strategy for signature-based algorithms to
  compute Groebner basis",http://arxiv.org/abs/1811.12663v1,2018-11-30T08:12:42Z,2018-11-30T08:12:42Z,"  This paper introduces a strategy for signature-based algorithms to compute
Groebner basis. The signature-based algorithms generate S-pairs instead of
S-polynomials, and use s-reduction instead of the usual reduction used in the
Buchberger algorithm. There are two strategies for s-reduction: one is the
only-top reduction strategy which is the way that only leading monomials are
s-reduced. The other is the full reduction strategy which is the way that all
monomials are s-reduced. A new strategy, which we call selective-full strategy,
for s-reduction of S-pairs is introduced in this paper. In the experiment, this
strategy is efficient for computing the reduced Groebner basis. For computing a
signature Groebner basis, it is the most efficient or not the worst of the
three strategies.
",['\nKosuke Sakata\n'],13 pages,,http://arxiv.org/abs/1811.12663v1,cs.SC,"['cs.SC', 'math.AC']",,,[]
Deep learning for pedestrians: backpropagation in CNNs,http://arxiv.org/abs/1811.11987v1,2018-11-29T07:00:09Z,2018-11-29T07:00:09Z,"  The goal of this document is to provide a pedagogical introduction to the
main concepts underpinning the training of deep neural networks using gradient
descent; a process known as backpropagation. Although we focus on a very
influential class of architectures called ""convolutional neural networks""
(CNNs) the approach is generic and useful to the machine learning community as
a whole. Motivated by the observation that derivations of backpropagation are
often obscured by clumsy index-heavy narratives that appear somewhat
mathemagical, we aim to offer a conceptually clear, vectorized description that
articulates well the higher level logic. Following the principle of ""writing is
nature's way of letting you know how sloppy your thinking is"", we try to make
the calculations meticulous, self-contained and yet as intuitive as possible.
Taking nothing for granted, ample illustrations serve as visual guides and an
extensive bibliography is provided for further explorations.
  (For the sake of clarity, long mathematical derivations and visualizations
have been broken up into short ""summarized views"" and longer ""detailed views""
encoded into the PDF as optional content groups. Some figures contain
animations designed to illustrate important concepts in a more engaging style.
For these reasons, we advise to download the document locally and open it using
Adobe Acrobat Reader. Other viewers were not tested and may not render the
detailed views, animations correctly.)
",['\nLaurent Boué\n'],,,http://arxiv.org/abs/1811.11987v1,cs.LG,"['cs.LG', 'cs.AI', 'cs.CV', 'cs.SC', 'stat.ML']",,,[]
Linear Differential Equations as a Data-Structure,http://arxiv.org/abs/1811.08616v1,2018-11-21T07:37:46Z,2018-11-21T07:37:46Z,"  A lot of information concerning solutions of linear differential equations
can be computed directly from the equation. It is therefore natural to consider
these equations as a data-structure, from which mathematical properties can be
computed. A variety of algorithms has thus been designed in recent years that
do not aim at ""solving"", but at computing with this representation. Many of
these results are surveyed here.
",['\nBruno Salvy\n'],Based on an invited talk at FoCM'2017,,http://arxiv.org/abs/1811.08616v1,cs.SC,"['cs.SC', '68W30, 33F10']",,,[]
"On Exact Reznick, Hilbert-Artin and Putinar's Representations",http://arxiv.org/abs/1811.10062v4,2018-11-25T17:51:55Z,2021-09-04T09:31:02Z,"  We consider the problem of computing exact sums of squares (SOS)
decompositions for certain classes of non-negative multivariate polynomials,
relying on semidefinite programming (SDP) solvers.
  We provide a hybrid numeric-symbolic algorithm computing exact rational SOS
decompositions with rational coefficients for polynomials lying in the interior
of the SOS cone. The first step of this algorithm computes an approximate SOS
decomposition for a perturbation of the input polynomial with an
arbitrary-precision SDP solver. Next, an exact SOS decomposition is obtained
thanks to the perturbation terms and a compensation phenomenon. We prove that
bit complexity estimates on output size and runtime are both singly exponential
in the cardinality of the Newton polytope (or doubly exponential in the number
of variables). Next, we apply this algorithm to compute exact Reznick,
Hilbert-Artin's representation and Putinar's representations respectively for
positive definite forms and positive polynomials over basic compact
semi-algebraic sets. We also report on practical experiments done with the
implementation of these algorithms and existing alternatives such as the
critical point method and cylindrical algebraic decomposition.
","['\nVictor Magron\n', '\nMohab Safey El Din\n']","35 pages, 4 tables, extended version of the paper from ISSAC'18
  conference (available at arXiv::1802.10339)",,http://arxiv.org/abs/1811.10062v4,cs.SC,['cs.SC'],,,[]
Chordal Graphs in Triangular Decomposition in Top-Down Style,http://arxiv.org/abs/1811.11023v1,2018-11-25T15:12:39Z,2018-11-25T15:12:39Z,"  In this paper, we first prove that when the associated graph of a polynomial
set is chordal, a particular triangular set computed by a general algorithm in
top-down style for computing the triangular decomposition of this polynomial
set has an associated graph as a subgraph of this chordal graph. Then for
Wang's method and a subresultant-based algorithm for triangular decomposition
in top-down style and for a subresultant-based algorithm for regular
decomposition in top-down style, we prove that all the polynomial sets
appearing in the process of triangular decomposition with any of these
algorithms have associated graphs as subgraphs of this chordal graph. These
theoretical results can be viewed as non-trivial polynomial generalization of
existing ones for sparse Gaussian elimination, inspired by which we further
propose an algorithm for sparse triangular decomposition in top-down style by
making use of the chordal structure of the polynomial set. The effectiveness of
the proposed algorithm for triangular decomposition, when the polynomial set is
chordal and sparse with respect to the variables, is demonstrated by
preliminary experimental results.
","['\nChenqi Mou\n', '\nYang Bai\n', '\nJiahua Lai\n']","26 pages. arXiv admin note: substantial text overlap with
  arXiv:1802.01752",,http://arxiv.org/abs/1811.11023v1,cs.SC,['cs.SC'],,,[]
"Fast Algorithms for Computing Eigenvectors of Matrices via Pseudo
  Annihilating Polynomials",http://arxiv.org/abs/1811.09149v2,2018-11-22T12:58:33Z,2019-02-17T13:30:50Z,"  An efficient algorithm for computing eigenvectors of a matrix of integers by
exact computation is proposed. The components of calculated eigenvectors are
expressed as polynomials in the eigenvalue to which the eigenvector is
associated, as a variable. The algorithm, in principle, utilizes the minimal
annihilating polynomials for eliminating redundant calculations. Furthermore,
in the actual computation, the algorithm computes candidates of eigenvectors by
utilizing pseudo annihilating polynomials and verifies their correctness. The
experimental results show that our algorithms have better performance compared
to conventional methods.
","['\nShinichi Tajima\n', '\nKatsuyoshi Ohara\n', '\nAkira Terui\n']",27 pages,,http://arxiv.org/abs/1811.09149v2,math.NA,"['math.NA', 'cs.SC', '15A18, 68W30']",,,[]
Simplification of tensor expressions in computer algebra,http://arxiv.org/abs/1811.07701v1,2018-11-19T14:14:52Z,2018-11-19T14:14:52Z,"  Computer algebra is widely used in various fields of mathematics, physics and
other sciences. The simplification of tensor expressions is an important
special case of computer algebra. In this paper, we consider the reduction of
tensor polynomials to canonical form, taking into account the properties of
symmetry under permutations of indices, the symmetries associated with the
renaming of summation indices, and also linear relations between tensors of a
general form. We give a definition of the canonical representation for
polynomial (multiplicative) expressions of variables with abstract indices,
which is the result of averaging of the original expression by the action of
some finite group (the signature stabilizer). In practice, the proposed
algorithms demonstrate high efficiency for expressions made of Riemann
curvature tensors.
","['\nA. Kryukov\n', '\nG. Shpiz\n']",6 pages,,http://dx.doi.org/10.1088/1742-6596/1163/1/012052,cs.SC,"['cs.SC', 'hep-ph', 'hep-th']",10.1088/1742-6596/1163/1/012052,,[]
A Fast Randomized Geometric Algorithm for Computing Riemann-Roch Spaces,http://arxiv.org/abs/1811.08237v8,2018-11-20T13:28:38Z,2020-10-19T12:34:26Z,"  We propose a probabilistic variant of Brill-Noether's algorithm for computing
a basis of the Riemann-Roch space $L(D)$ associated to a divisor $D$ on a
projective nodal plane curve $\mathcal C$ over a sufficiently large perfect
field $k$. Our main result shows that this algorithm requires at most
$O(\max(\mathrm{deg}(\mathcal C)^{2\omega}, \mathrm{deg}(D_+)^\omega))$
arithmetic operations in $k$, where $\omega$ is a feasible exponent for matrix
multiplication and $D_+$ is the smallest effective divisor such that $D_+\geq
D$. This improves the best known upper bounds on the complexity of computing
Riemann-Roch spaces. Our algorithm may fail, but we show that provided that a
few mild assumptions are satisfied, the failure probability is bounded by
$O(\max(\mathrm{deg}(\mathcal C)^4, \mathrm{deg}(D_+)^2)/\lvert \mathcal
E\rvert)$, where $\mathcal E$ is a finite subset of $k$ in which we pick
elements uniformly at random. We provide a freely available C++/NTL
implementation of the proposed algorithm and we present experimental data. In
particular, our implementation enjoys a speedup larger than 6 on many examples
(and larger than 200 on some instances over large finite fields) compared to
the reference implementation in the Magma computer algebra system. As a
by-product, our algorithm also yields a method for computing the group law on
the Jacobian of a smooth plane curve of genus $g$ within $O(g^\omega)$
operations in $k$, which equals the best known complexity for this problem.
","['\nAude Le Gluher\n', '\nPierre-Jean Spaenlehauer\n']",,"Mathematics of Computation 89 (2020), 2399-2433",http://dx.doi.org/10.1090/mcom/3517,cs.SC,"['cs.SC', 'cs.CC', 'math.AG']",10.1090/mcom/3517,,[]
"Kleene stars of the plane, polylogarithms and symmetries",http://arxiv.org/abs/1811.09091v2,2018-11-22T10:34:32Z,2020-02-20T10:39:45Z,"  We extend the definition and construct several bases for polylogarithms Li T
, where T are some series, recognizable by a finite state (multiplicity)
automaton of alphabet 4 X = {x 0 , x 1 }. The kernel of this new
""polylogarithmic map"" Li $\bullet$ is also characterized and provides a
rewriting process which terminates to a normal form. We concentrate on
algebraic and analytic aspects of this extension allowing index polylogarithms
at non positive multi-indices, by rational series and regularize polyzetas at
non positive multi-indices.
","['\nGérard Henry Edmond Duchamp\nLIPN\n', '\nVincel Hoang Ngoc Minh\n', '\nNgo Quoc Hoan\n']",,"Theoretical Computer Science, Elsevier, 2019, 800, pp.52-72",http://arxiv.org/abs/1811.09091v2,cs.SC,"['cs.SC', 'cs.DM', 'math.CO']",,,['LIPN']
"An Application of Rubi: Series Expansion of the Quark Mass
  Renormalization Group Equation",http://arxiv.org/abs/1811.04892v1,2018-11-12T18:21:09Z,2018-11-12T18:21:09Z,"  We highlight how Rule-based Integration (Rubi) is an enhanced method of
symbolic integration which allows for the integration of many difficult
integrals not accomplished by other computer algebra systems. Using Rubi, many
integration techniques become tractable. Integrals are approached using
step-wise simplification, hence distilling an integral (if the solution is
unknown) into composite integrals which highlight yet undiscovered integration
rules. The motivating example we use is the derivation of the updated series
expansion of the quark mass renormalization group equation (RGE) to five-loop
order. This series provides the relation between a light quark mass in the
modified minimal subtraction ($\overline{\text{MS}}$) scheme defined at some
given scale, e.g. at the tau-lepton mass scale, and another chosen energy
scale, $s$. This relation explicitly depicts the renormalization scheme
dependence of the running quark mass on the scale parameter, $s$, and is
important in accurately determining a light quark mass at a chosen scale. The
five-loop QCD $\beta(a_s)$ and $\gamma(a_s)$ functions are used in this
determination.
","['\nAlexes Mes\n', '\nJed Stephens\n']","Mathematica file used is attached as a supplementary resource, 9
  pages, 1 figure. Comments are invited",,http://arxiv.org/abs/1811.04892v1,hep-ph,"['hep-ph', 'cs.SC']",,,[]
"A SAT+CAS Approach to Finding Good Matrices: New Examples and
  Counterexamples",http://arxiv.org/abs/1811.05094v1,2018-11-13T04:07:56Z,2018-11-13T04:07:56Z,"  We enumerate all circulant good matrices with odd orders divisible by 3 up to
order 70. As a consequence of this we find a previously overlooked set of good
matrices of order 27 and a new set of good matrices of order 57. We also find
that circulant good matrices do not exist in the orders 51, 63, and 69, thereby
finding three new counterexamples to the conjecture that such matrices exist in
all odd orders. Additionally, we prove a new relationship between the entries
of good matrices and exploit this relationship in our enumeration algorithm.
Our method applies the SAT+CAS paradigm of combining computer algebra
functionality with modern SAT solvers to efficiently search large spaces which
are specified by both algebraic and logical constraints.
","['\nCurtis Bright\n', '\nDragomir Z. Djokovic\n', '\nIlias Kotsireas\n', '\nVijay Ganesh\n']",,,http://dx.doi.org/10.1609/aaai.v33i01.33011435,cs.LO,"['cs.LO', 'cs.SC', 'math.CO']",10.1609/aaai.v33i01.33011435,,[]
ATENSOR - REDUCE program for tensor simplification,http://arxiv.org/abs/1811.05409v1,2018-11-13T17:03:01Z,2018-11-13T17:03:01Z,"  The paper presents a REDUCE program for the simplification of tensor
expressions that are considered as formal indexed objects. The proposed
algorithm is based on the consideration of tensor expressions as vectors in
some linear space. This linear space is formed by all the elements of the group
algebra of the corresponding tensor expression. Such approach permits us to
simplify the tensor expressions possessing symmetry properties, summation
(dummy) indices and multiterm identities by unify manner. The canonical element
for the tensor expression is defined in terms of the basic vectors of this
linear space. The main restriction of the algorithm is the dimension of the
linear space that is equal to N!, where N is a number of indices of the tensor
expression. The program uses REDUCE as user interface.
","['\nV. A. Ilyin\n', '\nA. P. Kryukov\n']",21 pages,"Computer Physics Communications. Volume 96, Issue 1, July 1996,
  Pages 36-52",http://dx.doi.org/10.1016/0010-4655(96)00060-4,cs.SC,"['cs.SC', 'hep-ph', 'hep-th']",10.1016/0010-4655(96)00060-4,,[]
"Staging Human-computer Dialogs: An Application of the Futamura
  Projections",http://arxiv.org/abs/1811.05536v1,2018-11-13T21:45:20Z,2018-11-13T21:45:20Z,"  We demonstrate an application of the Futamura Projections to human-computer
interaction, and particularly to staging human-computer dialogs. Specifically,
by providing staging analogs to the classical Futamura Projections, we
demonstrate that the Futamura Projections can be applied to the staging of
human-computer dialogs in addition to the execution of programs.
","['\nBrandon M. Williams\n', '\nSaverio Perugini\n']","13 pages, 6 figures, and 3 tables",,http://arxiv.org/abs/1811.05536v1,cs.PL,"['cs.PL', 'cs.CL', 'cs.HC', 'cs.SC', 'cs.SE']",,,[]
"Temporal viability regulation for control affine systems with
  applications to mobile vehicle coordination under time-varying motion
  constraints",http://arxiv.org/abs/1811.06350v1,2018-11-15T14:03:33Z,2018-11-15T14:03:33Z,"  Controlled invariant set and viability regulation of dynamical control
systems have played important roles in many control and coordination
applications. In this paper we develop a temporal viability regulation theory
for general dynamical control systems, and in particular for control affine
systems. The time-varying viable set is parameterized by time-varying
constraint functions, with the aim to regulate a dynamical control system to be
invariant in the time-varying viable set so that temporal state-dependent
constraints are enforced. We consider both time-varying equality and inequality
constraints in defining a temporal viable set. We also present sufficient
conditions for the existence of feasible control input for the control affine
systems. The developed temporal viability regulation theory is applied to
mobile vehicle coordination.
","['\nMarcus Greiff\n', '\nZhiyong Sun\n', '\nAnders Robertsson\n', '\nRolf Johansson\n']","7 pages, 3 figures. Submitted to a conference for publication",,http://arxiv.org/abs/1811.06350v1,cs.SY,"['cs.SY', 'cs.MA', 'cs.RO', 'cs.SC', 'math.OC']",,,[]
A nearly optimal algorithm to decompose binary forms,http://arxiv.org/abs/1810.12588v2,2018-10-30T08:58:27Z,2019-09-11T11:42:27Z,"  Symmetric tensor decomposition is an important problem with applications in
several areas for example signal processing, statistics, data analysis and
computational neuroscience. It is equivalent to Waring's problem for
homogeneous polynomials, that is to write a homogeneous polynomial in n
variables of degree D as a sum of D-th powers of linear forms, using the
minimal number of summands. This minimal number is called the rank of the
polynomial/tensor. We focus on decomposing binary forms, a problem that
corresponds to the decomposition of symmetric tensors of dimension 2 and order
D. Under this formulation, the problem finds its roots in invariant theory
where the decompositions are known as canonical forms. In this context many
different algorithms were proposed. We introduce a superfast algorithm that
improves the previous approaches with results from structured linear algebra.
It achieves a softly linear arithmetic complexity bound. To the best of our
knowledge, the previously known algorithms have at least quadratic complexity
bounds. Our algorithm computes a symbolic decomposition in $O(M(D) log(D))$
arithmetic operations, where $M(D)$ is the complexity of multiplying two
polynomials of degree D. It is deterministic when the decomposition is unique.
When the decomposition is not unique, our algorithm is randomized. We present a
Monte Carlo version of it and we show how to modify it to a Las Vegas one,
within the same complexity. From the symbolic decomposition, we approximate the
terms of the decomposition with an error of $2^{--$\epsilon$}$ , in $O(D
log^2(D) (log^2(D) + log($\epsilon$)))$ arithmetic operations. We use results
from Kaltofen and Yagati (1989) to bound the size of the representation of the
coefficients involved in the decomposition and we bound the algebraic degree of
the problem by min(rank, D -- rank + 1). We show that this bound can be tight.
When the input polynomial has integer coefficients, our algorithm performs, up
to poly-logarithmic factors, $O\_{bit} (D{\ell} + D^4 + D^3 $\tau$)$ bit
operations, where $$\tau$$ is the maximum bitsize of the coefficients and
$2^{--{\ell}}$ is the relative error of the terms in the decomposition.
","['\nMatías Bender\nPolSys\n', '\nJean-Charles Faugère\nPolSys\n', '\nLudovic Perret\nPolSys\n', '\nElias Tsigaridas\nPolSys\n']",Accepted to JSC,,http://arxiv.org/abs/1810.12588v2,cs.SC,['cs.SC'],,,"['PolSys', 'PolSys', 'PolSys', 'PolSys']"
"On the complexity of class group computations for large degree number
  fields",http://arxiv.org/abs/1810.11396v1,2018-10-26T15:45:50Z,2018-10-26T15:45:50Z,"  In this paper, we examine the general algorithm for class group computations,
when we do not have a small defining polynomial for the number field. Based on
a result of Biasse and Fieker, we simplify their algorithm, improve the
complexity analysis and identify the optimal parameters to reduce the runtime.
We make use of the classes $\mathcal D$ defined in [GJ16] for classifying the
fields according to the size of the extension degree and prove that they enable
to describe all the number fields.
",['\nAlexandre Gélin\n'],,,http://arxiv.org/abs/1810.11396v1,math.NT,"['math.NT', 'cs.SC']",,,[]
"Reducing the complexity for class group computations using small
  defining polynomials",http://arxiv.org/abs/1810.12010v1,2018-10-29T09:10:39Z,2018-10-29T09:10:39Z,"  In this paper, we describe an algorithm that efficiently collect relations in
class groups of number fields defined by a small defining polynomial. This
conditional improvement consists in testing directly the smoothness of
principal ideals generated by small algebraic integers. This strategy leads to
an algorithm for computing the class group whose complexity is possibly as low
as $L_{|\Delta_{\mathbf K}|}\left(\frac{1}{3}\right)$.
",['\nAlexandre Gélin\n'],,,http://arxiv.org/abs/1810.12010v1,math.NT,"['math.NT', 'cs.SC']",,,[]
A Simple Recurrent Unit with Reduced Tensor Product Representations,http://arxiv.org/abs/1810.12456v6,2018-10-29T23:31:39Z,2019-11-05T10:38:38Z,"  idely used recurrent units, including Long-short Term Memory (LSTM) and the
Gated Recurrent Unit (GRU), perform well on natural language tasks, but their
ability to learn structured representations is still questionable. Exploiting
reduced Tensor Product Representations (TPRs) --- distributed representations
of symbolic structure in which vector-embedded symbols are bound to
vector-embedded structural positions --- we propose the TPRU, a simple
recurrent unit that, at each time step, explicitly executes structural-role
binding and unbinding operations to incorporate structural information into
learning. A gradient analysis of our proposed TPRU is conducted to support our
model design, and its performance on multiple datasets shows the effectiveness
of our design choices. Furthermore, observations on a linguistically grounded
study demonstrate the interpretability of our TPRU.
","['\nShuai Tang\n', '\nPaul Smolensky\n', '\nVirginia R. de Sa\n']",,,http://arxiv.org/abs/1810.12456v6,cs.NE,"['cs.NE', 'cs.LG', 'cs.SC']",,,[]
Computation of gcd chain over the power of an irreducible polynomial,http://arxiv.org/abs/1810.09056v2,2018-10-22T01:57:55Z,2018-12-27T00:07:46Z,"  A notion of gcd chain has been introduced by the author at ISSAC 2017 for two
univariate monic polynomials with coefficients in a ring R = k[x_1, ..., x_n
]/(T) where T is a primary triangular set of dimension zero. A complete
algorithm to compute such a gcd chain remains challenging. This work treats
completely the case of a triangular set T = (T_1 (x)) in one variable, namely a
power of an irreducible polynomial. This seemingly ""easy"" case reveals the main
steps necessary for treating the general case, and it allows to isolate the
particular one step that does not directly extend and requires more care.
",['\nXavier Dahan\n'],"(Added a full running example) 16 pages, 3 figures. Full version of
  extended abstract presented at ADG 2018",,http://arxiv.org/abs/1810.09056v2,cs.SC,['cs.SC'],,,[]
"Proceedings of the 15th International Workshop on the ACL2 Theorem
  Prover and Its Applications",http://arxiv.org/abs/1810.03762v2,2018-10-09T00:41:08Z,2018-10-29T04:51:01Z,"  This volume contains the proceedings of the Fifteenth International Workshop
on the ACL2 Theorem Prover and Its Applications (ACL2-2018), a two-day workshop
held in Austin, Texas, USA, on November 5-6, 2018, immediately after FMCAD'18.
The proceedings of ACL2-2018 include eleven long papers and two extended
abstracts.
","['\nShilpi Goel\nCentaur Technology, Inc.\n', '\nMatt Kaufmann\nThe University of Texas at Austin\n']",,"EPTCS 280, 2018",http://dx.doi.org/10.4204/EPTCS.280,cs.SC,"['cs.SC', 'cs.LO']",10.4204/EPTCS.280,,"['Centaur Technology, Inc.', 'The University of Texas at Austin']"
"Reconstruction of surfaces with ordinary singularities from their
  silhouettes",http://arxiv.org/abs/1810.05559v2,2018-10-12T14:52:53Z,2021-04-23T10:49:34Z,"  We present algorithms for reconstructing, up to unavoidable projective
automorphisms, surfaces with ordinary singularities in three dimensional space
starting from their silhouette, or ""apparent contour"" - namely the branching
locus of a projection on the plane - and the projection of their singular
locus.
","['\nMatteo Gallet\n', '\nNiels Lubbes\n', '\nJosef Schicho\n', '\nJan Vršek\n']",36 pages,"SIAM J. Appl. Algebra Geometry, 3(3), 472-506 (2019)",http://dx.doi.org/10.1137/18M1220911,math.AG,"['math.AG', 'cs.SC', '14Q10 (14J99)']",10.1137/18M1220911,,[]
Integration in terms of polylogarithm,http://arxiv.org/abs/1810.05865v2,2018-10-13T14:15:40Z,2019-06-21T11:43:36Z,"  This paper provides a Liouville principle for integration in terms of
dilogarithm and partial result for polylogarithm.
",['\nWaldemar Hebisch\n'],,,http://arxiv.org/abs/1810.05865v2,math.NT,"['math.NT', 'cs.SC', '12H05']",,,[]
Computing Elimination Ideals and Discriminants of Likelihood Equations,http://arxiv.org/abs/1810.05620v1,2018-10-12T17:30:01Z,2018-10-12T17:30:01Z,"  We develop a probabilistic algorithm for computing elimination ideals of
likelihood equations, which is for larger models by far more efficient than
directly computing Groebner bases or the interpolation method proposed in the
first author's previous work. The efficiency is improved by a theoretical
result showing that the sum of data variables appears in most coefficients of
the generator polynomial of elimination ideal. Furthermore, applying the known
structures of Newton polytopes of discriminants, we can also efficiently deduce
discriminants of the elimination ideals. For instance, the discriminants of 3
by 3 matrix model and one Jukes-Cantor model in phylogenetics (with sizes over
30 GB and 8 GB text files, respectively) can be computed by our methods.
","['\nXiaoxian Tang\n', '\nTimo De Wolff\n', '\nRukai Zhao\n']","31 pages, 1 figure, 3 tables",,http://arxiv.org/abs/1810.05620v1,cs.SC,"['cs.SC', 'math.AG', 'stat.CO']",,,[]
Bohemian Upper Hessenberg Matrices,http://arxiv.org/abs/1809.10653v1,2018-09-27T17:28:56Z,2018-09-27T17:28:56Z,"  We look at Bohemian matrices, specifically those with entries from $\{-1, 0,
{+1}\}$. More, we specialize the matrices to be upper Hessenberg, with
subdiagonal entries $\pm1$. Many properties remain after these specializations,
some of which surprised us. We find two recursive formulae for the
characteristic polynomials of upper Hessenberg matrices. Focusing on only those
matrices whose characteristic polynomials have maximal height allows us to
explicitly identify these polynomials and give a lower bound on their height.
This bound is exponential in the order of the matrix. We count stable matrices,
normal matrices, and neutral matrices, and tabulate the results of our
experiments. We prove a theorem about the only possible kinds of normal
matrices amongst a specific family of Bohemian upper Hessenberg matrices.
","['\nEunice Y. S. Chan\n', '\nRobert M. Corless\n', '\nLaureano Gonzalez-Vega\n', '\nJ. Rafael Sendra\n', '\nJuana Sendra\n', '\nSteven E. Thornton\n']","23 pages, 4 figures",,http://arxiv.org/abs/1809.10653v1,cs.SC,"['cs.SC', 'cs.NA', '15B05, 15B36, 11C20']",,,[]
Bohemian Upper Hessenberg Toeplitz Matrices,http://arxiv.org/abs/1809.10664v1,2018-09-27T17:45:31Z,2018-09-27T17:45:31Z,"  We look at Bohemian matrices, specifically those with entries from $\{-1, 0,
{+1}\}$. More, we specialize the matrices to be upper Hessenberg, with
subdiagonal entries $1$. Even more, we consider Toeplitz matrices of this kind.
Many properties remain after these specializations, some of which surprised us.
Focusing on only those matrices whose characteristic polynomials have maximal
height allows us to explicitly identify these polynomials and give a lower
bound on their height. This bound is exponential in the order of the matrix.
","['\nEunice Y. S. Chan\n', '\nRobert M. Corless\n', '\nLaureano Gonzalez-Vega\n', '\nJ. Rafael Sendra\n', '\nJuana Sendra\n', '\nSteven E. Thornton\n']","18 pages, 4 figures. arXiv admin note: text overlap with
  arXiv:1809.10653",,http://arxiv.org/abs/1809.10664v1,cs.SC,"['cs.SC', 'cs.NA', '15B05, 15B36, 11C20']",,,[]
A Pommaret Bases Approach to the Degree of a Polynomial Ideal,http://arxiv.org/abs/1809.10964v1,2018-09-28T11:15:56Z,2018-09-28T11:15:56Z,"  In this paper, we study first the relationship between Pommaret bases and
Hilbert series. Given a finite Pommaret basis, we derive new explicit formulas
for the Hilbert series and for the degree of the ideal generated by it which
exhibit more clearly the influence of each generator. Then we establish a new
dimension depending Bezout bound for the degree and use it to obtain a
dimension depending bound for the ideal membership problem.
","['\nBentolhoda Binaei\n', '\nAmir Hashemi\n', '\nWerner M. Seiler\n']",,"Applicable Algebra in Engineering, Communication and Computing,
  Volume 29, Issue 4, pages 283--301, 2018",http://arxiv.org/abs/1809.10964v1,math.AG,"['math.AG', 'cs.SC', 'math.AC']",,,[]
Computation of Pommaret Bases Using Syzygies,http://arxiv.org/abs/1809.10971v1,2018-09-28T11:39:46Z,2018-09-28T11:39:46Z,"  We investigate the application of syzygies for efficiently computing (finite)
Pommaret bases. For this purpose, we first describe a non-trivial variant of
Gerdt's algorithm to construct an involutive basis for the input ideal as well
as an involutive basis for the syzygy module of the output basis. Then we apply
this new algorithm in the context of Seiler's method to transform a given ideal
into quasi stable position to ensure the existence of a finite Pommaret basis.
This new approach allows us to avoid superfluous reductions in the iterative
computation of Janet bases required by this method. We conclude the paper by
proposing an involutive variant of the signature based algorithm of Gao et al.
to compute simultaneously a Grobner basis for a given ideal and for the syzygy
module of the input basis. All the presented algorithms have been implemented
in Maple and their performance is evaluated via a set of benchmark ideals.
","['\nBentolhoda Binaei\n', '\nAmir Hashemi\n', '\nWerner M. Seiler\n']","Computer Algebra in Scientific Computing (CASC 2018), Lille, France,
  2018","Lecture Notes in Computer Science, Volume 11077, pages 51--66,
  2018",http://arxiv.org/abs/1809.10971v1,math.AG,"['math.AG', 'cs.SC', 'math.AC']",,,[]
"A Novel Algebraic Geometry Compiling Framework for Adiabatic Quantum
  Computations",http://arxiv.org/abs/1810.01440v1,2018-10-02T18:21:13Z,2018-10-02T18:21:13Z,"  Adiabatic Quantum Computing (AQC) is an attractive paradigm for solving hard
integer polynomial optimization problems. Available hardware restricts the
Hamiltonians to be of a structure that allows only pairwise interactions. This
requires that the original optimization problem to be first converted -- from
its polynomial form -- to a quadratic unconstrained binary optimization (QUBO)
problem, which we frame as a problem in algebraic geometry. Additionally, the
hardware graph where such a QUBO-Hamiltonian needs to be embedded -- assigning
variables of the problem to the qubits of the physical optimizer -- is not a
complete graph, but rather one with limited connectivity. This ""problem graph
to hardware graph"" embedding can also be framed as a problem of computing a
Groebner basis of a certain specially constructed polynomial ideal. We develop
a systematic computational approach to prepare a given polynomial optimization
problem for AQC in three steps. The first step reduces an input polynomial
optimization problem into a QUBO through the computation of the Groebner basis
of a toric ideal generated from the monomials of the input objective function.
The second step computes feasible embeddings. The third step computes the
spectral gap of the adiabatic Hamiltonian associated to a given embedding.
These steps are applicable well beyond the integer polynomial optimization
problem. Our paper provides the first general purpose computational procedure
that can be used directly as a $translator$ to solve polynomial integer
optimization. Alternatively, it can be used as a test-bed (with small size
problems) to help design efficient heuristic quantum compilers by studying
various choices of reductions and embeddings in a systematic and comprehensive
manner. An added benefit of our framework is in designing Ising architectures
through the study of $\mathcal Y-$minor universal graphs.
","['\nRaouf Dridi\n', '\nHedayat Alghassi\n', '\nSridhar Tayur\n']","Key words: Polynomial Optimization, Adiabatic Quantum Computing,
  Ising Model, Graph Embedding, Spectral Gap, Groebner basis, Fiber Bundles,
  Classical Invariant Theory, Compilers",,http://arxiv.org/abs/1810.01440v1,quant-ph,"['quant-ph', 'cs.IT', 'cs.SC', 'math.AG', 'math.IT']",,,[]
New bounds and efficient algorithm for sparse difference resultant,http://arxiv.org/abs/1810.00057v3,2018-09-21T12:24:29Z,2021-04-20T02:54:50Z,"  The sparse difference resultant introduced in \citep{gao-2015} is a basic
concept in difference elimination theory.
  In this paper, we show that the sparse difference resultant of a generic
Laurent transformally essential system can be computed via the sparse resultant
of a simple algebraic system arising from the difference system. Moreover, new
order bounds of sparse difference resultant are found. Then we propose an
efficient algorithm to compute sparse difference resultant which is the
quotient of two determinants whose elements are the coefficients of the
polynomials in the algebraic system.
  The complexity of the algorithm is analyzed and experimental results show the
efficiency of the algorithm.
","['\nChun-Ming Yuan\n', '\nZhi-Yong Zhang\n']",22 pages,Journal of Symbolic Computation 2021,http://dx.doi.org/10.1016/j.jsc.2021.04.002,cs.SC,['cs.SC'],10.1016/j.jsc.2021.04.002,,[]
Towards a symbolic summation theory for unspecified sequences,http://arxiv.org/abs/1809.06578v1,2018-09-18T08:16:23Z,2018-09-18T08:16:23Z,"  The article addresses the problem whether indefinite double sums involving a
generic sequence can be simplified in terms of indefinite single sums.
Depending on the structure of the double sum, the proposed summation machinery
may provide such a simplification without exceptions. If it fails, it may
suggest a more advanced simplification introducing in addition a single nested
sum where the summand has to satisfy a particular constraint. More precisely,
an explicitly given parameterized telescoping equation must hold. Restricting
to the case that the arising unspecified sequences are specialized to the class
of indefinite nested sums defined over hypergeometric, multi-basic or mixed
hypergeometric products, it can be shown that this constraint is not only
sufficient but also necessary.
","['\nPeter Paule\n', '\nCarsten Schneider\n']",,,http://arxiv.org/abs/1809.06578v1,cs.SC,"['cs.SC', 'math.CO']",,,[]
Degree bound for toric envelope of a linear algebraic group,http://arxiv.org/abs/1809.06489v4,2018-09-18T00:24:37Z,2021-08-28T19:39:24Z,"  Algorithms working with linear algebraic groups often represent them via
defining polynomial equations. One can always choose defining equations for an
algebraic group to be of the degree at most the degree of the group as an
algebraic variety. However, the degree of a linear algebraic group $G \subset
\mathrm{GL}_n(C)$ can be arbitrarily large even for $n = 1$. One of the key
ingredients of Hrushovski's algorithm for computing the Galois group of a
linear differential equation was an idea to `approximate' every algebraic
subgroup of $\mathrm{GL}_n(C)$ by a `similar' group so that the degree of the
latter is bounded uniformly in $n$. Making this uniform bound computationally
feasible is crucial for making the algorithm practical.
  In this paper, we derive a single-exponential degree bound for such an
approximation (we call it toric envelope), which is qualitatively optimal. As
an application, we improve the quintuply exponential bound for the first step
of the Hrushovski's algorithm due to Feng to a single-exponential bound. For
the cases $n = 2, 3$ often arising in practice, we further refine our general
bound.
","['\nEli Amzallag\n', '\nAndrei Minchenko\n', '\nGleb Pogudin\n']","Accepted to Mathematics of Computation, 27 months between the
  submission and the first report",,http://arxiv.org/abs/1809.06489v4,math.AG,"['math.AG', 'cs.SC', 'math.CA']",,,[]
Machine-Assisted Proofs (ICM 2018 Panel),http://arxiv.org/abs/1809.08062v1,2018-09-21T12:33:22Z,2018-09-21T12:33:22Z,"  This submission to arXiv is the report of a panel session at the 2018
International Congress of Mathematicians (Rio de Janeiro, August). It is
intended that, while v1 is that report, this stays a living document containing
the panelists', and others', reflections on the topic.
","['\nJames Davenport\n', '\nBjorn Poonen\n', '\nJames Maynard\n', '\nHarald Helfgott\n', '\nPham Huu Tiep\n', '\nLuís Cruz-Filipe\n']",,,http://arxiv.org/abs/1809.08062v1,math.HO,"['math.HO', 'cs.SC', 'math.GR', 'math.NT', '03-04, 11-04, 20-04,']",,,[]
Computer algebra tools for Feynman integrals and related multi-sums,http://arxiv.org/abs/1809.06168v1,2018-09-17T12:53:15Z,2018-09-17T12:53:15Z,"  In perturbative calculations, e.g., in the setting of Quantum Chromodynamics
(QCD) one aims at the evaluation of Feynman integrals. Here one is often faced
with the problem to simplify multiple nested integrals or sums to expressions
in terms of indefinite nested integrals or sums. Furthermore, one seeks for
solutions of coupled systems of linear differential equations, that can be
represented in terms of indefinite nested sums (or integrals). In this article
we elaborate the main tools and the corresponding packages, that we have
developed and intensively used within the last 10 years in the course of our
QCD-calculations.
","['\nJohannes Blümlein\n', '\nCarsten Schneider\n']",,,http://arxiv.org/abs/1809.06168v1,cs.SC,"['cs.SC', 'hep-ph', 'hep-th', 'math-ph', 'math.MP']",,,[]
Detecting tropical defects of polynomial equations,http://arxiv.org/abs/1809.03350v2,2018-09-10T14:28:16Z,2019-11-11T13:00:26Z,"  We introduce the notion of tropical defects, certificates that a system of
polynomial equations is not a tropical basis, and provide two algorithms for
finding them in affine spaces of complementary dimension to the zero set. We
use these techniques to solve open problems regarding del Pezzo surfaces of
degree 3 and realizability of valuated gaussoids on 4 elements.
","['\nPaul Görlach\n', '\nYue Ren\n', '\nJeff Sommars\n']","Changes: improved presentation in Section 2. To appear in Journal of
  Algebraic Combinatorics",Journal of Algebraic Combinatorics (2019),http://dx.doi.org/10.1007/s10801-019-00916-4,math.AG,"['math.AG', 'cs.SC', '14T05, 13P10, 68W30']",10.1007/s10801-019-00916-4,,[]
"Smoothed Analysis for the Condition Number of Structured Real Polynomial
  Systems",http://arxiv.org/abs/1809.03626v3,2018-09-10T22:59:41Z,2021-06-20T17:35:08Z,"  We consider the sensitivity of real zeros of structured polynomial systems to
perturbations of their coefficients. In particular, we provide explicit
estimates for condition numbers of structured random real polynomial systems,
and extend these estimates to the smoothed analysis setting.
","['\nAlperen A. Ergür\n', '\nGrigoris Paouris\n', '\nJ. Maurice Rojas\n']","Title changed due to an interesting journal policy, to appear in
  Mathematics of Computation","Mathematics of Computation, 2021",http://arxiv.org/abs/1809.03626v3,math.AG,"['math.AG', 'cs.NA', 'cs.SC', 'math.MG', 'math.NA']",,,[]
Randomized Polynomial-Time Root Counting in Prime Power Rings,http://arxiv.org/abs/1808.10531v2,2018-08-30T21:50:17Z,2019-02-14T19:15:48Z,"  Suppose $k,p\!\in\!\mathbb{N}$ with $p$ prime and $f\!\in\!\mathbb{Z}[x]$ is
a univariate polynomial with degree $d$ and all coefficients having absolute
value less than $p^k$. We give a Las Vegas randomized algorithm that computes
the number of roots of $f$ in $\mathbb{Z}/\!\left(p^k\right)$ within time
$d^3(k\log p)^{2+o(1)}$. (We in fact prove a more intricate complexity bound
that is slightly better.) The best previous general algorithm had
(deterministic) complexity exponential in $k$. We also present some
experimental data evincing the potential practicality of our algorithm.
","['\nLeann Kopp\n', '\nNatalie Randall\n', '\nJ. Maurice Rojas\n', '\nYuyu Zhu\n']","11 pages, 3 figures. Qi Cheng just pointed out that [3, Cor. 4, Pg.
  16] proves a generalization of the main result (Theorem 1.1), and gives a
  sharper complexity bound. Nevertheless, the underlying algorithms are
  approached differently, so the development of our paper (the recursion tree
  structure, in particular) may still be of value",,http://arxiv.org/abs/1808.10531v2,math.NT,"['math.NT', 'cs.CC', 'cs.SC']",,,[]
Aligator.jl - A Julia Package for Loop Invariant Generation,http://arxiv.org/abs/1808.05394v1,2018-08-16T09:49:57Z,2018-08-16T09:49:57Z,"  We describe the Aligator.jl software package for automatically generating all
polynomial invariants of the rich class of extended P-solvable loops with
nested conditionals. Aligator.jl is written in the programming language Julia
and is open-source. Aligator.jl transforms program loops into a system of
algebraic recurrences and implements techniques from symbolic computation to
solve recurrences, derive closed form solutions of loop variables and infer the
ideal of polynomial invariants by variable elimination based on Gr\""obner basis
computation.
","['\nAndreas Humenberger\n', '\nMaximilian Jaroschek\n', '\nLaura Kovács\n']",,,http://dx.doi.org/10.1007/978-3-319-96812-4_10,cs.SC,['cs.SC'],10.1007/978-3-319-96812-4_10,,[]
"Bringing Together Dynamic Geometry Software and the Graphics Processing
  Unit",http://arxiv.org/abs/1808.04579v1,2018-08-14T08:20:36Z,2018-08-14T08:20:36Z,"  We equip dynamic geometry software (DGS) with a user-friendly method that
enables massively parallel calculations on the graphics processing unit (GPU).
This interplay of DGS and GPU opens up various applications in education and
mathematical research. The GPU-aided discovery of mathematical properties,
interactive visualizations of algebraic surfaces (raycasting), the mathematical
deformation of images and footage in real-time, and computationally demanding
numerical simulations of PDEs are examples from the long and versatile list of
new domains that our approach makes accessible within a DGS. We ease the
development of complex (mathematical) visualizations and provide a
rapid-prototyping scheme for general-purpose computations (GPGPU).
  The possibility to program both CPU and GPU with the use of only one
high-level (scripting) programming language is a crucial aspect of our concept.
We embed shader programming seamlessly within a high-level (scripting)
programming environment. The aforementioned requires the symbolic process of
the transcompilation of a high-level programming language into shader
programming language for GPU and, in this article, we address the challenge of
the automatic translation of a high-level programming language to a shader
language of the GPU. To maintain platform independence and the possibility to
use our technology on modern devices, we focus on a realization through WebGL.
","['\nAaron Montag\n', '\nJürgen Richter-Gebert\n']",,,http://arxiv.org/abs/1808.04579v1,cs.MS,"['cs.MS', 'cs.DC', 'cs.SC', 'math.AG', '68U05, 68W30, 68W10, 97R60, 14Q05, 14Q10, 68N20', 'G.4; D.3.3; D.3.4; I.3.1; I.6.8; K.3.1']",,,[]
"An Effective Framework for Constructing Exponent Lattice Basis of
  Nonzero Algebraic Numbers",http://arxiv.org/abs/1808.02712v3,2018-08-08T10:28:11Z,2019-05-08T18:53:05Z,"  Computing a basis for the exponent lattice of algebraic numbers is a basic
problem in the field of computational number theory with applications to many
other areas. The main cost of a well-known algorithm
\cite{ge1993algorithms,kauers2005algorithms} solving the problem is on
computing the primitive element of the extended field generated by the given
algebraic numbers. When the extended field is of large degree, the problem
seems intractable by the tool implementing the algorithm. In this paper, a
special kind of exponent lattice basis is introduced. An important feature of
the basis is that it can be inductively constructed, which allows us to deal
with the given algebraic numbers one by one when computing the basis. Based on
this, an effective framework for constructing exponent lattice basis is
proposed. Through computing a so-called pre-basis first and then solving some
linear Diophantine equations, the basis can be efficiently constructed. A new
certificate for multiplicative independence and some techniques for decreasing
degrees of algebraic numbers are provided to speed up the computation. The new
algorithm has been implemented with Mathematica and its effectiveness is
verified by testing various examples. Moreover, the algorithm is applied to
program verification for finding invariants of linear loops.
",['\nTao Zheng\n'],"8 pages,no figure,18 conferences",,http://dx.doi.org/10.1145/3326229.3326243,cs.SC,['cs.SC'],10.1145/3326229.3326243,,[]
Minimal solutions of the rational interpolation problem,http://arxiv.org/abs/1808.02575v2,2018-08-07T22:57:56Z,2019-10-23T15:59:56Z,"  We explore connections between the approach of solving the rational
interpolation problem via resolutions of ideals and syzygies with the standard
method provided by the Extended Euclidean Algorithm. As a consequence, we
obtain explicit descriptions for solutions of ""minimal"" degrees in terms of the
degrees of elements appearing in the EEA. This allows us to describe the
minimal degree in a $\mu$-basis of a polynomial planar parametrization in terms
of a ""critical"" degree arising in the EEA.
","['\nTeresa Cortadellas Benitez\n', ""\nCarlos D'Andrea\n"", '\nEulalia Montoro\n']","16 pages, revised version. Accepted for publication at Revista de la
  Union Matematica Argentina",,http://arxiv.org/abs/1808.02575v2,math.AC,"['math.AC', 'cs.SC']",,,[]
Computing Unit Groups of Curves,http://arxiv.org/abs/1808.02742v1,2018-08-08T13:04:44Z,2018-08-08T13:04:44Z,"  The group of units modulo constants of an affine variety over an
algebraically closed field is free abelian of finite rank. Computing this group
is difficult but of fundamental importance in tropical geometry, where it is
desirable to realize intrinsic tropicalizations. We present practical
algorithms for computing unit groups of smooth curves of low genus. Our
approach is rooted in divisor theory, based on interpolation in the case of
rational curves and on methods from algebraic number theory in the case of
elliptic curves.
","['\nJustin Chen\n', '\nSameera Vemulapalli\n', '\nLeon Zhang\n']",,,http://arxiv.org/abs/1808.02742v1,math.AG,"['math.AG', 'cs.SC']",,,[]
What Can (and Can't) we Do with Sparse Polynomials?,http://arxiv.org/abs/1807.08289v1,2018-07-22T14:40:34Z,2018-07-22T14:40:34Z,"  Simply put, a sparse polynomial is one whose zero coefficients are not
explicitly stored. Such objects are ubiquitous in exact computing, and so
naturally we would like to have efficient algorithms to handle them. However,
with this compact storage comes new algorithmic challenges, as fast algorithms
for dense polynomials may no longer be efficient. In this tutorial we examine
the state of the art for sparse polynomial algorithms in three areas:
arithmetic, interpolation, and factorization. The aim is to highlight recent
progress both in theory and in practice, as well as opportunities for future
work.
",['\nDaniel S. Roche\n'],"Tutorial at ISSAC 2018, Pages 25-30 of proceedings, In Proc. 43rd
  International Symposium on Symbolic and Algebraic Computation, ISSAC '18.
  ACM, New York, NY, 2018",,http://dx.doi.org/10.1145/3208976.3209027,cs.SC,['cs.SC'],10.1145/3208976.3209027,,[]
Fast transforms over finite fields of characteristic two,http://arxiv.org/abs/1807.07785v1,2018-07-20T10:51:22Z,2018-07-20T10:51:22Z,"  An additive fast Fourier transform over a finite field of characteristic two
efficiently evaluates polynomials at every element of an $\mathbb{F}_2$-linear
subspace of the field. We view these transforms as performing a change of basis
from the monomial basis to the associated Lagrange basis, and consider the
problem of performing the various conversions between these two bases, the
associated Newton basis, and the '' novel '' basis of Lin, Chung and Han (FOCS
2014). Existing algorithms are divided between two families, those designed for
arbitrary subspaces and more efficient algorithms designed for specially
constructed subspaces of fields with degree equal to a power of two. We
generalise techniques from both families to provide new conversion algorithms
that may be applied to arbitrary subspaces, but which benefit equally from the
specially constructed subspaces. We then construct subspaces of fields with
smooth degree for which our algorithms provide better performance than existing
algorithms.
",['\nNicholas Coxon\nGRACE\n'],,,http://arxiv.org/abs/1807.07785v1,cs.SC,"['cs.SC', 'cs.CC']",,,['GRACE']
Tropical recurrent sequences,http://arxiv.org/abs/1807.10714v5,2018-07-27T16:34:16Z,2020-02-04T20:15:38Z,"  Tropical recurrent sequences are introduced satisfying a given vector (being
a tropical counterpart of classical linear recurrent sequences). We consider
the case when Newton polygon of the vector has a single (bounded) edge. In this
case there are periodic tropical recurrent sequences which are similar to
classical linear recurrent sequences. A question is studied when there exists a
non-periodic tropical recurrent sequence satisfying a given vector, and partial
answers are provided to this question. Also an algorithm is designed which
tests existence of non-periodic tropical recurrent sequences satisfying a given
vector with integer coordinates. Finally, we introduce a tropical entropy of a
vector and provide some bounds on it.
",['\nDima Grigoriev\n'],it is described when the entropy is positive,,http://arxiv.org/abs/1807.10714v5,math.AG,"['math.AG', 'cs.SC', '14T05']",,,[]
"Uma análise comparativa de ferramentas de análise estática para
  deteção de erros de memória",http://arxiv.org/abs/1807.08015v1,2018-07-20T20:12:24Z,2018-07-20T20:12:24Z,"  --- Portuguese version
  As falhas de software est\~ao com frequ\^encia associadas a acidentes com
graves consequ\^encias econ\'omicas e/ou humanas, pelo que se torna imperioso
investir na valida\c{c}\~ao do software, nomeadamente daquele que \'e
cr\'itico. Este artigo endere\c{c}a a tem\'atica da qualidade do software
atrav\'es de uma an\'alise comparativa da usabilidade e efic\'acia de quatro
ferramentas de an\'alise est\'atica de programas em C/C++. Este estudo permitiu
compreender o grande potencial e o elevado impacto que as ferramentas de
an\'alise est\'atica podem ter na valida\c{c}\~ao e verifica\c{c}\~ao de
software. Como resultado complementar, foram identificados novos erros em
programas de c\'odigo aberto e com elevada popularidade, que foram reportados.
  --- English version
  Software bugs are frequently associated with accidents with serious
economical and/or human consequences, being thus imperative the investment in
the validation of software, namely of the critical one. This article addresses
the topic of software quality by making a comparative analysis of the usability
and efficiency of four static analysis tools for C/C++ programs. This study
allow to understand the big potential and high impact that these tools may have
in the validation and verification of software. As a complementary result, we
identified new errors in very popular open source projects, which have been
reported.
","['\nPatrícia Monteiro\n', '\nJoão Lourenço\n', '\nAntónio Ravara\n']","Article in Portuguese, accepted in the national informatics
  conference INForum (http://inforum.org.pt/INForum2018)",,http://arxiv.org/abs/1807.08015v1,cs.SE,"['cs.SE', 'cs.PL', 'cs.SC']",,,[]
"Toward an Optimal Quantum Algorithm for Polynomial Factorization over
  Finite Fields",http://arxiv.org/abs/1807.09675v1,2018-07-25T15:49:49Z,2018-07-25T15:49:49Z,"  We present a randomized quantum algorithm for polynomial factorization over
finite fields. For polynomials of degree $n$ over a finite field $\F_q$, the
average-case complexity of our algorithm is an expected $O(n^{1 + o(1)} \log^{2
+ o(1)}q)$ bit operations. Only for a negligible subset of polynomials of
degree $n$ our algorithm has a higher complexity of $O(n^{4 / 3 + o(1)} \log^{2
+ o(1)}q)$ bit operations. This breaks the classical $3/2$-exponent barrier for
polynomial factorization over finite fields \cite{guo2016alg}.
",['\nJavad Doliskani\n'],,,http://arxiv.org/abs/1807.09675v1,cs.SC,"['cs.SC', 'cs.CC', 'math.NT', 'quant-ph']",,,[]
Orbits of monomials and factorization into products of linear forms,http://arxiv.org/abs/1807.03663v1,2018-07-10T14:13:57Z,2018-07-10T14:13:57Z,"  This paper is devoted to the factorization of multivariate polynomials into
products of linear forms, a problem which has applications to differential
algebra, to the resolution of systems of polynomial equations and to Waring
decomposition (i.e., decomposition in sums of d-th powers of linear forms; this
problem is also known as symmetric tensor decomposition). We provide three
black box algorithms for this problem. Our main contribution is an algorithm
motivated by the application to Waring decomposition. This algorithm reduces
the corresponding factorization problem to simultaenous matrix diagonalization,
a standard task in linear algebra. The algorithm relies on ideas from invariant
theory, and more specifically on Lie algebras. Our second algorithm
reconstructs a factorization from several bi-variate projections. Our third
algorithm reconstructs it from the determination of the zero set of the input
polynomial, which is a union of hyperplanes.
","['\nPascal Koiran\nLIP\n', '\nNicolas Ressayre\nICJ\n']",,,http://arxiv.org/abs/1807.03663v1,cs.CC,"['cs.CC', 'cs.SC', 'math.AC']",,,"['LIP', 'ICJ']"
"Multistationarity and Bistability for Fewnomial Chemical Reaction
  Networks",http://arxiv.org/abs/1807.02991v2,2018-07-09T08:58:05Z,2018-12-03T17:12:39Z,"  Bistability and multistationarity are properties of reaction networks linked
to switch-like responses and connected to cell memory and cell decision making.
Determining whether and when a network exhibits bistability is a hard and open
mathematical problem. One successful strategy consists of analyzing small
networks and deducing that some of the properties are preserved upon passage to
the full network. Motivated by this we study chemical reaction networks with
few chemical complexes. Under mass-action kinetics the steady states of these
networks are described by fewnomial systems, that is polynomial systems having
few distinct monomials. Such systems of polynomials are often studied in real
algebraic geometry by the use of Gale dual systems. Using this Gale duality we
give precise conditions in terms of the reaction rate constants for the number
and stability of the steady states of families of reaction networks with one
non-flow reaction.
","['\nElisenda Feliu\n', '\nMartin Helmer\n']",,"Bulletin of Mathematical Biology, 2018",http://dx.doi.org/10.1007/s11538-018-00555-z,math.AG,"['math.AG', 'cs.SC', 'math.DS', 'q-bio.MN', 'q-bio.QM', '92Exx, 92C42, 14P10, 37N25, 14P05']",10.1007/s11538-018-00555-z,,[]
Machine Learning for Mathematical Software,http://arxiv.org/abs/1806.10920v1,2018-06-28T12:35:47Z,2018-06-28T12:35:47Z,"  While there has been some discussion on how Symbolic Computation could be
used for AI there is little literature on applications in the other direction.
However, recent results for quantifier elimination suggest that, given enough
example problems, there is scope for machine learning tools like Support Vector
Machines to improve the performance of Computer Algebra Systems. We survey the
authors own work and similar applications for other mathematical software.
  It may seem that the inherently probabilistic nature of machine learning
tools would invalidate the exact results prized by mathematical software.
However, algorithms and implementations often come with a range of choices
which have no effect on the mathematical correctness of the end result but a
great effect on the resources required to find it, and thus here, machine
learning can have a significant impact.
",['\nM. England\n'],To appear in Proc. ICMS 2018,"In: J.H. Davenport, M. Kauers, G. Labahn and J. Urban, eds.
  Mathematical Software - ICMS 2018, pp. 165-174. (Lecture Notes in Computer
  Science 10931). Springer, 2018",http://dx.doi.org/10.1007/978-3-319-96418-8_20,cs.SC,"['cs.SC', '68T05, 68W30', 'I.2.6; I.1.0']",10.1007/978-3-319-96418-8_20,,[]
"Non-linear Real Arithmetic Benchmarks derived from Automated Reasoning
  in Economics",http://arxiv.org/abs/1806.11447v1,2018-06-28T12:46:58Z,2018-06-28T12:46:58Z,"  We consider problems originating in economics that may be solved
automatically using mathematical software. We present and make freely available
a new benchmark set of such problems. The problems have been shown to fall
within the framework of non-linear real arithmetic, and so are in theory
soluble via Quantifier Elimination (QE) technology as usually implemented in
computer algebra systems. Further, they all can be phrased in prenex normal
form with only existential quantifiers and so are also admissible to those
Satisfiability Module Theory (SMT) solvers that support the QF_NRA. There is a
great body of work considering QE and SMT application in science and
engineering, but we demonstrate here that there is potential for this
technology also in the social sciences.
","['\nC. Mulligan\n', '\nR. Bradford\n', '\nJ. H. Davenport\n', '\nM. England\n', '\nZ. Tonks\n']","To appear in Proc. SC-Square 2018. Dataset described is hosted by
  Zenodo at: https://doi.org/10.5281/zenodo.1226892 . arXiv admin note:
  substantial text overlap with arXiv:1804.10037","In: A. Bigatti and M. Brain eds. Proceedings of the 3rd Workshop
  on Satisfiability Checking and Symbolic Computation (SC2 '18), pp. 48-60.
  CEUR Workshop Proceedings 2189, 2018",http://arxiv.org/abs/1806.11447v1,cs.SC,"['cs.SC', '68W30, 03C10, 91-04', 'I.1.2; J.4']",,,[]
"Verification Protocols with Sub-Linear Communication for Polynomial
  Matrix Operations",http://arxiv.org/abs/1807.01272v2,2018-07-03T16:44:21Z,2019-12-11T10:14:00Z,"  We design and analyze new protocols to verify the correctness of various
computations on matrices over the ring F[x] of univariate polynomials over a
field F. For the sake of efficiency, and because many of the properties we
verify are specific to matrices over a principal ideal domain, we cannot simply
rely on previously-developed linear algebra protocols for matrices over a
field. Our protocols are interactive, often randomized, and feature a constant
number of rounds of communication between the Prover and Verifier. We seek to
minimize the communication cost so that the amount of data sent during the
protocol is significantly smaller than the size of the result being verified,
which can be useful when combining protocols or in some multi-party settings.
The main tools we use are reductions to existing linear algebra verification
protocols and a new protocol to verify that a given vector is in the F[x]-row
space of a given matrix.
","['\nDavid Lucas\n', '\nVincent Neiger\n', '\nClément Pernet\n', '\nDaniel S. Roche\n', '\nJohan Rosenkilde\n']","39 pages, 20 protocols",,http://arxiv.org/abs/1807.01272v2,cs.SC,['cs.SC'],,,[]
Implementation of a Near-Optimal Complex Root Clustering Algorithm,http://arxiv.org/abs/1806.10584v3,2018-06-27T17:31:48Z,2018-08-01T18:50:36Z,"  We describe Ccluster, a software for computing natural $\epsilon$-clusters of
complex roots in a given box of the complex plane. This algorithm from Becker
et al.~(2016) is near-optimal when applied to the benchmark problem of
isolating all complex roots of an integer polynomial. It is one of the first
implementations of a near-optimal algorithm for complex roots. We describe some
low level techniques for speeding up the algorithm. Its performance is compared
with the well-known MPSolve library and Maple.
","['\nRémi Imbach\n', '\nVictor Y. Pan\n', '\nChee Yap\n']",,,http://arxiv.org/abs/1806.10584v3,cs.MS,"['cs.MS', 'cs.SC']",,,[]
TheoryGuru: A Mathematica Package to apply Quantifier Elimination,http://arxiv.org/abs/1806.10925v1,2018-06-28T12:53:08Z,2018-06-28T12:53:08Z,"  We consider the use of Quantifier Elimination (QE) technology for automated
reasoning in economics. There is a great body of work considering QE
applications in science and engineering but we demonstrate here that it also
has use in the social sciences. We explain how many suggested theorems in
economics could either be proven, or even have their hypotheses shown to be
inconsistent, automatically via QE.
  However, economists who this technology could benefit are usually unfamiliar
with QE, and the use of mathematical software generally. This motivated the
development of a Mathematica Package TheoryGuru, whose purpose is to lower the
costs of applying QE to economics. We describe the package's functionality and
give examples of its use.
","['\nC. Mulligan\n', '\nJ. H. Davenport\n', '\nM. England\n']",To appear in Proc ICMS 2018,"In: J.H. Davenport, M. Kauers, G. Labahn and J. Urban, eds.
  Mathematical Software - ICMS 2018, pp. 369-378. (Lecture Notes in Computer
  Science 10931). Springer, 2018",http://dx.doi.org/10.1007/978-3-319-96418-8_44,cs.SC,"['cs.SC', 'cs.CE', '68W30, 03C10, 91-04', 'I.1.2; J.4']",10.1007/978-3-319-96418-8_44,,[]
Proof-of-work certificates that can be efficiently computed in the cloud,http://arxiv.org/abs/1806.11293v2,2018-06-29T08:13:29Z,2018-07-21T15:53:18Z,"  In an emerging computing paradigm, computational capabilities, from
processing power to storage capacities, are offered to users over communication
networks as a cloud-based service. There, demanding computations are outsourced
in order to limit infrastructure costs. The idea of verifiable computing is to
associate a data structure, a proof-of-work certificate, to the result of the
outsourced computation. This allows a verification algorithm to prove the
validity of the result, faster than by recomputing it. We talk about a Prover
(the server performing the computations) and a Verifier. Goldwasser, Kalai and
Rothblum gave in 2008 a generic method to verify any parallelizable
computation, in almost linear time in the size of the, potentially structured,
inputs and the result. However, the extra cost of the computations for the
Prover (and therefore the extra cost to the customer), although only almost a
constant factor of the overall work, is nonetheless prohibitive in practice.
Differently, we will here present problem-specific procedures in computer
algebra, e.g. for exact linear algebra computations, that are Prover-optimal,
that is that have much less financial overhead.
",['\nJean-Guillaume Dumas\nLMC - IMAG\n'],"The 20th International Workshop on Computer Algebra in Scientific
  Computing, Sep 2018, Lille, France",,http://arxiv.org/abs/1806.11293v2,cs.SC,"['cs.SC', 'cs.CR']",,,['LMC - IMAG']
"Fast Hermite interpolation and evaluation over finite fields of
  characteristic two",http://arxiv.org/abs/1807.00645v1,2018-07-02T13:19:18Z,2018-07-02T13:19:18Z,"  This paper presents new fast algorithms for Hermite interpolation and
evaluation over finite fields of characteristic two. The algorithms reduce the
Hermite problems to instances of the standard multipoint interpolation and
evaluation problems, which are then solved by existing fast algorithms. The
reductions are simple to implement and free of multiplications, allowing low
overall multiplicative complexities to be obtained. The algorithms are suitable
for use in encoding and decoding algorithms for multiplicity codes.
",['\nNicholas Coxon\nGRACE\n'],,,http://arxiv.org/abs/1807.00645v1,cs.SC,"['cs.SC', 'cs.CC']",,,['GRACE']
A Purely Functional Computer Algebra System Embedded in Haskell,http://arxiv.org/abs/1807.01456v2,2018-07-04T06:22:02Z,2018-09-20T08:54:50Z,"  We demonstrate how methods in Functional Programming can be used to implement
a computer algebra system. As a proof-of-concept, we present the
computational-algebra package. It is a computer algebra system implemented as
an embedded domain-specific language in Haskell, a purely functional
programming language. Utilising methods in functional programming and prominent
features of Haskell, this library achieves safety, composability, and
correctness at the same time. To demonstrate the advantages of our approach, we
have implemented advanced Gr\""{o}bner basis algorithms, such as Faug\`{e}re's
$F_4$ and $F_5$, in a composable way.
",['\nHiromi Ishii\n'],"16 pages, Accepted to CASC 2018","Computer Algebra in Scientific Computing, pp. 288-303. 20th
  International Workshop, CASC 2018, Lille, France, September 17-21, 2018,
  Proceedings",http://dx.doi.org/10.1007/978-3-319-99639-4_20,cs.SC,"['cs.SC', 'cs.PL']",10.1007/978-3-319-99639-4_20,,[]
"A Strongly Consistent Finite Difference Scheme for Steady Stokes Flow
  and its Modified Equations",http://arxiv.org/abs/1807.00328v2,2018-07-01T13:13:18Z,2018-09-02T19:41:12Z,"  We construct and analyze a strongly consistent second-order finite difference
scheme for the steady two-dimensional Stokes flow. The pressure Poisson
equation is explicitly incorporated into the scheme. Our approach suggested by
the first two authors is based on a combination of the finite volume method,
difference elimination, and numerical integration. We make use of the
techniques of the differential and difference Janet/Groebner bases. In order to
prove strong consistency of the generated scheme we correlate the differential
ideal generated by the polynomials in the Stokes equations with the difference
ideal generated by the polynomials in the constructed difference scheme.
Additionally, we compute the modified differential system of the obtained
scheme and analyze the scheme's accuracy and strong consistency by considering
this system. An evaluation of our scheme against the established
marker-and-cell method is carried out.
","['\nYury A. Blinkov\n', '\nVladimir P. Gerdt\n', '\nDmitry A. Lyakhov\n', '\nDominik L. Michels\n']",15 pages,,http://arxiv.org/abs/1807.00328v2,math.NA,"['math.NA', 'cs.SC', 'math.RA', '65M06, 76D07, 12H05, 12H10']",,,[]
"Fast Coefficient Computation for Algebraic Power Series in Positive
  Characteristic",http://arxiv.org/abs/1806.06543v1,2018-06-18T08:18:04Z,2018-06-18T08:18:04Z,"  We revisit Christol's theorem on algebraic power series in positive
characteristic and propose yet another proof for it. This new proof combines
several ingredients and advantages of existing proofs, which make it very
well-suited for algorithmic purposes. We apply the construction used in the new
proof to the design of a new efficient algorithm for computing the $N$th
coefficient of a given algebraic power series over a perfect field of
characteristic~$p$. It has several nice features: it is more general, more
natural and more efficient than previous algorithms. Not only the arithmetic
complexity of the new algorithm is linear in $\log N$ and quasi-linear in~$p$,
but its dependency with respect to the degree of the input is much smaller than
in the previously best algorithm. {Moreover, when the ground field is finite,
the new approach yields an even faster algorithm, whose bit complexity is
linear in $\log N$ and quasi-linear in~$\sqrt{p}$}.
","['\nAlin Bostan\nSPECFUN\n', '\nXavier Caruso\nLAGA\n', '\nGilles Christol\nIMJ\n', '\nPhilippe Dumas\nSPECFUN\n']",,Open Book Series 2 (2019) 119-135,http://dx.doi.org/10.2140/obs.2019.2.119,math.NT,"['math.NT', 'cs.SC']",10.2140/obs.2019.2.119,,"['SPECFUN', 'LAGA', 'IMJ', 'SPECFUN']"
Real root finding for equivariant semi-algebraic systems,http://arxiv.org/abs/1806.08121v1,2018-06-21T09:00:29Z,2018-06-21T09:00:29Z,"  Let $R$ be a real closed field. We consider basic semi-algebraic sets defined
by $n$-variate equations/inequalities of $s$ symmetric polynomials and an
equivariant family of polynomials, all of them of degree bounded by $2d < n$.
Such a semi-algebraic set is invariant by the action of the symmetric group. We
show that such a set is either empty or it contains a point with at most $2d-1$
distinct coordinates. Combining this geometric result with efficient algorithms
for real root finding (based on the critical point method), one can decide the
emptiness of basic semi-algebraic sets defined by $s$ polynomials of degree $d$
in time $(sn)^{O(d)}$. This improves the state-of-the-art which is exponential
in $n$. When the variables $x_1, \ldots, x_n$ are quantified and the
coefficients of the input system depend on parameters $y_1, \ldots, y_t$, one
also demonstrates that the corresponding one-block quantifier elimination
problem can be solved in time $(sn)^{O(dt)}$.
","['\nCordian Riener\nUiT\n', '\nMohab Safey El Din\nPolSys\n']",,"Proceedings of the International Symposium on Symbolic and
  Algebraic Computation, 2018, New-York, United States",http://arxiv.org/abs/1806.08121v1,cs.SC,"['cs.SC', 'math.AG']",,,"['UiT', 'PolSys']"
Segre Class Computation and Practical Applications,http://arxiv.org/abs/1806.07408v4,2018-06-19T18:11:14Z,2019-05-30T17:43:32Z,"  Let $X \subset Y$ be closed (possibly singular) subschemes of a smooth
projective toric variety $T$. We show how to compute the Segre class $s(X,Y)$
as a class in the Chow group of $T$. Building on this, we give effective
methods to compute intersection products in projective varieties, to determine
algebraic multiplicity without working in local rings, and to test pairwise
containment of subvarieties of $T$. Our methods may be implemented without
using Groebner bases; in particular any algorithm to compute the number of
solutions of a zero-dimensional polynomial system may be used.
","['\nCorey Harris\n', '\nMartin Helmer\n']",,"Mathematics of Computation, DOI:10.1090/mcom/3448, 2019",http://dx.doi.org/10.1090/mcom/3448,math.AG,"['math.AG', 'cs.SC', 'math.AC', '14Qxx, 13Pxx, 13H15, 14C17, 14C20, 68W30, 65H10']",10.1090/mcom/3448,,[]
A fast algorithm for solving linearly recurrent sequences,http://arxiv.org/abs/1806.03554v1,2018-06-09T22:40:19Z,2018-06-09T22:40:19Z,"  We present an algorithm which computes the $D^{th}$ term of a sequence
satisfying a linear recurrence relation of order $d$ over a field $K$ in $O(
\mathsf{M}(\bar d)\log(D) + \mathsf{M}(d)\log(d))$ operations in $K$, where
$\bar d \leq d$ is the degree of the squarefree part of the annihilating
polynomial of the recurrence and $\mathsf{M}$ is the cost of polynomial
multiplication in $K$. This is a refinement of the previously optimal result of
$O( \mathsf{M}(d)\log(D) )$ operations, due to Fiduccia.
","['\nSeung Gyu Hyun\n', '\nStephen Melczer\n', '\nCatherine St-Pierre\n']",ISSAC 2018 poster abstract,,http://arxiv.org/abs/1806.03554v1,cs.SC,"['cs.SC', 'math.CO']",,,[]
"Efficient Differentiable Programming in a Functional Array-Processing
  Language",http://arxiv.org/abs/1806.02136v1,2018-06-06T11:54:34Z,2018-06-06T11:54:34Z,"  We present a system for the automatic differentiation of a higher-order
functional array-processing language. The core functional language underlying
this system simultaneously supports both source-to-source automatic
differentiation and global optimizations such as loop transformations. Thanks
to this feature, we demonstrate how for some real-world machine learning and
computer vision benchmarks, the system outperforms the state-of-the-art
automatic differentiation tools.
","['\nAmir Shaikhha\n', '\nAndrew Fitzgibbon\n', '\nDimitrios Vytiniotis\n', '\nSimon Peyton Jones\n', '\nChristoph Koch\n']",,,http://arxiv.org/abs/1806.02136v1,cs.MS,"['cs.MS', 'cs.LG', 'cs.PL', 'cs.SC', 'stat.ML']",,,[]
Tritangents and Their Space Sextics,http://arxiv.org/abs/1805.11702v1,2018-05-29T20:47:50Z,2018-05-29T20:47:50Z,"  Two classical results in algebraic geometry are that the branch curve of a
del Pezzo surface of degree 1 can be embedded as a space sextic curve and that
every space sextic curve has exactly 120 tritangents corresponding to its odd
theta characteristics. In this paper we revisit both results from the
computational perspective. Specifically, we give an algorithm to construct
space sextic curves that arise from blowing up projective plane at eight points
and provide algorithms to compute the 120 tritangents and their Steiner system
of any space sextic. Furthermore, we develop efficient inverses to the
aforementioned methods. We present an algorithm to either reconstruct the
original eight points in the projective plane from a space sextic or certify
that this is not possible. Moreover, we extend a construction of Lehavi which
recovers a space sextic from its tritangents and Steiner system. All algorithms
in this paper have been implemented in magma.
","['\nTurku Ozlum Celik\n', '\nAvinash Kulkarni\n', '\nYue Ren\n', '\nMahsa Sayyary Namin\n']","24 pages, 2 figures",,http://arxiv.org/abs/1805.11702v1,math.AG,"['math.AG', 'cs.SC', '14Q05, 14H50']",,,[]
Monodromy Solver: Sequential and Parallel,http://arxiv.org/abs/1805.12212v1,2018-05-30T20:23:11Z,2018-05-30T20:23:11Z,"  We describe, study, and experiment with an algorithm for finding all
solutions of systems of polynomial equations using homotopy continuation and
monodromy. This algorithm follows a framework developed in previous work and
can operate in the presence of a large number of failures of the homotopy
continuation subroutine. We give special attention to parallelization and
probabilistic analysis of a model adapted to parallelization and failures.
Apart from theoretical results, we developed a simulator that allows us to run
a large number of experiments without recomputing the outcomes of the
continuation subroutine.
","['\nNathan Bliss\n', '\nTimothy Duff\n', '\nAnton Leykin\n', '\nJeff Sommars\n']","19 pages, accepted to ISSAC 2018",,http://arxiv.org/abs/1805.12212v1,cs.SC,"['cs.SC', 'cs.DC', 'math.AG', '14Q99, 65H99, 68W20']",,,[]
"Bilinear systems with two supports: Koszul resultant matrices,
  eigenvalues, and eigenvectors",http://arxiv.org/abs/1805.05060v1,2018-05-14T08:34:37Z,2018-05-14T08:34:37Z,"  A fundamental problem in computational algebraic geometry is the computation
of the resultant. A central question is when and how to compute it as the
determinant of a matrix. whose elements are the coefficients of the input
polynomials up-to sign. This problem is well understood for unmixed
multihomogeneous systems, that is for systems consisting of multihomogeneous
polynomials with the * 1 same support. However, little is known for mixed
systems, that is for systems consisting of polynomials with different supports.
We consider the computation of the multihomogeneous resultant of bilinear
systems involving two different supports. We present a constructive approach
that expresses the resultant as the exact determinant of a Koszul resultant
matrix, that is a matrix constructed from maps in the Koszul complex. We
exploit the resultant matrix to propose an algorithm to solve such systems. In
the process we extend the classical eigenvalues and eigenvectors criterion to a
more general setting. Our extension of the eigenvalues criterion applies to a
general class of matrices, including the Sylvester-type and the Koszul-type
ones.
","['\nMatías Bender\nPolSys\n', '\nJean-Charles Faugère\nPolSys\n', '\nAngelos Mantzaflaris\nRICAM\n', '\nElias Tsigaridas\nPolSys\n']","Proceedings of the 43th International Symposium on Symbolic and
  Algebraic Computation, 2018, Jul 2018, New York, United States. 2018",,http://dx.doi.org/10.1145/3208976.3209011,cs.SC,['cs.SC'],10.1145/3208976.3209011,,"['PolSys', 'PolSys', 'RICAM', 'PolSys']"
On Affine Tropical F5 Algorithms,http://arxiv.org/abs/1805.06183v1,2018-05-16T08:13:25Z,2018-05-16T08:13:25Z,"  Let $K$ be a field equipped with a valuation. Tropical varieties over $K$ can
be defined with a theory of Gr{\""o}bner bases taking into account the valuation
of $K$.Because of the use of the valuation, the theory of tropical Gr{\""o}bner
bases has proved to provide settings for computations over polynomial rings
over a $p$-adic field that are more stable than that of classical Gr{\""o}bner
bases.Beforehand, these strategies were only available for homogeneous
polynomials. In this article, we extend the F5 strategy to a new definition of
tropical Gr{\""o}bner bases in an affine setting.We provide numerical examples
to illustrate time-complexity and $p$-adic stability of this tropical F5
algorithm.We also illustrate its merits as a first step before an FGLM
algorithm to compute (classical) lex bases over $p$-adics.
","['\nTristan Vaccon\nXLIM-MATHIS\n', '\nThibaut Verron\n', '\nKazuhiro Yokoyama\n']",,"ISSAC '18: 2018 ACM International Symposium on Symbolic and
  Algebraic Computation, 2018",http://dx.doi.org/10.1145/3208976.3209012,cs.SC,"['cs.SC', 'math.AC']",10.1145/3208976.3209012,,['XLIM-MATHIS']
"A Simple Re-Derivation of Onsager's Solution of the 2D Ising Model using
  Experimental Mathematics",http://arxiv.org/abs/1805.09057v1,2018-05-23T11:12:20Z,2018-05-23T11:12:20Z,"  In this case study, we illustrate the great potential of experimental
mathematics and symbolic computation, by rederiving, ab initio, Onsager's
celebrated solution of the twodimensional Ising model in zero magnetic field.
Onsager's derivation is extremely complicated and ad hoc, as are all the
subsequent proofs. Unlike Onsager's, our derivation is not rigorous, yet it is
absolutely certain (even if Onsager did not do it before), and should have been
acceptable to physicists who do not share mathematicians' fanatical (and often
misplaced) insistence on rigor.
","['\nManuel Kauers\n', '\nDoron Zeilberger\n']",,,http://arxiv.org/abs/1805.09057v1,math.CO,"['math.CO', 'cs.SC']",,,[]
Enumeration of Complex Golay Pairs via Programmatic SAT,http://arxiv.org/abs/1805.05488v2,2018-05-14T22:56:52Z,2018-11-08T02:51:10Z,"  We provide a complete enumeration of all complex Golay pairs of length up to
25, verifying that complex Golay pairs do not exist in lengths 23 and 25 but do
exist in length 24. This independently verifies work done by F. Fiedler in 2013
that confirms the 2002 conjecture of Craigen, Holzmann, and Kharaghani that
complex Golay pairs of length 23 don't exist. Our enumeration method relies on
the recently proposed SAT+CAS paradigm of combining computer algebra systems
with SAT solvers to take advantage of the advances made in the fields of
symbolic computation and satisfiability checking. The enumeration proceeds in
two stages: First, we use a fine-tuned computer program and functionality from
computer algebra systems to construct a list containing all sequences which
could appear as the first sequence in a complex Golay pair (up to equivalence).
Second, we use a programmatic SAT solver to construct all sequences (if any)
that pair off with the sequences constructed in the first stage to form a
complex Golay pair.
","['\nCurtis Bright\n', '\nIlias Kotsireas\n', '\nAlbert Heinle\n', '\nVijay Ganesh\n']",Corrected typos,,http://dx.doi.org/10.1145/3208976.3209006,cs.LO,"['cs.LO', 'cs.SC', 'math.CO']",10.1145/3208976.3209006,,[]
Computing an LLL-reduced basis of the orthogonal lattice,http://arxiv.org/abs/1805.03418v1,2018-05-09T09:00:24Z,2018-05-09T09:00:24Z,"  As a typical application, the Lenstra-Lenstra-Lovasz lattice basis reduction
algorithm (LLL) is used to compute a reduced basis of the orthogonal lattice
for a given integer matrix, via reducing a special kind of lattice bases. With
such bases in input, we propose a new technique for bounding from above the
number of iterations required by the LLL algorithm. The main technical
ingredient is a variant of the classical LLL potential, which could prove
useful to understand the behavior of LLL for other families of input bases.
","['\nJingwei Chen\n', '\nDamien Stehlé\n', '\nGilles Villard\n']",ISSAC'18,,http://arxiv.org/abs/1805.03418v1,cs.SC,['cs.SC'],,,[]
"Generalized Hermite Reduction, Creative Telescoping and Definite
  Integration of D-Finite Functions",http://arxiv.org/abs/1805.03445v1,2018-05-09T10:14:50Z,2018-05-09T10:14:50Z,"  Hermite reduction is a classical algorithmic tool in symbolic integration. It
is used to decompose a given rational function as a sum of a function with
simple poles and the derivative of another rational function. We extend Hermite
reduction to arbitrary linear differential operators instead of the pure
derivative, and develop efficient algorithms for this reduction. We then apply
the generalized Hermite reduction to the computation of linear operators
satisfied by single definite integrals of D-finite functions of several
continuous or discrete parameters. The resulting algorithm is a generalization
of reduction-based methods for creative telescoping.
","['\nAlin Bostan\n', '\nFrédéric Chyzak\n', '\nPierre Lairez\n', '\nBruno Salvy\n']","Accepted for publication in the proceedings of the conference
  ISSAC'18 (Jul 16-19, 2018)","Proceedings of ISSAC 2018 (New York, NY, USA)",http://dx.doi.org/10.1145/3208976.3208992,cs.SC,"['cs.SC', 'I.1.2']",10.1145/3208976.3208992,,[]
"Towards Mixed Gr{ö}bner Basis Algorithms: the Multihomogeneous and
  Sparse Case",http://arxiv.org/abs/1805.03577v2,2018-05-09T15:07:16Z,2018-05-15T12:38:32Z,"  One of the biggest open problems in computational algebra is the design of
efficient algorithms for Gr{\""o}bner basis computations that take into account
the sparsity of the input polynomials. We can perform such computations in the
case of unmixed polynomial systems, that is systems with polynomials having the
same support, using the approach of Faug{\`e}re, Spaenlehauer, and Svartz
[ISSAC'14]. We present two algorithms for sparse Gr{\""o}bner bases computations
for mixed systems. The first one computes with mixed sparse systems and
exploits the supports of the polynomials. Under regularity assumptions, it
performs no reductions to zero. For mixed, square, and 0-dimensional
multihomogeneous polynomial systems, we present a dedicated, and potentially
more efficient, algorithm that exploits different algebraic properties that
performs no reduction to zero. We give an explicit bound for the maximal degree
appearing in the computations.
","['\nMatías Bender\nPolSys\n', '\nJean-Charles Faugère\nPolSys\n', '\nElias Tsigaridas\nPolSys\n']",,"ISSAC 2018 - 43th International Symposium on Symbolic and
  Algebraic Computation, Jul 2018, New York, United States. 2018",http://dx.doi.org/10.1145/3208976.3209018,cs.SC,['cs.SC'],10.1145/3208976.3209018,,"['PolSys', 'PolSys', 'PolSys']"
On the Annihilator Ideal of an Inverse Form. A Simplification,http://arxiv.org/abs/1805.03995v1,2018-05-09T06:14:48Z,2018-05-09T06:14:48Z,"  We simplify an earlier paper of the same title by not using syzygy
polynomials and by not using a trichotomy of inverse forms. Let $\K$ be a field
and $\M=\K[x^{-1},z^{-1}]$ denote Macaulay's $\K[x,z]$ module of inverse
polynomials; here $z$ and $z^{-1}$ are homogenising variables. An inverse form
$F\in\M$ has a homogeneous annihilator ideal, $\I_F$\,. In an earlier paper we
inductively constructed an ordered pair ($f_1$\,,\,$f_2$) of forms in $\K[x,z]$
which generate $\I_F$. We used syzygy polynomials to show that the intermediate
forms give a minimal grlex Groebner basis, which can be efficiently reduced.
  We give a significantly shorter proof that the intermediate forms are a
minimal grlex Groebner basis for $\I_F$\,. We also simplify our proof that
either $ F$ is already reduced or a monomial of $f_1$ can be reduced by
$f_2$\,. The algorithm that computes $f_1\,,f_2$ yields a variant of the
Berlekamp-Massey algorithm which does not use the last 'length change' approach
of Massey.
  These new proofs avoid the three separate cases, 'triples' and the technical
factorisation of intermediate 'essential' forms. We also show that $f_1,f_2$ is
a maximal $\R$ regular sequence for $\I_F$\,, so that $\I_F$ is a complete
intersection.
",['\nGraham H. Norton\n'],arXiv admin note: substantial text overlap with arXiv:1710.07731,,http://arxiv.org/abs/1805.03995v1,cs.SC,"['cs.SC', 'I.1']",,,[]
RealCertify: a Maple package for certifying non-negativity,http://arxiv.org/abs/1805.02201v1,2018-05-06T12:31:55Z,2018-05-06T12:31:55Z,"  Let $\mathbb{Q}$ (resp. $\mathbb{R}$) be the field of rational (resp. real)
numbers and $X = (X_1, \ldots, X_n)$ be variables. Deciding the non-negativity
of polynomials in $\mathbb{Q}[X]$ over $\mathbb{R}^n$ or over semi-algebraic
domains defined by polynomial constraints in $\mathbb{Q}[X]$ is a classical
algorithmic problem for symbolic computation.
  The Maple package \textsc{RealCertify} tackles this decision problem by
computing sum of squares certificates of non-negativity for inputs where such
certificates hold over the rational numbers. It can be applied to numerous
problems coming from engineering sciences, program verification and
cyber-physical systems. It is based on hybrid symbolic-numeric algorithms based
on semi-definite programming.
","['\nVictor Magron\n', '\nMohab Safey El Din\n']","4 pages, 2 tables",,http://arxiv.org/abs/1805.02201v1,cs.SC,"['cs.SC', 'cs.MS']",,,[]
Computing basepoints of linear series in the plane,http://arxiv.org/abs/1805.03452v1,2018-05-09T10:47:54Z,2018-05-09T10:47:54Z,"  We present an algorithm for detecting basepoints of linear series of curves
in the plane. Moreover, we give an algorithm for constructing a linear series
of curves in the plane for given basepoints. The underlying method of these
algorithms is the classical procedure of blowing up points in the plane. We
motivate the algorithmic version of this procedure with several applications.
",['\nNiels Lubbes\n'],,,http://arxiv.org/abs/1805.03452v1,math.AG,"['math.AG', 'cs.SC', '14C20, 14Q10, 68W30']",,,[]
Computing curves on real rational surfaces,http://arxiv.org/abs/1805.03860v1,2018-05-10T07:31:02Z,2018-05-10T07:31:02Z,"  We present an algorithm for computing curves and families of curves of
prescribed degree and geometric genus on real rational surfaces.
",['\nNiels Lubbes\n'],,,http://arxiv.org/abs/1805.03860v1,math.AG,"['math.AG', 'cs.SC', '14Q10, 14D99, 68W30']",,,[]
"Implementing a Method for Stochastization of One-Step Processes in a
  Computer Algebra System",http://arxiv.org/abs/1805.03190v1,2018-05-08T17:48:59Z,2018-05-08T17:48:59Z,"  When modeling such phenomena as population dynamics, controllable ows, etc.,
a problem arises of adapting the existing models to a phenomenon under study.
For this purpose, we propose to derive new models from the rst principles by
stochastization of one-step processes. Research can be represented as an
iterative process that consists in obtaining a model and its further re nement.
The number of such iterations can be extremely large. This work is aimed at
software implementation (by means of computer algebra) of a method for
stochastization of one-step processes. As a basis of the software
implementation, we use the SymPy computer algebra system. Based on a developed
algorithm, we derive stochastic di erential equations and their interaction
schemes. The operation of the program is demonstrated on the Verhulst and
Lotka-Volterra models.
","['\nD. S. Kulyabov\n', '\nM. N. Gevorkyan\n', '\nA. V. Demidova\n', '\nT. R. Velieva\n', '\nA. V. Korolkova\n', '\nL. A. Sevastianov\n']",in English; in Russian,,http://dx.doi.org/10.1134/S0361768818020044,cs.SC,"['cs.SC', 'math-ph', 'math.MP']",10.1134/S0361768818020044,,[]
"Summer Research Report: Towards Incremental Lazard Cylindrical Algebraic
  Decomposition",http://arxiv.org/abs/1804.08564v1,2018-04-23T16:59:00Z,2018-04-23T16:59:00Z,"  Cylindrical Algebraic Decomposition (CAD) is an important tool within
computational real algebraic geometry, capable of solving many problems to do
with polynomial systems over the reals, but known to have worst-case
computational complexity doubly exponential in the number of variables. It has
long been studied by the Symbolic Computation community and is implemented in a
variety of computer algebra systems, however, it has also found recent interest
in the Satisfiability Checking community for use with SMT-solvers. The SCSC
Project seeks to build bridges between these communities.
  The present report describes progress made during a Research Internship in
Summer 2017 funded by the EU H2020 SCSC CSA. We describe a proof of concept
implementation of an Incremental CAD algorithm in Maple, where CADs are built
and refined incrementally by polynomial constraint, in contrast to the usual
approach of a single computation from a single input. This advance would make
CAD of use to SMT-solvers who search for solutions by constantly reformulating
logical formula and querying solvers like CAD for whether a logical solution is
admissible. We describe experiments for the proof of concept, which clearly
display the computational advantages when compared to iterated re-computation.
In addition, the project implemented this work under the recently verified
Lazard projection scheme (with corresponding Lazard evaluation). That is the
minimal complete CAD method in theory, and this is the first documented
implementation.
","['\nAlexander I. Cowen-Rivers\n', '\nMatthew England\n']","33 pages, 6 tables, 12 figures, 9 algorithms",,http://arxiv.org/abs/1804.08564v1,cs.SC,['cs.SC'],,,[]
Quantifier Elimination for Reasoning in Economics,http://arxiv.org/abs/1804.10037v2,2018-04-26T13:05:16Z,2018-05-15T10:30:31Z,"  We consider the use of Quantifier Elimination (QE) technology for automated
reasoning in economics. QE dates back to Tarski's work in the 1940s with
software to perform it dating to the 1970s. There is a great body of work
considering its application in science and engineering but we show here how it
can also find application in the social sciences. We explain how many suggested
theorems in economics could either be proven, or even have their hypotheses
shown to be inconsistent, automatically; and describe the application of this
in both economics education and research. We describe a bank of QE examples
gathered from economics literature and note the structure of these are, on
average, quite different to those occurring in the computer algebra literature.
This leads us to suggest a new incremental QE approach based on result
memorization of commonly occurring generic QE results.
","['\nCasey B. Mulligan\n', '\nRussell Bradford\n', '\nJames H. Davenport\n', '\nMatthew England\n', '\nZak Tonks\n']",,,http://arxiv.org/abs/1804.10037v2,cs.SC,"['cs.SC', '68W30, 03C10, 91-04', 'I.1.2; J.4']",,,[]
"Positive Solutions of Systems of Signed Parametric Polynomial
  Inequalities",http://arxiv.org/abs/1804.09705v1,2018-04-25T17:53:25Z,2018-04-25T17:53:25Z,"  We consider systems of strict multivariate polynomial inequalities over the
reals. All polynomial coefficients are parameters ranging over the reals, where
for each coefficient we prescribe its sign. We are interested in the existence
of positive real solutions of our system for all choices of coefficients
subject to our sign conditions. We give a decision procedure for the existence
of such solutions. In the positive case our procedure yields a parametric
positive solution as a rational function in the coefficients. Our framework
allows to reformulate heuristic subtropical approaches for non-parametric
systems of polynomial inequalities that have been recently used in qualitative
biological network analysis and, independently, in satisfiability modulo theory
solving. We apply our results to characterize the incompleteness of those
methods.
","['\nHoon Hong\n', '\nThomas Sturm\n']",,"Proc. CASC 2018, LNCS 11077, pp.238-253, Springer 2018",http://dx.doi.org/10.1007/978-3-319-99639-4_17,cs.SC,"['cs.SC', 'cs.LO']",10.1007/978-3-319-99639-4_17,,[]
Using Machine Learning to Improve Cylindrical Algebraic Decomposition,http://arxiv.org/abs/1804.10520v1,2018-04-26T12:56:51Z,2018-04-26T12:56:51Z,"  Cylindrical Algebraic Decomposition (CAD) is a key tool in computational
algebraic geometry, best known as a procedure to enable Quantifier Elimination
over real-closed fields. However, it has a worst case complexity doubly
exponential in the size of the input, which is often encountered in practice.
It has been observed that for many problems a change in algorithm settings or
problem formulation can cause huge differences in runtime costs, changing
problem instances from intractable to easy. A number of heuristics have been
developed to help with such choices, but the complicated nature of the
geometric relationships involved means these are imperfect and can sometimes
make poor choices. We investigate the use of machine learning (specifically
support vector machines) to make such choices instead.
  Machine learning is the process of fitting a computer model to a complex
function based on properties learned from measured data. In this paper we apply
it in two case studies: the first to select between heuristics for choosing a
CAD variable ordering; the second to identify when a CAD problem instance would
benefit from Groebner Basis preconditioning. These appear to be the first such
applications of machine learning to Symbolic Computation. We demonstrate in
both cases that the machine learned choice outperforms human developed
heuristics.
","['\nZongyan Huang\n', '\nMatthew England\n', '\nDavid Wilson\n', '\nJames H. Davenport\n', '\nLawrence C. Paulson\n']","arXiv admin note: text overlap with arXiv:1608.04219, arXiv:1404.6369","Mathematics in Computer Science, 13:4, pp. 461 - 488, Springer,
  2019",http://dx.doi.org/10.1007/s11786-019-00394-8,cs.SC,"['cs.SC', 'cs.LG', '68W30, 68T05, 03C10', 'I.2.6; I.1.0']",10.1007/s11786-019-00394-8,,[]
"Gröbner Bases of Modules and Faugère's $F_4$ Algorithm in
  Isabelle/HOL",http://arxiv.org/abs/1805.00304v1,2018-05-01T13:06:56Z,2018-05-01T13:06:56Z,"  We present an elegant, generic and extensive formalization of Gr\""obner bases
in Isabelle/HOL. The formalization covers all of the essentials of the theory
(polynomial reduction, S-polynomials, Buchberger's algorithm, Buchberger's
criteria for avoiding useless pairs), but also includes more advanced features
like reduced Gr\""obner bases. Particular highlights are the first-time
formalization of Faug\`ere's matrix-based $F_4$ algorithm and the fact that the
entire theory is formulated for modules and submodules rather than rings and
ideals. All formalized algorithms can be translated into executable code
operating on concrete data structures, enabling the certified computation of
(reduced) Gr\""obner bases and syzygy modules.
","['\nAlexander Maletzky\n', '\nFabian Immler\n']",extended version of paper submitted to CICM2018,,http://arxiv.org/abs/1805.00304v1,cs.LO,"['cs.LO', 'cs.SC']",,,[]
Diagonal asymptotics for symmetric rational functions via ACSV,http://arxiv.org/abs/1804.10929v1,2018-04-29T13:43:51Z,2018-04-29T13:43:51Z,"  We consider asymptotics of power series coefficients of rational functions of
the form $1/Q$ where $Q$ is a symmetric multilinear polynomial. We review a
number of such cases from the literature, chiefly concerned either with
positivity of coefficients or diagonal asymptotics. We then analyze coefficient
asymptotics using ACSV (Analytic Combinatorics in Several Variables) methods.
While ACSV sometimes requires considerable overhead and geometric computation,
in the case of symmetric multilinear rational functions there are some
reductions that streamline the analysis. Our results include diagonal
asymptotics across entire classes of functions, for example the general
3-variable case and the Gillis-Reznick-Zeilberger (GRZ) case, where the
denominator in terms of elementary symmetric functions is $1 - e_1 + c e_d$ in
any number $d$ of variables. The ACSV analysis also explains a discontinuous
drop in exponential growth rate for the GRZ class at the parameter value $c =
(d-1)^{d-1}$, previously observed for $d=4$ only by separately computing
diagonal recurrences for critical and noncritical values of $c$.
","['\nYuliy Baryshnikov\n', '\nStephen Melczer\n', '\nRobin Pemantle\n', '\nArmin Straub\n']",To appear in LIPIcs Proceedings of Analysis of Algorithms 2018,,http://arxiv.org/abs/1804.10929v1,math.CO,"['math.CO', 'cs.SC', 'math.NT']",,,[]
Freeness and invariants of rational plane curves,http://arxiv.org/abs/1804.06194v2,2018-04-17T12:23:42Z,2020-01-22T15:13:18Z,"  Given a parameterization $\phi$ of a rational plane curve C, we study some
invariants of C via $\phi$. We first focus on the characterization of rational
cuspidal curves, in particular we establish a relation between the discriminant
of the pull-back of a line via $\phi$, the dual curve of C and its singular
points. Then, by analyzing the pull-backs of the global differential forms via
$\phi$, we prove that the (nearly) freeness of a rational curve can be tested
by inspecting the Hilbert function of the kernel of a canonical map. As a by
product, we also show that the global Tjurina number of a rational curve can be
computed directly from one of its parameterization, without relying on the
computation of an equation of C.
","['\nLaurent Busé\nAROMATH\n', '\nAlexandru Dimca\nJAD, AROMATH\n', '\nGabriel Sticlaru\n']","Mathematics of Computation, American Mathematical Society, In press",,http://dx.doi.org/10.1090/mcom/3495,math.AG,"['math.AG', 'cs.SC', 'math.AC']",10.1090/mcom/3495,,"['AROMATH', 'JAD, AROMATH']"
A Blackbox Polynomial System Solver on Parallel Shared Memory Computers,http://arxiv.org/abs/1804.03807v2,2018-04-11T04:41:49Z,2018-06-17T19:57:36Z,"  A numerical irreducible decomposition for a polynomial system provides
representations for the irreducible factors of all positive dimensional
solution sets of the system, separated from its isolated solutions. Homotopy
continuation methods are applied to compute a numerical irreducible
decomposition. Load balancing and pipelining are techniques in a parallel
implementation on a computer with multicore processors. The application of the
parallel algorithms is illustrated on solving the cyclic $n$-roots problems, in
particular for $n = 8, 9$, and~12.
",['\nJan Verschelde\n'],Accepted for publication in the proceedings of CASC 2018,,http://arxiv.org/abs/1804.03807v2,cs.MS,"['cs.MS', 'cs.DC', 'cs.SC', 'math.AG', 'math.NA']",,,[]
"Definite Sums as Solutions of Linear Recurrences With Polynomial
  Coefficients",http://arxiv.org/abs/1804.02964v1,2018-04-09T13:17:15Z,2018-04-09T13:17:15Z,"  We present an algorithm which, given a linear recurrence operator $L$ with
polynomial coefficients, $m \in \mathbb{N}\setminus\{0\}$, $a_1,a_2,\ldots,a_m
\in \mathbb{N}\setminus\{0\}$ and $b_1,b_2,\ldots,b_m \in \mathbb{K}$, returns
a linear recurrence operator $L'$ with rational coefficients such that for
every sequence $h$, \[ L\left(\sum_{k=0}^\infty \prod_{i=1}^m \binom{a_i n +
b_i}{k} h_k\right) = 0 \] if and only if $L' h = 0$.
",['\nMarko Petkovšek\n'],,,http://arxiv.org/abs/1804.02964v1,cs.SC,"['cs.SC', '68W30, 33F10']",,,[]
"Normal and Triangular Determinantal Representations of Multivariate
  Polynomials",http://arxiv.org/abs/1804.00145v1,2018-03-31T09:53:15Z,2018-03-31T09:53:15Z,"  In this paper we give a new and simple algorithm to put any multivariate
polynomial into a normal determinant form in which each entry has the form ,
and in each column the same variable appears. We also apply the algorithm to
obtain a triangular determinant representation, a reduced determinant
representation, and a uniform determinant representation of any multivariable
polynomial. The algorithm could be useful for obtaining representations of
dimensions smaller than those available up to now to solve numerical problems.
",['\nMassimo Salvi\n'],26 pages,"JP Journal of Algebra, Number Theory and Applications Volume 40,
  Issue 6, 2018",http://dx.doi.org/10.17654/NT040061043,math.NA,"['math.NA', 'cs.SC']",10.17654/NT040061043,,[]
"Simulation-Based Reachability Analysis for High-Index Large Linear
  Differential Algebraic Equations",http://arxiv.org/abs/1804.03227v1,2018-04-09T20:34:03Z,2018-04-09T20:34:03Z,"  Reachability analysis is a fundamental problem for safety verification and
falsification of Cyber-Physical Systems (CPS) whose dynamics follow physical
laws usually represented as differential equations. In the last two decades,
numerous reachability analysis methods and tools have been proposed for a
common class of dynamics in CPS known as ordinary differential equations (ODE).
However, there is lack of methods dealing with differential algebraic equations
(DAE) which is a more general class of dynamics that is widely used to describe
a variety of problems from engineering and science such as multibody mechanics,
electrical cicuit design, incompressible fluids, molecular dynamics and
chemcial process control. Reachability analysis for DAE systems is more complex
than ODE systems, especially for high-index DAEs because they contain both a
differential part (i.e., ODE) and algebraic constraints (AC). In this paper, we
extend the recent scalable simulation-based reachability analysis in
combination with decoupling techniques for a class of high-index large linear
DAEs. In particular, a high-index linear DAE is first decoupled into one ODE
and one or several AC subsystems based on the well-known Marz decoupling method
ultilizing admissible projectors. Then, the discrete reachable set of the DAE,
represented as a list of star-sets, is computed using simulation. Unlike ODE
reachability analysis where the initial condition is freely defined by a user,
in DAE cases, the consistency of the inititial condition is an essential
requirement to guarantee a feasible solution. Therefore, a thorough check for
the consistency is invoked before computing the discrete reachable set. Our
approach sucessfully verifies (or falsifies) a wide range of practical,
high-index linear DAE systems in which the number of state variables varies
from several to thousands.
","['\nHoang-Dung Tran\n', '\nWeiming Xiang\n', '\nNathaniel Hamilton\n', '\nTaylor T. Johnson\n']",,,http://arxiv.org/abs/1804.03227v1,cs.SC,"['cs.SC', 'cs.SY']",,,[]
"Applying Computer Algebra Systems with SAT Solvers to the Williamson
  Conjecture",http://arxiv.org/abs/1804.01172v2,2018-04-03T21:12:29Z,2019-05-14T03:51:28Z,"  We employ tools from the fields of symbolic computation and satisfiability
checking---namely, computer algebra systems and SAT solvers---to study the
Williamson conjecture from combinatorial design theory and increase the bounds
to which Williamson matrices have been enumerated. In particular, we completely
enumerate all Williamson matrices of even order up to and including 70 which
gives us deeper insight into the behaviour and distribution of Williamson
matrices. We find that, in contrast to the case when the order is odd,
Williamson matrices of even order are quite plentiful and exist in every even
order up to and including 70. As a consequence of this and a new construction
for 8-Williamson matrices we construct 8-Williamson matrices in all odd orders
up to and including 35. We additionally enumerate all Williamson matrices whose
orders are divisible by 3 and less than 70, finding one previously unknown set
of Williamson matrices of order 63.
","['\nCurtis Bright\n', '\nIlias Kotsireas\n', '\nVijay Ganesh\n']",To appear in the Journal of Symbolic Computation,,http://dx.doi.org/10.1016/j.jsc.2019.07.024,cs.LO,"['cs.LO', 'cs.SC', 'math.CO']",10.1016/j.jsc.2019.07.024,,[]
The Geometry of SDP-Exactness in Quadratic Optimization,http://arxiv.org/abs/1804.01796v2,2018-04-05T12:02:53Z,2019-01-05T13:52:08Z,"  Consider the problem of minimizing a quadratic objective subject to quadratic
equations. We study the semialgebraic region of objective functions for which
this problem is solved by its semidefinite relaxation. For the Euclidean
distance problem, this is a bundle of spectrahedral shadows surrounding the
given variety. We characterize the algebraic boundary of this region and we
derive a formula for its degree.
","['\nDiego Cifuentes\n', '\nCorey Harris\n', '\nBernd Sturmfels\n']","26 pages, 9 figures",,http://arxiv.org/abs/1804.01796v2,math.OC,"['math.OC', 'cs.SC', 'math.AG', '90C22 (Primary) 14C17, 14P10, 68W30 (Secondary)']",,,[]
Convolutions of Liouvillian Sequences,http://arxiv.org/abs/1803.08747v1,2018-03-23T11:32:21Z,2018-03-23T11:32:21Z,"  While Liouvillian sequences are closed under many operations, simple examples
show that they are not closed under convolution, and the same goes for
d'Alembertian sequences. Nevertheless, we show that d'Alembertian sequences are
closed under convolution with rationally d'Alembertian sequences, and that
Liouvillian sequences are closed under convolution with rationally Liouvillian
sequences.
","['\nSergei A. Abramov\n', '\nMarko Petkovšek\n', '\nHelena Zakrajšek\n']",,,http://arxiv.org/abs/1803.08747v1,cs.SC,"['cs.SC', '68W30, 33F10']",,,[]
"On Existence and Uniqueness of Formal Power Series Solutions of
  Algebraic Ordinary Differential Equations",http://arxiv.org/abs/1803.09646v3,2018-03-26T15:11:10Z,2021-07-02T13:53:26Z,"  Given an algebraic ordinary differential equation (AODE), we propose a
computational method to determine when a truncated power series can be extended
to a formal power series solution. If a certain regularity condition on the
given AODE or on the initial values is fulfilled, we compute all of the
solutions. Moreover, when the existence is confirmed, we present the algebraic
structure of the set of all formal power series solutions.
","['\nSebastian Falkensteiner\n', '\nYi Zhang\n', '\nThieu N. Vo\n']",,,http://arxiv.org/abs/1803.09646v3,cs.SC,['cs.SC'],,,[]
Computing Periods of Hypersurfaces,http://arxiv.org/abs/1803.08068v2,2018-03-21T18:04:18Z,2019-04-23T13:03:52Z,"  We give an algorithm to compute the periods of smooth projective
hypersurfaces of any dimension. This is an improvement over existing algorithms
which could only compute the periods of plane curves. Our algorithm reduces the
evaluation of period integrals to an initial value problem for ordinary
differential equations of Picard-Fuchs type. In this way, the periods can be
computed to extreme-precision in order to study their arithmetic properties.
The initial conditions are obtained by an exact determination of the cohomology
pairing on Fermat hypersurfaces with respect to a natural basis.
",['\nEmre Can Sertöz\n'],"33 pages; Final version. Fixed typos, minor expository changes.
  Changed code repository link",,http://dx.doi.org/10.1090/mcom/3430,math.AG,"['math.AG', 'cs.SC', '32G20, 14C30, 14D07, 14K20, 68W30']",10.1090/mcom/3430,,[]
"Truncated Normal Forms for Solving Polynomial Systems: Generalized and
  Efficient Algorithms",http://arxiv.org/abs/1803.07974v3,2018-03-21T15:48:12Z,2018-12-07T12:10:59Z,"  We consider the problem of finding the isolated common roots of a set of
polynomial functions defining a zero-dimensional ideal I in a ring R of
polynomials over C. Normal form algorithms provide an algebraic approach to
solve this problem. The framework presented in Telen et al. (2018) uses
truncated normal forms (TNFs) to compute the algebra structure of R/I and the
solutions of I. This framework allows for the use of much more general bases
than the standard monomials for R/I. This is exploited in this paper to
introduce the use of two special (nonmonomial) types of basis functions with
nice properties. This allows, for instance, to adapt the basis functions to the
expected location of the roots of I. We also propose algorithms for efficient
computation of TNFs and a generalization of the construction of TNFs in the
case of non-generic zero-dimensional systems. The potential of the TNF method
and usefulness of the new results are exposed by many experiments.
","['\nBernard Mourrain\nAROMATH\n', '\nSimon Telen\n', '\nMarc Van Barel\n']",,,http://arxiv.org/abs/1803.07974v3,math.AG,"['math.AG', 'cs.SC', 'math.NA']",,,['AROMATH']
Groebner bases of reaction networks with intermediate species,http://arxiv.org/abs/1804.01381v1,2018-03-21T12:49:04Z,2018-03-21T12:49:04Z,"  In this work we consider the computation of Groebner bases of the steady
state ideal of reaction networks equipped with mass-action kinetics.
Specifically, we focus on the role of intermediate species and the relation
between the extended network (with intermediate species) and the core network
(without intermediate species).
  We show that a Groebner basis of the steady state ideal of the core network
always lifts to a Groebner basis of the steady state ideal of the extended
network by means of linear algebra, with a suitable choice of monomial order.
As illustrated with examples, this contributes to a substantial reduction of
the computation time, due mainly to the reduction in the number of variables
and polynomials. We further show that if the steady state ideal of the core
network is binomial, then so is the case for the extended network, as long as
an extra condition is fulfilled. For standard networks, this extra condition
can be visually explored from the network structure alone.
","['\nAmirHosein Sadeghimanesh\n', '\nElisenda Feliu\n']",,,http://arxiv.org/abs/1804.01381v1,cs.SC,"['cs.SC', 'math.AG', 'q-bio.MN']",,,[]
Probabilistic Analysis of Block Wiedemann for Leading Invariant Factors,http://arxiv.org/abs/1803.03864v1,2018-03-10T21:48:12Z,2018-03-10T21:48:12Z,"  We determine the probability, structure dependent, that the block Wiedemann
algorithm correctly computes leading invariant factors. This leads to a tight
lower bound for the probability, structure independent. We show, using block
size slightly larger than $r$, that the leading $r$ invariant factors are
computed correctly with high probability over any field. Moreover, an algorithm
is provided to compute the probability bound for a given matrix size and thus
to select the block size needed to obtain the desired probability. The worst
case probability bound is improved, post hoc, by incorporating the partial
information about the invariant factors.
","['\nGavin Harrison\n', '\nJeremy Johnson\n', '\nB. David Saunders\n']",17 pages,,http://arxiv.org/abs/1803.03864v1,cs.SC,"['cs.SC', 'F.2.1; I.1.2']",,,[]
"Solving First Order Autonomous Algebraic Ordinary Differential Equations
  by Places",http://arxiv.org/abs/1803.04731v2,2018-03-13T11:13:02Z,2018-11-14T10:35:47Z,"  Given a first-order autonomous algebraic ordinary differential equation, we
present a method for computing formal power series solutions by means of
places. We provide an algorithm for computing a full characterization of
possible initial values, classified in terms of the number of distinct formal
power series solutions extending them. In addition, if a particular initial
value is given, we present a second algorithm that computes all the formal
power series solutions, up to a suitable degree, corresponding to it.
Furthermore, when the ground field is the field of the complex numbers, we
prove that the computed formal power series solutions are all convergent in
suitable neighborhoods.
","['\nSebastian Falkensteiner\n', '\nJ. Rafael Sendra\n']",,,http://arxiv.org/abs/1803.04731v2,cs.SC,['cs.SC'],,,[]
Regular cylindrical algebraic decomposition,http://arxiv.org/abs/1803.04029v1,2018-03-11T19:48:45Z,2018-03-11T19:48:45Z,"  We show that a strong well-based cylindrical algebraic decomposition P of a
bounded semi-algebraic set is a regular cell decomposition, in any dimension
and independently of the method by which P is constructed. Being well-based is
a global condition on P that holds for the output of many widely used
algorithms. We also show the same for S of dimension at most 3 and P a strong
cylindrical algebraic decomposition that is locally boundary simply connected:
this is a purely local extra condition.
","['\nJ. H. Davenport\n', '\nA. F. Locatelli\n', '\nG. K. Sankaran\n']",,,http://dx.doi.org/10.1112/jlms.12257,math.AG,"['math.AG', 'cs.SC', 'math.AT', '14P10, 57N99, 68W30']",10.1112/jlms.12257,,[]
On Probabilistic Term Rewriting,http://arxiv.org/abs/1802.09774v1,2018-02-27T08:47:46Z,2018-02-27T08:47:46Z,"  We study the termination problem for probabilistic term rewrite systems. We
prove that the interpretation method is sound and complete for a strengthening
of positive almost sure termination, when abstract reduction systems and term
rewrite systems are considered. Two instances of the interpretation method -
polynomial and matrix interpretations - are analyzed and shown to capture
interesting and nontrivial examples when automated. We capture probabilistic
computation in a novel way by way of multidistribution reduction sequences,
this way accounting for both the nondeterminism in the choice of the redex and
the probabilism intrinsic in firing each rule.
","['\nMartin Avanzini\n', '\nUgo Dal Lago\n', '\nAkihisa Yamada\n']",Technical Report of our FLOPS'18 paper,,http://arxiv.org/abs/1802.09774v1,cs.SC,['cs.SC'],,,[]
On Exact Polya and Putinar's Representations,http://arxiv.org/abs/1802.10339v1,2018-02-28T10:17:04Z,2018-02-28T10:17:04Z,"  We consider the problem of finding exact sums of squares (SOS) decompositions
for certain classes of non-negative multivariate polynomials, relying on
semidefinite programming (SDP) solvers.
  We start by providing a hybrid numeric-symbolic algorithm computing exact
rational SOS decompositions for polynomials lying in the interior of the SOS
cone. It computes an approximate SOS decomposition for a perturbation of the
input polynomial with an arbitrary-precision SDP solver. An exact SOS
decomposition is obtained thanks to the perturbation terms. We prove that bit
complexity estimates on output size and runtime are both polynomial in the
degree of the input polynomial and simply exponential in the number of
variables. Next, we apply this algorithm to compute exact Polya and Putinar's
representations respectively for positive definite forms and positive
polynomials over basic compact semi-algebraic sets. We also compare the
implementation of our algorithms with existing methods in computer algebra
including cylindrical algebraic decomposition and critical point method.
","['\nVictor Magron\n', '\nMohab Safey El Din\n']","19 pages, 4 algorithms, 3 tables",,http://arxiv.org/abs/1802.10339v1,cs.SC,['cs.SC'],,,[]
Solving determinantal systems using homotopy techniques,http://arxiv.org/abs/1802.10409v1,2018-02-28T13:49:28Z,2018-02-28T13:49:28Z,"  Let $\K$ be a field of characteristic zero and $\Kbar$ be an algebraic
closure of $\K$. Consider a sequence of polynomials$G=(g\_1,\dots,g\_s)$ in
$\K[X\_1,\dots,X\_n]$, a polynomial matrix $\F=[f\_{i,j}] \in
\K[X\_1,\dots,X\_n]^{p \times q}$, with $p \leq q$,and the algebraic set
$V\_p(F, G)$ of points in $\KKbar$ at which all polynomials in $\G$ and all
$p$-minors of $\F$vanish. Such polynomial systems appear naturally in e.g.
polynomial optimization, computational geometry.We provide bounds on the number
of isolated points in $V\_p(F, G)$ depending on the maxima of the degrees in
rows (resp. columns) of $\F$. Next, we design homotopy algorithms for computing
those points. These algorithms take advantage of the determinantal structure of
the system defining $V\_p(F, G)$. In particular, the algorithms run in time
that is polynomial in the bound on the number of isolated points.
","['\nJonathan D. Hauenstein\nPolSys\n', '\nMohab Safey El Din\nPolSys\n', '\nÉric Schost\nCS\n', '\nThi Xuan Vu\nPolSys, CS\n']",,,http://arxiv.org/abs/1802.10409v1,cs.SC,['cs.SC'],,,"['PolSys', 'PolSys', 'CS', 'PolSys, CS']"
How to generate all possible rational Wilf-Zeilberger pairs?,http://arxiv.org/abs/1802.09798v4,2018-02-27T09:56:00Z,2018-06-12T04:32:58Z,"  A Wilf--Zeilberger pair $(F, G)$ in the discrete case satisfies the equation
$ F(n+1, k) - F(n, k) = G(n, k+1) - G(n, k)$. We present a structural
description of all possible rational Wilf--Zeilberger pairs and their
continuous and mixed analogues.
",['\nShaoshi Chen\n'],"17 pages, add the notion of pseudo residues in the differential case,
  and some related papers in the reference, ACMES special volume in the Fields
  Institute Communications series, 2018",,http://arxiv.org/abs/1802.09798v4,math.CO,"['math.CO', 'cs.SC', '33F10']",,,[]
OpenMath and SMT-LIB,http://arxiv.org/abs/1803.01592v1,2018-03-05T10:33:50Z,2018-03-05T10:33:50Z,"  OpenMath and SMT-LIB are languages with very different origins, but both
""represent mathematics"". We describe SMT-LIB for the OpenMath community and
consider adaptations for both languages to support the growing SC-Square
initiative.
","['\nJames H. Davenport\n', '\nMatthew England\n', '\nRoberto Sebastiani\n', '\nPatrick Trentin\n']","Presented in the OpenMath 2017 Workshop, at CICM 2017, Edinburgh, UK",,http://arxiv.org/abs/1803.01592v1,cs.SC,"['cs.SC', 'cs.MS', 'H.3.5']",,,[]
"Formal Analysis of Galois Field Arithmetics - Parallel Verification and
  Reverse Engineering",http://arxiv.org/abs/1802.06870v1,2018-02-16T08:44:57Z,2018-02-16T08:44:57Z,"  Galois field (GF) arithmetic circuits find numerous applications in
communications, signal processing, and security engineering. Formal
verification techniques of GF circuits are scarce and limited to circuits with
known bit positions of the primary inputs and outputs. They also require
knowledge of the irreducible polynomial $P(x)$, which affects final hardware
implementation. This paper presents a computer algebra technique that performs
verification and reverse engineering of GF($2^m$) multipliers directly from the
gate-level implementation. The approach is based on extracting a unique
irreducible polynomial in a parallel fashion and proceeds in three steps: 1)
determine the bit position of the output bits; 2) determine the bit position of
the input bits; and 3) extract the irreducible polynomial used in the design.
We demonstrate that this method is able to reverse engineer GF($2^m$)
multipliers in \textit{m} threads. Experiments performed on synthesized
\textit{Mastrovito} and \textit{Montgomery} multipliers with different $P(x)$,
including NIST-recommended polynomials, demonstrate high efficiency of the
proposed method.
","['\nCunxi Yu\n', '\nMaciej Ciesielski\n']","To appear in IEEE Transactions on Computer-Aided Design of Integrated
  Circuits and Systems. (TCAD'18); extended version. arXiv admin note: text
  overlap with arXiv:1611.05101",,http://arxiv.org/abs/1802.06870v1,cs.SC,"['cs.SC', 'cs.CR']",,,[]
Faster integer multiplication using short lattice vectors,http://arxiv.org/abs/1802.07932v1,2018-02-22T08:01:59Z,2018-02-22T08:01:59Z,"  We prove that $n$-bit integers may be multiplied in $O(n \log n \, 4^{\log^*
n})$ bit operations. This complexity bound had been achieved previously by
several authors, assuming various unproved number-theoretic hypotheses. Our
proof is unconditional, and depends in an essential way on Minkowski's theorem
concerning lattice vectors in symmetric convex sets.
","['\nDavid Harvey\n', '\nJoris van der Hoeven\n']",16 pages,Open Book Series 2 (2019) 293-310,http://dx.doi.org/10.2140/obs.2019.2.293,cs.SC,"['cs.SC', 'cs.DS', 'math.NT', '68W30, 68Q17, 68W40', 'G.1.0; F.2.1']",10.2140/obs.2019.2.293,,[]
ZpL: a p-adic precision package,http://arxiv.org/abs/1802.08532v1,2018-02-23T13:55:12Z,2018-02-23T13:55:12Z,"  We present a new package ZpL for the mathematical software system SM. It
implements a sharp tracking of precision on p-adic numbers, following the
theory of ultrametric precision introduced in [4]. The underlying algorithms
are mostly based on automatic dierentiation techniques. We introduce them,
study their complexity and discuss our design choices. We illustrate the
bene-ts of our package (in comparison with previous implementations) with a
large sample of examples coming from linear algebra, com-mutative algebra and
dierential equations.
","['\nXavier Caruso\nLAGA\n', '\nDavid Roe\nMIT\n', '\nTristan Vaccon\nXLIM-MATHIS\n']",,,http://arxiv.org/abs/1802.08532v1,math.NT,"['math.NT', 'cs.NA', 'cs.SC']",,,"['LAGA', 'MIT', 'XLIM-MATHIS']"
"Desingularization of First Order Linear Difference Systems with Rational
  Function Coefficients",http://arxiv.org/abs/1802.01150v1,2018-02-04T15:47:16Z,2018-02-04T15:47:16Z,"  It is well known that for a first order system of linear difference equations
with rational function coefficients, a solution that is holomorphic in some
left half plane can be analytically continued to a meromorphic solution in the
whole complex plane. The poles stem from the singularities of the rational
function coefficients of the system. Just as for differential equations, not
all of these singularities necessarily lead to poles in solutions, as they
might be what is called removable. In our work, we show how to detect and
remove these singularities and further study the connection between poles of
solutions and removable singularities. We describe two algorithms to
(partially) desingularize a given difference system and present a
characterization of removable singularities in terms of shifts of the original
system.
","['\nMoulay A. Barkatou\n', '\nMaximilian Jaroschek\n']",,,http://arxiv.org/abs/1802.01150v1,cs.SC,['cs.SC'],,,[]
"A Signature-based Algorithm for computing Computing Gröbner Bases over
  Principal Ideal Domains",http://arxiv.org/abs/1802.01388v3,2018-02-05T13:52:18Z,2019-05-27T12:32:39Z,"  Signature-based algorithms have become a standard approach for Gr\""obner
basis computations for polynomial systems over fields, but how to extend these
techniques to coefficients in general rings is not yet as well understood.
  In this paper, we present a proof-of-concept signature-based algorithm for
computing Gr\""obner bases over commutative integral domains. It is adapted from
a general version of M\""oller's algorithm (1988) which considers reductions by
multiple polynomials at each step. This algorithm performs reductions with
non-decreasing signatures, and in particular, signature drops do not occur.
When the coefficients are from a principal ideal domain (e.g. the ring of
integers or the ring of univariate polynomials over a field), we prove
correctness and termination of the algorithm, and we show how to use signature
properties to implement classic signature-based criteria to eliminate some
redundant reductions. In particular, if the input is a regular sequence, the
algorithm operates without any reduction to 0.
  We have written a toy implementation of the algorithm in Magma. Early
experimental results suggest that the algorithm might even be correct and
terminate in a more general setting, for polynomials over a unique
factorization domain (e.g. the ring of multivariate polynomials over a field or
a PID).
","['\nMaria Francis\n', '\nThibaut Verron\n']","17 pages, accepted for the special issue of Mathematics for Computer
  Science for the conference ACA 2018",,http://arxiv.org/abs/1802.01388v3,cs.SC,['cs.SC'],,,[]
"On the chordality of polynomial sets in triangular decomposition in
  top-down style",http://arxiv.org/abs/1802.01752v1,2018-02-06T01:12:56Z,2018-02-06T01:12:56Z,"  In this paper the chordal graph structures of polynomial sets appearing in
triangular decomposition in top-down style are studied when the input
polynomial set to decompose has a chordal associated graph. In particular, we
prove that the associated graph of one specific triangular set computed in any
algorithm for triangular decomposition in top-down style is a subgraph of the
chordal graph of the input polynomial set and that all the polynomial sets
including all the computed triangular sets appearing in one specific
simply-structured algorithm for triangular decomposition in top-down style
(Wang's method) have associated graphs which are subgraphs of the the chordal
graph of the input polynomial set. These subgraph structures in triangular
decomposition in top-down style are multivariate generalization of existing
results for Gaussian elimination and may lead to specialized efficient
algorithms and refined complexity analyses for triangular decomposition of
chordal polynomial sets.
","['\nChenqi Mou\n', '\nYang Bai\n']",20 pages,,http://arxiv.org/abs/1802.01752v1,cs.SC,['cs.SC'],,,[]
Certification of minimal approximant bases,http://arxiv.org/abs/1802.01920v2,2018-02-06T12:57:47Z,2018-05-17T20:35:04Z,"  For a given computational problem, a certificate is a piece of data that one
(the prover) attaches to the output with the aim of allowing efficient
verification (by the verifier) that this output is correct. Here, we consider
the minimal approximant basis problem, for which the fastest known algorithms
output a polynomial matrix of dimensions $m \times m$ and average degree $D/m$
using $O\tilde{~}(m^\omega \frac{D}{m})$ field operations. We propose a
certificate which, for typical instances of the problem, is computed by the
prover using $O(m^\omega \frac{D}{m})$ additional field operations and allows
verification of the approximant basis by a Monte Carlo algorithm with cost
bound $O(m^\omega + m D)$.
  Besides theoretical interest, our motivation also comes from the fact that
approximant bases arise in most of the fastest known algorithms for linear
algebra over the univariate polynomials; thus, this work may help in designing
certificates for other polynomial matrix computations. Furthermore,
cryptographic challenges such as breaking records for discrete logarithm
computations or for integer factorization rely in particular on computing
minimal approximant bases for large instances: certificates can then be used to
provide reliable computation on outsourced and error-prone clusters.
","['\nPascal Giorgi\n', '\nVincent Neiger\n']","ISSAC 2018. 8 pages, 3 algorithms, acmart sigconf style",,http://arxiv.org/abs/1802.01920v2,cs.SC,['cs.SC'],,,[]
Computing Popov and Hermite forms of rectangular polynomial matrices,http://arxiv.org/abs/1802.01928v2,2018-02-06T13:08:56Z,2018-05-17T21:11:43Z,"  We consider the computation of two normal forms for matrices over the
univariate polynomials: the Popov form and the Hermite form. For matrices which
are square and nonsingular, deterministic algorithms with satisfactory cost
bounds are known. Here, we present deterministic, fast algorithms for
rectangular input matrices. The obtained cost bound for the Popov form matches
the previous best known randomized algorithm, while the cost bound for the
Hermite form improves on the previous best known ones by a factor which is at
least the largest dimension of the input matrix.
","['\nVincent Neiger\n', '\nJohan Rosenkilde\n', '\nGrigory Solomatov\n']","ISSAC 2018, 8 pages, 4 algorithms, acmart sigconf style",,http://arxiv.org/abs/1802.01928v2,cs.SC,['cs.SC'],,,[]
Additive Decompositions in Primitive Extensions,http://arxiv.org/abs/1802.02329v1,2018-02-07T07:40:23Z,2018-02-07T07:40:23Z,"  This paper extends the classical Ostrogradsky-Hermite reduction for rational
functions to more general functions in primitive extensions of certain types.
For an element $f$ in such an extension $K$, the extended reduction decomposes
$f$ as the sum of a derivative in $K$ and another element $r$ such that $f$ has
an antiderivative in $K$ if and only if $r=0$; and $f$ has an elementary
antiderivative over $K$ if and only if $r$ is a linear combination of
logarithmic derivatives over the constants when $K$ is a logarithmic extension.
Moreover, $r$ is minimal in some sense. Additive decompositions may lead to
reduction-based creative-telescoping methods for nested logarithmic functions,
which are not necessarily $D$-finite.
","['\nShaoshi Chen\n', '\nHao Du\n', '\nZiming Li\n']",,,http://arxiv.org/abs/1802.02329v1,cs.SC,"['cs.SC', '33F10, 68W30, 12H05']",,,[]
Exact algorithms for semidefinite programs with degenerate feasible set,http://arxiv.org/abs/1802.02834v2,2018-02-08T12:44:56Z,2020-06-10T11:01:28Z,"  Given symmetric matrices $A_0, A_1, \ldots, A_n$ of size $m$ with rational
entries, the set of real vectors $x = (x_1, \ldots, x_n)$ such that the matrix
$A_0 + x_1 A_1 + \cdots + x_n A_n$ has non-negative eigenvalues is called a
spectrahedron. Minimization of linear functions over spectrahedra is called
semidefinite programming. Such problems appear frequently in control theory and
real algebra, especially in the context of nonnegativity certificates for
multivariate polynomials based on sums of squares. Numerical software for
semidefinite programming are mostly based on interior point methods, assuming
non-degeneracy properties such as the existence of an interior point in the
spectrahedron. In this paper, we design an exact algorithm based on symbolic
homotopy for solving semidefinite programs without assumptions on the feasible
set, and we analyze its complexity. Because of the exactness of the output, it
cannot compete with numerical routines in practice. However, we prove that
solving such problems can be done in polynomial time if either $n$ or $m$ is
fixed.
","['\nDidier Henrion\n', '\nSimone Naldi\n', '\nMohab Safey El Din\n']","26 pages, 1 figure, extended version (the original paper is published
  in the Proceedings of ISSAC 2018)",,http://arxiv.org/abs/1802.02834v2,cs.SC,['cs.SC'],,,[]
Error correction in fast matrix multiplication and inverse,http://arxiv.org/abs/1802.02270v1,2018-02-07T00:20:12Z,2018-02-07T00:20:12Z,"  We present new algorithms to detect and correct errors in the product of two
matrices, or the inverse of a matrix, over an arbitrary field. Our algorithms
do not require any additional information or encoding other than the original
inputs and the erroneous output. Their running time is softly linear in the
number of nonzero entries in these matrices when the number of errors is
sufficiently small, and they also incorporate fast matrix multiplication so
that the cost scales well when the number of errors is large. These algorithms
build on the recent result of Gasieniec et al (2017) on correcting matrix
products, as well as existing work on verification algorithms, sparse low-rank
linear algebra, and sparse polynomial interpolation.
",['\nDaniel S. Roche\n'],,,http://arxiv.org/abs/1802.02270v1,cs.SC,"['cs.SC', 'cs.DS']",,,[]
Frobenius Additive Fast Fourier Transform,http://arxiv.org/abs/1802.03932v1,2018-02-12T08:40:08Z,2018-02-12T08:40:08Z,"  In ISSAC 2017, van der Hoeven and Larrieu showed that evaluating a polynomial
P in GF(q)[x] of degree <n at all n-th roots of unity in GF($q^d$) can
essentially be computed d-time faster than evaluating Q in GF($q^d$)[x] at all
these roots, assuming GF($q^d$) contains a primitive n-th root of unity. Termed
the Frobenius FFT, this discovery has a profound impact on polynomial
multiplication, especially for multiplying binary polynomials, which finds
ample application in coding theory and cryptography. In this paper, we show
that the theory of Frobenius FFT beautifully generalizes to a class of additive
FFT developed by Cantor and Gao-Mateer. Furthermore, we demonstrate the power
of Frobenius additive FFT for q=2: to multiply two binary polynomials whose
product is of degree <256, the new technique requires only 29,005 bit
operations, while the best result previously reported was 33,397. To the best
of our knowledge, this is the first time that FFT-based multiplication
outperforms Karatsuba and the like at such a low degree in terms of
bit-operation count.
","['\nWen-Ding Li\n', '\nMing-Shing Chen\n', '\nPo-Chun Kuo\n', '\nChen-Mou Cheng\n', '\nBo-Yin Yang\n']",,,http://arxiv.org/abs/1802.03932v1,cs.SC,"['cs.SC', 'cs.CC']",,,[]
Symmetries of Quantified Boolean Formulas,http://arxiv.org/abs/1802.03993v1,2018-02-12T11:49:42Z,2018-02-12T11:49:42Z,"  While symmetries are well understood for Boolean formulas and successfully
exploited in practical SAT solving, less is known about symmetries in
quantified Boolean formulas (QBF). There are some works introducing adaptions
of propositional symmetry breaking techniques, with a theory covering only very
specific parts of QBF symmetries. We present a general framework that gives a
concise characterization of symmetries of QBF. Our framework naturally
incorporates the duality of universal and existential symmetries resulting in a
general basis for QBF symmetry breaking.
","['\nManuel Kauers\n', '\nMartina Seidl\n']",,,http://arxiv.org/abs/1802.03993v1,cs.LO,"['cs.LO', 'cs.SC']",,,[]
Unbounded Software Model Checking with Incremental SAT-Solving,http://arxiv.org/abs/1802.04174v1,2018-02-12T16:43:06Z,2018-02-12T16:43:06Z,"  This paper describes a novel unbounded software model checking approach to
find errors in programs written in the C language based on incremental
SAT-solving. Instead of using the traditional assumption based API to
incremental SAT solvers we use the DimSpec format that is used in SAT based
automated planning. A DimSpec formula consists of four CNF formulas
representing the initial, goal and intermediate states and the relations
between each pair of neighboring states of a transition system. We present a
new tool called LLUMC which encodes the presence of certain errors in a C
program into a DimSpec formula, which can be solved by either an incremental
SAT-based DimSpec solver or the IC3 algorithm for invariant checking. We
evaluate the approach in the context of SAT-based model checking for both the
incremental SAT-solving and the IC3 algorithm. We show that our encoding
expands the functionality of bounded model checkers by also covering large and
infinite loops, while still maintaining a feasible time performance.
Furthermore, we demonstrate that our approach offers the opportunity to
generate runtime-optimizations by utilizing parallel SAT-solving.
","['\nMarko Kleine Büning\n', '\nTomas Balyo\n', '\nCarsten Sinz\n']",,,http://arxiv.org/abs/1802.04174v1,cs.SC,"['cs.SC', 'cs.PL']",,,[]
"Integration in terms of exponential integrals and incomplete gamma
  functions I",http://arxiv.org/abs/1802.05544v2,2018-02-09T18:08:02Z,2018-02-21T20:01:30Z,"  This paper provides a Liouville principle for integration in terms of
exponential integrals and incomplete gamma functions.
",['\nWaldemar Hebisch\n'],,,http://arxiv.org/abs/1802.05544v2,math.NT,"['math.NT', 'cs.SC', '12H05']",,,[]
"Quantum Algorithm for Optimization and Polynomial System Solving over
  Finite Field and Application to Cryptanalysis",http://arxiv.org/abs/1802.03856v2,2018-02-12T01:26:36Z,2018-10-07T08:49:41Z,"  In this paper, we give quantum algorithms for two fundamental computation
problems: solving polynomial systems over finite fields and optimization where
the arguments of the objective function and constraints take values from a
finite field or a bounded interval of integers. The quantum algorithms can
solve these problems with any given success probability and have polynomial
runtime complexities in the size of the input, the degree of the inequality
constraints, and the condition number of certain matrices derived from the
problem. So, we achieved exponential speedup for these problems when their
condition numbers are small. As applications, quantum algorithms are given to
three basic computational problems in cryptography: the polynomial system with
noise problem, the short integer solution problem, the shortest vector problem,
as well as the cryptanalysis for the lattice based NTRU cryptosystem. It is
shown that these problems and NTRU can against quantum computer attacks only if
their condition numbers are large, so the condition number could be used as a
new criterion for the lattice based post-quantum cryptosystems.
","['\nYu-Ao Chen\n', '\nXiao-Shan Gao\n', '\nChun-Ming Yuan\n']",,,http://arxiv.org/abs/1802.03856v2,cs.SC,"['cs.SC', 'cs.CC', 'cs.CR', 'quant-ph']",,,[]
Block SOS Decomposition,http://arxiv.org/abs/1801.07954v2,2018-01-24T12:26:24Z,2018-01-30T08:34:44Z,"  A widely used method for solving SOS (Sum Of Squares) decomposition problem
is to reduce it to the problem of semi-definite programs (SDPs) which can be
efficiently solved in theory. In practice, although many SDP solvers can work
out some problems of big scale, the efficiency and reliability of such method
decrease greatly while the input size increases. Recently, by exploiting the
sparsity of the input SOS decomposition problem, some preprocessing algorithms
were proposed [5,17], which first divide the input problem satisfying special
definitions or properties into smaller SDP problems and then pass the smaller
ones to SDP solvers to obtain reliable results efficiently. A natural question
is that to what extent the above mentioned preprocessing algorithms work. That
is, how many polynomials satisfying those definitions or properties are there
in the SOS polynomials? In this paper, we define a concept of block SOS
decomposable polynomials which is a generalization of those special classes in
[5] and [17]. Roughly speaking, it is a class of polynomials whose SOS
decomposition problem can be transformed into smaller ones (in other words, the
corresponding SDP matrices can be block-diagnolized) by considering their
supports only (coefficients are not considered). Then we prove that the set of
block SOS decomposable polynomials has measure zero in the set of SOS
polynomials. That means if we only consider supports (not with coefficients) of
polynomials, such algorithms decreasing the size of SDPs for those SDP-based
SOS solvers can only work on very few polynomials. As a result, this shows that
the SOS decomposition problems that can be optimized by the above mentioned
preprocessing algorithms are very few.
","['\nHaokun Li\n', '\nBican Xia\n']",,,http://arxiv.org/abs/1801.07954v2,cs.SC,['cs.SC'],,,[]
"Fast Algorithm for Calculating the Minimal Annihilating Polynomials of
  Matrices via Pseudo Annihilating Polynomials",http://arxiv.org/abs/1801.08437v3,2018-01-25T15:01:21Z,2018-06-12T09:37:34Z,"  Minimal annihilating polynomials are very useful in a wide variety of
algorithms in exact linear algebra. A new efficient method is proposed for
calculating the minimal annihilating polynomials for all the unit vectors, for
a square matrix over a field of characteristic zero. Key ideas of the proposed
method are the concept of pseudo annihilating polynomial and the use of binary
splitting technique. Efficiency of the resulting algorithms is shown by
arithmetic time complexity analysis.
","['\nShinichi Tajima\n', '\nKatsuyoshi Ohara\n', '\nAkira Terui\n']",,,http://arxiv.org/abs/1801.08437v3,math.AC,"['math.AC', 'cs.SC', '15A18, 65F15, 68W30']",,,[]
"Symmetries and similarities of planar algebraic curves using harmonic
  polynomials",http://arxiv.org/abs/1801.09962v1,2018-01-30T12:44:39Z,2018-01-30T12:44:39Z,"  We present novel, deterministic, efficient algorithms to compute the
symmetries of a planar algebraic curve, implicitly defined, and to check
whether or not two given implicit planar algebraic curves are similar, i.e.
equal up to a similarity transformation. Both algorithms are based on the fact,
well-known in Harmonic Analysis, that the Laplacian operator commutes with
orthogonal transformations, and on efficient algorithms to find the
symmetriessimilarities of a harmonic algebraic curvetwo given harmonic
algebraic curves. In fact, we show that in general the problem can be reduced
to the harmonic case, except for some special cases, easy to treat.
","['\nJuan Gerardo Alcázar\n', '\nMiroslav Lávička\n', '\nJan Vršek\n']",,,http://arxiv.org/abs/1801.09962v1,math.AG,"['math.AG', 'cs.SC']",,,[]
Algorithmic Linearly Constrained Gaussian Processes,http://arxiv.org/abs/1801.09197v3,2018-01-28T09:07:05Z,2019-01-04T17:33:23Z,"  We algorithmically construct multi-output Gaussian process priors which
satisfy linear differential equations. Our approach attempts to parametrize all
solutions of the equations using Gr\""obner bases. If successful, a push forward
Gaussian process along the paramerization is the desired prior. We consider
several examples from physics, geomathematics and control, among them the full
inhomogeneous system of Maxwell's equations. By bringing together stochastic
learning and computer algebra in a novel way, we combine noisy observations
with precise algebraic computations.
",['\nMarkus Lange-Hegermann\n'],NIPS 2018,,http://arxiv.org/abs/1801.09197v3,stat.ML,"['stat.ML', 'cs.LG', 'cs.SC', 'math.AC', '60G15, 62M30, 62G08, 12H05, 68W30, 13P10, 13P20, 13J30, 13P25,\n  60B11, 35Q61']",,,[]
"An Algorithm to Decompose Permutation Representations of Finite Groups:
  Polynomial Algebra Approach",http://arxiv.org/abs/1801.09786v2,2018-01-29T22:40:35Z,2018-03-03T16:32:06Z,"  We describe an algorithm for splitting permutation representations of finite
group over fields of characteristic zero into irreducible components. The
algorithm is based on the fact that the components of the invariant inner
product in invariant subspaces are operators of projection into these
subspaces. An important element of the algorithm is the calculation of
Gr\""obner bases of polynomial ideals. A preliminary implementation of the
algorithm splits representations up to dimensions of tens of thousands. Some
examples of computations are given in appendix.
",['\nVladimir V. Kornyak\n'],"V2: more complicated examples are given, 16 pages",,http://arxiv.org/abs/1801.09786v2,math.RT,"['math.RT', 'cs.SC', 'math.GR']",,,[]
"An efficient algorithm for global interval solution of nonlinear
  algebraic equations and its GPGPU implementation",http://arxiv.org/abs/1802.00330v1,2018-01-31T13:29:50Z,2018-01-31T13:29:50Z,"  Solving nonlinear algebraic equations is a classic mathematics problem, and
common in scientific researches and engineering applications. There are many
numeric, symbolic and numeric-symbolic methods of solving (real) solutions.
Unlucky, these methods are constrained by some factors, e.g., high complexity,
slow serial calculation, and the notorious intermediate expression expansion.
Especially when the count of variables is larger than six, the efficiency is
decreasing drastically. In this paper, according to the property of physical
world, we pay attention to nonlinear algebraic equations whose variables are in
fixed constraints, and get meaningful real solutions. Combining with
parallelism of GPGPU, we present an efficient algorithm, by searching the
solution space globally and solving the nonlinear algebraic equations with real
interval solutions. Furthermore, we realize the Hansen-Sengupta method on
GPGPU. The experiments show that our method can solve many nonlinear algebraic
equations, and the results are accurate and more efficient compared to
traditional serial methods.
","['\nDang Lin\n', '\nLiangyu Chen\n']","21pages, 1 figure",,http://arxiv.org/abs/1802.00330v1,cs.NA,"['cs.NA', 'cs.DC', 'cs.SC']",,,[]
Fast computation of approximant bases in canonical form,http://arxiv.org/abs/1801.04553v2,2018-01-14T13:14:41Z,2019-04-06T14:25:36Z,"  In this article, we design fast algorithms for the computation of approximant
bases in shifted Popov normal form. We first recall the algorithm known as
PM-Basis, which will be our second fundamental engine after polynomial matrix
multiplication: most other fast approximant basis algorithms basically aim at
efficiently reducing the input instance to instances for which PM-Basis is
fast. Such reductions usually involve partial linearization techniques due to
Storjohann, which have the effect of balancing the degrees and dimensions in
the manipulated matrices.
  Following these ideas, Zhou and Labahn gave two algorithms which are faster
than PM-Basis for important cases including Hermite-Pade approximation, yet
only for shifts whose values are concentrated around the minimum or the maximum
value. The three mentioned algorithms were designed for balanced orders and
compute approximant bases that are generally not normalized. Here, we show how
they can be modified to return the shifted Popov basis without impact on their
cost bound; besides, we extend Zhou and Labahn's algorithms to arbitrary
orders.
  Furthermore, we give an algorithm which handles arbitrary shifts with one
extra logarithmic factor in the cost bound compared to the above algorithms. To
the best of our knowledge, this improves upon previously known algorithms for
arbitrary shifts, including for particular cases such as Hermite-Pade
approximation. This algorithm is based on a recent divide and conquer approach
which reduces the general case to the case where information on the output
degree is available. As outlined above, we solve the latter case via partial
linearizations and PM-Basis.
","['\nClaude-Pierre Jeannerod\n', '\nVincent Neiger\n', '\nGilles Villard\n']","37 pages, 8 algorithms",,http://arxiv.org/abs/1801.04553v2,cs.SC,['cs.SC'],,,[]
The Complexity of Subdivision for Diameter-Distance Tests,http://arxiv.org/abs/1801.05864v1,2018-01-17T20:55:22Z,2018-01-17T20:55:22Z,"  We present a general framework for analyzing the complexity of
subdivision-based algorithms whose tests are based on the sizes of regions and
their distance to certain sets (often varieties) intrinsic to the problem under
study. We call such tests diameter-distance tests. We illustrate that
diameter-distance tests are common in the literature by proving that many
interval arithmetic-based tests are, in fact, diameter-distance tests. For this
class of algorithms, we provide both non-adaptive bounds for the complexity,
based on separation bounds, as well as adaptive bounds, by applying the
framework of continuous amortization.
  Using this structure, we provide the first complexity analysis for the
algorithm by Plantinga and Vegeter for approximating real implicit curves and
surfaces. We present both adaptive and non-adaptive a priori worst-case bounds
on the complexity of this algorithm both in terms of the number of subregions
constructed and in terms of the bit complexity for the construction. Finally,
we construct families of hypersurfaces to prove that our bounds are tight.
","['\nMichael Burr\n', '\nShuhong Gao\n', '\nElias Tsigaridas\n']",,,http://arxiv.org/abs/1801.05864v1,cs.SC,"['cs.SC', '68W30, 65Y20']",,,[]
A random walk through experimental mathematics,http://arxiv.org/abs/1801.05423v2,2018-01-16T17:35:19Z,2018-02-08T01:07:48Z,"  We describe our adventures in creating a new first-year course in
Experimental Mathematics that uses active learning. We used a state-of-the-art
facility, called The Western Active Learning Space, and got the students to
""drive the spaceship"" (at least a little bit). This paper describes some of our
techniques for pedagogy, some of the vignettes of experimental mathematics that
we used, and some of the outcomes. EYSC was a student in the
simultaneously-taught senior sister course ""Open Problems in Experimental
Mathematics"" the first time it was taught and an unofficial co-instructor the
second time. Jon Borwein attended the Project Presentation Day (the second
time) and gave thoughtful feedback to each student. This paper is dedicated to
his memory.
","['\nEunice Y. S. Chan\n', '\nRobert M. Corless\n']","25 pages, 7 figures",,http://arxiv.org/abs/1801.05423v2,math.HO,"['math.HO', 'cs.SC', '68W30, 30B70, 37N30']",,,[]
Resolving zero-divisors using Hensel lifting,http://arxiv.org/abs/1801.03161v1,2018-01-09T22:23:39Z,2018-01-09T22:23:39Z,"  Algorithms which compute modulo triangular sets must respect the presence of
zero-divisors. We present Hensel lifting as a tool for dealing with them. We
give an application: a modular algorithm for computing GCDs of univariate
polynomials with coefficients modulo a radical triangular set over the
rationals. Our modular algorithm naturally generalizes previous work from
algebraic number theory. We have implemented our algorithm using Maple's RECDEN
package. We compare our implementation with the procedure RegularGcd in the
RegularChains package.
","['\nJohn Kluesner\n', '\nMichael Monagan\n']",Shorter version to appear in Proceedings of SYNASC 2017,,http://arxiv.org/abs/1801.03161v1,cs.SC,['cs.SC'],,,[]
"Deciding and Interpolating Algebraic Data Types by Reduction (Technical
  Report)",http://arxiv.org/abs/1801.02367v1,2018-01-08T10:16:18Z,2018-01-08T10:16:18Z,"  Recursive algebraic data types (term algebras, ADTs) are one of the most
well-studied theories in logic, and find application in contexts including
functional programming, modelling languages, proof assistants, and
verification. At this point, several state-of-the-art theorem provers and SMT
solvers include tailor-made decision procedures for ADTs, and version 2.6 of
the SMT-LIB standard includes support for ADTs. We study an extremely simple
approach to decide satisfiability of ADT constraints, the reduction of ADT
constraints to equisatisfiable constraints over uninterpreted functions (EUF)
and linear integer arithmetic (LIA). We show that the reduction approach gives
rise to both decision and Craig interpolation procedures in (extensions of)
ADTs.
","['\nHossein Hojjat\nRochester Institute of Technology\n', '\nPhilipp Rümmer\nUppsala University\n']","Extended version of a paper presented at SYNASC 2017, Timisoara,
  Romania",,http://arxiv.org/abs/1801.02367v1,cs.LO,"['cs.LO', 'cs.SC']",,,"['Rochester Institute of Technology', 'Uppsala University']"
On Division Polynomial PIT and Supersingularity,http://arxiv.org/abs/1801.02664v2,2018-01-08T20:04:23Z,2018-01-15T19:53:20Z,"  For an elliptic curve $E$ over a finite field $\F_q$, where $q$ is a prime
power, we propose new algorithms for testing the supersingularity of $E$. Our
algorithms are based on the Polynomial Identity Testing (PIT) problem for the
$p$-th division polynomial of $E$. In particular, an efficient algorithm using
points of high order on $E$ is given.
",['\nJavad Doliskani\n'],,,http://arxiv.org/abs/1801.02664v2,cs.SC,"['cs.SC', 'math.NT', '11Y16, 14H52 (Primary), 12Y05 (Secondary)']",,,[]
Invariant Generation for Multi-Path Loops with Polynomial Assignments,http://arxiv.org/abs/1801.03967v1,2018-01-11T19:55:51Z,2018-01-11T19:55:51Z,"  Program analysis requires the generation of program properties expressing
conditions to hold at intermediate program locations. When it comes to programs
with loops, these properties are typically expressed as loop invariants. In
this paper we study a class of multi-path program loops with numeric variables,
in particular nested loops with conditionals, where assignments to program
variables are polynomial expressions over program variables. We call this class
of loops extended P-solvable and introduce an algorithm for generating all
polynomial invariants of such loops. By an iterative procedure employing
Gr\""obner basis computation, our approach computes the polynomial ideal of the
polynomial invariants of each program path and combines these ideals
sequentially until a fixed point is reached. This fixed point represents the
polynomial ideal of all polynomial invariants of the given extended P-solvable
loop. We prove termination of our method and show that the maximal number of
iterations for reaching the fixed point depends linearly on the number of
program variables and the number of inner loops. In particular, for a loop with
m program variables and r conditional branches we prove an upper bound of m*r
iterations. We implemented our approach in the Aligator software package.
Furthermore, we evaluated it on 18 programs with polynomial arithmetic and
compared it to existing methods in invariant generation. The results show the
efficiency of our approach.
","['\nAndreas Humenberger\n', '\nMaximilian Jaroschek\n', '\nLaura Kovács\n']",,,http://dx.doi.org/10.1007/978-3-319-73721-8_11,cs.PL,"['cs.PL', 'cs.SC']",10.1007/978-3-319-73721-8_11,,[]
"Computing the Inverse Mellin Transform of Holonomic Sequences using
  Kovacic's Algorithm",http://arxiv.org/abs/1801.01039v1,2018-01-02T09:20:03Z,2018-01-02T09:20:03Z,"  We describe how the extension of a solver for linear differential equations
by Kovacic's algorithm helps to improve a method to compute the inverse Mellin
transform of holonomic sequences. The method is implemented in the computer
algebra package HarmonicSums.
",['\nJakob Ablinger\n'],8 pages. arXiv admin note: text overlap with arXiv:1606.02845,,http://arxiv.org/abs/1801.01039v1,cs.SC,"['cs.SC', 'math-ph', 'math.CO', 'math.MP']",,,[]
Computer Algebra Methods in Control Systems,http://arxiv.org/abs/1712.09163v2,2017-12-26T02:15:34Z,2017-12-29T03:59:56Z,"  As dynamic and control systems become more complex, relying purely on
numerical computations for systems analysis and design might become extremely
expensive or totally infeasible. Computer algebra can act as an enabler for
analysis and design of such complex systems. It also provides means for
characterization of all solutions and studying them before realizing a
particular solution. This note provides a brief survey on some of the
applications of symbolic computations in control systems analysis and design.
",['\nMasoud Abbaszadeh\n'],10 pages,,http://arxiv.org/abs/1712.09163v2,cs.SY,"['cs.SY', 'cs.SC', 'math.OC', '15A60, 37M99', 'B.1.1; B.1.2; F.2.2; I.1.3; I.1.4']",,,[]
Computing Lower Rank Approximations of Matrix Polynomials,http://arxiv.org/abs/1712.04007v1,2017-12-11T20:30:21Z,2017-12-11T20:30:21Z,"  Given an input matrix polynomial whose coefficients are floating point
numbers, we consider the problem of finding the nearest matrix polynomial which
has rank at most a specified value. This generalizes the problem of finding a
nearest matrix polynomial that is algebraically singular with a prescribed
lower bound on the dimension given in a previous paper by the authors. In this
paper we prove that such lower rank matrices at minimal distance always exist,
satisfy regularity conditions, and are all isolated and surrounded by a basin
of attraction of non-minimal solutions. In addition, we present an iterative
algorithm which, on given input sufficiently close to a rank-at-most matrix,
produces that matrix. The algorithm is efficient and is proven to converge
quadratically given a sufficiently good starting point. An implementation
demonstrates the effectiveness and numerical robustness of our algorithm in
practice.
","['\nMark Giesbrecht\n', '\nJoseph Haraldson\n', '\nGeorge Labahn\n']",31 Pages,,http://arxiv.org/abs/1712.04007v1,cs.SC,['cs.SC'],,,[]
Block-Krylov techniques in the context of sparse-FGLM algorithms,http://arxiv.org/abs/1712.04177v2,2017-12-12T08:56:20Z,2019-01-15T12:26:57Z,"  Consider a zero-dimensional ideal $I$ in $\mathbb{K}[X_1,\dots,X_n]$.
Inspired by Faug\`ere and Mou's Sparse FGLM algorithm, we use Krylov sequences
based on multiplication matrices of $I$ in order to compute a description of
its zero set by means of univariate polynomials.
  Steel recently showed how to use Coppersmith's block-Wiedemann algorithm in
this context; he describes an algorithm that can be easily parallelized, but
only computes parts of the output in this manner. Using generating series
expressions going back to work of Bostan, Salvy, and Schost, we show how to
compute the entire output for a small overhead, without making any assumption
on the ideal $I$ other than it having dimension zero. We then propose a
refinement of this idea that partially avoids the introduction of a generic
linear form. We comment on experimental results obtained by an implementation
based on the C++ libraries Eigen, LinBox and NTL.
","['\nSeung Gyu Hyun\n', '\nVincent Neiger\n', '\nHamid Rahkooy\n', '\nEric Schost\n']","32 pages, 7 algorithms, 2 tables",,http://arxiv.org/abs/1712.04177v2,cs.SC,['cs.SC'],,,[]
"Revisit Sparse Polynomial Interpolation based on Randomized Kronecker
  Substitution",http://arxiv.org/abs/1712.05481v2,2017-12-15T00:08:50Z,2018-07-17T13:10:35Z,"  In this paper, a new reduction based interpolation algorithm for black-box
multivariate polynomials over finite fields is given. The method is based on
two main ingredients. A new Monte Carlo method is given to reduce black-box
multivariate polynomial interpolation to black-box univariate polynomial
interpolation over any ring. The reduction algorithm leads to multivariate
interpolation algorithms with better or the same complexities most cases when
combining with various univariate interpolation algorithms. We also propose a
modified univariate Ben-or and Tiwarri algorithm over the finite field, which
has better total complexity than the Lagrange interpolation algorithm.
Combining our reduction method and the modified univariate Ben-or and Tiwarri
algorithm, we give a Monte Carlo multivariate interpolation algorithm, which
has better total complexity in most cases for sparse interpolation of black-box
polynomial over finite fields.
","['\nQiao-Long Huang\n', '\nXiao-Shan Gao\n']",,,http://arxiv.org/abs/1712.05481v2,cs.SC,['cs.SC'],,,[]
"Can one design a geometry engine? On the (un)decidability of affine
  Euclidean geometries",http://arxiv.org/abs/1712.07474v3,2017-12-20T13:29:12Z,2018-06-01T07:57:08Z,"  We survey the status of decidabilty of the consequence relation in various
axiomatizations of Euclidean geometry. We draw attention to a widely overlooked
result by Martin Ziegler from 1980, which proves Tarski's conjecture on the
undecidability of finitely axiomatizable theories of fields. We elaborate on
how to use Ziegler's theorem to show that the consequence relations for the
first order theory of the Hilbert plane and the Euclidean plane are
undecidable. As new results we add: (A) The first order consequence relations
for Wu's orthogonal and metric geometries (Wen-Ts\""un Wu, 1984), and for the
axiomatization of Origami geometry (J. Justin 1986, H. Huzita 1991)are
undecidable.
  It was already known that the universal theory of Hilbert planes and Wu's
orthogonal geometry is decidable. We show here using elementary model theoretic
tools that (B) the universal first order consequences of any geometric theory
$T$ of Pappian planes which is consistent with the analytic geometry of the
reals is decidable.
",['\nJ. A. Makowsky\n'],"28 pages, revised version, May 25, 2018",,http://arxiv.org/abs/1712.07474v3,cs.SC,"['cs.SC', '03, 03D35']",,,[]
"Computing effectively stabilizing controllers for a class of $n$D
  systems",http://arxiv.org/abs/1801.04982v1,2017-12-19T09:55:01Z,2017-12-19T09:55:01Z,"  In this paper, we study the internal stabilizability and internal
stabilization problems for multidimensional (nD) systems. Within the fractional
representation approach, a multidimen-sional system can be studied by means of
matrices with entries in the integral domain of structurally stable rational
fractions, namely the ring of rational functions which have no poles in the
closed unit polydisc U n = {z = (z 1 ,. .. , z n) $\in$ C n | |z 1 | 1,. .. ,
|z n | 1}. It is known that the internal stabilizability of a multidimensional
system can be investigated by studying a certain polynomial ideal I = p 1 ,. ..
, p r that can be explicitly described in terms of the transfer matrix of the
plant. More precisely the system is stabilizable if and only if V (I) = {z
$\in$ C n | p 1 (z) = $\times$ $\times$ $\times$ = p r (z) = 0} $\cap$ U n =
$\emptyset$. In the present article, we consider the specific class of linear
nD systems (which includes the class of 2D systems) for which the ideal I is
zero-dimensional, i.e., the p i 's have only a finite number of common complex
zeros. We propose effective symbolic-numeric algorithms for testing if V (I)
$\cap$ U n = $\emptyset$, as well as for computing, if it exists, a stable
polynomial p $\in$ I which allows the effective computation of a stabilizing
controller. We illustrate our algorithms through an example and finally provide
running times of prototype implementations for 2D and 3D systems.
","['\nYacine Bouzidi\nVEGAS\n', '\nThomas Cluzeau\nVEGAS\n', '\nGuillaume Moroz\nVEGAS\n', '\nAlban Quadrat\nDISCO\n']",,"IFAC-PapersOnLine, 2017, 50 (1), pp.1847 - 1852",http://dx.doi.org/10.1016/j.ifacol.2017.08.200,math.NA,"['math.NA', 'cs.SC']",10.1016/j.ifacol.2017.08.200,,"['VEGAS', 'VEGAS', 'VEGAS', 'DISCO']"
"Faster integer and polynomial multiplication using cyclotomic
  coefficient rings",http://arxiv.org/abs/1712.03693v1,2017-12-11T09:37:57Z,2017-12-11T09:37:57Z,"  We present an algorithm that computes the product of two n-bit integers in
O(n log n (4\sqrt 2)^{log^* n}) bit operations. Previously, the best known
bound was O(n log n 6^{log^* n}). We also prove that for a fixed prime p,
polynomials in F_p[X] of degree n may be multiplied in O(n log n 4^{log^* n})
bit operations; the previous best bound was O(n log n 8^{log^* n}).
","['\nDavid Harvey\n', '\nJoris van der Hoeven\n']",28 pages,,http://arxiv.org/abs/1712.03693v1,cs.SC,"['cs.SC', 'cs.DS', 'math.NT', '68W30, 68Q17, 68W40', 'G.1.0; F.2.1']",,,[]
Counting Solutions of a Polynomial System Locally and Exactly,http://arxiv.org/abs/1712.05487v1,2017-12-15T00:33:30Z,2017-12-15T00:33:30Z,"  We propose a symbolic-numeric algorithm to count the number of solutions of a
polynomial system within a local region. More specifically, given a
zero-dimensional system $f_1=\cdots=f_n=0$, with
$f_i\in\mathbb{C}[x_1,\ldots,x_n]$, and a polydisc
$\mathbf{\Delta}\subset\mathbb{C}^n$, our method aims to certify the existence
of $k$ solutions (counted with multiplicity) within the polydisc.
  In case of success, it yields the correct result under guarantee. Otherwise,
no information is given. However, we show that our algorithm always succeeds if
$\mathbf{\Delta}$ is sufficiently small and well-isolating for a $k$-fold
solution $\mathbf{z}$ of the system.
  Our analysis of the algorithm further yields a bound on the size of the
polydisc for which our algorithm succeeds under guarantee. This bound depends
on local parameters such as the size and multiplicity of $\mathbf{z}$ as well
as the distances between $\mathbf{z}$ and all other solutions. Efficiency of
our method stems from the fact that we reduce the problem of counting the roots
in $\mathbf{\Delta}$ of the original system to the problem of solving a
truncated system of degree $k$. In particular, if the multiplicity $k$ of
$\mathbf{z}$ is small compared to the total degrees of the polynomials $f_i$,
our method considerably improves upon known complete and certified methods.
  For the special case of a bivariate system, we report on an implementation of
our algorithm, and show experimentally that our algorithm leads to a
significant improvement, when integrated as inclusion predicate into an
elimination method.
","['\nRuben Becker\n', '\nMichael Sagraloff\n']",,,http://arxiv.org/abs/1712.05487v1,cs.SC,"['cs.SC', 'cs.NA', 'math.NA']",,,[]
"Quantum Algorithms for Boolean Equation Solving and Quantum Algebraic
  Attack on Cryptosystems",http://arxiv.org/abs/1712.06239v3,2017-12-18T03:34:10Z,2018-08-06T02:33:16Z,"  Decision of whether a Boolean equation system has a solution is an NPC
problem and finding a solution is NP hard. In this paper, we present a quantum
algorithm to decide whether a Boolean equation system FS has a solution and
compute one if FS does have solutions with any given success probability. The
runtime complexity of the algorithm is polynomial in the size of FS and the
condition number of FS. As a consequence, we give a polynomial-time quantum
algorithm for solving Boolean equation systems if their condition numbers are
small, say polynomial in the size of FS. We apply our quantum algorithm for
solving Boolean equations to the cryptanalysis of several important
cryptosystems: the stream cipher Trivum, the block cipher AES, the hash
function SHA-3/Keccak, and the multivariate public key cryptosystems, and show
that they are secure under quantum algebraic attack only if the condition
numbers of the corresponding equation systems are large. This leads to a new
criterion for designing cryptosystems that can against the attack of quantum
computers: their corresponding equation systems must have large condition
numbers.
","['\nYu-Ao Chen\n', '\nXiao-Shan Gao\n']",,,http://arxiv.org/abs/1712.06239v3,quant-ph,"['quant-ph', 'cs.CC', 'cs.CR', 'cs.SC']",,,[]
Partial Predicate Abstraction and Counter-Example Guided Refinement,http://arxiv.org/abs/1712.01734v1,2017-12-05T16:19:21Z,2017-12-05T16:19:21Z,"  In this paper we present a counter-example guided abstraction and
approximation refinement (CEGAAR) technique for {\em partial predicate
abstraction}, which combines predicate abstraction and fixpoint approximations
for model checking infinite-state systems. The proposed approach incrementally
considers growing sets of predicates for abstraction refinement. The novelty of
the approach stems from recognizing source of the imprecision: abstraction or
approximation. We use Craig interpolation to deal with imprecision due to
abstraction. In the case of imprecision due to approximation, we delay
application of the approximation. Our experimental results on a variety of
models provide insights into effectiveness of partial predicate abstraction as
well as refinement techniques in this context.
",['\nTuba Yavuz\n'],,,http://arxiv.org/abs/1712.01734v1,cs.LO,"['cs.LO', 'cs.SC']",,,[]
Symbolic-Numeric Integration of Rational Functions,http://arxiv.org/abs/1712.01752v2,2017-12-05T16:49:08Z,2018-10-25T15:09:01Z,"  We consider the problem of symbolic-numeric integration of symbolic
functions, focusing on rational functions. Using a hybrid method allows the
stable yet efficient computation of symbolic antiderivatives while avoiding
issues of ill-conditioning to which numerical methods are susceptible. We
propose two alternative methods for exact input that compute the rational part
of the integral using Hermite reduction and then compute the transcendental
part two different ways using a combination of exact integration and efficient
numerical computation of roots. The symbolic computation is done within BPAS,
or Basic Polynomial Algebra Subprograms, which is a highly optimized
environment for polynomial computation on parallel architectures, while the
numerical computation is done using the highly optimized multiprecision
rootfinding package MPSolve. We show that both methods are forward and backward
stable in a structured sense and away from singularities tolerance
proportionality is achieved by adjusting the precision of the rootfinding
tasks.
","['\nRobert M. Corless\n', '\nRobert H. C. Moir\n', '\nMarc Moreno Maza\n', '\nNing Xie\n']","25 pages, 4 figures; added a footnote and page numbers",,http://arxiv.org/abs/1712.01752v2,cs.SC,"['cs.SC', 'math.NA', '68W30 (Primary), 65D99, 68N99 (Secondary)']",,,[]
An Extensible Ad Hoc Interface between Lean and Mathematica,http://arxiv.org/abs/1712.09288v1,2017-12-05T05:47:47Z,2017-12-05T05:47:47Z,"  We implement a user-extensible ad hoc connection between the Lean proof
assistant and the computer algebra system Mathematica. By reflecting the syntax
of each system in the other and providing a flexible interface for extending
translation, our connection allows for the exchange of arbitrary information
between the two systems. We show how to make use of the Lean metaprogramming
framework to verify certain Mathematica computations, so that the rigor of the
proof assistant is not compromised.
",['\nRobert Y. Lewis\nCarnegie Mellon University\n'],"In Proceedings PxTP 2017, arXiv:1712.00898","EPTCS 262, 2017, pp. 23-37",http://dx.doi.org/10.4204/EPTCS.262.4,cs.LO,"['cs.LO', 'cs.SC']",10.4204/EPTCS.262.4,,['Carnegie Mellon University']
"Drinfeld Modules with Complex Multiplication, Hasse Invariants and
  Factoring Polynomials over Finite Fields",http://arxiv.org/abs/1712.00669v2,2017-12-02T21:35:13Z,2018-08-26T18:39:56Z,"  We present a novel randomized algorithm to factor polynomials over a finite
field $\F_q$ of odd characteristic using rank $2$ Drinfeld modules with complex
multiplication. The main idea is to compute a lift of the Hasse invariant
(modulo the polynomial $f \in \F_q[x]$ to be factored) with respect to a random
Drinfeld module $\phi$ with complex multiplication. Factors of $f$ supported on
prime ideals with supersingular reduction at $\phi$ have vanishing Hasse
invariant and can be separated from the rest. Incorporating a Drinfeld module
analogue of Deligne's congruence, we devise an algorithm to compute the Hasse
invariant lift, which turns out to be the crux of our algorithm. The resulting
expected runtime of $n^{3/2+\varepsilon} (\log q)^{1+o(1)}+n^{1+\varepsilon}
(\log q)^{2+o(1)}$ to factor polynomials of degree $n$ over $\F_q$ matches the
fastest previously known algorithm, the Kedlaya-Umans implementation of the
Kaltofen-Shoup algorithm.
","['\nJavad Doliskani\n', '\nAnand Kumar Narayanan\n', '\nÉric Schost\n']",A mistake in Lemma 3.1 is corrected,,http://arxiv.org/abs/1712.00669v2,cs.CG,"['cs.CG', 'cs.SC', 'math.NT', '11G09, 11Y16, 12Y05']",,,[]
Constructive Arithmetics in Ore Localizations of Domains,http://arxiv.org/abs/1712.01773v1,2017-12-05T17:26:19Z,2017-12-05T17:26:19Z,"  For a non-commutative domain $R$ and a multiplicatively closed set $S$ the
(left) Ore localization of $R$ at $S$ exists if and only if $S$ satisfies the
(left) Ore property. Since the concept has been introduced by Ore back in the
1930's, Ore localizations have been widely used in theory and in applications.
We investigate the arithmetics of the localized ring $S^{-1}R$ from both
theoretical and practical points of view. We show that the key component of the
arithmetics is the computation of the intersection of a left ideal with a
submonoid $S$ of $R$. It is not known yet, whether there exists an algorithmic
solution of this problem in general. Still, we provide such solutions for cases
where $S$ is equipped with additional structure by distilling three most
frequently occurring types of Ore sets. We introduce the notion of the (left)
saturation closure and prove that it is a canonical form for (left) Ore sets in
$R$. We provide an implementation of arithmetics over the ubiquitous
$G$-algebras in \textsc{Singular:Plural} and discuss questions arising in this
context. Numerous examples illustrate the effectiveness of the proposed
approach.
","['\nJohannes Hoffmann\n', '\nViktor Levandovskyy\n']",24 pages,Journal of Symbolic Computation 98 (2020) 23-46,http://dx.doi.org/10.1016/j.jsc.2019.07.005,math.RA,"['math.RA', 'cs.MS', 'cs.SC', '16U20, 16Z05, 68W30']",10.1016/j.jsc.2019.07.005,,[]
Rings: an efficient Java/Scala library for polynomial rings,http://arxiv.org/abs/1712.02329v3,2017-12-06T18:52:14Z,2018-09-21T19:32:48Z,"  In this paper we briefly discuss \Rings --- an efficient lightweight library
for commutative algebra. Polynomial arithmetic, GCDs, polynomial factorization
and Gr\""obner bases are implemented with the use of modern asymptotically fast
algorithms. \Rings can be easily interacted or embedded in applications in
high-energy physics and other research areas via a simple API with fully typed
hierarchy of algebraic structures and algorithms for commutative algebra. The
use of the Scala language brings a quite novel powerful, strongly typed
functional programming model allowing to write short, expressive, and fast code
for applications. At the same time Rings shows one of the best performances
among existing software for algebraic calculations. \Rings is available from
http://github.com/PoslavskySV/rings
",['\nStanislav Poslavsky\n'],Computer Physics Communications (2018),,http://dx.doi.org/10.1016/j.cpc.2018.09.005,cs.SC,"['cs.SC', 'cs.MS', 'hep-ph', 'math.AC', 'math.RA']",10.1016/j.cpc.2018.09.005,,[]
Computation of the Adjoint Matrix,http://arxiv.org/abs/1711.09450v1,2017-11-26T20:25:23Z,2017-11-26T20:25:23Z,"  The best method for computing the adjoint matrix of an order $n$ matrix in an
arbitrary commutative ring requires $O(n^{\beta+1/3}\log n \log \log n)$
operations, provided the complexity of the algorithm for multiplying two
matrices is $\gamma n^\beta+o(n^\beta)$. For a commutative domain -- and under
the same assumptions -- the complexity of the best method is ${6\gamma
n^\beta}/{(2^{\beta}-2)}+o(n^\beta)$. In the present work a new method is
presented for the computation of the adjoint matrix in a commutative domain.
Despite the fact that the number of operations required is now 1.5 times more,
than that of the best method, this new method permits a better parallelization
of the computational process and may be successfully employed for computations
in parallel computational systems.
","['\nAlkiviadis Akritas\n', '\nGennadi Malaschonok\n']",,"Computational Science - ICCS 2006: 6th Int. Conf., UK, May 28-31,
  2006. Proc., Part II, LNCS 3992, Springer Berlin, 486-489",http://dx.doi.org/10.1007/11758525_65,cs.SC,['cs.SC'],10.1007/11758525_65,,[]
Solution of a System of Linear Equations in an Integral Ring,http://arxiv.org/abs/1711.09452v1,2017-11-26T20:33:24Z,2017-11-26T20:33:24Z,"  A modified Gauss's algorithm for solving a system of linear equations in an
integral ring is proposed, as well as an appropriate algorithm for calculating
the elements of the adjoint matrix.
",['\nGennadi Malaschonok\n'],7 pages,"USSR J. of Comput. Math. and Math. Phys., V.23, No. 6, 1983.
  1497-1500",http://arxiv.org/abs/1711.09452v1,cs.SC,['cs.SC'],,,[]
Effective Matrix Methods in Commutative Domains,http://arxiv.org/abs/1711.09456v1,2017-11-26T20:50:43Z,2017-11-26T20:50:43Z,"  Effective matrix methods for solving standard linear algebra problems in a
commutative domains are discussed. Two of them are new. There are a methods for
computing adjoined matrices and solving system of linear equations in a
commutative domains.
",['\nGennadi Malaschonok\n'],11 pages,"Formal Power Series and Algebraic Combinatorics, (Ed. by D.Krob,
  A.A.Mikhalev, A.V.Mikhalev), Springer Berlin, 2000, 506-517",http://arxiv.org/abs/1711.09456v1,cs.SC,['cs.SC'],,,[]
"Algorithms for the solution of systems of linear equations in
  commutative ring",http://arxiv.org/abs/1711.11471v1,2017-11-26T20:57:09Z,2017-11-26T20:57:09Z,"  Solution methods for linear equation systems in a commutative ring are
discussed. Four methods are compared, in the setting of several different
rings: Dodgson's method [1], Bareiss's method [2] and two methods of the author
- method by forward and back-up procedures [3] and a one-pass method [4]. We
show that for the number of coefficient operations, or for the number of
operations in the finite rings, or for modular computation in the polynomial
rings the one-pass method [4] is the best. The method of forward and back-up
procedures [3] is the best for the polynomial rings when we make use of
classical algorithms for polynomial operations.
",['\nGennadi Malaschonok\n'],"10 pages. arXiv admin note: substantial text overlap with
  arXiv:1711.11472","Effective Methods in Algebraic Geometry, ed. by T. Mora and C.
  Traverso, Progress in Mathematics 94, Birkhauser, 1991, 289--298",http://arxiv.org/abs/1711.11471v1,cs.SC,['cs.SC'],,,[]
Algorithms for the Computing Determinants in Commutative Rings,http://arxiv.org/abs/1711.11472v1,2017-11-26T21:32:54Z,2017-11-26T21:32:54Z,"  Two known computation methods and one new computation method for matrix
determinant over an integral domain are discussed. For each of the methods we
evaluate the computation times for different rings and show that the new method
is the best.
",['\nGennadi Malaschonok\n'],"9 pages. arXiv admin note: substantial text overlap with
  arXiv:1711.11471","Diskretnaya Matematika, V. 7, No.4, 1995, 68-76",http://arxiv.org/abs/1711.11472v1,cs.SC,['cs.SC'],,,[]
"The Potential and Challenges of CAD with Equational Constraints for
  SC-Square",http://arxiv.org/abs/1711.00312v1,2017-11-01T12:36:15Z,2017-11-01T12:36:15Z,"  Cylindrical algebraic decomposition (CAD) is a core algorithm within Symbolic
Computation, particularly for quantifier elimination over the reals and
polynomial systems solving more generally. It is now finding increased
application as a decision procedure for Satisfiability Modulo Theories (SMT)
solvers when working with non-linear real arithmetic. We discuss the potentials
from increased focus on the logical structure of the input brought by the SMT
applications and SC-Square project, particularly the presence of equational
constraints. We also highlight the challenges for exploiting these: primitivity
restrictions, well-orientedness questions, and the prospect of incrementality.
","['\nJames H. Davenport\n', '\nMatthew England\n']",Accepted into proceedings of MACIS 2017,"In: Blomer J., Kotsireas I., Kutsia T., Simos D. (eds)
  Mathematical Aspects of Computer and Information Sciences (Proc. MACIS '17),
  pp. 280-285. (Lecture Notes in Computer Science 10693). Springer
  International, 2017",http://dx.doi.org/10.1007/978-3-319-72453-9_22,cs.SC,"['cs.SC', '68W30, 03C10', 'I.1.2']",10.1007/978-3-319-72453-9_22,,[]
Automatic Differentiation for Tensor Algebras,http://arxiv.org/abs/1711.01348v1,2017-11-03T22:24:47Z,2017-11-03T22:24:47Z,"  Kjolstad et. al. proposed a tensor algebra compiler. It takes expressions
that define a tensor element-wise, such as $f_{ij}(a,b,c,d) =
\exp\left[-\sum_{k=0}^4 \left((a_{ik}+b_{jk})^2\, c_{ii} + d_{i+k}^3 \right)
\right]$, and generates the corresponding compute kernel code.
  For machine learning, especially deep learning, it is often necessary to
compute the gradient of a loss function $l(a,b,c,d)=l(f(a,b,c,d))$ with respect
to parameters $a,b,c,d$. If tensor compilers are to be applied in this field,
it is necessary to derive expressions for the derivatives of element-wise
defined tensors, i.e. expressions for $(da)_{ik}=\partial l/\partial a_{ik}$.
  When the mapping between function indices and argument indices is not 1:1,
special attention is required. For the function $f_{ij} (x) = x_i^2$, the
derivative of the loss is $(dx)_i=\partial l/\partial x_i=\sum_j
(df)_{ij}2x_i$; the sum is necessary because index $j$ does not appear in the
indices of $f$. Another example is $f_{i}(x)=x_{ii}^2$, where $x$ is a matrix;
here we have $(dx)_{ij}=\delta_{ij}(df)_i2x_{ii}$; the Kronecker delta is
necessary because the derivative is zero for off-diagonal elements. Another
indexing scheme is used by $f_{ij}(x)=\exp x_{i+j}$; here the correct
derivative is $(dx)_{k}=\sum_i (df)_{i,k-i} \exp x_{k}$, where the range of the
sum must be chosen appropriately.
  In this publication we present an algorithm that can handle any case in which
the indices of an argument are an arbitrary linear combination of the indices
of the function, thus all the above examples can be handled. Sums (and their
ranges) and Kronecker deltas are automatically inserted into the derivatives as
necessary. Additionally, the indices are transformed, if required (as in the
last example). The algorithm outputs a symbolic expression that can be
subsequently fed into a tensor algebra compiler.
  Source code is provided.
","['\nSebastian Urban\n', '\nPatrick van der Smagt\n']",Technical Report,,http://arxiv.org/abs/1711.01348v1,cs.SC,"['cs.SC', 'stat.ML']",,,[]
Counting Roots of Polynomials Over Prime Power Rings,http://arxiv.org/abs/1711.01355v1,2017-11-03T23:02:22Z,2017-11-03T23:02:22Z,"  Suppose $p$ is a prime, $t$ is a positive integer, and
$f\!\in\!\mathbb{Z}[x]$ is a univariate polynomial of degree $d$ with
coefficients of absolute value $<\!p^t$. We show that for any fixed $t$, we can
compute the number of roots in $\mathbb{Z}/(p^t)$ of $f$ in deterministic time
$(d+\log p)^{O(1)}$. This fixed parameter tractability appears to be new for
$t\!\geq\!3$. A consequence for arithmetic geometry is that we can efficiently
compute Igusa zeta functions $Z$, for univariate polynomials, assuming the
degree of $Z$ is fixed.
","['\nQi Cheng\n', '\nShuhong Gao\n', '\nJ. Maurice Rojas\n', '\nDaqing Wan\n']","title page, plus 11 pages, no illustrations, submitted to a
  conference",Open Book Series 2 (2019) 191-205,http://dx.doi.org/10.2140/obs.2019.2.191,math.NT,"['math.NT', 'cs.CC', 'cs.SC']",10.2140/obs.2019.2.191,,[]
On the bit-size of non-radical triangular sets,http://arxiv.org/abs/1710.06396v1,2017-10-17T17:07:25Z,2017-10-17T17:07:25Z,"  We present upper bounds on the bit-size of coefficients of non-radical
lexicographical Groebner bases in purely triangular form (triangular sets) of
dimension zero. This extends a previous work [Dahan-Schost, Issac'2004],
constrained to radical triangular sets; it follows the same technical steps,
based on interpolation. However, key notion of height of varieties is not
available for points with multiplicities; therefore the bounds obtained are
less universal and depend on some input data. We also introduce a related
family of non- monic polynomials that have smaller coefficients, and smaller
bounds. It is not obvious to compute them from the initial triangular set
though.
",['\nXavier Dahan\n'],Extended abstract,,http://arxiv.org/abs/1710.06396v1,cs.SC,"['cs.SC', 'I.1.2; G.1.1']",,,[]
Univariate Contraction and Multivariate Desingularization of Ore Ideals,http://arxiv.org/abs/1710.07445v1,2017-10-20T08:03:55Z,2017-10-20T08:03:55Z,"  Ore operators with polynomial coefficients form a common algebraic
abstraction for representing D-finite functions. They form the Ore ring
$K(x)[D_x]$, where $K$ is the constant field. Suppose $K$ is the quotient field
of some principal ideal domain $R$. The ring $R[x][D_x]$ consists of elements
in $K(x)[D_x]$ without ""denominator"".
  Given $L \in K(x)[D_x]$, it generates a left ideal $I$ in $K(x)[D_x]$. We
call $I \cap R[x][D_x]$ the univariate contraction of $I$.
  When $L$ is a linear ordinary differential or difference operator, we design
a contraction algorithm for $L$ by using desingularized operators as proposed
by Chen, Jaroschek, Kauers and Singer. When $L$ is an ordinary differential
operator and $R = K$, our algorithm is more elementary than known algorithms.
In other cases, our results are new.
  We propose the notion of completely desingularized operators, study their
properties, and design an algorithm for computing them. Completely
desingularized operators have interesting applications such as certifying
integer sequences and checking special cases of a conjecture of Krattenthaler.
  A D-finite system is a finite set of linear homogeneous partial differential
equations in several variables, whose solution space is of finite dimension.
For such systems, we give the notion of a singularity in terms of the
polynomials appearing in them. We show that a point is a singularity of the
system unless it admits a basis of power series solutions in which the starting
monomials are as small as possible with respect to some term order. Then a
singularity is apparent if the system admits a full basis of power series
solutions, the starting terms of which are not as small as possible. We prove
that apparent singularities in the multivariate case can be removed like in the
univariate case by adding suitable additional solutions to the original system.
",['\nYi Zhang\n'],,,http://arxiv.org/abs/1710.07445v1,cs.SC,['cs.SC'],,,[]
On the Annihilator Ideal of an Inverse Form,http://arxiv.org/abs/1710.07731v2,2017-10-20T23:51:08Z,2018-05-11T05:20:18Z,"  Let $K$ be a field. We simplify and extend work of Althaler \& D\""ur on
finite sequences over $K$ by regarding $K[x^{-1},z^{-1}]$ as a $K[x,z]$ module,
and studying forms in $K[x^{-1},z^{-1}]$ from first principles. Then we apply
our results to finite sequences.
  First we define the annihilator ideal $I_F$ of a non-zero form $F\in
K[x^{-1},z^{-1}]$, a homogeneous ideal. We inductively construct an ordered
pair ($f_1$\,,\,$f_2$) of forms which generate $I_F$\,; our generators are
special in that $z$ does not divide the leading grlex monomial of $f_1$ but $z$
divides $f_2$\,, and the sum of their total degrees is always $2-|F|$, where
$|F|$ is the total degree of $F$. We show that $f_1,f_2$ is a maximal regular
sequence for $I_F$, so that the height of $I_F$ is 2. The corresponding
algorithm is $\sim |F|^2/2$.
  The row vector obtained by accumulating intermediate forms of the
construction gives a minimal grlex Gr\""obner basis for $I_F$ for no extra
computational cost other than storage and apply this to determining $\dim_K
(K[x,z] /I_F)$\,. We show that either the form vector is reduced or a monomial
of $f_1$ can be reduced by $f_2$\,. This enables us to efficiently construct
the unique reduced Gr\""obner basis for $I_F$ from the vector extension of our
algorithm.
  Then we specialise to the inverse form of a finite sequence, obtaining
generator forms for its annihilator ideal and a corresponding algorithm which
does not use the last 'length change' of Massey. We compute the intersection of
two annihilator ideals using syzygies in $K[x,z]^5$. This improves a result of
Althaler \& D\""ur. Finally, dehomogenisation induces a one-to-one
correspondence ($f_1$\,,$f_2$) $\mapsto$ (minimal polynomial, auxiliary
polynomial), the output of the author's variant of the Berlekamp-Massey
algorithm. So we can also solve the LFSR synthesis problem via the
corresponding algorithm for sequences.
",['\nGraham H. Norton\n'],"We have improved the proof of the main construction, made minor
  improvements to the presentation and corrected some typos. We have also moved
  Subsection 4.5 on the maximal regular sequence to ArXiv 1805.03995",J. AAECC (2017) 28: 31-78,http://dx.doi.org/10.1007/s00200-016-0295-6,cs.SC,"['cs.SC', 'I.1']",10.1007/s00200-016-0295-6,,[]
"Definite Sums of Hypergeometric Terms and Limits of P-Recursive
  Sequences",http://arxiv.org/abs/1710.08566v1,2017-10-24T01:18:29Z,2017-10-24T01:18:29Z,"  The ubiquity of the class of D-finite functions and P-recursive sequences in
symbolic computation is widely recognized. In this thesis, the presented work
consists of two parts related to this class.
  In the first part, we generalize the reduction-based creative telescoping
algorithms to the hypergeometric setting, which allows to deal with definite
sums of hypergeometric terms more quickly. We first modify the
Abramov-Petkovsek reduction, and then design a new algorithm to compute minimal
telescopers for bivariate hypergeometric terms based on the modified reduction.
This new algorithm can avoid the costly computation of certificates, and
outperforms the classical Zeilberger algorithm no matter whether certificates
are computed or not according to the computational experiments. Moreover, we
also derive order bounds for minimal telescopers. These bounds are sometimes
better, and never worse than the known ones.
  In the second part of the thesis, we study the class of D-finite numbers. It
consists of the limits of convergent P-recursive sequences. Typically, this
class contains many well-known mathematical constants in addition to the
algebraic numbers. Our definition of the class of D-finite numbers depends on
two subrings of the field of complex numbers. We investigate how different
choices of these two subrings affect the class. Moreover, we show that D-finite
numbers over the Gaussian rational field are essentially the same as the values
of D-finite functions at non-singular algebraic number arguments (so-called the
regular holonomic constants). This result makes it easier to recognize certain
numbers as belonging to this class.
",['\nHui Huang\n'],PhD thesis,,http://arxiv.org/abs/1710.08566v1,cs.SC,['cs.SC'],,,[]
Lower bounds on the number of realizations of rigid graphs,http://arxiv.org/abs/1710.08237v2,2017-10-23T12:40:04Z,2018-04-11T15:28:00Z,"  Computing the number of realizations of a minimally rigid graph is a
notoriously difficult problem. Towards this goal, for graphs that are minimally
rigid in the plane, we take advantage of a recently published algorithm, which
is the fastest available method, although its complexity is still exponential.
Combining computational results with the theory of constructing new rigid
graphs by gluing, we give a new lower bound on the maximal possible number of
(complex) realizations for graphs with a given number of vertices. We extend
these ideas to rigid graphs in three dimensions and we derive similar lower
bounds, by exploiting data from extensive Gr\""obner basis computations.
","['\nGeorg Grasegger\n', '\nChristoph Koutschan\n', '\nElias Tsigaridas\n']",,"Experimental Mathematics, 2018",http://dx.doi.org/10.1080/10586458.2018.1437851,math.CO,"['math.CO', 'cs.CG', 'cs.SC', '52C25, 05C04, 68W30']",10.1080/10586458.2018.1437851,,[]
Symbolic Computations of First Integrals for Polynomial Vector Fields,http://arxiv.org/abs/1710.08225v2,2017-10-23T12:13:37Z,2018-12-19T15:34:04Z,"  In this article we show how to generalize to the Darbouxian, Liouvillian and
Riccati case the extactic curve introduced by J. Pereira. With this approach,
we get new algorithms for computing, if it exists, a rational, Darbouxian,
Liouvillian or Riccati first integral with bounded degree of a polynomial
planar vector field. We give probabilistic and deterministic algorithms. The
arithmetic complexity of our probabilistic algorithm is in
$\tilde{\mathcal{O}}(N^{\omega+1})$, where $N$ is the bound on the degree of a
representation of the first integral and $\omega \in [2;3]$ is the exponent of
linear algebra. This result improves previous algorithms. Our algorithms have
been implemented in Maple and are available on authors' websites. In the last
section, we give some examples showing the efficiency of these algorithms.
","['\nGuillaume Chèze\nIMT\n', '\nThierry Combot\nIMB\n']",,,http://arxiv.org/abs/1710.08225v2,cs.SC,"['cs.SC', 'cs.CC', 'math.CA', 'math.DS', 'nlin.SI']",,,"['IMT', 'IMB']"
Improved Complexity Bounds for Counting Points on Hyperelliptic Curves,http://arxiv.org/abs/1710.03448v2,2017-10-10T08:42:35Z,2018-06-07T07:13:40Z,"  We present a probabilistic Las Vegas algorithm for computing the local zeta
function of a hyperelliptic curve of genus $g$ defined over $\mathbb{F}_q$. It
is based on the approaches by Schoof and Pila combined with a modeling of the
$\ell$-torsion by structured polynomial systems. Our main result improves on
previously known complexity bounds by showing that there exists a constant
$c>0$ such that, for any fixed $g$, this algorithm has expected time and space
complexity $O((\log q)^{cg})$ as $q$ grows and the characteristic is large
enough.
","['\nSimon Abelard\n', '\nPierrick Gaudry\n', '\nPierre-Jean Spaenlehauer\n']",To appear in Foundations of Computational Mathematics,,http://arxiv.org/abs/1710.03448v2,math.NT,"['math.NT', 'cs.SC', 'math.AG']",,,[]
Compact Formulae in Sparse Elimination,http://arxiv.org/abs/1710.04856v1,2017-10-13T09:43:30Z,2017-10-13T09:43:30Z,"  It has by now become a standard approach to use the theory of sparse (or
toric) elimination, based on the Newton polytope of a polynomial, in order to
reveal and exploit the structure of algebraic systems. This talk surveys
compact formulae, including older and recent results, in sparse elimination. We
start with root bounds and juxtapose two recent formulae: a generating function
of the m-B{\'e}zout bound and a closed-form expression for the mixed volume by
means of a matrix permanent. For the sparse resultant, a bevy of results have
established determinantal or rational formulae for a large class of systems,
starting with Macaulay. The discriminant is closely related to the resultant
but admits no compact formula except for very simple cases. We offer a new
determinantal formula for the discriminant of a sparse multilinear system
arising in computing Nash equilibria. We introduce an alternative notion of
compact formula, namely the Newton polytope of the unknown polynomial. It is
possible to compute it efficiently for sparse resultants, discriminants, as
well as the implicit equation of a parameterized variety. This leads us to
consider implicit matrix representations of geometric objects.
","['\nIoannis Emiris\nAthens, AROMATH\n']",,"ISSAC 2016 - International Symposium on Symbolic and Algebraic
  Computation, Jul 2016, Waterloo, Canada. pp.1 - 8, 2016, ISSAC '16 -
  Proceedings of the ACM on International Symposium on Symbolic and Algebraic
  Computation",http://dx.doi.org/10.1145/2930889.2930943,cs.CC,"['cs.CC', 'cs.CG', 'cs.DM', 'cs.SC']",10.1145/2930889.2930943,,"['Athens, AROMATH']"
"Faster Interpolation Algorithms for Sparse Multivariate Polynomials
  Given by Straight-Line Programs\",http://arxiv.org/abs/1709.08979v4,2017-09-26T12:40:09Z,2018-07-17T13:01:23Z,"  In this paper, we propose new deterministic and Monte Carlo interpolation
algorithms for sparse multivariate polynomials represented by straight-line
programs. Let $f$ be an $n$-variate polynomial given by a straight-line
program, which has a degree bound $D$ and a term bound $T$. Our deterministic
algorithm is quadratic in $n,T$ and cubic in $\log D$ in the Soft-Oh sense,
which has better complexities than existing deterministic interpolation
algorithms in most cases. Our Monte Carlo interpolation algorithms have better
complexities than existing Monte Carlo interpolation algorithms and are the
first algorithms whose complexities are linear in $nT$ in the Soft-Oh sense.
Since $nT$ is a factor of the size of $f$, our Monte Carlo algorithms are
optimal in $n$ and $T$ in the Soft-Oh sense.
","['\nQiao-Long Huang\n', '\nXiao-Shan Gao\n']",,,http://arxiv.org/abs/1709.08979v4,cs.SC,['cs.SC'],,,[]
"Deterministic Interpolation of Sparse Black-box Multivariate Polynomials
  using Kronecker Type Substitutions",http://arxiv.org/abs/1710.01301v2,2017-10-03T11:38:08Z,2018-08-08T07:26:33Z,"  In this paper, we propose two new deterministic interpolation algorithms for
a sparse multivariate polynomial given as a standard black-box by introducing
new Kronecker type substitutions. Let $f\in \RB[x_1,\dots,x_n]$ be a sparse
black-box polynomial with a degree bound $D$. When $\RB=\C$ or a finite field,
our algorithms either have better bit complexity or better bit complexity in
$D$ than existing deterministic algorithms. In particular, in the case of
deterministic algorithms for standard black-box models, our second algorithm
has the current best complexity in $D$ which is the dominant factor in the
complexity.
","['\nQiao-Long Huang\n', '\nXiao-Shan Gao\n']",,,http://arxiv.org/abs/1710.01301v2,cs.SC,['cs.SC'],,,[]
"In-depth comparison of the Berlekamp -- Massey -- Sakata and the
  Scalar-FGLM algorithms: the non adaptive variants",http://arxiv.org/abs/1709.07168v1,2017-09-21T06:17:42Z,2017-09-21T06:17:42Z,"  We compare thoroughly the Berlekamp -- Massey -- Sakata algorithm and the
Scalar-FGLM algorithm, which compute both the ideal of relations of a
multi-dimensional linear recurrent sequence. Suprisingly, their behaviors
differ. We detail in which way they do and prove that it is not possible to
tweak one of the algorithms in order to mimic exactly the behavior of the
other.
","['\nJérémy Berthomieu\nPolSys\n', '\nJean-Charles Faugère\nPolSys\n']",,,http://arxiv.org/abs/1709.07168v1,cs.SC,['cs.SC'],,,"['PolSys', 'PolSys']"
"Analytic Combinatorics in Several Variables: Effective Asymptotics and
  Lattice Path Enumeration",http://arxiv.org/abs/1709.05051v1,2017-09-15T04:09:09Z,2017-09-15T04:09:09Z,"  The field of analytic combinatorics, which studies the asymptotic behaviour
of sequences through analytic properties of their generating functions, has led
to the development of deep and powerful tools with applications across
mathematics and the natural sciences. In addition to the now classical
univariate theory, recent work in the study of analytic combinatorics in
several variables (ACSV) has shown how to derive asymptotics for the
coefficients of certain D-finite functions represented by diagonals of
multivariate rational functions. We give a pedagogical introduction to the
methods of ACSV from a computer algebra viewpoint, developing rigorous
algorithms and giving the first complexity results in this area under
conditions which are broadly satisfied. Furthermore, we give several new
applications of ACSV to the enumeration of lattice walks restricted to certain
regions. In addition to proving several open conjectures on the asymptotics of
such walks, a detailed study of lattice walk models with weighted steps is
undertaken.
",['\nStephen Melczer\n'],"PhD thesis, University of Waterloo and ENS Lyon - 259 pages",,http://arxiv.org/abs/1709.05051v1,math.CO,"['math.CO', 'cs.SC']",,,[]
"High Degree Sum of Squares Proofs, Bienstock-Zuckerberg hierarchy and
  Chvatal-Gomory cuts",http://arxiv.org/abs/1709.07966v7,2017-09-22T22:18:47Z,2019-12-23T13:47:16Z,"  Chvatal-Gomory (CG) cuts and the Bienstock-Zuckerberg hierarchy capture
useful linear programs that the standard bounded degree Lasserre/Sum-of-Squares
SOS hierarchy fails to capture.
  In this paper we present a novel polynomial time SOS hierarchy for 0/1
problems with a custom subspace of high degree polynomials (not the standard
subspace of low-degree polynomials). We show that the new SOS hierarchy
recovers the Bienstock-Zuckerberg hierarchy. Our result implies a linear
program that reproduces the Bienstock-Zuckerberg hierarchy as a polynomial
sized, efficiently constructive extended formulation that satisfies all
constant pitch inequalities. The construction is also very simple, and it is
fully defined by giving the supporting polynomials. Moreover, for a class of
polytopes (e.g. set covering and packing problems), the resulting SOS hierarchy
optimizes in polynomial time over the polytope resulting from any constant
rounds of CG-cuts, up to an arbitrarily small error.
  Arguably, this is the first example where different basis functions can be
useful in asymmetric situations to obtain a hierarchy of relaxations.
",['\nMonaldo Mastrolilli\n'],Revised version with some small typos corrected,,http://arxiv.org/abs/1709.07966v7,math.OC,"['math.OC', 'cs.CC', 'cs.SC']",,,[]
"Exact Inference for Relational Graphical Models with Interpreted
  Functions: Lifted Probabilistic Inference Modulo Theories",http://arxiv.org/abs/1709.01122v1,2017-09-04T19:08:37Z,2017-09-04T19:08:37Z,"  Probabilistic Inference Modulo Theories (PIMT) is a recent framework that
expands exact inference on graphical models to use richer languages that
include arithmetic, equalities, and inequalities on both integers and real
numbers. In this paper, we expand PIMT to a lifted version that also processes
random functions and relations. This enhancement is achieved by adapting
Inversion, a method from Lifted First-Order Probabilistic Inference literature,
to also be modulo theories. This results in the first algorithm for exact
probabilistic inference that efficiently and simultaneously exploits random
relations and functions, arithmetic, equalities and inequalities.
","['\nRodrigo de Salvo Braz\n', ""\nCiaran O'Reilly\n""]","Appeared in the Uncertainty in Artificial Intelligence Conference,
  August 2017",,http://arxiv.org/abs/1709.01122v1,cs.AI,"['cs.AI', 'cs.SC']",,,[]
"A Curious Family of Binomial Determinants That Count Rhombus Tilings of
  a Holey Hexagon",http://arxiv.org/abs/1709.02616v2,2017-09-08T09:40:47Z,2018-07-03T08:56:26Z,"  We evaluate a curious determinant, first mentioned by George Andrews in 1980
in the context of descending plane partitions. Our strategy is to combine the
famous Desnanot-Jacobi-Dodgson identity with automated proof techniques. More
precisely, we follow the holonomic ansatz that was proposed by Doron Zeilberger
in 2007. We derive a compact and nice formula for Andrews's determinant, and
use it to solve a challenge problem that we posed in a previous paper. By
noting that Andrews's determinant is a special case of a two-parameter family
of determinants, we find closed forms for several one-parameter subfamilies.
The interest in these determinants arises because they count cyclically
symmetric rhombus tilings of a hexagon with several triangular holes inside.
","['\nChristoph Koutschan\n', '\nThotsaporn Thanatipanonda\n']",,"Journal of Combinatorial Theory, Series A, vol. 166, pp. 352-381,
  2019",http://dx.doi.org/10.1016/j.jcta.2019.03.001,math.CO,"['math.CO', 'cs.SC']",10.1016/j.jcta.2019.03.001,,[]
Root Separation for Trinomials,http://arxiv.org/abs/1709.03294v3,2017-09-11T08:42:01Z,2018-10-25T12:03:36Z,"  We give a separation bound for the complex roots of a trinomial $f \in
\mathbb{Z}[X]$. The logarithm of the inverse of our separation bound is
polynomial in the size of the sparse encoding of $f$; in particular, it is
polynomial in $\log (\deg f)$. It is known that no such bound is possible for
4-nomials (polynomials with 4 monomials). For trinomials, the classical results
(which are based on the degree of $f$ rather than the number of monomials) give
separation bounds that are exponentially worse.As an algorithmic application,
we show that the number of real roots of a trinomial $f$ can be computed in
time polynomial in the size of the sparse encoding of~$f$. The same problem is
open for 4-nomials.
",['\nPascal Koiran\nLIP\n'],,,http://arxiv.org/abs/1709.03294v3,cs.SC,"['cs.SC', 'cs.CC', 'math.NT']",,,['LIP']
Strassen's 2x2 matrix multiplication algorithm: A conceptual perspective,http://arxiv.org/abs/1708.08083v2,2017-08-27T13:27:32Z,2019-02-13T10:59:50Z,"  The main purpose of this paper is pedagogical.
  Despite its importance, all proofs of the correctness of Strassen's famous
1969 algorithm to multiply two 2x2 matrices with only seven multiplications
involve some basis-dependent calculations such as explicitly multiplying
specific 2x2 matrices, expanding expressions to cancel terms with opposing
signs, or expanding tensors over the standard basis. This makes the proof
nontrivial to memorize and many presentations of the proof avoid showing all
the details and leave a significant amount of verifications to the reader.
  In this note we give a short, self-contained, basis-independent proof of the
existence of Strassen's algorithm that avoids these types of calculations. We
achieve this by focusing on symmetries and algebraic properties.
  Our proof can be seen as a coordinate-free version of the construction of
Clausen from 1988, combined with recent work on the geometry of Strassen's
algorithm by Chiantini, Ikenmeyer, Landsberg, and Ottaviani from 2016.
","['\nChristian Ikenmeyer\n', '\nVladimir Lysikov\n']",6 pages,"Annali dell'Universit\`a di Ferrara. Sezione VII: Science
  matematiche. 65(2), pp. 241-248. (2019)",http://dx.doi.org/10.1007/s11565-019-00318-1,cs.DS,"['cs.DS', 'cs.SC', 'I.1.2']",10.1007/s11565-019-00318-1,,[]
Syzygies among reduction operators,http://arxiv.org/abs/1708.08709v3,2017-08-29T11:52:48Z,2018-04-09T08:28:24Z,"  We introduce the notion of syzygy for a set of reduction operators and relate
it to the notion of syzygy for presentations of algebras. We give a method for
constructing a linear basis of the space of syzygies for a set of reduction
operators. We interpret these syzygies in terms of the confluence property from
rewriting theory. This enables us to optimise the completion procedure for
reduction operators based on a criterion for detecting useless reductions. We
illustrate this criterion with an example of construction of commutative
Gr{\""o}bner basis.
","['\nCyrille Chenavier\nIRIF, PI.R2\n']",,,http://arxiv.org/abs/1708.08709v3,math.RA,"['math.RA', 'cs.SC']",,,"['IRIF, PI.R2']"
Faster Multiplication for Long Binary Polynomials,http://arxiv.org/abs/1708.09746v3,2017-08-31T14:30:44Z,2018-01-05T15:30:47Z,"  We set new speed records for multiplying long polynomials over finite fields
of characteristic two. Our multiplication algorithm is based on an additive FFT
(Fast Fourier Transform) by Lin, Chung, and Huang in 2014 comparing to
previously best results based on multiplicative FFTs. Both methods have similar
complexity for arithmetic operations on underlying finite field; however, our
implementation shows that the additive FFT has less overhead. For further
optimization, we employ a tower field construction because the multipliers in
the additive FFT naturally fall into small subfields, which leads to speed-ups
using table-lookup instructions in modern CPUs. Benchmarks show that our method
saves about $40 \%$ computing time when multiplying polynomials of $2^{28}$ and
$2^{29}$ bits comparing to previous multiplicative FFT implementations.
","['\nMing-Shing Chen\n', '\nChen-Mou Cheng\n', '\nPo-Chun Kuo\n', '\nWen-Ding Li\n', '\nBo-Yin Yang\n']",,,http://arxiv.org/abs/1708.09746v3,cs.SC,"['cs.SC', 'math.NT']",,,[]
Counting Roots of Polynomials over $\mathbb{Z}/p^2\mathbb{Z}$,http://arxiv.org/abs/1708.04713v2,2017-08-15T22:58:59Z,2017-12-12T20:14:28Z,"  Until recently, the only known method of finding the roots of polynomials
over prime power rings, other than fields, was brute force. One reason for this
is the lack of a division algorithm, obstructing the use of greatest common
divisors. Fix a prime $p \in \mathbb{Z}$ and $f \in ( \mathbb{Z}/p^n \mathbb{Z}
) [x]$ any nonzero polynomial of degree $d$ whose coefficients are not all
divisible by $p$. For the case $n=2$, we prove a new efficient algorithm to
count the roots of $f$ in $\mathbb{Z}/p^2\mathbb{Z}$ within time polynomial in
$(d+\operatorname{size}(f)+\log{p})$, and record a concise formula for the
number of roots, formulated by Cheng, Gao, Rojas, and Wan.
","['\nTrajan Hammonds\n', '\nJeremy Johnson\n', '\nAngela Patini\n', '\nRobert M. Walker\n']","6 pages, comments welcome! Rewritten to address referee feedback.
  Bibliography updated. There is a new Corollary 3.3 giving a formula for the
  number of degenerate roots modulo p that fail to lift to roots modulo p^2","Houston Journal of Mathematics (2018), Vol. 44, no. 4, pp.
  1111-1119",http://arxiv.org/abs/1708.04713v2,math.NT,"['math.NT', 'cs.CC', 'cs.SC', 'math.AC', '11Y05, 11Y16, 13F20 (Primary). 11M38, 11S05, 11T06 (Secondary)']",,,[]
"Robust Computer Algebra, Theorem Proving, and Oracle AI",http://arxiv.org/abs/1708.02553v2,2017-08-08T16:35:40Z,2017-12-31T18:43:40Z,"  In the context of superintelligent AI systems, the term ""oracle"" has two
meanings. One refers to modular systems queried for domain-specific tasks.
Another usage, referring to a class of systems which may be useful for
addressing the value alignment and AI control problems, is a superintelligent
AI system that only answers questions. The aim of this manuscript is to survey
contemporary research problems related to oracles which align with long-term
research goals of AI safety. We examine existing question answering systems and
argue that their high degree of architectural heterogeneity makes them poor
candidates for rigorous analysis as oracles. On the other hand, we identify
computer algebra systems (CASs) as being primitive examples of domain-specific
oracles for mathematics and argue that efforts to integrate computer algebra
systems with theorem provers, systems which have largely been developed
independent of one another, provide a concrete set of problems related to the
notion of provable safety that has emerged in the AI safety community. We
review approaches to interfacing CASs with theorem provers, describe
well-defined architectural deficiencies that have been identified with CASs,
and suggest possible lines of research and practical software projects for
scientists interested in AI safety.
","['\nGopal P. Sarma\n', '\nNick J. Hay\n']","15 pages, 3 figures",Informatica Vol. 41 No. 3 (2017),http://arxiv.org/abs/1708.02553v2,cs.AI,"['cs.AI', 'cs.SC']",,,[]
"Complexity of Model Testing for Dynamical Systems with Toric Steady
  States",http://arxiv.org/abs/1707.07650v3,2017-07-24T17:17:59Z,2019-12-06T12:16:10Z,"  In this paper we investigate the complexity of model selection and model
testing for dynamical systems with toric steady states. Such systems frequently
arise in the study of chemical reaction networks. We do this by formulating
these tasks as a constrained optimization problem in Euclidean space. This
optimization problem is known as a Euclidean distance problem; the complexity
of solving this problem is measured by an invariant called the Euclidean
distance (ED) degree. We determine closed-form expressions for the ED degree of
the steady states of several families of chemical reaction networks with toric
steady states and arbitrarily many reactions. To illustrate the utility of this
work we show how the ED degree can be used as a tool for estimating the
computational cost of solving the model testing and model selection problems.
","['\nMichael F Adamer\n', '\nMartin Helmer\n']",,,http://arxiv.org/abs/1707.07650v3,q-bio.QM,"['q-bio.QM', 'cs.SC', 'math.AG', 'q-bio.MN']",,,[]
"Dealing with Rational Second Order Ordinary Differential Equations where
  both Darboux and Lie Find It Difficult: The $S$-function Method",http://arxiv.org/abs/1707.09007v1,2017-07-27T19:22:36Z,2017-07-27T19:22:36Z,"  Here we present a new approach to search for first order invariants (first
integrals) of rational second order ordinary differential equations. This
method is an alternative to the Darbouxian and symmetry approaches. Our
procedure can succeed in many cases where these two approaches fail. We also
present here a Maple implementation of the theoretical results and methods,
hereby introduced, in a computational package -- {\it InSyDE}. The package is
designed, apart from materializing the algorithms presented, to provide a set
of tools to allow the user to analyse the intermediary steps of the process.
","['\nJ. Avellar\n', '\nM. S. Cardoso\n', '\nL. G. S. Duarte\n', '\nL. A. C. P. da Mota\n']",42 pages,"Computer Physics Communications, V. 234c, P. 302-314, 2019",http://dx.doi.org/10.1016/j.cpc.2018.05.009,math-ph,"['math-ph', 'cs.SC', 'math.MP']",10.1016/j.cpc.2018.05.009,,[]
The PSLQ Algorithm for Empirical Data,http://arxiv.org/abs/1707.05037v2,2017-07-17T08:27:42Z,2018-06-21T05:43:55Z,"  The celebrated integer relation finding algorithm PSLQ has been successfully
used in many applications. PSLQ was only analyzed theoretically for exact input
data, however, when the input data are irrational numbers, they must be
approximate ones due to the finite precision of the computer. When the
algorithm takes empirical data (inexact data with error bounded) instead of
exact real numbers as its input, how do we theoretically ensure the output of
the algorithm to be an exact integer relation?
  In this paper, we investigate the PSLQ algorithm for empirical data as its
input. Firstly, we give a termination condition for this case. Secondly, we
analyze a perturbation on the hyperplane matrix constructed from the input data
and hence disclose a relationship between the accuracy of the input data and
the output quality (an upper bound on the absolute value of the inner product
of the exact data and the computed integer relation), which naturally leads to
an error control strategy for PSLQ. Further, we analyze the complexity bound of
the PSLQ algorithm for empirical data. Examples on transcendental numbers and
algebraic numbers show the meaningfulness of our error control strategy.
","['\nYong Feng\n', '\nJingwei Chen\n', '\nWenyuan Wu\n']",23 pages; Math. Comp,,http://arxiv.org/abs/1707.05037v2,cs.SC,"['cs.SC', 'math.NT', '11A05, 11Y16, 68-04']",,,[]
"A non-commutative algorithm for multiplying 5x5 matrices using 99
  multiplications",http://arxiv.org/abs/1707.06860v2,2017-07-18T08:57:28Z,2017-12-20T12:28:38Z,"  We present a non-commutative algorithm for multiplying 5x5 matrices using 99
multiplications. This algorithm is a minor modification of Makarov's algorithm
which exhibit the previous best known bound with 100 multiplications.
",['\nAlexandre Sedoglavic\nCRIStAL\n'],,,http://arxiv.org/abs/1707.06860v2,cs.CC,"['cs.CC', 'cs.SC']",,,['CRIStAL']
Algorithms for zero-dimensional ideals using linear recurrent sequences,http://arxiv.org/abs/1707.01971v1,2017-07-06T21:34:07Z,2017-07-06T21:34:07Z,"  Inspired by Faug\`ere and Mou's sparse FGLM algorithm, we show how using
linear recurrent multi-dimensional sequences can allow one to perform
operations such as the primary decomposition of an ideal, by computing the
annihilator of one or several such sequences.
","['\nVincent Neiger\n', '\nHamid Rahkooy\n', '\nÉric Schost\n']","LNCS, Computer Algebra in Scientific Computing CASC 2017",,http://arxiv.org/abs/1707.01971v1,cs.SC,['cs.SC'],,,[]
Measured Multiseries and Integration,http://arxiv.org/abs/1707.02235v1,2017-07-07T15:42:11Z,2017-07-07T15:42:11Z,"  A paper by Bruno Salvy and the author introduced measured multiseries and
gave an algorithm to compute these for a large class of elementary functions,
modulo a zero-equivalence method for constants. This gave a theoretical
background for the implementation that Salvy was developing at that time. The
main result of the present article is an algorithm to calculate measured
multiseries for integrals of functions of the form h*sin G, where h and G
belong to a Hardy field. The process can reiterated with the resulting algebra,
and also applied to solutions of a second order differential equation of a
particular form.
",['\nJohn Shackell\n'],"28 pages, 0 figures",,http://arxiv.org/abs/1707.02235v1,cs.SC,"['cs.SC', 'I.1.2']",,,[]
Compiling LATEX to computer algebra-enabled HTML5,http://arxiv.org/abs/1707.01271v1,2017-07-05T09:24:46Z,2017-07-05T09:24:46Z,"  This document explains how to create or modify an existing LATEX document
with commands enabling computations in the HTML5 output: when the reader opens
the HTML5 output, he can run a computation in his browser, or modify the
command to be executed and run it. This is done by combining different
softwares: hevea for compilation to HTML5, giac.js for the CAS computing kernel
(itself compiled from the C++ Giac library with emscripten), and a modified
version of itex2MML for fast and nice rendering in MathML in browsers that
support MathML.
",['\nBernard Parisse\n'],"The interactive HTML5/MathML version of the document is available at
  https://www-fourier.ujf-grenoble.fr/~parisse/giac/castex.htmlThe LaTeX source
  will not compile properly to PDF without installing the software described in
  the document",,http://arxiv.org/abs/1707.01271v1,cs.SC,"['cs.SC', 'cs.MS']",,,[]
"$\mathcal{P}$-schemes and Deterministic Polynomial Factoring over Finite
  Fields",http://arxiv.org/abs/1706.10028v2,2017-06-30T05:42:59Z,2017-09-23T09:44:03Z,"  We introduce a family of mathematical objects called $\mathcal{P}$-schemes,
where $\mathcal{P}$ is a poset of subgroups of a finite group $G$. A
$\mathcal{P}$-scheme is a collection of partitions of the right coset spaces
$H\backslash G$, indexed by $H\in\mathcal{P}$, that satisfies a list of axioms.
These objects generalize the classical notion of association schemes as well as
the notion of $m$-schemes (Ivanyos et al. 2009).
  Based on $\mathcal{P}$-schemes, we develop a unifying framework for the
problem of deterministic factoring of univariate polynomials over finite fields
under the generalized Riemann hypothesis (GRH).
",['\nZeyu Guo\n'],PhD thesis,,http://arxiv.org/abs/1706.10028v2,cs.CC,"['cs.CC', 'cs.SC', 'math.GR', 'math.NT']",,,[]
"Symbolic Versus Numerical Computation and Visualization of Parameter
  Regions for Multistationarity of Biological Networks",http://arxiv.org/abs/1706.08794v1,2017-06-27T11:45:35Z,2017-06-27T11:45:35Z,"  We investigate models of the mitogenactivated protein kinases (MAPK) network,
with the aim of determining where in parameter space there exist multiple
positive steady states. We build on recent progress which combines various
symbolic computation methods for mixed systems of equalities and inequalities.
We demonstrate that those techniques benefit tremendously from a newly
implemented graph theoretical symbolic preprocessing method. We compare
computation times and quality of results of numerical continuation methods with
our symbolic approach before and after the application of our preprocessing.
","['\nMatthew England\n', '\nHassan Errami\n', '\nDima Grigoriev\n', '\nOvidiu Radulescu\n', '\nThomas Sturm\n', '\nAndreas Weber\n']",Accepted into Proc. CASC 2017,"In: V. Gerdt, W. Koepf, W. Seiler, E. Vorozhtsov, eds. Computer
  Algebra in Scientific Computing (Proc. CASC '17), pp. 93-108. (Lecture Notes
  in Computer Science, 10490). Springer International, 2017",http://dx.doi.org/10.1007/978-3-319-66320-3_8,cs.SC,"['cs.SC', 'I.1.4']",10.1007/978-3-319-66320-3_8,,[]
"Algorithms for Weighted Sums of Squares Decomposition of Non-negative
  Univariate Polynomials",http://arxiv.org/abs/1706.03941v1,2017-06-13T08:04:53Z,2017-06-13T08:04:53Z,"  It is well-known that every non-negative univariate real polynomial can be
written as the sum of two polynomial squares with real coefficients. When one
allows a weighted sum of finitely many squares instead of a sum of two squares,
then one can choose all coefficients in the representation to lie in the field
generated by the coefficients of the polynomial.
  In this article, we describe, analyze and compare both from the theoretical
and practical points of view, two algorithms computing such a weighted sums of
squares decomposition for univariate polynomials with rational coefficients.
  The first algorithm, due to the third author relies on real root isolation,
quadratic approximations of positive polynomials and square-free decomposition
but its complexity was not analyzed. We provide bit complexity estimates, both
on runtime and output size of this algorithm. They are exponential in the
degree of the input univariate polynomial and linear in the maximum bitsize of
its complexity. This analysis is obtained using quantifier elimination and root
isolation bounds.
  The second algorithm, due to Chevillard, Harrison, Joldes and Lauter, relies
on complex root isolation and square-free decomposition and has been introduced
for certifying positiveness of polynomials in the context of computer
arithmetics. Again, its complexity was not analyzed. We provide bit complexity
estimates, both on runtime and output size of this algorithm, which are
polynomial in the degree of the input polynomial and linear in the maximum
bitsize of its complexity. This analysis is obtained using Vieta's formula and
root isolation bounds.
  Finally, we report on our implementations of both algorithms. While the
second algorithm is, as expected from the complexity result, more efficient on
most of examples, we exhibit families of non-negative polynomials for which the
first algorithm is better.
","['\nVictor Magron\n', '\nMohab Safey El Din\n', '\nMarkus Schweighofer\n']","22 pages, 4 tables",,http://arxiv.org/abs/1706.03941v1,cs.SC,['cs.SC'],,,[]
Beyond Polyhedral Homotopies,http://arxiv.org/abs/1706.03520v1,2017-06-12T09:03:34Z,2017-06-12T09:03:34Z,"  We present a new algorithmic framework which utilizes tropical geometry and
homotopy continuation for solving systems of polynomial equations where some of
the polynomials are generic elements in linear subspaces of the polynomial
ring. This approach generalizes the polyhedral homotopies by Huber and
Sturmfels.
","['\nAnton Leykin\n', '\nJosephine Yu\n']","8 pages, 1 figure",,http://arxiv.org/abs/1706.03520v1,math.AG,"['math.AG', 'cs.SC', '14Q99, 14T05']",,,[]
"Automatic differentiation of hybrid models Illustrated by Diffedge
  Graphic Methodology. (Survey)",http://arxiv.org/abs/1706.03549v1,2017-06-12T10:20:59Z,2017-06-12T10:20:59Z,"  We investigate the automatic differentiation of hybrid models, viz. models
that may contain delays, logical tests and discontinuities or loops. We
consider differentiation with respect to parameters, initial conditions or the
time. We emphasize the case of a small number of derivations and iterated
differentiations are mostly treated with a foccus on high order iterations of
the same derivation. The models we consider may involve arithmetic operations,
elementary functions, logical tests but also more elaborate components such as
delays, integrators, equations and differential equations solvers. This survey
has no pretention to exhaustivity but tries to fil a gap in the litterature
where each kind of of component may be documented, but seldom their common use.
  The general approach is illustrated by computer algebra experiments,
stressing the interest of performing differentiation, whenever possible, on
high level objects, before any translation in Fortran or C code. We include
ordinary differential systems with discontinuity, with a special interest for
those comming from discontinuous Lagrangians.
  We conclude with an overview of the graphic methodology developped in the
Diffedge software for Simulink hybrid models. Not all possibilities are
covered, but the methodology can be adapted. The result of automatic
differentiation is a new block diagram and so it can be easily translated to
produce real time embedded programs.
  We welcome any comments or suggestions of references that we may have missed.
","['\nJohn Masse\n', '\nClara Masse\n', '\nFrançois Ollivier\n']",47 p. Source files from computer experiments available,,http://arxiv.org/abs/1706.03549v1,cs.SY,"['cs.SY', 'cs.SC']",,,[]
Refined Holonomic Summation Algorithms in Particle Physics,http://arxiv.org/abs/1706.03677v2,2017-06-12T14:58:19Z,2017-10-30T11:55:51Z,"  An improved multi-summation approach is introduced and discussed that enables
one to simultaneously handle indefinite nested sums and products in the setting
of difference rings and holonomic sequences. Relevant mathematics is reviewed
and the underlying advanced difference ring machinery is elaborated upon. The
flexibility of this new toolbox contributed substantially to evaluating
complicated multi-sums coming from particle physics. Illustrative examples of
the functionality of the new software package RhoSum are given.
","['\nJohannes Blümlein\n', '\nMark Round\n', '\nCarsten Schneider\n']",Modified Proposition 2.1 and Corollary 2.1,,http://arxiv.org/abs/1706.03677v2,cs.SC,"['cs.SC', 'hep-ph']",,,[]
When is a polynomial ideal binomial after an ambient automorphism?,http://arxiv.org/abs/1706.03629v1,2017-06-12T13:39:19Z,2017-06-12T13:39:19Z,"  Can an ideal I in a polynomial ring k[x] over a field be moved by a change of
coordinates into a position where it is generated by binomials $x^a - cx^b$
with c in k, or by unital binomials (i.e., with c = 0 or 1)? Can a variety be
moved into a position where it is toric? By fibering the G-translates of I over
an algebraic group G acting on affine space, these problems are special cases
of questions about a family F of ideals over an arbitrary base B. The main
results in this general setting are algorithms to find the locus of points in B
over which the fiber of F
  - is contained in the fiber of a second family F' of ideals over B;
  - defines a variety of dimension at least d;
  - is generated by binomials; or
  - is generated by unital binomials.
  A faster containment algorithm is also presented when the fibers of F are
prime. The big-fiber algorithm is probabilistic but likely faster than known
deterministic ones. Applications include the setting where a second group T
acts on affine space, in addition to G, in which case algorithms compute the
set of G-translates of I
  - whose stabilizer subgroups in T have maximal dimension; or
  - that admit a faithful multigrading by $Z^r$ of maximal rank r.
  Even with no ambient group action given, the final application is an
algorithm to
  - decide whether a normal projective variety is abstractly toric.
  All of these loci in B and subsets of G are constructible; in some cases they
are closed.
","['\nLukas Katthän\n', '\nMateusz Michałek\n', '\nEzra Miller\n']",22 pages,,http://arxiv.org/abs/1706.03629v1,math.AC,"['math.AC', 'cs.SC', 'math.AG', 'Primary: 14Q99, 13P99, 14L30, 13A50, 14M25, 68W30, Secondary: 13F20,\n  14D06, 14L40']",,,[]
"On the Complexity of Exact Counting of Dynamically Irreducible
  Polynomials",http://arxiv.org/abs/1706.04392v2,2017-06-14T10:18:09Z,2018-11-19T21:54:36Z,"  We give an efficient algorithm to enumerate all sets of $r\ge 1$ quadratic
polynomials over a finite field, which remain irreducible under iterations and
compositions.
","['\nDomingo Gómez-Pérez\n', '\nLászló Mérai\n', '\nIgor E. Shparlinski\n']",,,http://arxiv.org/abs/1706.04392v2,math.NT,"['math.NT', 'cs.SC', 'math.DS']",,,[]
Computing Canonical Bases of Modules of Univariate Relations,http://arxiv.org/abs/1705.10649v1,2017-05-30T13:56:27Z,2017-05-30T13:56:27Z,"  We study the computation of canonical bases of sets of univariate relations
$(p_1,\ldots,p_m) \in \mathbb{K}[x]^{m}$ such that $p_1 f_1 + \cdots + p_m f_m
= 0$; here, the input elements $f_1,\ldots,f_m$ are from a quotient
$\mathbb{K}[x]^n/\mathcal{M}$, where $\mathcal{M}$ is a $\mathbb{K}[x]$-module
of rank $n$ given by a basis $\mathbf{M}\in\mathbb{K}[x]^{n\times n}$ in
Hermite form. We exploit the triangular shape of $\mathbf{M}$ to generalize a
divide-and-conquer approach which originates from fast minimal approximant
basis algorithms. Besides recent techniques for this approach, we rely on
high-order lifting to perform fast modular products of polynomial matrices of
the form $\mathbf{P}\mathbf{F} \bmod \mathbf{M}$.
  Our algorithm uses $O\tilde{~}(m^{\omega-1}D + n^{\omega} D/m)$ operations in
$\mathbb{K}$, where $D = \mathrm{deg}(\det(\mathbf{M}))$ is the
$\mathbb{K}$-vector space dimension of $\mathbb{K}[x]^n/\mathcal{M}$,
$O\tilde{~}(\cdot)$ indicates that logarithmic factors are omitted, and
$\omega$ is the exponent of matrix multiplication. This had previously only
been achieved for a diagonal matrix $\mathbf{M}$. Furthermore, our algorithm
can be used to compute the shifted Popov form of a nonsingular matrix within
the same cost bound, up to logarithmic factors, as the previously fastest known
algorithm, which is randomized.
","['\nVincent Neiger\n', '\nThi Xuan Vu\n']","8 pages, uses acmart sigconf",,http://dx.doi.org/10.1145/3087604.3087656,cs.SC,['cs.SC'],10.1145/3087604.3087656,,[]
"Fast Computation of the Roots of Polynomials Over the Ring of Power
  Series",http://arxiv.org/abs/1705.10658v1,2017-05-30T14:12:31Z,2017-05-30T14:12:31Z,"  We give an algorithm for computing all roots of polynomials over a univariate
power series ring over an exact field $\mathbb{K}$. More precisely, given a
precision $d$, and a polynomial $Q$ whose coefficients are power series in $x$,
the algorithm computes a representation of all power series $f(x)$ such that
$Q(f(x)) = 0 \bmod x^d$. The algorithm works unconditionally, in particular
also with multiple roots, where Newton iteration fails. Our main motivation
comes from coding theory where instances of this problem arise and multiple
roots must be handled.
  The cost bound for our algorithm matches the worst-case input and output size
$d \deg(Q)$, up to logarithmic factors. This improves upon previous algorithms
which were quadratic in at least one of $d$ and $\deg(Q)$. Our algorithm is a
refinement of a divide \& conquer algorithm by Alekhnovich (2005), where the
cost of recursive steps is better controlled via the computation of a factor of
$Q$ which has a smaller degree while preserving the roots.
","['\nVincent Neiger\n', '\nJohan Rosenkilde\n', '\nEric Schost\n']","8 pages, uses acmart sigconf",,http://dx.doi.org/10.1145/3087604.3087642,cs.SC,['cs.SC'],10.1145/3087604.3087642,,[]
"Sparse Rational Function Interpolation with Finitely Many Values for the
  Coefficients",http://arxiv.org/abs/1706.00914v1,2017-06-03T09:03:39Z,2017-06-03T09:03:39Z,"  In this paper, we give new sparse interpolation algorithms for black box
univariate and multivariate rational functions h=f/g whose coefficients are
integers with an upper bound. The main idea is as follows: choose a proper
integer beta and let h(beta) = a/b with gcd(a,b)=1. Then f and g can be
computed by solving the polynomial interpolation problems f(beta)=ka and
g(beta)=ka for some integer k. It is shown that the univariate interpolation
algorithm is almost optimal and multivariate interpolation algorithm has low
complexity in T but the data size is exponential in n.
","['\nQiao-Long Huang\n', '\nXiao-Shan Gao\n']",,,http://arxiv.org/abs/1706.00914v1,cs.SC,['cs.SC'],,,[]
Power series expansions for the planar monomer-dimer problem,http://arxiv.org/abs/1705.10121v3,2017-05-29T11:16:16Z,2017-08-17T16:09:51Z,"  We compute the free energy of the planar monomer-dimer model. Unlike the
classical planar dimer model, an exact solution is not known in this case. Even
the computation of the low-density power series expansion requires heavy and
nontrivial computations. Despite of the exponential computational complexity,
we compute almost three times more terms than were previously known. Such an
expansion provides both lower and upper bound for the free energy, and allows
to obtain more accurate numerical values than previously possible. We expect
that our methods can be applied to other similar problems.
",['\nGleb Pogudin\n'],,"Phys. Rev. E 96, 033303 (2017)",http://dx.doi.org/10.1103/PhysRevE.96.033303,cs.SC,"['cs.SC', 'math-ph', 'math.MP']",10.1103/PhysRevE.96.033303,,[]
Automatic Differentiation using Constraint Handling Rules in Prolog,http://arxiv.org/abs/1706.00231v1,2017-06-01T09:55:10Z,2017-06-01T09:55:10Z,"  Automatic differentiation is a technique which allows a programmer to define
a numerical computation via compositions of a broad range of numeric and
computational primitives and have the underlying system support the computation
of partial derivatives of the result with respect to any of its inputs, without
making any finite difference approximations, and without manipulating large
symbolic expressions representing the computation. This note describes a novel
approach to reverse mode automatic differentiation using constraint logic
programmming, specifically, the constraint handling rules (CHR) library of SWI
Prolog, resulting in a very small (50 lines of code) implementation. When
applied to a differentiation-based implementation of the inside-outside
algorithm for parameter learning in probabilistic grammars, the CHR based
implementations outperformed two well-known frameworks for optimising
differentiable functions, Theano and TensorFlow, by a large margin.
",['\nSamer Abdallah\n'],,,http://arxiv.org/abs/1706.00231v1,cs.MS,"['cs.MS', 'cs.PL', 'cs.SC']",,,[]
Iterated Elliptic and Hypergeometric Integrals for Feynman Diagrams,http://arxiv.org/abs/1706.01299v1,2017-06-05T12:52:32Z,2017-06-05T12:52:32Z,"  We calculate 3-loop master integrals for heavy quark correlators and the
3-loop QCD corrections to the $\rho$-parameter. They obey non-factorizing
differential equations of second order with more than three singularities,
which cannot be factorized in Mellin-$N$ space either. The solution of the
homogeneous equations is possible in terms of convergent close integer power
series as $_2F_1$ Gau\ss{} hypergeometric functions at rational argument. In
some cases, integrals of this type can be mapped to complete elliptic integrals
at rational argument. This class of functions appears to be the next one
arising in the calculation of more complicated Feynman integrals following the
harmonic polylogarithms, generalized polylogarithms, cyclotomic harmonic
polylogarithms, square-root valued iterated integrals, and combinations
thereof, which appear in simpler cases. The inhomogeneous solution of the
corresponding differential equations can be given in terms of iterative
integrals, where the new innermost letter itself is not an iterative integral.
A new class of iterative integrals is introduced containing letters in which
(multiple) definite integrals appear as factors. For the elliptic case, we also
derive the solution in terms of integrals over modular functions and also
modular forms, using $q$-product and series representations implied by Jacobi's
$\vartheta_i$ functions and Dedekind's $\eta$-function. The corresponding
representations can be traced back to polynomials out of Lambert--Eisenstein
series, having representations also as elliptic polylogarithms, a $q$-factorial
$1/\eta^k(\tau)$, logarithms and polylogarithms of $q$ and their $q$-integrals.
Due to the specific form of the physical variable $x(q)$ for different
processes, different representations do usually appear. Numerical results are
also presented.
","['\nJ. Ablinger\n', '\nJ. Blümlein\n', '\nA. De Freitas\n', '\nM. van Hoeij\n', '\nE. Imamoglu\n', '\nC. G. Raab\n', '\nC. -S. Radu\n', '\nC. Schneider\n']","68 pages LATEX, 10 Figures",,http://dx.doi.org/10.1063/1.4986417,hep-th,"['hep-th', 'cs.SC', 'hep-ph', 'math-ph', 'math.AG', 'math.MP']",10.1063/1.4986417,,[]
A Tropical F5 algorithm,http://arxiv.org/abs/1705.05571v1,2017-05-16T08:02:41Z,2017-05-16T08:02:41Z,"  Let K be a field equipped with a valuation. Tropical varieties over K can be
defined with a theory of Gr{\""o}bner bases taking into account the valuation of
K. While generalizing the classical theory of Gr{\""o}bner bases, it is not
clear how modern algorithms for computing Gr{\""o}bner bases can be adapted to
the tropical case. Among them, one of the most efficient is the celebrated F5
Algorithm of Faug{\`e}re. In this article, we prove that, for homogeneous
ideals, it can be adapted to the tropical case. We prove termination and
correctness. Because of the use of the valuation, the theory of tropical
Gr{\""o}b-ner bases is promising for stable computations over polynomial rings
over a p-adic field. We provide numerical examples to illustrate
time-complexity and p-adic stability of this tropical F5 algorithm.
","['\nTristan Vaccon\nXLIM-MATHIS\n', '\nKazuhiro Yokoyama\n']",,,http://arxiv.org/abs/1705.05571v1,cs.SC,"['cs.SC', 'math.AC']",,,['XLIM-MATHIS']
"Nemo/Hecke: Computer Algebra and Number Theory Packages for the Julia
  Programming Language",http://arxiv.org/abs/1705.06134v1,2017-05-17T13:10:32Z,2017-05-17T13:10:32Z,"  We introduce two new packages, Nemo and Hecke, written in the Julia
programming language for computer algebra and number theory. We demonstrate
that high performance generic algorithms can be implemented in Julia, without
the need to resort to a low-level C implementation. For specialised algorithms,
we use Julia's efficient native C interface to wrap existing C/C++ libraries
such as Flint, Arb, Antic and Singular. We give examples of how to use Hecke
and Nemo and discuss some algorithms that we have implemented to provide high
performance basic arithmetic.
","['\nClaus Fieker\n', '\nWilliam Hart\n', '\nTommy Hofmann\n', '\nFredrik Johansson\n']","ISSAC '17, Kaiserslautern, Germany, July 25-28, 2017, 8 pages",,http://dx.doi.org/10.1145/3087604.3087611,cs.MS,"['cs.MS', 'cs.SC']",10.1145/3087604.3087611,,[]
A lower bound on the positive semidefinite rank of convex bodies,http://arxiv.org/abs/1705.06996v2,2017-05-19T13:54:57Z,2017-12-05T16:49:46Z,"  The positive semidefinite rank of a convex body $C$ is the size of its
smallest positive semidefinite formulation. We show that the positive
semidefinite rank of any convex body $C$ is at least $\sqrt{\log d}$ where $d$
is the smallest degree of a polynomial that vanishes on the boundary of the
polar of $C$. This improves on the existing bound which relies on results from
quantifier elimination. The proof relies on the B\'ezout bound applied to the
Karush-Kuhn-Tucker conditions of optimality. We discuss the connection with the
algebraic degree of semidefinite programming and show that the bound is tight
(up to constant factor) for random spectrahedra of suitable dimension.
","['\nHamza Fawzi\n', '\nMohab Safey El Din\n']","v2: 14 pages - minor changes following comments by referees; v1: 13
  pages",,http://arxiv.org/abs/1705.06996v2,math.OC,"['math.OC', 'cs.CC', 'cs.SC']",,,[]
"Automated Generation of Non-Linear Loop Invariants Utilizing
  Hypergeometric Sequences",http://arxiv.org/abs/1705.02863v2,2017-05-08T13:34:18Z,2017-05-11T09:39:57Z,"  Analyzing and reasoning about safety properties of software systems becomes
an especially challenging task for programs with complex flow and, in
particular, with loops or recursion. For such programs one needs additional
information, for example in the form of loop invariants, expressing properties
to hold at intermediate program points. In this paper we study program loops
with non-trivial arithmetic, implementing addition and multiplication among
numeric program variables. We present a new approach for automatically
generating all polynomial invariants of a class of such programs. Our approach
turns programs into linear ordinary recurrence equations and computes closed
form solutions of these equations. These closed forms express the most precise
inductive property, and hence invariant. We apply Gr\""obner basis computation
to obtain a basis of the polynomial invariant ideal, yielding thus a finite
representation of all polynomial invariants. Our work significantly extends the
class of so-called P-solvable loops by handling multiplication with the loop
counter variable. We implemented our method in the Mathematica package Aligator
and showcase the practical use of our approach.
","['\nAndreas Humenberger\n', '\nMaximilian Jaroschek\n', '\nLaura Kovács\n']","A revised version of this paper is published in the proceedings of
  ISSAC 2017",,http://dx.doi.org/10.1145/3087604.3087623,cs.SC,['cs.SC'],10.1145/3087604.3087623,,[]
"Denominator Bounds and Polynomial Solutions for Systems of q-Recurrences
  over K(t) for Constant K",http://arxiv.org/abs/1705.04188v1,2017-05-11T14:03:44Z,2017-05-11T14:03:44Z,"  We consider systems A_\ell(t) y(q^\ell t) + ... + A_0(t) y(t) = b(t) of
higher order q-recurrence equations with rational coefficients. We extend a
method for finding a bound on the maximal power of t in the denominator of
arbitrary rational solutions y(t) as well as a method for bounding the degree
of polynomial solutions from the scalar case to the systems case. The approach
is direct and does not rely on uncoupling or reduction to a first order system.
Unlike in the scalar case this usually requires an initial transformation of
the system.
",['\nJohannes Middeke\n'],8 pages,,http://arxiv.org/abs/1705.04188v1,math.CO,"['math.CO', 'cs.SC']",,,[]
Dimension-Dependent Upper Bounds for Grobner Bases,http://arxiv.org/abs/1705.02776v1,2017-05-08T08:39:01Z,2017-05-08T08:39:01Z,"  We improve certain degree bounds for Grobner bases of polynomial ideals in
generic position. We work exclusively in deterministically verifiable and
achievable generic positions of a combinatorial nature, namely either strongly
stable position or quasi stable position. Furthermore, we exhibit new
dimension- (and depth-)dependent upper bounds for the Castelnuovo-Mumford
regularity and the degrees of the elements of the reduced Grobner basis (w.r.t.
the degree reverse lexicographical ordering) of a homogeneous ideal in these
positions.
","['\nAmir Hashemi\n', '\nWerner M. Seiler\n']","8 pages, to be published in the proceedings of ISSAC'17",,http://arxiv.org/abs/1705.02776v1,cs.SC,"['cs.SC', 'math.AC', 'math.AG']",,,[]
Deterministic Genericity for Polynomial Ideals,http://arxiv.org/abs/1705.02797v1,2017-05-08T09:38:05Z,2017-05-08T09:38:05Z,"  We consider several notions of genericity appearing in algebraic geometry and
commutative algebra. Special emphasis is put on various stability notions which
are defined in a combinatorial manner and for which a number of equivalent
algebraic characterisations are provided. It is shown that in characteristic
zero the corresponding generic positions can be obtained with a simple
deterministic algorithm. In positive characteristic, only adapted stable
positions are reachable except for quasi-stability which is obtainable in any
characteristic.
","['\nAmir Hashemi\n', '\nMichael Schweinfurter\n', '\nWerner M. Seiler\n']","42 pages, to be published in Journal of Symbolic Computation, 2017",,http://arxiv.org/abs/1705.02797v1,cs.SC,"['cs.SC', 'math.AC', 'math.AG']",,,[]
Improved Computation of Involutive Bases,http://arxiv.org/abs/1705.03441v1,2017-05-09T17:34:08Z,2017-05-09T17:34:08Z,"  In this paper, we describe improved algorithms to compute Janet and Pommaret
bases. To this end, based on the method proposed by Moller et al., we present a
more efficient variant of Gerdt's algorithm (than the algorithm presented by
Gerdt-Hashemi-M.Alizadeh) to compute minimal involutive bases. Further, by
using the involutive version of Hilbert driven technique, along with the new
variant of Gerdt's algorithm, we modify the algorithm, given by Seiler, to
compute a linear change of coordinates for a given homogeneous ideal so that
the new ideal (after performing this change) possesses a finite Pommaret basis.
All the proposed algorithms have been implemented in Maple and their efficiency
is discussed via a set of benchmark polynomials.
","['\nBentolhoda Binaei\n', '\nAmir Hashemi\n', '\nWerner M. Seiler\n']","15 pages, Computer Algebra in Scientific Computing (CASC 2016),
  Bucharest, Romania, 2016. Lecture Notes in Computer Science, Volume 9890,
  pages 58--72, 2016",,http://arxiv.org/abs/1705.03441v1,cs.SC,"['cs.SC', 'math.AC', 'math.AG']",,,[]
"Improved method for finding optimal formulae for bilinear maps in a
  finite field",http://arxiv.org/abs/1705.07728v3,2017-05-09T06:20:33Z,2018-12-07T16:10:46Z,"  In 2012, Barbulescu, Detrey, Estibals and Zimmermann proposed a new framework
to exhaustively search for optimal formulae for evaluating bilinear maps, such
as Strassen or Karatsuba formulae. The main contribution of this work is a new
criterion to aggressively prune useless branches in the exhaustive search, thus
leading to the computation of new optimal formulae, in particular for the short
product modulo X 5 and the circulant product modulo (X 5 -- 1). Moreover , we
are able to prove that there is essentially only one optimal decomposition of
the product of 3 x 2 by 2 x 3 matrices up to the action of some group of
automorphisms.
",['\nSvyatoslav Covanov\nCARAMBA\n'],,,http://arxiv.org/abs/1705.07728v3,cs.DS,"['cs.DS', 'cs.CC', 'cs.DM', 'cs.SC']",,,['CARAMBA']
A Case Study on the Parametric Occurrence of Multiple Steady States,http://arxiv.org/abs/1704.08997v1,2017-04-28T16:37:12Z,2017-04-28T16:37:12Z,"  We consider the problem of determining multiple steady states for positive
real values in models of biological networks. Investigating the potential for
these in models of the mitogen-activated protein kinases (MAPK) network has
consumed considerable effort using special insights into the structure of
corresponding models. Here we apply combinations of symbolic computation
methods for mixed equality/inequality systems, specifically virtual
substitution, lazy real triangularization and cylindrical algebraic
decomposition. We determine multistationarity of an 11-dimensional MAPK network
when numeric values are known for all but potentially one parameter. More
precisely, our considered model has 11 equations in 11 variables and 19
parameters, 3 of which are of interest for symbolic treatment, and furthermore
positivity conditions on all variables and parameters.
","['\nRussell Bradford\n', '\nJames H. Davenport\n', '\nMatthew England\n', '\nHassan Errami\n', '\nVladimir Gerdt\n', '\nDima Grigoriev\n', '\nCharles Hoyt\n', '\nMarek Kosta\n', '\nOvidiu Radulescu\n', '\nThomas Sturm\n', '\nAndreas Weber\n']","Accepted into ISSAC 2017. This version has additional page showing
  all 11 CAD trees discussed in Section 2.1.1","Proceedings of the 42nd International Symposium on Symbolic and
  Algebraic Computation (ISSAC '17), pp. 45-52, ACM, 2017",http://dx.doi.org/10.1145/3087604.3087622,cs.SC,"['cs.SC', 'I.1.4']",10.1145/3087604.3087622,,[]
"Denominator Bounds for Systems of Recurrence Equations using
  $ΠΣ$-Extensions",http://arxiv.org/abs/1705.00280v1,2017-04-30T07:28:37Z,2017-04-30T07:28:37Z,"  We consider linear systems of recurrence equations whose coefficients are
given in terms of indefinite nested sums and products covering, e.g., the
harmonic numbers, hypergeometric products, $q$-hypergeometric products or their
mixed versions. These linear systems are formulated in the setting of
$\Pi\Sigma$-extensions and our goal is to find a denominator bound (also known
as universal denominator) for the solutions; i.e., a non-zero polynomial $d$
such that the denominator of every solution of the system divides $d$. This is
the first step in computing all rational solutions of such a rather general
recurrence system. Once the denominator bound is known, the problem of solving
for rational solutions is reduced to the problem of solving for polynomial
solutions.
","['\nJohannes Middeke\n', '\nCarsten Schneider\n']",,,http://arxiv.org/abs/1705.00280v1,cs.SC,['cs.SC'],,,[]
Apparent Singularities of D-finite Systems,http://arxiv.org/abs/1705.00838v1,2017-05-02T07:51:21Z,2017-05-02T07:51:21Z,"  We generalize the notions of singularities and ordinary points from linear
ordinary differential equations to D-finite systems. Ordinary points of a
D-finite system are characterized in terms of its formal power series
solutions. We also show that apparent singularities can be removed like in the
univariate case by adding suitable additional solutions to the system at hand.
Several algorithms are presented for removing and detecting apparent
singularities. In addition, an algorithm is given for computing formal power
series solutions of a D-finite system at apparent singularities.
","['\nShaoshi Chen\n', '\nManuel Kauers\n', '\nZiming Li\n', '\nYi Zhang\n']",,,http://arxiv.org/abs/1705.00838v1,cs.SC,['cs.SC'],,,[]
"Non-linear Associative-Commutative Many-to-One Pattern Matching with
  Sequence Variables",http://arxiv.org/abs/1705.00907v1,2017-05-02T11:10:05Z,2017-05-02T11:10:05Z,"  Pattern matching is a powerful tool which is part of many functional
programming languages as well as computer algebra systems such as Mathematica.
Among the existing systems, Mathematica offers the most expressive pattern
matching. Unfortunately, no open source alternative has comparable pattern
matching capabilities. Notably, these features include support for associative
and/or commutative function symbols and sequence variables. While those
features have individually been subject of previous research, their
comprehensive combination has not yet been investigated. Furthermore, in many
applications, a fixed set of patterns is matched repeatedly against different
subjects. This many-to-one matching can be sped up by exploiting similarities
between patterns. Discrimination nets are the state-of-the-art solution for
many-to-one matching. In this thesis, a generalized discrimination net which
supports the full feature set is presented. All algorithms have been
implemented as an open-source library for Python. In experiments on real world
examples, significant speedups of many-to-one over one-to-one matching have
been observed.
",['\nManuel Krebber\n'],Master Thesis,,http://arxiv.org/abs/1705.00907v1,cs.SC,"['cs.SC', 'D.3.3; E.1; F.2.2; F.4.2; F.4.3; I.1.1; I.1.2']",,,[]
"Representing ($q$--)hypergeometric products and mixed versions in
  difference rings",http://arxiv.org/abs/1705.01368v2,2017-05-03T11:36:42Z,2017-10-30T11:35:06Z,"  In recent years, Karr's difference field theory has been extended to the
so-called $R\Pi\Sigma$-extensions in which one can represent not only
indefinite nested sums and products that can be expressed by transcendental
ring extensions, but one can also handle algebraic products of the form
$\alpha^n$ where $\alpha$ is a root of unity. In this article we supplement
this summation theory substantially by the following building block. We provide
new algorithms that represent a finite number of hypergeometric or mixed
$(q_1,...,q_e)$-multibasic hypergeometric products in such a difference ring.
This new insight provides a complete summation machinery that enables one to
formulate such products and indefinite nested sums defined over such products
in $R\Pi\Sigma$-extensions fully automatically. As a side-product, one obtains
compactified expressions where the products are algebraically independent among
each other, and one can solve the zero-recognition problem for such products.
","['\nEvans Doe Ocansey\n', '\nCarsten Schneider\n']",Removed various typos,,http://arxiv.org/abs/1705.01368v2,cs.SC,['cs.SC'],,,[]
"Computing representation matrices for the action of Frobenius to
  cohomology groups",http://arxiv.org/abs/1704.08110v1,2017-04-26T13:43:21Z,2017-04-26T13:43:21Z,"  This paper is concerned with the computation of representation matrices for
the action of Frobenius to the cohomology groups of algebraic varieties.
Specifically we shall give an algorithm to compute the matrices for arbitrary
algebraic varieties with defining equations over perfect fields of positive
characteristic, and estimate its complexity. Moreover, we propose a specific
efficient method, which works for complete intersections.
",['\nMomonari Kudo\n'],,"Journal of Symbolic Computation, Vol. 109, 441-464, 2022",http://dx.doi.org/10.1016/j.jsc.2020.07.015,math.AG,"['math.AG', 'cs.SC']",10.1016/j.jsc.2020.07.015,,[]
A Special Homotopy Continuation Method For A Class of Polynomial Systems,http://arxiv.org/abs/1704.07536v1,2017-04-25T04:42:11Z,2017-04-25T04:42:11Z,"  A special homotopy continuation method, as a combination of the polyhedral
homotopy and the linear product homotopy, is proposed for computing all the
isolated solutions to a special class of polynomial systems. The root number
bound of this method is between the total degree bound and the mixed volume
bound and can be easily computed. The new algorithm has been implemented as a
program called LPH using C++. Our experiments show its efficiency compared to
the polyhedral or other homotopies on such systems. As an application, the
algorithm can be used to find witness points on each connected component of a
real variety.
","['\nYu Wang\n', '\nWenyuan Wu\n', '\nBican Xia\n']",,,http://arxiv.org/abs/1704.07536v1,cs.SC,"['cs.SC', 'math.AG', 'math.NA']",,,[]
Computing isomorphisms and embeddings of finite fields,http://arxiv.org/abs/1705.01221v1,2017-05-03T01:33:29Z,2017-05-03T01:33:29Z,"  Let $\mathbb{F}_q$ be a finite field. Given two irreducible polynomials $f,g$
over $\mathbb{F}_q$, with $\mathrm{deg} f$ dividing $\mathrm{deg} g$, the
finite field embedding problem asks to compute an explicit description of a
field embedding of $\mathbb{F}_q[X]/f(X)$ into $\mathbb{F}_q[Y]/g(Y)$. When
$\mathrm{deg} f = \mathrm{deg} g$, this is also known as the isomorphism
problem.
  This problem, a special instance of polynomial factorization, plays a central
role in computer algebra software. We review previous algorithms, due to
Lenstra, Allombert, Rains, and Narayanan, and propose improvements and
generalizations. Our detailed complexity analysis shows that our newly proposed
variants are at least as efficient as previously known algorithms, and in many
cases significantly better.
  We also implement most of the presented algorithms, compare them with the
state of the art computer algebra software, and make the code available as open
source. Our experiments show that our new variants consistently outperform
available software.
","['\nLudovic Brieulle\n', '\nLuca De Feo\n', '\nJavad Doliskani\n', '\nJean-Pierre Flori\n', '\nÉric Schost\n']",,,http://dx.doi.org/10.1090/mcom/3363,cs.SC,"['cs.SC', 'cs.MS', 'math.NT']",10.1090/mcom/3363,,[]
On Drinfel'd associators,http://arxiv.org/abs/1705.01882v1,2017-05-03T14:42:08Z,2017-05-03T14:42:08Z,"  In 1986, in order to study the linear representations of the braid group
$B\_n$coming from the monodromy of the Knizhnik-Zamolodchikov differential
equations,Drinfel'd introduced a class of formal power series $\Phi$on
noncommutative variables. These formal series can be considered as a class of
associators. We here give an interpretation of them as well as some new tools
over Noncommutative Evolution Equations. Asymptotic phenomena are also
discussed.
","['\nGérard Duchamp\nLIPN\n', '\nNgoc Minh\nLIPN\n', '\nK Penson\nLPTMC\n']",,,http://arxiv.org/abs/1705.01882v1,math.CA,"['math.CA', 'cs.SC', 'math.CO']",,,"['LIPN', 'LIPN', 'LPTMC']"
"Sparse Polynomial Interpolation with Finitely Many Values for the
  Coefficients",http://arxiv.org/abs/1704.04359v2,2017-04-14T06:04:44Z,2017-06-22T07:24:50Z,"  In this paper, we give new sparse interpolation algorithms for black box
polynomial f whose coefficients are from a finite set. In the univariate case,
we recover f from one evaluation of f(a) for a sufficiently large number a. In
the multivariate case, we introduce the modified Kronecker substitution to
reduce the interpolation of a multivariate polynomial to the univariate case.
Both algorithms have polynomial bit-size complexity.
","['\nQiao-Long Huang\n', '\nXiao-Shan Gao\n']",,,http://arxiv.org/abs/1704.04359v2,cs.SC,['cs.SC'],,,[]
CAD Adjacency Computation Using Validated Numerics,http://arxiv.org/abs/1704.06856v1,2017-04-22T23:26:57Z,2017-04-22T23:26:57Z,"  We present an algorithm for computation of cell adjacencies for well-based
cylindrical algebraic decomposition. Cell adjacency information can be used to
compute topological operations e.g. closure, boundary, connected components,
and topological properties e.g. homology groups. Other applications include
visualization and path planning. Our algorithm determines cell adjacency
information using validated numerical methods similar to those used in CAD
construction, thus computing CAD with adjacency information in time comparable
to that of computing CAD without adjacency information. We report on
implementation of the algorithm and present empirical data.
",['\nAdam Strzebonski\n'],20 pages,,http://arxiv.org/abs/1704.06856v1,cs.SC,"['cs.SC', 'cs.CG', '68W30', 'I.1.2; G.4']",,,[]
"Prover efficient public verification of dense or sparse/structured
  matrix-vector multiplication",http://arxiv.org/abs/1704.02768v1,2017-04-10T09:05:53Z,2017-04-10T09:05:53Z,"  With the emergence of cloud computing services, computationally weak devices
(Clients) can delegate expensive tasks to more powerful entities (Servers).
This raises the question of verifying a result at a lower cost than that of
recomputing it. This verification can be private, between the Client and the
Server, or public, when the result can be verified by any third party. We here
present protocols for the verification of matrix-vector multiplications, that
are secure against malicious Servers. The obtained algorithms are essentially
optimal in the amortized model: the overhead for the Server is limited to a
very small constant factor, even in the sparse or structured matrix case; and
the computational time for the public Verifier is linear in the dimension. Our
protocols combine probabilistic checks and cryptographic operations, but
minimize the latter to preserve practical efficiency. Therefore our protocols
are overall more than two orders of magnitude faster than existing ones.
","['\nJean-Guillaume Dumas\nCASYS\n', '\nVincent Zucca\nPEQUAN\n']",,"ACISP 2017, 22nd Australasian Conference on Information Security
  and Privacy, Jul 2017, Auckland, New Zealand. Lecture Notes in Computer
  Science, 2017",http://arxiv.org/abs/1704.02768v1,cs.CR,"['cs.CR', 'cs.SC']",,,"['CASYS', 'PEQUAN']"
Modular Techniques For Noncommutative Gröbner Bases,http://arxiv.org/abs/1704.02852v1,2017-04-10T13:39:13Z,2017-04-10T13:39:13Z,"  In this note, we extend modular techniques for computing Gr\""obner bases from
the commutative setting to the vast class of noncommutative $G$-algebras. As in
the commutative case, an effective verification test is only known to us in the
graded case. In the general case, our algorithm is probabilistic in the sense
that the resulting Gr\""obner basis can only be expected to generate the given
ideal, with high probability. We have implemented our algorithm in the computer
algebra system {\sc{Singular}} and give timings to compare its performance with
that of other instances of Buchberger's algorithm, testing examples from
$D$-module theory as well as classical benchmark examples. A particular feature
of the modular algorithm is that it allows parallel runs.
","['\nWolfram Decker\n', '\nChristian Eder\n', '\nViktor Levandovskyy\n', '\nSharwan K. Tiwari\n']",,,http://arxiv.org/abs/1704.02852v1,math.RA,"['math.RA', 'cs.SC']",,,[]
"Laderman matrix multiplication algorithm can be constructed using
  Strassen algorithm and related tensor's isotropies",http://arxiv.org/abs/1703.08298v4,2017-03-24T07:22:09Z,2017-05-10T11:51:07Z,"  In 1969, V. Strassen improves the classical~2x2 matrix multiplication
algorithm. The current upper bound for 3x3 matrix multiplication was reached by
J.B. Laderman in 1976. This note presents a geometric relationship between
Strassen and Laderman algorithms. By doing so, we retrieve a geometric
formulation of results very similar to those presented by O. Sykora in 1977.
",['\nAlexandre Sedoglavic\nCRIStAL\n'],,,http://arxiv.org/abs/1703.08298v4,cs.SC,['cs.SC'],,,['CRIStAL']
Recursive Method for the Solution of Systems of Linear Equations,http://arxiv.org/abs/1703.10232v1,2017-03-29T20:20:18Z,2017-03-29T20:20:18Z,"  New solution method for the systems of linear equations in commutative
integral domains is proposed. Its complexity is the same that the complexity of
the matrix multiplication.
",['\nGennadi Malaschonok\n'],,"Proceedings of the 15th IMACS World Congress, Vol. I, (Berlin,
  August 1997), Wissenschaft & Technik Verlag, 475--480",http://arxiv.org/abs/1703.10232v1,cs.DS,"['cs.DS', 'cs.SC']",,,[]
Roots multiplicity without companion matrices,http://arxiv.org/abs/1703.06120v1,2017-03-17T17:28:23Z,2017-03-17T17:28:23Z,"  We show a method for constructing a polynomial interpolating roots'
multiplicities of another polynomial, that does not use companion matrices.
This leads to a modification to Guersenzvaig--Szechtman square-free
decomposition algorithm that is more efficient both in theory and in practice.
",['\nPrzemysław Koprowski\n'],,,http://arxiv.org/abs/1703.06120v1,cs.SC,"['cs.SC', '12D05, 13A05']",,,[]
A clever elimination strategy for efficient minimal solvers,http://arxiv.org/abs/1703.05289v1,2017-03-15T17:44:37Z,2017-03-15T17:44:37Z,"  We present a new insight into the systematic generation of minimal solvers in
computer vision, which leads to smaller and faster solvers. Many minimal
problem formulations are coupled sets of linear and polynomial equations where
image measurements enter the linear equations only. We show that it is useful
to solve such systems by first eliminating all the unknowns that do not appear
in the linear equations and then extending solutions to the rest of unknowns.
This can be generalized to fully non-linear systems by linearization via
lifting. We demonstrate that this approach leads to more efficient solvers in
three problems of partially calibrated relative camera pose computation with
unknown focal length and/or radial distortion. Our approach also generates new
interesting constraints on the fundamental matrices of partially calibrated
cameras, which were not known before.
","['\nZuzana Kukelova\n', '\nJoe Kileel\n', '\nBernd Sturmfels\n', '\nTomas Pajdla\n']","13 pages, 7 figures","IEEE Conference on Computer Vision and Pattern Recognition 2017
  (CVPR 2017)",http://arxiv.org/abs/1703.05289v1,cs.CV,"['cs.CV', 'cs.SC']",,,[]
Ultimate Positivity of Diagonals of Quasi-rational Functions,http://arxiv.org/abs/1703.05580v1,2017-03-16T12:21:51Z,2017-03-16T12:21:51Z,"  The problem to decide whether a given multivariate (quasi-)rational function
has only positive coefficients in its power series expansion has a long
history. It dates back to Szego in 1933 who showed certain quasi-rational
function to be positive, in the sense that all the series coefficients are
positive, using an involved theory of special functions. In contrast to the
simplicity of the statement, the method was surprisingly difficult. This
dependency motivated further research for positivity of (quasi-)rational
functions. More and more (quasi-)rational functions have been proven to be
positive, and some of the proofs are even quite simple. However, there are also
others whose positivity are still open conjectures. In this talk, we focus on a
less difficult but also interesting question to decide whether the diagonal of
a given quasi-rational function is ultimately positive, especially for the one
conjectured to be positive by Kauers in 2007. To solve this question, it
suffices to compute the asymptotics of the diagonal coefficients, which can be
done by the multivariate singularity analysis developed by Baryshnikov,
Pemantle and Wilson. Note that the ultimate positivity is a necessary condition
for the positivity, and therefore can be used to either exclude the nonpositive
cases or further support the conjectural positivity.
",['\nHui Huang\n'],"5 pages, extended abstract",,http://arxiv.org/abs/1703.05580v1,cs.SC,"['cs.SC', 'math.CO']",,,[]
Orbital Graphs,http://arxiv.org/abs/1703.04272v2,2017-03-13T07:05:24Z,2017-05-23T07:43:33Z,"  We introduce orbital graphs and discuss some of their basic properties. Then
we focus on their usefulness for search algorithms for permutation groups,
including finding the intersection of groups and the stabilizer of sets in a
group.
","['\nPaula Hähndel\n', '\nChristopher Jefferson\n', '\nMarkus Pfeiffer\n', '\nRebecca Waldecker\n']",,,http://arxiv.org/abs/1703.04272v2,math.GR,"['math.GR', 'cs.SC', 'math.CO']",,,[]
Decomposition of polynomial sets into characteristic pairs,http://arxiv.org/abs/1702.08664v1,2017-02-28T06:33:50Z,2017-02-28T06:33:50Z,"  A characteristic pair is a pair (G,C) of polynomial sets in which G is a
reduced lexicographic Groebner basis, C is the minimal triangular set contained
in G, and C is normal. In this paper, we show that any finite polynomial set P
can be decomposed algorithmically into finitely many characteristic pairs with
associated zero relations, which provide representations for the zero set of P
in terms of those of Groebner bases and those of triangular sets. The algorithm
we propose for the decomposition makes use of the inherent connection between
Ritt characteristic sets and lexicographic Groebner bases and is based
essentially on the structural properties and the computation of lexicographic
Groebner bases. Several nice properties about the decomposition and the
resulting characteristic pairs, in particular relationships between the
Groebner basis and the triangular set in each pair, are established. Examples
are given to illustrate the algorithm and some of the properties.
","['\nDongming Wang\n', '\nRina Dong\n', '\nChenqi Mou\n']",19 pages,,http://arxiv.org/abs/1702.08664v1,cs.SC,['cs.SC'],,,[]
A lattice formulation of the F4 completion procedure,http://arxiv.org/abs/1703.02077v3,2017-03-06T19:31:36Z,2018-01-30T12:34:12Z,"  We write a procedure for constructing noncommutative Groebner bases.
Reductions are done by particular linear projectors, called reduction
operators. The operators enable us to use a lattice construction to reduce
simultaneously each S-polynomial into a unique normal form. We write an
implementation as well as an example to illustrate our procedure. Moreover, the
lattice construction is done by Gaussian elimination, which relates our
procedure to the F4 algorithm for constructing commutative Groebner bases.
",['\nChenavier Cyrille\n'],,,http://arxiv.org/abs/1703.02077v3,cs.SC,['cs.SC'],,,[]
Faster truncated integer multiplication,http://arxiv.org/abs/1703.00640v2,2017-03-02T07:12:12Z,2023-08-02T03:06:38Z,"  We present new algorithms for computing the low $n$ bits or the high $n$ bits
of the product of two $n$-bit integers. We show that these problems may be
solved in asymptotically 75% of the time required to compute the full $2n$-bit
product, assuming that the underlying integer multiplication algorithm relies
on computing cyclic convolutions of real sequences.
",['\nDavid Harvey\n'],"32 pages. Improved exposition, updated timings",,http://arxiv.org/abs/1703.00640v2,cs.SC,"['cs.SC', 'cs.DS', '68W30 (Primary)', 'G.1.0; F.2.1']",,,[]
"Algorithm for computing semi-Fourier sequences of expressions involving
  exponentiations and integrations",http://arxiv.org/abs/1702.07060v1,2017-02-23T01:24:29Z,2017-02-23T01:24:29Z,"  We provide an algorithm for computing semi-Fourier sequences for expressions
constructed from arithmetic operations, exponentiations and integrations. The
semi-Fourier sequence is a relaxed version of Fourier sequence for polynomials
(expressions made of additions and multiplications).
","['\nHoon Hong\n', '\nAdam Strzebonski\n']",20 pages,,http://arxiv.org/abs/1702.07060v1,cs.SC,"['cs.SC', '68W30, 26-04, 26A99', 'I.1.2; G.4']",,,[]
Fast generalized Bruhat decomposition,http://arxiv.org/abs/1702.07242v1,2017-02-23T14:49:37Z,2017-02-23T14:49:37Z,"  The deterministic recursive pivot-free algorithms for the computation of
generalized Bruhat decomposition of the matrix in the field and for the
computation of the inverse matrix are presented. This method has the same
complexity as algorithm of matrix multiplication and it is suitable for the
parallel computer systems.
",['\nGennadi Malaschonok\n'],,"Computer Algebra in Scientific Computing, LNCS 6244, Springer,
  Berlin, 2010. P. 194-202",http://dx.doi.org/10.1007/978-3-642-15274-0_16,cs.SC,['cs.SC'],10.1007/978-3-642-15274-0_16,,[]
Triangular Decomposition of Matrices in a Domain,http://arxiv.org/abs/1702.07243v1,2017-02-23T14:50:00Z,2017-02-23T14:50:00Z,"  Deterministic recursive algorithms for the computation of matrix triangular
decompositions with permutations like LU and Bruhat decomposition are presented
for the case of commutative domains. This decomposition can be considered as a
generalization of LU and Bruhat decompositions, because they both may be easily
obtained from this triangular decomposition. Algorithms have the same
complexity as the algorithm of matrix multiplication.
","['\nGennadi Malaschonok\n', '\nAnton Scherbinin\n']",,"Computer Algebra in Scientific Computing. LNCS 9301, Springer,
  Switzerland, 2015, P.290-304",http://dx.doi.org/10.1007/978-3-319-24021-3_22,cs.SC,['cs.SC'],10.1007/978-3-319-24021-3_22,,[]
Generalized Bruhat decomposition in commutative domains,http://arxiv.org/abs/1702.07248v1,2017-02-23T14:59:12Z,2017-02-23T14:59:12Z,"  Deterministic recursive algorithms for the computation of generalized Bruhat
decomposition of the matrix in commutative domain are presented. This method
has the same complexity as the algorithm of matrix multiplication.
",['\nGennadi Malaschonok\n'],,,http://dx.doi.org/10.1007/978-3-319-02297-0_20,cs.SC,['cs.SC'],10.1007/978-3-319-02297-0_20,,[]
Bad Primes in Computational Algebraic Geometry,http://arxiv.org/abs/1702.06920v1,2017-02-22T18:03:38Z,2017-02-22T18:03:38Z,"  Computations over the rational numbers often suffer from intermediate
coefficient swell. One solution to this problem is to apply the given algorithm
modulo a number of primes and then lift the modular results to the rationals.
This method is guaranteed to work if we use a sufficiently large set of good
primes. In many applications, however, there is no efficient way of excluding
bad primes. In this note, we describe a technique for rational reconstruction
which will nevertheless return the correct result, provided the number of good
primes in the selected set of primes is large enough. We give a number of
illustrating examples which are implemented using the computer algebra system
Singular and the programming language Julia. We discuss applications of our
technique in computational algebraic geometry.
","['\nJanko Boehm\n', '\nWolfram Decker\n', '\nClaus Fieker\n', '\nSantiago Laplagne\n', '\nGerhard Pfister\n']","8 pages, 1 figure, 1 table","LNCS 9725 (2016), 93-102",http://dx.doi.org/10.1007/978-3-319-42432-3_12,math.AG,"['math.AG', 'cs.SC', '13P10 (Primary) 68W10, 52C05 (Secondary)']",10.1007/978-3-319-42432-3_12,,[]
Computing and Using Minimal Polynomials,http://arxiv.org/abs/1702.07262v3,2017-02-23T15:34:29Z,2019-08-07T09:02:38Z,"  Given a zero-dimensional ideal I in a polynomial ring, many computations
start by finding univariate polynomials in I. Searching for a univariate
polynomial in I is a particular case of considering the minimal polynomial of
an element in P/I. It is well known that minimal polynomials may be computed
via elimination, therefore this is considered to be a ""resolved problem"". But
being the key of so many computations, it is worth investigating its meaning,
its optimization, its applications (e.g. testing if a zero-dimensional ideal is
radical, primary or maximal). We present efficient algorithms for computing the
minimal polynomial of an element of P/I. For the specific case where the
coefficients are in Q, we show how to use modular methods to obtain a
guaranteed result. We also present some applications of minimal polynomials,
namely algorithms for computing radicals and primary decompositions of
zero-dimensional ideals, and also for testing radicality and maximality.
","['\nJohn Abbott\n', '\nAnna Maria Bigatti\n', '\nElisa Palezzato\n', '\nLorenzo Robbiano\n']","This is a fully revised version. To be published in Journal of
  Symbolic Computation, special Issue on Symbolic Computation and
  Satisfiability Checking",JSC 2019,http://arxiv.org/abs/1702.07262v3,math.AC,"['math.AC', 'cs.SC', '13P25, 13P10, 13-04, 14Q10, 68W30']",,,[]
"The natural algorithmic approach of mixed trigonometric-polynomial
  problems",http://arxiv.org/abs/1702.07911v1,2017-02-25T16:02:45Z,2017-02-25T16:02:45Z,"  The aim of this paper is to present a new algorithm for proving mixed
trigonometric-polynomial inequalities by reducing to polynomial inequalities.
Finally, we show the great applicability of this algorithm and as examples, we
use it to analyze some new rational (Pade) approximations of the function
$\cos^2(x)$, and to improve a class of inequalities by Z.-H. Yang. The results
of our analysis could be implemented by means of an automated proof assistant,
so our work is a contribution to the library of automatic support tools for
proving various analytic inequalities.
","['\nTatjana Lutovac\n', '\nBranko Malesevic\n', '\nCristinel Mortici\n']",,"Journal of Inequalities and Applications 2017:116, (2017)",http://dx.doi.org/10.1186/s13660-017-1392-1,math.CA,"['math.CA', 'cs.SC', '41A10, 26D05, 12L05, 41A58']",10.1186/s13660-017-1392-1,,[]
Certificates for triangular equivalence and rank profiles,http://arxiv.org/abs/1702.03755v2,2017-02-13T13:03:55Z,2019-10-25T13:10:49Z,"  In this paper, we give novel certificates for triangular equivalence and rank
profiles. These certificates enable to verify the row or column rank profiles
or the whole rank profile matrix faster than recomputing them, with a
negligible overall overhead. We first provide quadratic time and space
non-interactive certificates saving the logarithmic factors of previously known
ones. Then we propose interactive certificates for the same problems whose
Monte Carlo verification complexity requires a small constant number of
matrix-vector multiplications, a linear space, and a linear number of extra
field operations. As an application we also give an interactive protocol,
certifying the determinant of dense matrices, faster than the best previously
known one.
","['\nJean-Guillaume Dumas\nCASYS\n', '\nDavid Lucas\nCASYS\n', '\nClément Pernet\nCASYS\n']",,"ACM International Symposium on Symbolic and Algebraic Computations
  - ISSAC'17, Jul 2017, Kaiserslautern, Germany. pp.133-140,
  \&\#x27E8;10.1145/3087604.3087609\&\#x27E9",http://arxiv.org/abs/1702.03755v2,cs.SC,['cs.SC'],,,"['CASYS', 'CASYS', 'CASYS']"
Symbolic Solutions of Simultaneous First-order PDEs in One Unknown,http://arxiv.org/abs/1703.01974v1,2017-02-14T20:13:52Z,2017-02-14T20:13:52Z,"  We propose and implement an algorithm for solving an overdetermined system of
partial differential equations in one unknown. Our approach relies on
Bour-Mayer method to determine compatibility conditions via Jacobi-Mayer
brackets. We solve compatible systems recursively by imitating what one would
do with pen and paper: Solve one equation, substitute the solution into the
remaining equations and iterate the process until the equations of the system
are exhausted. The method we employ for assessing the consistency of the
underlying system differs from the traditional use of differential Gr\""obner
bases yet seems more efficient and straightforward to implement. We are not
aware of a computer algebra system that adopts the procedure we advocate in
this work.
",['\nCélestin Wafo Soh\n'],,,http://arxiv.org/abs/1703.01974v1,cs.SC,['cs.SC'],,,[]
Characteristic polynomials of p-adic matrices,http://arxiv.org/abs/1702.01653v1,2017-02-06T15:28:31Z,2017-02-06T15:28:31Z,"  We analyze the precision of the characteristic polynomial of an $n\times n$
p-adic matrix A using differential precision methods developed previously. When
A is integral with precision O(p^N), we give a criterion (checkable in time
O~(n^omega)) for $\chi$(A) to have precision exactly O(p^N). We also give a
O~(n^3) algorithm for determining the optimal precision when the criterion is
not satisfied, and give examples when the precision is larger than O(p^N).
","['\nXavier Caruso\nIRMAR\n', '\nDavid Roe\nXLIM-MATHIS, UNILIM\n', '\nTristan Vaccon\nXLIM-MATHIS, UNILIM\n']",,,http://arxiv.org/abs/1702.01653v1,math.NT,"['math.NT', 'cs.SC']",,,"['IRMAR', 'XLIM-MATHIS, UNILIM', 'XLIM-MATHIS, UNILIM']"
Fast multiplication for skew polynomials,http://arxiv.org/abs/1702.01665v1,2017-02-06T15:47:28Z,2017-02-06T15:47:28Z,"  We describe an algorithm for fast multiplication of skew polynomials. It is
based on fast modular multiplication of such skew polynomials, for which we
give an algorithm relying on evaluation and interpolation on normal bases. Our
algorithms improve the best known complexity for these problems, and reach the
optimal asymptotic complexity bound for large degree. We also give an
adaptation of our algorithm for polynomials of small degree. Finally, we use
our methods to improve on the best known complexities for various arithmetics
problems.
","['\nXavier Caruso\nIRMAR\n', '\nJérémy Le Borgne\nENS Rennes, IRMAR\n']",,,http://dx.doi.org/10.1145/1235,cs.SC,"['cs.SC', 'math.RA']",10.1145/1235,,"['IRMAR', 'ENS Rennes, IRMAR']"
"Algorithmic Verification of Linearizability for Ordinary Differential
  Equations",http://arxiv.org/abs/1702.03829v3,2017-02-13T15:42:19Z,2017-04-27T13:30:51Z,"  For a nonlinear ordinary differential equation solved with respect to the
highest order derivative and rational in the other derivatives and in the
independent variable, we devise two algorithms to check if the equation can be
reduced to a linear one by a point transformation of the dependent and
independent variables. The first algorithm is based on a construction of the
Lie point symmetry algebra and on the computation of its derived algebra. The
second algorithm exploits the differential Thomas decomposition and allows not
only to test the linearizability, but also to generate a system of nonlinear
partial differential equations that determines the point transformation and the
coefficients of the linearized equation. Both algorithms have been implemented
in Maple and their application is illustrated using several examples.
","['\nDmitry Lyakhov\n', '\nVladimir Gerdt\n', '\nDominik Michels\n']",8 pages,,http://arxiv.org/abs/1702.03829v3,math.CA,"['math.CA', 'cs.SC', '34A05, 34A34, 68W30', 'I.1.2; G.1.7; G.4']",,,[]
Hybrid System Modelling and Simulation with Dirac Deltas,http://arxiv.org/abs/1702.04274v1,2017-02-14T16:04:20Z,2017-02-14T16:04:20Z,"  For a wide variety of problems, creating detailed continuous models of
(continuous) physical systems is, at the very least, impractical. Hybrid models
can abstract away short transient behaviour (thus introducing discontinuities)
in order to simplify the study of such systems. For example, when modelling a
bouncing ball, the bounce can be abstracted as a discontinuous change of the
velocity, instead of resorting to the physics of the ball (de-)compression to
keep the velocity signal continuous. Impulsive differential equations can be
used to model and simulate hybrid systems such as the bouncing ball. In this
approach, the force acted on the ball by the floor is abstracted as an
infinitely large function in an infinitely small interval of time, that is, an
impulse. Current simulators cannot handle such approximations well due to the
limitations of machine precision.
  In this paper, we explore the simulation of impulsive differential equations,
where impulses are first class citizens. We present two approaches for the
simulation of impulses: symbolic and numerical. Our contribution is a
theoretically founded description of the implementation of both approaches in a
Causal Block Diagram modelling and simulation tool. Furthermore, we investigate
the conditions for which one approach is better than the other.
","['\nCláudio Gomes\n', '\nYentl Van Tendeloo\n', '\nJoachim Denil\n', '\nPaul De Meulenaere\n', '\nHans Vangheluwe\n']",,,http://arxiv.org/abs/1702.04274v1,cs.NA,"['cs.NA', 'cs.SC']",,,[]
Discriminants of complete intersection space curves,http://arxiv.org/abs/1702.01694v1,2017-02-06T16:35:42Z,2017-02-06T16:35:42Z,"  In this paper, we develop a new approach to the discrimi-nant of a complete
intersection curve in the 3-dimensional projective space. By relying on the
resultant theory, we first prove a new formula that allows us to define this
discrimi-nant without ambiguity and over any commutative ring, in particular in
any characteristic. This formula also provides a new method for evaluating and
computing this discrimi-nant efficiently, without the need to introduce new
variables as with the well-known Cayley trick. Then, we obtain new properties
and computational rules such as the covariance and the invariance formulas.
Finally, we show that our definition of the discriminant satisfies to the
expected geometric property and hence yields an effective smoothness criterion
for complete intersection space curves. Actually, we show that in the generic
setting, it is the defining equation of the discriminant scheme if the ground
ring is assumed to be a unique factorization domain.
","['\nLaurent Busé\nAROMATH\n', '\nIbrahim Nonkané\n']",,,http://arxiv.org/abs/1702.01694v1,cs.SC,"['cs.SC', 'math.AG', 'math.NT']",,,['AROMATH']
Bounds for Substituting Algebraic Functions into D-finite Functions,http://arxiv.org/abs/1701.07802v3,2017-01-26T18:12:52Z,2017-05-26T09:53:44Z,"  It is well known that the composition of a D-finite function with an
algebraic function is again D-finite. We give the first estimates for the
orders and the degrees of annihilating operators for the compositions. We find
that the analysis of removable singularities leads to an order-degree curve
which is much more accurate than the order-degree curve obtained from the usual
linear algebra reasoning.
","['\nManuel Kauers\n', '\nGleb Pogudin\n']",,,http://arxiv.org/abs/1701.07802v3,cs.SC,['cs.SC'],,,[]
Riemann Tensor Polynomial Canonicalization by Graph Algebra Extension,http://arxiv.org/abs/1701.08487v1,2017-01-30T05:50:51Z,2017-01-30T05:50:51Z,"  Tensor expression simplification is an ""ancient"" topic in computer algebra, a
representative of which is the canonicalization of Riemann tensor polynomials.
Practically fast algorithms exist for monoterm canonicalization, but not for
multiterm canonicalization. Targeting the multiterm difficulty, in this paper
we establish the extension theory of graph algebra, and propose a
canonicalization algorithm for Riemann tensor polynomials based on this theory.
","['\nHongbo Li\n', '\nZhang Li\n', '\nYang Li\n']",,,http://arxiv.org/abs/1701.08487v1,cs.SC,['cs.SC'],,,[]
"A proof of Hilbert's theorem on ternary quartic forms with the ladder
  technique",http://arxiv.org/abs/1701.08294v2,2017-01-28T15:43:32Z,2017-03-21T06:46:08Z,"  This paper proposes a totally constructive approach for the proof of
Hilbert's theorem on ternary quartic forms. The main contribution is the ladder
technique, with which the Hilbert's theorem is proved vividly.
","['\nJia Xu\n', '\nYong Yao\n']",9 pages,,http://arxiv.org/abs/1701.08294v2,cs.SC,"['cs.SC', 'math.AG', '11E25', 'F.2.1']",,,[]
"Criteria for Finite Difference Groebner Bases of Normal Binomial
  Difference Ideals",http://arxiv.org/abs/1701.06248v1,2017-01-23T01:39:32Z,2017-01-23T01:39:32Z,"  In this paper, we give decision criteria for normal binomial difference
polynomial ideals in the univariate difference polynomial ring F{y} to have
finite difference Groebner bases and an algorithm to compute the finite
difference Groebner bases if these criteria are satisfied. The novelty of these
criteria lies in the fact that complicated properties about difference
polynomial ideals are reduced to elementary properties of univariate
polynomials in Z[x].
","['\nYu-Ao Chen\n', '\nXiao-Shan Gao\n']",,,http://arxiv.org/abs/1701.06248v1,cs.SC,['cs.SC'],,,[]
On Bezout Inequalities for non-homogeneous Polynomial Ideals,http://arxiv.org/abs/1701.04341v1,2017-01-16T15:59:40Z,2017-01-16T15:59:40Z,"  We introduce a ""workable"" notion of degree for non-homogeneous polynomial
ideals and formulate and prove ideal theoretic B\'ezout Inequalities for the
sum of two ideals in terms of this notion of degree and the degree of
generators. We compute probabilistically the degree of an equidimensional
ideal.
","['\nAmir Hashemi\n', '\nJoos Heintz\n', '\nLuis Miguel Pardo\n', '\nPablo Solernó\n']",,,http://arxiv.org/abs/1701.04341v1,cs.SC,"['cs.SC', 'cs.CC', 'math.AC', 'math.AG', '13F20, 14A10, 13P10']",,,[]
The number of realizations of a Laman graph,http://arxiv.org/abs/1701.05500v2,2017-01-19T16:24:27Z,2017-11-28T13:39:41Z,"  Laman graphs model planar frameworks that are rigid for a general choice of
distances between the vertices. There are finitely many ways, up to isometries,
to realize a Laman graph in the plane. Such realizations can be seen as
solutions of systems of quadratic equations prescribing the distances between
pairs of points. Using ideas from algebraic and tropical geometry, we provide a
recursive formula for the number of complex solutions of such systems.
","['\nJose Capco\n', '\nMatteo Gallet\n', '\nGeorg Grasegger\n', '\nChristoph Koutschan\n', '\nNiels Lubbes\n', '\nJosef Schicho\n']",36 pages,"SIAM Journal on Applied Algebra and Geometry, Volume 2, Issue 1,
  p. 94-125, 2018",http://dx.doi.org/10.1137/17M1118312,math.AG,"['math.AG', 'cs.CG', 'cs.SC', 'math.CO', '14T05, 14N99, 52C25, 05C99']",10.1137/17M1118312,,[]
Computations with p-adic numbers,http://arxiv.org/abs/1701.06794v1,2017-01-24T10:22:45Z,2017-01-24T10:22:45Z,"  This document contains the notes of a lecture I gave at the ""Journ\'ees
Nationales du Calcul Formel"" (JNCF) on January 2017. The aim of the lecture was
to discuss low-level algorithmics for p-adic numbers. It is divided into two
main parts: first, we present various implementations of p-adic numbers and
compare them and second, we introduce a general framework for studying
precision issues and apply it in several concrete situations.
",['\nXavier Caruso\nIRMAR\n'],,,http://arxiv.org/abs/1701.06794v1,math.NT,"['math.NT', 'cs.NA', 'cs.SC']",,,['IRMAR']
"The Method of Arbitrarily Large Moments to Calculate Single Scale
  Processes in Quantum Field Theory",http://arxiv.org/abs/1701.04614v1,2017-01-17T10:50:33Z,2017-01-17T10:50:33Z,"  We device a new method to calculate a large number of Mellin moments of
single scale quantities using the systems of differential and/or difference
equations obtained by integration-by-parts identities between the corresponding
Feynman integrals of loop corrections to physical quantities. These scalar
quantities have a much simpler mathematical structure than the complete
quantity. A sufficiently large set of moments may even allow the analytic
reconstruction of the whole quantity considered, holding in case of first order
factorizing systems. In any case, one may derive highly precise numerical
representations in general using this method, which is otherwise completely
analytic.
","['\nJohannes Blümlein\n', '\nCarsten Schneider\n']",4 pages LATEX,,http://dx.doi.org/10.1016/j.physletb.2017.05.001,hep-ph,"['hep-ph', 'cs.SC', 'hep-lat', 'hep-th', 'math-ph', 'math.MP']",10.1016/j.physletb.2017.05.001,,[]
Functional Decomposition using Principal Subfields,http://arxiv.org/abs/1701.03529v2,2017-01-12T23:14:56Z,2017-05-26T19:31:14Z,"  Let $f\in K(t)$ be a univariate rational function. It is well known that any
non-trivial decomposition $g \circ h$, with $g,h\in K(t)$, corresponds to a
non-trivial subfield $K(f(t))\subsetneq L \subsetneq K(t)$ and vice-versa. In
this paper we use the idea of principal subfields and fast
subfield-intersection techniques to compute the subfield lattice of
$K(t)/K(f(t))$. This yields a Las Vegas type algorithm with improved complexity
and better run times for finding all non-equivalent complete decompositions of
$f$.
","['\nLuiz E. Allem\n', '\nJuliane Capaverde\n', '\nMark van Hoeij\n', '\nJonas Szutkoski\n']","8 pages, accepted for ISSAC'17",,http://dx.doi.org/10.1145/3087604.3087608,cs.SC,"['cs.SC', '68W30', 'I.1.2']",10.1145/3087604.3087608,,[]
"Computing Approximate Greatest Common Right Divisors of Differential
  Polynomials",http://arxiv.org/abs/1701.01994v2,2017-01-08T18:06:21Z,2019-04-26T18:40:29Z,"  Differential (Ore) type polynomials with ""approximate"" polynomial
coefficients are introduced. These provide an effective notion of approximate
differential operators, with a strong algebraic structure. We introduce the
approximate Greatest Common Right Divisor Problem (GCRD) of differential
polynomials, as a non-commutative generalization of the well-studied
approximate GCD problem.
  Given two differential polynomials, we present an algorithm to find nearby
differential polynomials with a non-trivial GCRD, where nearby is defined with
respect to a suitable coefficient norm. Intuitively, given two linear
differential polynomials as input, the (approximate) GCRD problem corresponds
to finding the (approximate) differential polynomial whose solution space is
the intersection of the solution spaces of the two inputs.
  The approximate GCRD problem is proven to be locally well-posed. A method
based on the singular value decomposition of a differential Sylvester matrix is
developed to produce an initial approximation of the GCRD. With a sufficiently
good initial approximation, Newton iteration is shown to converge quadratically
to an optimal solution. Finally, sufficient conditions for existence of a
solution to the global problem are presented along with examples demonstrating
that no solution exists when these conditions are not satisfied.
","['\nMark Giesbrecht\n', '\nJoseph Haraldson\n', '\nErich Kaltofen\n']",,,http://arxiv.org/abs/1701.01994v2,cs.SC,"['cs.SC', 'cs.NA']",,,[]
Semialgebraic Invariant Synthesis for the Kannan-Lipton Orbit Problem,http://arxiv.org/abs/1701.02162v1,2017-01-09T13:00:53Z,2017-01-09T13:00:53Z,"  The \emph{Orbit Problem} consists of determining, given a linear
transformation $A$ on $\mathbb{Q}^d$, together with vectors $x$ and $y$,
whether the orbit of $x$ under repeated applications of $A$ can ever reach $y$.
This problem was famously shown to be decidable by Kannan and Lipton in the
1980s.
  In this paper, we are concerned with the problem of synthesising suitable
\emph{invariants} $\mathcal{P} \subseteq \mathbb{R}^d$, \emph{i.e.}, sets that
are stable under $A$ and contain $x$ and not $y$, thereby providing compact and
versatile certificates of non-reachability. We show that whether a given
instance of the Orbit Problem admits a semialgebraic invariant is decidable,
and moreover in positive instances we provide an algorithm to synthesise
suitable invariants of polynomial size.
  It is worth noting that the existence of \emph{semilinear} invariants, on the
other hand, is (to the best of our knowledge) not known to be decidable.
","['\nNathanaël Fijalkow\n', '\nPierre Ohlmann\n', '\nJoël Ouaknine\n', '\nAmaury Pouly\n', '\nJames Worrell\n']",,,http://arxiv.org/abs/1701.02162v1,cs.CC,"['cs.CC', 'cs.LO', 'cs.SC', 'math.AG', 'math.NT']",,,[]
Time and space efficient generators for quasiseparable matrices,http://arxiv.org/abs/1701.00396v2,2017-01-02T14:12:20Z,2019-10-21T11:13:17Z,"  The class of quasiseparable matrices is defined by the property that any
submatrix entirely below or above the main diagonal has small rank, namely
below a bound called the order of quasiseparability. These matrices arise
naturally in solving PDE's for particle interaction with the Fast Multi-pole
Method (FMM), or computing generalized eigenvalues. From these application
fields, structured representations and algorithms have been designed in
numerical linear algebra to compute with these matrices in time linear in the
matrix dimension and either quadratic or cubic in the quasiseparability order.
Motivated by the design of the general purpose exact linear algebra library
LinBox, and by algorithmic applications in algebraic computing, we adapt
existing techniques introduce novel ones to use quasiseparable matrices in
exact linear algebra, where sub-cubic matrix arithmetic is available. In
particular, we will show, the connection between the notion of
quasiseparability and the rank profile matrix invariant, that we have
introduced in 2015. It results in two new structured representations, one being
a simpler variation on the hierarchically semiseparable storage, and the second
one exploiting the generalized Bruhat decomposition. As a consequence, most
basic operations, such as computing the quasiseparability orders, applying a
vector, a block vector, multiplying two quasiseparable matrices together,
inverting a quasiseparable matrix, can be at least as fast and often faster
than previous existing algorithms.
","['\nClement Pernet\nARIC\n', '\nArne Storjohann\n']",,"Journal of Symbolic Computation, Elsevier, 2018, Special issue on
  the 41th International Symposium on Symbolic and Alge-braic Computation
  (ISSAC'16), 85, pp.224-246. \&\#x27E8;10.1016/j.jsc.2017.07.010\&\#x27E9",http://arxiv.org/abs/1701.00396v2,cs.SC,['cs.SC'],,,['ARIC']
Computing in quotients of rings of integers,http://arxiv.org/abs/1612.09176v1,2016-12-29T15:45:01Z,2016-12-29T15:45:01Z,"  We develop algorithms to turn quotients of rings of rings of integers into
effective Euclidean rings by giving polynomial algorithms for all fundamental
ring operations. In addition, we study normal forms for modules over such rings
and their behavior under certain quotients. We illustrate the power of our
ideas in a new modular normal form algorithm for modules over rings of
integers, vastly outperforming classical algorithms.
","['\nTommy Hofmann\n', '\nClaus Fieker\n']",,"LMS Journal of Computation and Mathematics, 17(A) (2014), 349-365",http://dx.doi.org/10.1112/S1461157014000291,math.NT,"['math.NT', 'cs.SC', '11Y40 (Primary), 11-04 (Secondary)']",10.1112/S1461157014000291,,[]
"On the computation of the HNF of a module over the ring of integers of a
  number field",http://arxiv.org/abs/1612.09428v1,2016-12-30T09:15:17Z,2016-12-30T09:15:17Z,"  We present a variation of the modular algorithm for computing the Hermite
normal form of an $\mathcal O_K$-module presented by Cohen, where $\mathcal
O_K$ is the ring of integers of a number field $K$. An approach presented in
(Cohen 1996) based on reductions modulo ideals was conjectured to run in
polynomial time by Cohen, but so far, no such proof was available in the
literature. In this paper, we present a modification of the approach of Cohen
to prevent the coefficient swell and we rigorously assess its complexity with
respect to the size of the input and the invariants of the field $K$.
","['\nJean-François Biasse\n', '\nClaus Fieker\n', '\nTommy Hofmann\n']",,"Journal of Symbolic Computation, Volume 80 (2017), Pages 581-615",http://dx.doi.org/10.1016/j.jsc.2016.07.027,math.NT,"['math.NT', 'cs.SC', '11Y40']",10.1016/j.jsc.2016.07.027,,[]
Reverse Engineering of Irreducible Polynomials in GF(2^m) Arithmetic,http://arxiv.org/abs/1612.04588v1,2016-12-14T11:41:05Z,2016-12-14T11:41:05Z,"  Current techniques for formally verifying circuits implemented in Galois
field (GF) arithmetic are limited to those with a known irreducible polynomial
P(x). This paper presents a computer algebra based technique that extracts the
irreducible polynomial P(x) used in the implementation of a multiplier in
GF(2^m). The method is based on first extracting a unique polynomial in Galois
field of each output bit independently. P(x) is then obtained by analyzing the
algebraic expression in GF(2^m) of each output bit. We demonstrate that this
method is able to reverse engineer the irreducible polynomial of an n-bit GF
multiplier in n threads. Experiments were performed on Mastrovito and
Montgomery multipliers with different P (x), including NIST-recommended
polynomials and optimal polynomials for different microprocessor architectures.
","['\nCunxi Yu\n', '\nDaniel Holcomb\n', '\nMaciej Ciesielski\n']","6 pages, 4 figures, DATE 2017, Lausanne, Switzerland, March 27-31,
  2017",,http://arxiv.org/abs/1612.04588v1,cs.SC,['cs.SC'],,,[]
Efficient sparse polynomial factoring using the Funnel heap,http://arxiv.org/abs/1612.05403v1,2016-12-16T09:42:59Z,2016-12-16T09:42:59Z,"  This work is a comprehensive extension of Abu-Salem et al. (2015) that
investigates the prowess of the Funnel Heap for implementing sums of products
in the polytope method for factoring polynomials, when the polynomials are in
sparse distributed representation. We exploit that the work and cache
complexity of an Insert operation using Funnel Heap can be refined to de- pend
on the rank of the inserted monomial product, where rank corresponds to its
lifetime in Funnel Heap. By optimising on the pattern by which insertions and
extractions occur during the Hensel lifting phase of the polytope method, we
are able to obtain an adaptive Funnel Heap that minimises all of the work,
cache, and space complexity of this phase. Additionally, we conduct a detailed
empirical study confirming the superiority of Funnel Heap over the generic
Binary Heap once swaps to external memory begin to take place. We demonstrate
that Funnel Heap is a more efficient merger than the cache oblivious k-merger,
which fails to achieve its optimal (and amortised) cache complexity when used
for performing sums of products. This provides an empirical proof of concept
that the overlapping approach for perform- ing sums of products using one
global Funnel Heap is more suited than the serialised approach, even when the
latter uses the best merging structures available.
","['\nFatima K. Abu Salem\n', '\nKhalil El-Harake\n', '\nKarl Gemayel\n']",,,http://arxiv.org/abs/1612.05403v1,cs.SC,['cs.SC'],,,[]
Computing solutions of linear Mahler equations,http://arxiv.org/abs/1612.05518v2,2016-12-16T15:56:27Z,2018-04-10T12:47:01Z,"  Mahler equations relate evaluations of the same function $f$ at iterated
$b$th powers of the variable. They arise in particular in the study of
automatic sequences and in the complexity analysis of divide-and-conquer
algorithms. Recently, the problem of solving Mahler equations in closed form
has occurred in connection with number-theoretic questions. A difficulty in the
manipulation of Mahler equations is the exponential blow-up of degrees when
applying a Mahler operator to a polynomial. In this work, we present algorithms
for solving linear Mahler equations for series, polynomials, and rational
functions, and get polynomial-time complexity under a mild assumption.
Incidentally, we develop an algorithm for computing the gcrd of a family of
linear Mahler operators.
","['\nFrédéric Chyzak\n', '\nThomas Dreyfus\n', '\nPhilippe Dumas\n', '\nMarc Mezzarobba\n']",Minor changes + New section 3.6,"Mathematics of Computation, 87 (2018), 2977-3021",http://dx.doi.org/10.1090/mcom/3359,cs.SC,['cs.SC'],10.1090/mcom/3359,,[]
Fast Matrix Multiplication and Symbolic Computation,http://arxiv.org/abs/1612.05766v1,2016-12-17T13:51:17Z,2016-12-17T13:51:17Z,"  The complexity of matrix multiplication (hereafter MM) has been intensively
studied since 1969, when Strassen surprisingly decreased the exponent 3 in the
cubic cost of the straightforward classical MM to log 2 (7) $\approx$ 2.8074.
Applications to some fundamental problems of Linear Algebra and Computer
Science have been immediately recognized, but the researchers in Computer
Algebra keep discovering more and more applications even today, with no sign of
slowdown. We survey the unfinished history of decreasing the exponent towards
its information lower bound 2, recall some important techniques discovered in
this process and linked to other fields of computing, reveal sample surprising
applications to fast computation of the inner products of two vectors and
summation of integers, and discuss the curse of recursion, which separates the
progress in fast MM into its most acclaimed and purely theoretical part and
into valuable acceleration of MM of feasible sizes. Then, in the second part of
our paper, we cover fast MM in realistic symbolic computations and discuss
applications and implementation of fast exact matrix multiplication. We first
review how most of exact linear algebra can be reduced to matrix multiplication
over small finite fields. Then we highlight the differences in the design of
approximate and exact implementations of fast MM, taking into account nowadays
processor and memory hierarchies. In the concluding section we comment on
current perspectives of the study of fast MM.
","['\nJean-Guillaume Dumas\nCASYS\n', '\nVictor Pan\n']",,,http://arxiv.org/abs/1612.05766v1,cs.SC,['cs.SC'],,,['CASYS']
"Comparative study of space filling curves for cache oblivious TU
  Decomposition",http://arxiv.org/abs/1612.06069v1,2016-12-19T08:07:16Z,2016-12-19T08:07:16Z,"  We examine several matrix layouts based on space-filling curves that allow
for a cache-oblivious adaptation of parallel TU decomposition for rectangular
matrices over finite fields. The TU algorithm of \cite{Dumas} requires index
conversion routines for which the cost to encode and decode the chosen curve is
significant. Using a detailed analysis of the number of bit operations required
for the encoding and decoding procedures, and filtering the cost of lookup
tables that represent the recursive decomposition of the Hilbert curve, we show
that the Morton-hybrid order incurs the least cost for index conversion
routines that are required throughout the matrix decomposition as compared to
the Hilbert, Peano, or Morton orders. The motivation lies in that cache
efficient parallel adaptations for which the natural sequential evaluation
order demonstrates lower cache miss rate result in overall faster performance
on parallel machines with private or shared caches, on GPU's, or even cloud
computing platforms. We report on preliminary experiments that demonstrate how
the TURBO algorithm in Morton-hybrid layout attains orders of magnitude
improvement in performance as the input matrices increase in size. For example,
when $N = 2^{13}$, the row major TURBO algorithm concludes within about 38.6
hours, whilst the Morton-hybrid algorithm with truncation size equal to $64$
concludes within 10.6 hours.
","['\nFatima K. Abu Salem\n', '\nMira Al Arab\n']",,,http://arxiv.org/abs/1612.06069v1,cs.SC,['cs.SC'],,,[]
Parallel Integer Polynomial Multiplication,http://arxiv.org/abs/1612.05778v1,2016-12-17T14:54:52Z,2016-12-17T14:54:52Z,"  We propose a new algorithm for multiplying dense polynomials with integer
coefficients in a parallel fashion, targeting multi-core processor
architectures. Complexity estimates and experimental comparisons demonstrate
the advantages of this new approach.
","['\nChangbo Chen\n', '\nSvyatoslav Covanov\n', '\nFarnam Mansouri\n', '\nMarc Moreno Maza\n', '\nNing Xie\n', '\nYuzhen Xie\n']",,,http://arxiv.org/abs/1612.05778v1,cs.SC,"['cs.SC', 'cs.MS']",,,[]
"The Method of Gauss-Newton to Compute Power Series Solutions of
  Polynomial Homotopies",http://arxiv.org/abs/1612.05313v4,2016-12-15T23:52:29Z,2017-10-26T00:54:59Z,"  We consider the extension of the method of Gauss-Newton from complex
floating-point arithmetic to the field of truncated power series with complex
floating-point coefficients. With linearization we formulate a linear system
where the coefficient matrix is a series with matrix coefficients, and provide
a characterization for when the matrix series is regular based on the algebraic
variety of an augmented system. The structure of the linear system leads to a
block triangular system. In the regular case, solving the linear system is
equivalent to solving a Hermite interpolation problem. We show that this
solution has cost cubic in the problem size. In general, at singular points, we
rely on methods of tropical algebraic geometry to compute Puiseux series. With
a few illustrative examples, we demonstrate the application to polynomial
homotopy continuation.
","['\nNathan Bliss\n', '\nJan Verschelde\n']","21 pages, 9 figures",,http://arxiv.org/abs/1612.05313v4,math.NA,"['math.NA', 'cs.MS', 'cs.SC', 'math.AG']",,,[]
"Automatic Differentiation: a look through Tensor and Operational
  Calculus",http://arxiv.org/abs/1612.02731v3,2016-12-08T17:06:29Z,2018-08-30T19:33:50Z,"  In this paper we take a look at Automatic Differentiation through the eyes of
Tensor and Operational Calculus. This work is best consumed as supplementary
material for learning tensor and operational calculus by those already familiar
with automatic differentiation. To that purpose, we provide a simple
implementation of automatic differentiation, where the steps taken are
explained in the language tensor and operational calculus.
",['\nŽiga Sajovic\n'],,,http://arxiv.org/abs/1612.02731v3,cs.SC,"['cs.SC', 'math.FA']",,,[]
Computer Algebra and Material Design,http://arxiv.org/abs/1612.02275v4,2016-12-06T07:16:40Z,2018-02-17T00:05:27Z,"  This article is intended to an introductory lecture in material physics, in
which the modern computational group theory and the electronic structure
calculation are in collaboration. The effort of mathematicians in field of the
group theory, have ripened as a new trend, called ""computer algebra"", outcomes
of which now can be available as handy computational packages, and would also
be useful to physicists with practical purposes. This article, in the former
part, explains how to use the computer algebra for the applications in the
solid-state simulation, by means of one of the computer algebra package, the
GAP system. The computer algebra enables us to obtain various group theoretical
properties with ease, such as the representations, the character tables, the
subgroups, etc. Furthermore it would grant us a new perspective of material
design, which could be executed in mathematically rigorous and systematic way.
Some technical details and some computations which require the knowledge of a
little higher mathematics (but computable easily by the computer algebra) are
also given. The selected topics will provide the reader with some insights
toward the dominating role of the symmetry in crystal, or, the ""mathematical
first principles"" in it. In the latter part of the article, we analyze the
relation between the structural symmetry and the electronic structure in
C$_{60}$ (as an example to the sysmem without periodicity). The principal
object of the study is to illustrate the hierarchical change of the
quantum-physical properties of the molecule, in accordance with the reduction
of the symmetry (as it descends down in the ladder of subgroups). In order to
serve the common interest of the researchers, the details of the computations
(the required initial data and the small programs developed for the purpose)
are explained as minutely as possible.
",['\nAkihito Kikuchi\n'],"The 4th version: some parts of the article (statements, equations,
  figures, references, and the appendix ) are rewritten. It contains 227 pages",,http://arxiv.org/abs/1612.02275v4,cond-mat.mtrl-sci,"['cond-mat.mtrl-sci', 'cs.CE', 'cs.SC', 'math.GR']",,,[]
Baby-Step Giant-Step Algorithms for the Symmetric Group,http://arxiv.org/abs/1612.03456v1,2016-12-11T19:15:48Z,2016-12-11T19:15:48Z,"  We study discrete logarithms in the setting of group actions. Suppose that
$G$ is a group that acts on a set $S$. When $r,s \in S$, a solution $g \in G$
to $r^g = s$ can be thought of as a kind of logarithm. In this paper, we study
the case where $G = S_n$, and develop analogs to the Shanks baby-step /
giant-step procedure for ordinary discrete logarithms. Specifically, we compute
two sets $A, B \subseteq S_n$ such that every permutation of $S_n$ can be
written as a product $ab$ of elements $a \in A$ and $b \in B$. Our
deterministic procedure is optimal up to constant factors, in the sense that
$A$ and $B$ can be computed in optimal asymptotic complexity, and $|A|$ and
$|B|$ are a small constant from $\sqrt{n!}$ in size. We also analyze randomized
""collision"" algorithms for the same problem.
","['\nEric Bach\n', '\nBryce Sandlund\n']",,,http://arxiv.org/abs/1612.03456v1,cs.SC,"['cs.SC', 'cs.DS', 'math.GR']",,,[]
"A-Discriminants for Complex Exponents, and Counting Real Isotopy Types",http://arxiv.org/abs/1612.03458v2,2016-12-11T19:22:40Z,2017-10-29T14:23:54Z,"  We extend the definition of $\mathcal{A}$-discriminant varieties, and
Kapranov's parametrization of $\mathcal{A}$-discriminant varieties, to complex
exponents. As an application, we study the special case where $\mathcal{A}$ is
a fixed real $n\times (n+3)$ matrix whose columns form the spectrum of an
$n$-variate exponential sum $g$ with fixed sign vector for its coefficients: We
prove that the number of possible isotopy types for the real zero set of $g$ is
$O(n^2)$. The best previous upper bound was $2^{O(n^4)}$. Along the way, we
also show that the singular loci of our generalized $\mathcal{A}$-discriminants
are images of low-degree algebraic sets under certain analytic maps.
","['\nJ. Maurice Rojas\n', '\nKorben Rusek\n']","14 pages, 13 illustrations, submitted for publication. (Previous
  version was accepted and presented at MEGA 2017.)",,http://arxiv.org/abs/1612.03458v2,math.AG,"['math.AG', 'cs.SC', 'math.CV']",,,[]
Reduction-Based Creative Telescoping for Fuchsian D-finite Functions,http://arxiv.org/abs/1611.07421v1,2016-11-22T17:05:04Z,2016-11-22T17:05:04Z,"  Continuing a series of articles in the past few years on creative telescoping
using reductions, we adapt Trager's Hermite reduction for algebraic functions
to fuchsian D-finite functions and develop a reduction-based creative
telescoping algorithm for this class of functions, thereby generalizing our
recent reduction-based algorithm for algebraic functions, presented at ISSAC
2016.
","['\nShaoshi Chen\n', '\nMark van Hoeij\n', '\nManuel Kauers\n', '\nChristoph Koutschan\n']",arXiv admin note: text overlap with arXiv:1602.00424,,http://arxiv.org/abs/1611.07421v1,cs.SC,['cs.SC'],,,[]
Groebner Bases for Everyone with CoCoA-5 and CoCoALib,http://arxiv.org/abs/1611.07306v2,2016-11-22T14:10:27Z,2017-04-18T15:44:03Z,"  We present a survey on the developments related to Groebner bases, and show
explicit examples in CoCoA. The CoCoA project dates back to 1987: its aim was
to create a ""mathematician""-friendly computational laboratory for studying
Commutative Algebra, most especially Groebner bases. Always maintaining this
""friendly"" tradition, the project has grown and evolved, and the software has
been completely rewritten. CoCoA offers Groebner bases for all levels of
interest: from the basic, explicit call in the interactive system CoCoA-5, to
problem-specific optimized implementations, to the computer--computer
communication with the open source C++ software library, CoCoALib, or the
prototype OpenMath-based server. The openness and clean design of CoCoALib and
CoCoA-5 are intended to offer different levels of usage, and to encourage
external contributions.
","['\nJohn Abbott\n', '\nAnna Maria Bigatti\n']",24 pages,"Advanced Studies in Pure Mathematics 75, 2017",http://arxiv.org/abs/1611.07306v2,math.AC,"['math.AC', 'cs.SC', '13P15, 13P10, 13-04, 14Q10, 68W30, 05E40, 68W24']",,,[]
Symmetries of Canal Surfaces and Dupin Cyclides,http://arxiv.org/abs/1611.06768v2,2016-11-21T12:57:43Z,2017-08-14T09:15:54Z,"  We develop a characterization for the existence of symmetries of canal
surfaces defined by a rational spine curve and rational radius function. In
turn, this characterization inspires an algorithm for computing the symmetries
of such canal surfaces. For Dupin cyclides in canonical form, we apply the
characterization to derive an intrinsic description of their symmetries and
symmetry groups, which gives rise to a method for computing the symmetries of a
Dupin cyclide not necessarily in canonical form. As a final application, we
discuss the construction of patches and blends of rational canal surfaces with
a prescribed symmetry.
","['\nJuan Gerardo Alcázar\n', '\nHeidi E. I. Dahl\n', '\nGeorg Muntingh\n']",27 pages,,http://arxiv.org/abs/1611.06768v2,math.AG,"['math.AG', 'cs.CG', 'cs.SC', '14Q10, 68W30']",,,[]
Faster integer multiplication using plain vanilla FFT primes,http://arxiv.org/abs/1611.07144v2,2016-11-22T04:35:14Z,2017-10-16T05:01:45Z,"  Assuming a conjectural upper bound for the least prime in an arithmetic
progression, we show that n-bit integers may be multiplied in O(n log n
4^(log^* n)) bit operations.
","['\nDavid Harvey\n', '\nJoris van der Hoeven\n']","14 pages, to appear in Mathematics of Computation",,http://arxiv.org/abs/1611.07144v2,cs.SC,"['cs.SC', 'cs.CC', 'math.NT', '68W30, 68Q17, 68W40', 'G.1.0; F.2.1']",,,[]
"Symbolic Representation for Analog Realization of A Family of Fractional
  Order Controller Structures via Continued Fraction Expansion",http://arxiv.org/abs/1611.09795v1,2016-11-29T19:19:22Z,2016-11-29T19:19:22Z,"  This paper uses the Continued Fraction Expansion (CFE) method for analog
realization of fractional order differ-integrator and few special classes of
fractional order (FO) controllers viz. Fractional Order
Proportional-Integral-Derivative (FOPID) controller, FO[PD] controller and FO
lead-lag compensator. Contemporary researchers have given several formulations
for rational approximation of fractional order elements. However, approximation
of the controllers studied in this paper, due to having fractional power of a
rational transfer function, is not available in analog domain; although its
digital realization already exists. This motivates us for applying CFE based
analog realization technique for complicated FO controller structures to get
equivalent rational transfer functions in terms of the controller tuning
parameters. The symbolic expressions for rationalized transfer function in
terms of the controller tuning parameters are especially important as ready
references, without the need of running CFE algorithm every time and also helps
in the synthesis of analog circuits for such FO controllers.
","['\nAnindya Pakhira\n', '\nSaptarshi Das\n', '\nIndranil Pan\n', '\nShantanu Das\n']","21 pages, 11 figures","ISA Transactions, Volume 57, July 2015, Pages 390-402",http://dx.doi.org/10.1016/j.isatra.2015.01.007,cs.SY,"['cs.SY', 'cs.SC', 'math.DS']",10.1016/j.isatra.2015.01.007,,[]
D-finite Numbers,http://arxiv.org/abs/1611.05901v3,2016-11-17T21:20:58Z,2018-05-26T15:13:04Z,"  D-finite functions and P-recursive sequences are defined in terms of linear
differential and recurrence equations with polynomial coefficients. In this
paper, we introduce a class of numbers closely related to D-finite functions
and P-recursive sequences. It consists of the limits of convergent P-recursive
sequences. Typically, this class contains many well-known mathematical
constants in addition to the algebraic numbers. Our definition of the class of
D-finite numbers depends on two subrings of the field of complex numbers. We
investigate how different choices of these two subrings affect the class.
Moreover, we show that D-finite numbers are essentially limits of D-finite
functions at the point one, and evaluating D-finite functions at non-singular
algebraic points typically yields D-finite numbers. This result makes it easier
to recognize certain numbers to be D-finite.
","['\nHui Huang\n', '\nManuel Kauers\n']",19 pages,,http://dx.doi.org/10.1142/S1793042118501099,math.NT,"['math.NT', 'cs.SC']",10.1142/S1793042118501099,,[]
Efficient Parallel Verification of Galois Field Multipliers,http://arxiv.org/abs/1611.05101v2,2016-11-16T00:03:28Z,2019-01-24T15:50:19Z,"  Galois field (GF) arithmetic is used to implement critical arithmetic
components in communication and security-related hardware, and verification of
such components is of prime importance. Current techniques for formally
verifying such components are based on computer algebra methods that proved
successful in verification of integer arithmetic circuits. However, these
methods are sequential in nature and do not offer any parallelism. This paper
presents an algebraic functional verification technique of gate-level GF (2m )
multipliers, in which verification is performed in bit-parallel fashion. The
method is based on extracting a unique polynomial in Galois field of each
output bit independently. We demonstrate that this method is able to verify an
n-bit GF multiplier in n threads. Experiments performed on pre- and
post-synthesized Mastrovito and Montgomery multipliers show high efficiency up
to 571 bits.
","['\nCunxi Yu\n', '\nMaciej Ciesielski\n']","6 pages, 22nd Asia and South Pacific Design Automation Conference
  (ASP-DAC 2017), Japan",,http://arxiv.org/abs/1611.05101v2,cs.SC,"['cs.SC', 'cs.CR', 'cs.LO']",,,[]
"Sparse multivariate factorization by mean of a few bivariate
  factorizations",http://arxiv.org/abs/1611.02569v1,2016-11-08T15:53:13Z,2016-11-08T15:53:13Z,"  We describe an algorithm to factor sparse multivariate polynomials using O(d)
bivariate factorizations where d is the number of variables. This algorithm is
implemented in the Giac/Xcas computer algebra system.
",['\nBernard Parisse\nIF\n'],,,http://arxiv.org/abs/1611.02569v1,cs.SC,['cs.SC'],,,['IF']
"SPECTRA -- a Maple library for solving linear matrix inequalities in
  exact arithmetic",http://arxiv.org/abs/1611.01947v3,2016-11-07T09:19:26Z,2020-02-11T10:51:32Z,"  This document describes our freely distributed Maple library {\sc spectra},
for Semidefinite Programming solved Exactly with Computational Tools of Real
Algebra. It solves linear matrix inequalities with symbolic computation in
exact arithmetic and it is targeted to small-size, possibly degenerate problems
for which symbolic infeasibility or feasibility certificates are required.
","['\nMohab Safey El Din\nPolSys\n', '\nDidier Henrion\nLAAS-MAC, CTU\n', '\nSimone Naldi\nTU\n', '\nMohab Safey\nPolSys\n', '\nEl Din\nPolSys\n']",Significantly extended version,"Optimization Methods and Software, Taylor \& Francis, 2019, 34
  (1), pp.62-78. \&\#x27E8;10.1080/10556788.2017.1341505\&\#x27E9",http://arxiv.org/abs/1611.01947v3,math.OC,"['math.OC', 'cs.SC', 'math.AG']",,,"['PolSys', 'LAAS-MAC, CTU', 'TU', 'PolSys', 'PolSys']"
Explicit equivalence of quadratic forms over $\mathbb{F}_q(t)$,http://arxiv.org/abs/1610.08671v2,2016-10-27T09:24:08Z,2018-09-10T09:29:50Z,"  We propose a randomized polynomial time algorithm for computing nontrivial
zeros of quadratic forms in 4 or more variables over $\mathbb{F}_q(t)$, where
$\mathbb{F}_q$ is a finite field of odd characteristic. The algorithm is based
on a suitable splitting of the form into two forms and finding a common value
they both represent. We make use of an effective formula for the number of
fixed degree irreducible polynomials in a given residue class. We apply our
algorithms for computing a Witt decomposition of a quadratic form, for
computing an explicit isometry between quadratic forms and finding zero
divisors in quaternion algebras over quadratic extensions of $\mathbb{F}_q(t)$.
","['\nGábor Ivanyos\n', '\nPéter Kutas\n', '\nLajos Rónyai\n']","27 pages, revised version, accepted for publication in Finite Fields
  and Their Applications",,http://arxiv.org/abs/1610.08671v2,math.RA,"['math.RA', 'cs.SC', 'math.NT']",,,[]
"Bounds for elimination of unknowns in systems of differential-algebraic
  equations",http://arxiv.org/abs/1610.04022v7,2016-10-13T11:03:31Z,2020-10-05T14:31:59Z,"  Elimination of unknowns in systems of equations, starting with Gaussian
elimination, is a problem of general interest. The problem of finding an a
priori upper bound for the number of differentiations in elimination of
unknowns in a system of differential-algebraic equations (DAEs) is an important
challenge, going back to Ritt (1932). The first characterization of this via an
asymptotic analysis is due to Grigoriev's result (1989) on quantifier
elimination in differential fields, but the challenge still remained.
  In this paper, we present a new bound, which is a major improvement over the
previously known results. We also present a new lower bound, which shows
asymptotic tightness of our upper bound in low dimensions, which are frequently
occurring in applications. Finally, we discuss applications of our results to
designing new algorithms for elimination of unknowns in systems of DAEs.
","['\nAlexey Ovchinnikov\n', '\nGleb Pogudin\n', '\nN. Thieu Vo\n']",minor revision,,http://arxiv.org/abs/1610.04022v7,math.AC,"['math.AC', 'cs.SC', 'math.AG', '12H05, 12H20, 14Q20, 34A09']",,,[]
"Experience with Heuristics, Benchmarks & Standards for Cylindrical
  Algebraic Decomposition",http://arxiv.org/abs/1609.09269v1,2016-09-29T09:30:22Z,2016-09-29T09:30:22Z,"  In the paper which inspired the SC-Square project, [E. Abraham, Building
Bridges between Symbolic Computation and Satisfiability Checking, Proc. ISSAC
'15, pp. 1-6, ACM, 2015] the author identified the use of sophisticated
heuristics as a technique that the Satisfiability Checking community excels in
and from which it is likely the Symbolic Computation community could learn and
prosper. To start this learning process we summarise our experience with
heuristic development for the computer algebra algorithm Cylindrical Algebraic
Decomposition. We also propose and discuss standards and benchmarks as another
area where Symbolic Computation could prosper from Satisfiability Checking
expertise, noting that these have been identified as initial actions for the
new SC-Square community in the CSA project, as described in [E.~Abraham et al.,
SC$^2$: Satisfiability Checking meets Symbolic Computation (Project Paper)},
Intelligent Computer Mathematics (LNCS 9761), pp. 28--43, Springer, 2015].
","['\nMatthew England\n', '\nJames H. Davenport\n']","Presented at the 1st International Workshop on Satisfiability
  Checking and Symbolic Computation (SC-Square 2016)","Proceedings of the 1st Workshop on Satisfiability Checking and
  Symbolic Computation (SC2 '16), E. Abraham, J.H. Davenport and P. Fontaine
  eds. CEUR Workshop Proceedings 1804, 2016. http://ceur-ws.org/Vol-1804/",http://arxiv.org/abs/1609.09269v1,cs.SC,"['cs.SC', 'cs.LO', '68W30, 68T05', 'I.1.2, I.2.6']",,,[]
Algebraic and algorithmic aspects of radical parametrizations,http://arxiv.org/abs/1609.09760v2,2016-09-30T14:48:24Z,2017-02-01T12:08:17Z,"  In this article algebraic constructions are introduced in order to study the
variety defined by a radical parametrization (a tuple of functions involving
complex numbers, $n$ variables, the four field operations and radical
extractions). We provide algorithms to implicitize radical parametrizations and
to check whether a radical parametrization can be reparametrized into a
rational parametrization.
","['\nJ. Rafael Sendra\n', '\nDavid Sevilla\n', '\nCarlos Villarino\n']","26 pages; revised version accepted for publication in Computer Aided
  Geometric Design",,http://dx.doi.org/10.1016/j.cagd.2017.01.002,math.AG,"['math.AG', 'cs.SC']",10.1016/j.cagd.2017.01.002,,[]
"Some results on counting roots of polynomials and the Sylvester
  resultant",http://arxiv.org/abs/1609.08712v1,2016-09-27T23:59:31Z,2016-09-27T23:59:31Z,"  We present two results, the first on the distribution of the roots of a
polynomial over the ring of integers modulo $n$ and the second on the
distribution of the roots of the Sylvester resultant of two multivariate
polynomials. The second result has application to polynomial GCD computation
and solving polynomial diophantine equations.
","['\nMichael Monagan\n', '\nBaris Tuncer\n']","Presented at FPSAC 2016, Vancouver, Canada, 2016",,http://arxiv.org/abs/1609.08712v1,math.CO,"['math.CO', 'cs.DM', 'cs.SC']",,,[]
Automatic Library Generation for Modular Polynomial Multiplication,http://arxiv.org/abs/1609.01010v1,2016-09-05T01:11:45Z,2016-09-05T01:11:45Z,"  Polynomial multiplication is a key algorithm underlying computer algebra
systems (CAS) and its efficient implementation is crucial for the performance
of CAS. In this paper we design and implement algorithms for polynomial
multiplication using approaches based the fast Fourier transform (FFT) and the
truncated Fourier transform (TFT). We improve on the state-of-the-art in both
theoretical and practical performance. The {\SPIRAL} library generation system
is extended and used to automatically generate and tune the performance of a
polynomial multiplication library that is optimized for memory hierarchy,
vectorization and multi-threading, using new and existing algorithms. The
performance tuning has been aided by the use of automation where many code
choices are generated and intelligent search is utilized to find the ""best""
implementation on a given architecture. The performance of autotuned
implementations is comparable to, and in some cases better than, the best
hand-tuned code.
",['\nLingchuan Meng\n'],"15 pages, 3 figures, based on the thesis work of Lingchuan Meng",,http://arxiv.org/abs/1609.01010v1,cs.SC,['cs.SC'],,,[]
Some Open Problems related to Creative Telescoping,http://arxiv.org/abs/1609.03768v1,2016-09-13T11:16:29Z,2016-09-13T11:16:29Z,"  Creative telescoping is the method of choice for obtaining information about
definite sums or integrals. It has been intensively studied since the early
1990s, and can now be considered as a classical technique in computer algebra.
At the same time, it is still subject of ongoing research. In this paper, we
present a selection of open problems in this context. We would be curious to
hear about any substantial progress on any of these problems.
","['\nShaoshi Chen\n', '\nManuel Kauers\n']",,,http://arxiv.org/abs/1609.03768v1,cs.SC,['cs.SC'],,,[]
A Fast Algorithm for Computing the Truncated Resultant,http://arxiv.org/abs/1609.04259v1,2016-09-14T13:25:33Z,2016-09-14T13:25:33Z,"  Let P and Q be two polynomials in K[x, y] with degree at most d, where K is a
field. Denoting by R $\in$ K[x] the resultant of P and Q with respect to y, we
present an algorithm to compute R mod x^k in O~(kd) arithmetic operations in K,
where the O~ notation indicates that we omit polylogarithmic factors. This is
an improvement over state-of-the-art algorithms that require to compute R in
O~(d^3) operations before computing its first k coefficients.
","['\nGuillaume Moroz\nVEGAS\n', '\nÉric Schost\n']",,"ISSAC '16, Jul 2016, Waterloo, Canada. ACM, Proceedings of the ACM
  on International Symposium on Symbolic and Algebraic Computation, pp.341-348,
  2016",http://dx.doi.org/10.1145/2930889.2930931,cs.SC,['cs.SC'],10.1145/2930889.2930931,,['VEGAS']
"Proceedings 9th International Workshop on Computing with Terms and
  Graphs",http://arxiv.org/abs/1609.03014v1,2016-09-10T08:38:13Z,2016-09-10T08:38:13Z,"  This volume contains the proceedings of TERMGRAPH 2016, the Ninth
International Workshop on Computing with Terms and Graphs which was held on
April 8, 2016 in Eindhoven, The Netherlands, as a satellite event of the
European Joint Conferences on Theory and Practice of Software (ETAPS 2016).
","['\nAndrea Corradini\nPisa\n', '\nHans Zantema\nEindhoven\n']",,"EPTCS 225, 2016",http://dx.doi.org/10.4204/EPTCS.225,cs.LO,"['cs.LO', 'cs.SC']",10.4204/EPTCS.225,,"['Pisa', 'Eindhoven']"
"OpenSBLI: A framework for the automated derivation and parallel
  execution of finite difference solvers on a range of computer architectures",http://arxiv.org/abs/1609.01277v2,2016-09-05T10:11:31Z,2016-11-14T17:17:48Z,"  Exascale computing will feature novel and potentially disruptive hardware
architectures. Exploiting these to their full potential is non-trivial.
Numerical modelling frameworks involving finite difference methods are
currently limited by the 'static' nature of the hand-coded discretisation
schemes and repeatedly may have to be re-written to run efficiently on new
hardware. In contrast, OpenSBLI uses code generation to derive the model's code
from a high-level specification. Users focus on the equations to solve, whilst
not concerning themselves with the detailed implementation. Source-to-source
translation is used to tailor the code and enable its execution on a variety of
hardware.
","['\nChristian T. Jacobs\n', '\nSatya P. Jammy\n', '\nNeil D. Sandham\n']","Author accepted version, with a small amendment: the link in the
  ""Code Availability"" section has been updated, and now refers to the OpenSBLI
  source code repository on GitHub. Accepted for publication in the Journal of
  Computational Science on 8 November 2016",Journal of Computational Science 18 (2017) 12-23,http://dx.doi.org/10.1016/j.jocs.2016.11.001,cs.MS,"['cs.MS', 'cs.SC', 'cs.SE', 'physics.comp-ph']",10.1016/j.jocs.2016.11.001,,[]
"A modular architecture for transparent computation in Recurrent Neural
  Networks",http://arxiv.org/abs/1609.01926v1,2016-09-07T10:44:28Z,2016-09-07T10:44:28Z,"  Computation is classically studied in terms of automata, formal languages and
algorithms; yet, the relation between neural dynamics and symbolic
representations and operations is still unclear in traditional eliminative
connectionism. Therefore, we suggest a unique perspective on this central
issue, to which we would like to refer as to transparent connectionism, by
proposing accounts of how symbolic computation can be implemented in neural
substrates. In this study we first introduce a new model of dynamics on a
symbolic space, the versatile shift, showing that it supports the real-time
simulation of a range of automata. We then show that the Goedelization of
versatile shifts defines nonlinear dynamical automata, dynamical systems
evolving on a vectorial space. Finally, we present a mapping between nonlinear
dynamical automata and recurrent artificial neural networks. The mapping
defines an architecture characterized by its granular modularity, where data,
symbolic operations and their control are not only distinguishable in
activation space, but also spatially localizable in the network itself, while
maintaining a distributed encoding of symbolic representations. The resulting
networks simulate automata in real-time and are programmed directly, in absence
of network training. To discuss the unique characteristics of the architecture
and their consequences, we present two examples: i) the design of a Central
Pattern Generator from a finite-state locomotive controller, and ii) the
creation of a network simulating a system of interactive automata that supports
the parsing of garden-path sentences as investigated in psycholinguistics
experiments.
","['\nGiovanni Sirio Carmantini\n', '\nPeter beim Graben\n', '\nMathieu Desroches\n', '\nSerafim Rodrigues\n']",,,http://dx.doi.org/10.1016/j.neunet.2016.09.001,cs.NE,"['cs.NE', 'cs.AI', 'cs.CL', 'cs.FL', 'cs.SC']",10.1016/j.neunet.2016.09.001,,[]
Lopsided Approximation of Amoebas,http://arxiv.org/abs/1608.08663v3,2016-08-30T21:41:27Z,2017-09-09T09:51:09Z,"  The amoeba of a Laurent polynomial is the image of the corresponding
hypersurface under the coordinatewise log absolute value map. In this article,
we demonstrate that a theoretical amoeba approximation method due to Purbhoo
can be used efficiently in practice. To do this, we resolve the main bottleneck
in Purbhoo's method by exploiting relations between cyclic resultants. We use
the same approach to give an approximation of the Log preimage of the amoeba of
a Laurent polynomial using semi-algebraic sets. We also provide a SINGULAR/SAGE
implementation of these algorithms, which shows a significant speedup when our
specialized cyclic resultant computation is used, versus a general purpose
resultant algorithm.
","['\nJens Forsgård\n', '\nLaura Felicia Matusevich\n', '\nNathan Mehlhop\n', '\nTimo de Wolff\n']","Minor revision; final version; 15 pages, 2 figures, 2 tables",,http://arxiv.org/abs/1608.08663v3,cs.SC,"['cs.SC', 'math.AC', 'math.AG', '13P15, 14Q20, 14T05 (Primary), 90C59, 90C90 (Secondary)']",,,[]
"Using Machine Learning to Decide When to Precondition Cylindrical
  Algebraic Decomposition With Groebner Bases",http://arxiv.org/abs/1608.04219v1,2016-08-15T09:44:29Z,2016-08-15T09:44:29Z,"  Cylindrical Algebraic Decomposition (CAD) is a key tool in computational
algebraic geometry, particularly for quantifier elimination over real-closed
fields. However, it can be expensive, with worst case complexity doubly
exponential in the size of the input. Hence it is important to formulate the
problem in the best manner for the CAD algorithm. One possibility is to
precondition the input polynomials using Groebner Basis (GB) theory. Previous
experiments have shown that while this can often be very beneficial to the CAD
algorithm, for some problems it can significantly worsen the CAD performance.
  In the present paper we investigate whether machine learning, specifically a
support vector machine (SVM), may be used to identify those CAD problems which
benefit from GB preconditioning. We run experiments with over 1000 problems
(many times larger than previous studies) and find that the machine learned
choice does better than the human-made heuristic.
","['\nZongyan Huang\n', '\nMatthew England\n', '\nJames H. Davenport\n', '\nLawrence C. Paulson\n']",,"Proceedings of the 18th International Symposium on Symbolic and
  Numeric Algorithms for Scientific Computing (SYNASC '16), pp. 45--52. IEEE,
  2016",http://dx.doi.org/10.1109/SYNASC.2016.020,cs.SC,"['cs.SC', 'cs.LG', '68W30, 68T05', 'I.2.6; I.1.0']",10.1109/SYNASC.2016.020,,[]
"Algorithms to solve coupled systems of differential equations in terms
  of power series",http://arxiv.org/abs/1608.05376v1,2016-08-18T19:06:27Z,2016-08-18T19:06:27Z,"  Using integration by parts relations, Feynman integrals can be represented in
terms of coupled systems of differential equations. In the following we suppose
that the unknown Feynman integrals can be given in power series
representations, and that sufficiently many initial values of the integrals are
given. Then there exist algorithms that decide constructively if the
coefficients of their power series representations can be given within the
class of nested sums over hypergeometric products. In this article we will work
out the calculation steps that solve this problem. First, we will present a
successful tactic that has been applied recently to challenging problems coming
from massive 3-loop Feynman integrals. Here our main tool is to solve scalar
linear recurrences within the class of nested sums over hypergeometric
products. Second, we will present a new variation of this tactic which relies
on more involved summation technologies but succeeds in reducing the problem to
solve scalar recurrences with lower recurrence orders. The article will work
out the different challenges of this new tactic and demonstrates how they can
be treated efficiently with our existing summation technologies.
","['\nJakob Ablinger\n', '\nArnd Behring\n', '\nJohannes Bluemlein\n', '\nAbilio de Freitas\n', '\nCarsten Schneider\n']",,,http://arxiv.org/abs/1608.05376v1,cs.SC,"['cs.SC', 'hep-ph', 'math-ph', 'math.MP']",,,[]
Congruences and Concurrent Lines in Multi-View Geometry,http://arxiv.org/abs/1608.05924v2,2016-08-21T12:07:14Z,2016-12-25T20:52:46Z,"  We present a new framework for multi-view geometry in computer vision. A
camera is a mapping between $\mathbb{P}^3$ and a line congruence. This model,
which ignores image planes and measurements, is a natural abstraction of
traditional pinhole cameras. It includes two-slit cameras, pushbroom cameras,
catadioptric cameras, and many more. We study the concurrent lines variety,
which consists of $n$-tuples of lines in $\mathbb{P}^3$ that intersect at a
point. Combining its equations with those of various congruences, we derive
constraints for corresponding images in multiple views. We also study
photographic cameras which use image measurements and are modeled as rational
maps from $\mathbb{P}^3$ to $\mathbb{P}^2$ or $\mathbb{P}^1\times
\mathbb{P}^1$.
","['\nJean Ponce\n', '\nBernd Sturmfels\n', '\nMatthew Trager\n']",26 pages,,http://arxiv.org/abs/1608.05924v2,math.AG,"['math.AG', 'cs.CV', 'cs.SC']",,,[]
Satisfiability Checking and Symbolic Computation,http://arxiv.org/abs/1607.06945v1,2016-07-23T14:52:23Z,2016-07-23T14:52:23Z,"  Symbolic Computation and Satisfiability Checking are viewed as individual
research areas, but they share common interests in the development,
implementation and application of decision procedures for arithmetic theories.
Despite these commonalities, the two communities are currently only weakly
connected. We introduce a new project SC-square to build a joint community in
this area, supported by a newly accepted EU (H2020-FETOPEN-CSA) project of the
same name. We aim to strengthen the connection between these communities by
creating common platforms, initiating interaction and exchange, identifying
common challenges, and developing a common roadmap. This abstract and
accompanying poster describes the motivation and aims for the project, and
reports on the first activities.
","['\nE. Abraham\n', '\nJ. Abbott\n', '\nB. Becker\n', '\nA. M. Bigatti\n', '\nM. Brain\n', '\nB. Buchberger\n', '\nA. Cimatti\n', '\nJ. H. Davenport\n', '\nM. England\n', '\nP. Fontaine\n', '\nS. Forrest\n', '\nA. Griggio\n', '\nD. Kroening\n', '\nW. M. Seiler\n', '\nT. Sturm\n']","3 page Extended Abstract to accompany an ISSAC 2016 poster. Poster
  available at http://www.sc-square.org/SC2-AnnouncementPoster.pdf","ACM Communications in Computer Algebra, 50:4 (issue 198), pp.
  145-147, ACM, 2016",http://dx.doi.org/10.1145/3055282.3055285,cs.SC,"['cs.SC', 'cs.LO']",10.1145/3055282.3055285,,[]
Satisfiability Checking meets Symbolic Computation (Project Paper),http://arxiv.org/abs/1607.08028v1,2016-07-27T10:38:53Z,2016-07-27T10:38:53Z,"  Symbolic Computation and Satisfiability Checking are two research areas, both
having their individual scientific focus but sharing also common interests in
the development, implementation and application of decision procedures for
arithmetic theories. Despite their commonalities, the two communities are
rather weakly connected. The aim of our newly accepted SC-square project
(H2020-FETOPEN-CSA) is to strengthen the connection between these communities
by creating common platforms, initiating interaction and exchange, identifying
common challenges, and developing a common roadmap from theory along the way to
tools and (industrial) applications. In this paper we report on the aims and on
the first activities of this project, and formalise some relevant challenges
for the unified SC-square community.
","['\nE. Abraham\n', '\nJ. Abbott\n', '\nB. Becker\n', '\nA. M. Bigatti\n', '\nM. Brain\n', '\nB. Buchberger\n', '\nA. Cimatti\n', '\nJ. H. Davenport\n', '\nM. England\n', '\nP. Fontaine\n', '\nS. Forrest\n', '\nA. Griggio\n', '\nD. Kroening\n', '\nW. M. Seiler\n', '\nT. Sturm\n']",,"M. Kohlhase, M. Johansson, B. Miller, L. de Moura, F. Tompa, eds.,
  Intelligent Computer Mathematics (Proceedings of CICM 2016), pp. 28-43,
  (Lecture Notes in Computer Science, 9791). Springer International Publishing,
  2016",http://dx.doi.org/10.1007/978-3-319-42547-4_3,cs.SC,"['cs.SC', 'cs.LO']",10.1007/978-3-319-42547-4_3,,[]
"Fast, deterministic computation of the Hermite normal form and
  determinant of a polynomial matrix",http://arxiv.org/abs/1607.04176v2,2016-07-14T16:01:59Z,2017-03-29T21:24:35Z,"  Given a nonsingular $n \times n$ matrix of univariate polynomials over a
field $\mathbb{K}$, we give fast and deterministic algorithms to compute its
determinant and its Hermite normal form. Our algorithms use
$\widetilde{\mathcal{O}}(n^\omega \lceil s \rceil)$ operations in $\mathbb{K}$,
where $s$ is bounded from above by both the average of the degrees of the rows
and that of the columns of the matrix and $\omega$ is the exponent of matrix
multiplication. The soft-$O$ notation indicates that logarithmic factors in the
big-$O$ are omitted while the ceiling function indicates that the cost is
$\widetilde{\mathcal{O}}(n^\omega)$ when $s = o(1)$. Our algorithms are based
on a fast and deterministic triangularization method for computing the diagonal
entries of the Hermite form of a nonsingular matrix.
","['\nGeorge Labahn\n', '\nVincent Neiger\n', '\nWei Zhou\n']","34 pages, 3 algorithms",,http://arxiv.org/abs/1607.04176v2,cs.SC,['cs.SC'],,,[]
Private Multi-party Matrix Multiplication and Trust Computations,http://arxiv.org/abs/1607.03629v1,2016-07-13T08:03:57Z,2016-07-13T08:03:57Z,"  This paper deals with distributed matrix multiplication. Each player owns
only one row of both matrices and wishes to learn about one distinct row of the
product matrix, without revealing its input to the other players. We first
improve on a weighted average protocol, in order to securely compute a
dot-product with a quadratic volume of communications and linear number of
rounds. We also propose a protocol with five communication rounds, using a
Paillier-like underlying homomorphic public key cryptosystem, which is secure
in the semi-honest model or secure with high probability in the malicious
adversary model. Using ProVerif, a cryptographic protocol verification tool, we
are able to check the security of the protocol and provide a countermeasure for
each attack found by the tool. We also give a randomization method to avoid
collusion attacks. As an application, we show that this protocol enables a
distributed and secure evaluation of trust relationships in a network, for a
large class of trust evaluation schemes.
","['\nJean-Guillaume Dumas\nCASYS\n', '\nPascal Lafourcade\nLIMOS\n', '\nJean-Baptiste Orfila\nCASYS\n', '\nMaxime Puys\nVERIMAG - IMAG\n']","Pierangela Samarati. SECRYPT 2016 : 13th International Conference on
  Security and Cryptography, Lisbonne, Portugal, 26--28 Juillet 2016. 2016",,http://arxiv.org/abs/1607.03629v1,cs.CR,"['cs.CR', 'cs.SC']",,,"['CASYS', 'LIMOS', 'CASYS', 'VERIMAG - IMAG']"
Evaluation of binomial double sums involving absolute values,http://arxiv.org/abs/1607.05314v3,2016-07-18T20:25:16Z,2020-12-04T10:31:39Z,"  We show that double sums of the form $$ \sum_{i,j=-n} ^{n}
|i^sj^t(i^k-j^k)^\beta| \binom {2n} {n+i} \binom {2n} {n+j} $$ can always be
expressed in terms of a linear combination of just four functions, namely
$\binom {4n}{2n}$, ${\binom {2n}n}^2$, $4^n\binom {2n}n$, and $16^n$, with
coefficients that are rational in $n$. We provide two different proofs: one is
algorithmic and uses the second author's computer algebra package Sigma; the
second is based on complex contour integrals. In many instances, these results
are extended to double sums of the above form where $\binom {2n}{n+j}$ is
replaced by $\binom {2m}{m+j}$ with independent parameter $m$.
","['\nChristian Krattenthaler\nUniversität Wien\n', '\nCarsten Schneider\nJohannes Kepler Universität Linz\n']","AmS-LaTeX, 42 pages; final version","in: Algorithmic Combinatorics: Enumerative Combinatorics, Special
  Functions and Computer Algebra, V. Pillwein and C. Schneider (eds.),
  Springer-Verlag, 2020, pp. 249-296",http://arxiv.org/abs/1607.05314v3,math.CO,"['math.CO', 'cs.SC', 'Primary 05A19, Secondary 05A10, 11B65, 33C70, 68R05, 68W30']",,,"['Universität Wien', 'Johannes Kepler Universität Linz']"
Reconstruction Algorithms for Sums of Affine Powers,http://arxiv.org/abs/1607.05420v3,2016-07-19T06:28:56Z,2017-10-24T11:45:46Z,"  In this paper we study sums of powers of affine functions in (mostly) one
variable. Although quite simple, this model is a generalization of two
well-studied models: Waring decomposition and sparsest shift. For these three
models there are natural extensions to several variables, but this paper is
mostly focused on univariate polynomials. We present structural results which
compare the expressive power of the three models; and we propose algorithms
that find the smallest decomposition of f in the first model (sums of affine
powers) for an input polynomial f given in dense representation. We also begin
a study of the multivariate case. This work could be extended in several
directions. In particular, just as for Sparsest Shift and Waring decomposition,
one could consider extensions to ""supersparse"" polynomials and attempt a fuller
study of the multi-variate case. We also point out that the basic univariate
problem studied in the present paper is far from completely solved: our
algorithms all rely on some assumptions for the exponents in an optimal
decomposition, and some algorithms also rely on a distinctness assumption for
the shifts. It would be very interesting to weaken these assumptions, or even
to remove them entirely. Another related and poorly understood issue is that of
the bit size of the constants appearing in an optimal decomposition: is it
always polynomially related to the bit size of the input polynomial given in
dense representation?
","['\nIgnacio Garcia-Marco\nLIP\n', '\nPascal Koiran\nLIP\n', '\nTimothée Pecatte\nLIP\n']",This version improves on several algorithmic results,,http://arxiv.org/abs/1607.05420v3,cs.CC,"['cs.CC', 'cs.SC']",,,"['LIP', 'LIP', 'LIP']"
Rigorous Multiple-Precision Evaluation of D-Finite Functions in SageMath,http://arxiv.org/abs/1607.01967v1,2016-07-07T11:33:10Z,2016-07-07T11:33:10Z,"  We present a new open source implementation in the SageMath computer algebra
system of algorithms for the numerical solution of linear ODEs with polynomial
coefficients. Our code supports regular singular connection problems and
provides rigorous error bounds.
","['\nMarc Mezzarobba\nCNRS, LIP6, PEQUAN\n']",,"5th International Congress on Mathematical Software (ICMS~2016),
  Jul 2016, Berlin, Germany",http://arxiv.org/abs/1607.01967v1,cs.SC,['cs.SC'],,,"['CNRS, LIP6, PEQUAN']"
Finding binomials in polynomial ideals,http://arxiv.org/abs/1607.02135v2,2016-07-07T19:56:31Z,2017-04-18T14:43:29Z,"  We describe an algorithm which finds binomials in a given ideal
$I\subset\mathbb{Q}[x_1,\dots,x_n]$ and in particular decides whether binomials
exist in $I$ at all. Binomials in polynomial ideals can be well hidden. For
example, the lowest degree of a binomial cannot be bounded as a function of the
number of indeterminates, the degree of the generators, or the
Castelnuovo--Mumford regularity. We approach the detection problem by reduction
to the Artinian case using tropical geometry. The Artinian case is solved with
algorithms from computational number theory.
","['\nAnders Jensen\n', '\nThomas Kahle\n', '\nLukas Katthän\n']","11 pages, v2: final version, to appear in Res. Math. Sci",,http://arxiv.org/abs/1607.02135v2,math.AC,"['math.AC', 'cs.SC', '68W99 (Primary), 11R04, 11Y16, 11Y40, 13P05, 13P99, 14T05, 68W30\n  (Secondary)']",,,[]
"Asymptotic and exact results on the complexity of the
  Novelli-Pak-Stoyanovskii algorithm",http://arxiv.org/abs/1606.07597v2,2016-06-24T08:21:54Z,2017-05-24T22:13:02Z,"  The Novelli-Pak-Stoyanovskii algorithm is a sorting algorithm for Young
tableaux of a fixed shape that was originally devised to give a bijective proof
of the hook-length formula. We obtain new asymptotic results on the average
case and worst case complexity of this algorithm as the underlying shape tends
to a fixed limit curve. Furthermore, using the summation package Sigma we prove
an exact formula for the average case complexity when the underlying shape
consists of only two rows. We thereby answer questions posed by Krattenthaler
and M\""uller.
","['\nCarsten Schneider\n', '\nRobin Sulzgruber\n']",,,http://arxiv.org/abs/1606.07597v2,math.CO,"['math.CO', 'cs.SC']",,,[]
Computing hypergeometric functions rigorously,http://arxiv.org/abs/1606.06977v2,2016-06-22T15:07:11Z,2016-07-05T11:58:14Z,"  We present an efficient implementation of hypergeometric functions in
arbitrary-precision interval arithmetic. The functions ${}_0F_1$, ${}_1F_1$,
${}_2F_1$ and ${}_2F_0$ (or the Kummer $U$-function) are supported for
unrestricted complex parameters and argument, and by extension, we cover
exponential and trigonometric integrals, error functions, Fresnel integrals,
incomplete gamma and beta functions, Bessel functions, Airy functions, Legendre
functions, Jacobi polynomials, complete elliptic integrals, and other special
functions. The output can be used directly for interval computations or to
generate provably correct floating-point approximations in any format.
Performance is competitive with earlier arbitrary-precision software, and
sometimes orders of magnitude faster. We also partially cover the generalized
hypergeometric function ${}_pF_q$ and computation of high-order parameter
derivatives.
",['\nFredrik Johansson\n'],"v2: corrected example in section 3.1; corrected timing data for case
  E-G in section 8.5 (table 6, figure 2); adjusted paper size",,http://arxiv.org/abs/1606.06977v2,cs.MS,"['cs.MS', 'cs.NA', 'cs.SC', '33F05, 33C20, 33C05, 33C10, 33C15, 65G30, 65Y20, 65D20, 97N80,\n  33B15, 33B20, 33C45']",,,[]
"Hypergeometric Expressions for Generating Functions of Walks with Small
  Steps in the Quarter Plane",http://arxiv.org/abs/1606.02982v3,2016-06-09T14:56:04Z,2016-10-20T08:42:18Z,"  We study nearest-neighbors walks on the two-dimensional square lattice, that
is, models of walks on $\mathbb{Z}^2$ defined by a fixed step set that is a
subset of the non-zero vectors with coordinates 0, 1 or $-1$. We concern
ourselves with the enumeration of such walks starting at the origin and
constrained to remain in the quarter plane $\mathbb{N}^2$, counted by their
length and by the position of their ending point. Bousquet-M\'elou and Mishna
[Contemp. Math., pp. 1--39, Amer. Math. Soc., 2010] identified 19 models of
walks that possess a D-finite generating function; linear differential
equations have then been guessed in these cases by Bostan and Kauers [FPSAC
2009, Discrete Math. Theor. Comput. Sci. Proc., pp. 201--215, 2009]. We give
here the first proof that these equations are indeed satisfied by the
corresponding generating functions. As a first corollary, we prove that all
these 19 generating functions can be expressed in terms of Gauss'
hypergeometric functions that are intimately related to elliptic integrals. As
a second corollary, we show that all the 19 generating functions are
transcendental, and that among their $19 \times 4$ combinatorially meaningful
specializations only four are algebraic functions.
","['\nAlin Bostan\n', '\nFrédéric Chyzak\n', '\nMark van Hoeij\n', '\nManuel Kauers\n', '\nLucien Pech\n']","29 pages, 6 tables",,http://arxiv.org/abs/1606.02982v3,math.CO,"['math.CO', 'cs.SC', '05A15, 14N10, 33F10, 68W30 (Primary), 33C05, 97N80, 11J89\n  (Secondary)']",,,[]
Inverse Mellin Transform of Holonomic Sequences,http://arxiv.org/abs/1606.02845v1,2016-06-09T07:24:05Z,2016-06-09T07:24:05Z,"  We describe a method to compute the inverse Mellin transform of holonomic
sequences, that is based on a method to compute the Mellin transform of
holonomic functions. Both methods are implemented in the computer algebra
package HarmonicSums.
",['\nJakob Ablinger\n'],,,http://arxiv.org/abs/1606.02845v1,cs.SC,"['cs.SC', 'math-ph', 'math.CO', 'math.MP']",,,[]
"Algebraic Problems Equivalent to Beating Exponent 3/2 for Polynomial
  Factorization over Finite Fields",http://arxiv.org/abs/1606.04592v1,2016-06-14T23:32:48Z,2016-06-14T23:32:48Z,"  The fastest known algorithm for factoring univariate polynomials over finite
fields is the Kedlaya-Umans (fast modular composition) implementation of the
Kaltofen-Shoup algorithm. It is randomized and takes $\widetilde{O}(n^{3/2}\log
q + n \log^2 q)$ time to factor polynomials of degree $n$ over the finite field
$\mathbb{F}_q$ with $q$ elements. A significant open problem is if the $3/2$
exponent can be improved. We study a collection of algebraic problems and
establish a web of reductions between them. A consequence is that an algorithm
for any one of these problems with exponent better than $3/2$ would yield an
algorithm for polynomial factorization with exponent better than $3/2$.
","['\nZeyu Guo\n', '\nAnand Kumar Narayanan\n', '\nChris Umans\n']",,,http://arxiv.org/abs/1606.04592v1,cs.CC,"['cs.CC', 'cs.DS', 'cs.SC']",,,[]
"Computing all Space Curve Solutions of Polynomial Systems by Polyhedral
  Methods",http://arxiv.org/abs/1606.05563v1,2016-06-17T15:28:40Z,2016-06-17T15:28:40Z,"  A polyhedral method to solve a system of polynomial equations exploits its
sparse structure via the Newton polytopes of the polynomials. We propose a
hybrid symbolic-numeric method to compute a Puiseux series expansion for every
space curve that is a solution of a polynomial system. The focus of this paper
concerns the difficult case when the leading powers of the Puiseux series of
the space curve are contained in the relative interior of a higher dimensional
cone of the tropical prevariety. We show that this difficult case does not
occur for polynomials with generic coefficients. To resolve this case, we
propose to apply polyhedral end games to recover tropisms hidden in the
tropical prevariety.
","['\nNathan Bliss\n', '\nJan Verschelde\n']","14 pages, 1 figure, accepted for presentation at Computer Algebra in
  Scientific Computing, CASC 2016",,http://arxiv.org/abs/1606.05563v1,cs.SC,"['cs.SC', 'cs.MS', 'math.AG', 'math.NA']",,,[]
The Complexity of Computing all Subfields of an Algebraic Number Field,http://arxiv.org/abs/1606.01140v3,2016-06-03T15:31:45Z,2017-11-20T15:03:50Z,"  For a finite separable field extension K/k, all subfields can be obtained by
intersecting so-called principal subfields of K/k. In this work we present a
way to quickly compute these intersections. If the number of subfields is high,
then this leads to faster run times and an improved complexity.
","['\nJonas Szutkoski\n', '\nMark van Hoeij\n']","Slides available at:
  http://www.math.fsu.edu/~hoeij/2017/Presentation.pdf",,http://arxiv.org/abs/1606.01140v3,cs.SC,"['cs.SC', 'math.NT', '11Y16, 12Y05, 13P05, 68W30', 'I.1.2; F.2.1']",,,[]
"Computing Hypergeometric Solutions of Second Order Linear Differential
  Equations using Quotients of Formal Solutions and Integral Bases",http://arxiv.org/abs/1606.01576v1,2016-06-05T22:41:58Z,2016-06-05T22:41:58Z,"  We present two algorithms for computing hypergeometric solutions of second
order linear differential operators with rational function coefficients. Our
first algorithm searches for solutions of the form \[ \exp(\int r \,
dx)\cdot{_{2}F_1}(a_1,a_2;b_1;f) \] where $r,f \in \overline{\mathbb{Q}(x)}$,
and $a_1,a_2,b_1 \in \mathbb{Q}$. It uses modular reduction and Hensel lifting.
Our second algorithm tries to find solutions in the form \[ \exp(\int r \,
dx)\cdot \left( r_0 \cdot{_{2}F_1}(a_1,a_2;b_1;f) + r_1
\cdot{_{2}F_1}'(a_1,a_2;b_1;f) \right) \] where $r_0, r_1 \in
\overline{\mathbb{Q}(x)}$, as follows: It tries to transform the input equation
to another equation with solutions of the first type, and then uses the first
algorithm.
","['\nErdal Imamoglu\n', '\nMark van Hoeij\n']",,,http://arxiv.org/abs/1606.01576v1,cs.SC,"['cs.SC', 'math.CA', '68W30', 'I.1.2']",,,[]
"Factoring Polynomials over Finite Fields using Drinfeld Modules with
  Complex Multiplication",http://arxiv.org/abs/1606.00898v1,2016-06-02T21:09:01Z,2016-06-02T21:09:01Z,"  We present novel algorithms to factor polynomials over a finite field $\F_q$
of odd characteristic using rank $2$ Drinfeld modules with complex
multiplication. The main idea is to compute a lift of the Hasse invariant
(modulo the polynomial $f(x) \in \F_q[x]$ to be factored) with respect to a
Drinfeld module $\phi$ with complex multiplication. Factors of $f(x)$ supported
on prime ideals with supersingular reduction at $\phi$ have vanishing Hasse
invariant and can be separated from the rest. A Drinfeld module analogue of
Deligne's congruence plays a key role in computing the Hasse invariant lift. We
present two algorithms based on this idea. The first algorithm chooses Drinfeld
modules with complex multiplication at random and has a quadratic expected run
time. The second is a deterministic algorithm with $O(\sqrt{p})$ run time
dependence on the characteristic $p$ of $\F_q$.
",['\nAnand Kumar Narayanan\n'],,,http://arxiv.org/abs/1606.00898v1,math.NT,"['math.NT', 'cs.CC', 'cs.DS', 'cs.SC']",,,[]
Splitting quaternion algebras over quadratic number fields,http://arxiv.org/abs/1606.01053v2,2016-06-03T11:59:56Z,2018-09-10T09:23:26Z,"  We propose an algorithm for finding zero divisors in quaternion algebras over
quadratic number fields, or equivalently, solving homogeneous quadratic
equations in three variables over $\mathbb{Q}(\sqrt{d})$ where $d$ is a
square-free integer. The algorithm is randomized and runs in polynomial time if
one is allowed to call oracles for factoring integers.
",['\nPéter Kutas\n'],"12 pages, revised version, accepted for publication in Journal of
  Symbolic Computation",,http://arxiv.org/abs/1606.01053v2,math.RA,"['math.RA', 'cs.SC', 'math.NT']",,,[]
"Computing Small Certificates of Inconsistency of Quadratic Fewnomial
  Systems",http://arxiv.org/abs/1605.05889v1,2016-05-19T11:25:17Z,2016-05-19T11:25:17Z,"  B{\'e}zout 's theorem states that dense generic systems of n multivariate
quadratic equations in n variables have 2 n solutions over algebraically closed
fields. When only a small subset M of monomials appear in the equations
(fewnomial systems), the number of solutions may decrease dramatically. We
focus in this work on subsets of quadratic monomials M such that generic
systems with support M do not admit any solution at all. For these systems,
Hilbert's Nullstellensatz ensures the existence of algebraic certificates of
inconsistency. However, up to our knowledge all known bounds on the sizes of
such certificates -including those which take into account the Newton polytopes
of the polynomials- are exponential in n. Our main results show that if the
inequality 2|M| -- 2n $\le$ $\sqrt$ 1 + 8{\nu} -- 1 holds for a quadratic
fewnomial system -- where {\nu} is the matching number of a graph associated
with M, and |M| is the cardinality of M -- then there exists generically a
certificate of inconsistency of linear size (measured as the number of
coefficients in the ground field K). Moreover this certificate can be computed
within a polynomial number of arithmetic operations. Next, we evaluate how
often this inequality holds, and we give evidence that the probability that the
inequality is satisfied depends strongly on the number of squares. More
precisely, we show that if M is picked uniformly at random among the subsets of
n + k + 1 quadratic monomials containing at least $\Omega$(n 1/2+$\epsilon$)
squares, then the probability that the inequality holds tends to 1 as n grows.
Interestingly, this phenomenon is related with the matching number of random
graphs in the Erd{\""o}s-Renyi model. Finally, we provide experimental results
showing that certificates in inconsistency can be computed for systems with
more than 10000 variables and equations.
","['\nJean-Charles Faugere\nPolSys\n', '\nPierre-Jean Spaenlehauer\nCARAMBA\n', '\nJules Svartz\nPolSys\n']","ISSAC 2016, Jul 2016, Waterloo, Canada. Proceedings of ISSAC 2016",,http://arxiv.org/abs/1605.05889v1,cs.SC,['cs.SC'],,,"['PolSys', 'CARAMBA', 'PolSys']"
Computation of the Similarity Class of the p-Curvature,http://arxiv.org/abs/1605.06126v1,2016-05-19T20:05:49Z,2016-05-19T20:05:49Z,"  The $p$-curvature of a system of linear differential equations in positive
characteristic $p$ is a matrix that measures how far the system is from having
a basis of polynomial solutions. We show that the similarity class of the
$p$-curvature can be determined without computing the $p$-curvature itself.
More precisely, we design an algorithm that computes the invariant factors of
the $p$-curvature in time quasi-linear in $\sqrt p$. This is much less than the
size of the $p$-curvature, which is generally linear in $p$. The new algorithm
allows to answer a question originating from the study of the Ising model in
statistical physics.
","['\nAlin Bostan\n', '\nXavier Caruso\n', '\nEric Schost\n']","Proceedings of ISSAC 2016, to appear",,http://dx.doi.org/10.1145/2930889.2930897,cs.SC,"['cs.SC', '11Y16, 68W30,', 'I.1.2']",10.1145/2930889.2930897,,[]
"Bit complexity for multi-homogeneous polynomial system solving
  Application to polynomial minimization",http://arxiv.org/abs/1605.07433v2,2016-05-24T13:01:34Z,2017-12-11T08:39:31Z,"  Multi-homogeneous polynomial systems arise in many applications. We provide
bit complexity estimates for solving them which, up to a few extra other
factors, are quadratic in the number of solutions and linear in the height of
the input system under some genericity assumptions. The assumptions essentially
imply that the Jacobian matrix of the system under study has maximal rank at
the solution set and that this solution set if finite. The algorithm is
probabilistic and a probability analysis is provided. Next, we apply these
results to the problem of optimizing a linear map on the real trace of an
algebraic set. Under some genericity assumptions, we provide bit complexity
estimates for solving this polynomial minimization problem.
","['\nMohab Safey El Din\nPolSys\n', '\nEric Schost\nCS\n']","Journal of Symbolic Computation, Elsevier, A Para{\^i}tre",,http://arxiv.org/abs/1605.07433v2,cs.SC,['cs.SC'],,,"['PolSys', 'CS']"
"Numeric Deduction in Symbolic Computation. Application to Normalizing
  Transformations",http://arxiv.org/abs/1607.02016v1,2016-05-27T12:52:49Z,2016-05-27T12:52:49Z,"  Algorithms of numeric (in exact arithmetic) deduction of analytical
expressions, proposed and described by Shevchenko and Vasiliev (1993), are
developed and implemented in a computer algebra code. This code is built as a
superstructure for the computer algebra package by Shevchenko and Sokolsky
(1993a) for normalization of Hamiltonian systems of ordinary differential
equations, in order that high complexity problems of normalization could be
solved. As an example, a resonant normal form of a Hamiltonian describing the
hyperboloidal precession of a dynamically symmetric satellite is derived by
means of the numeric deduction technique. The technique provides a considerable
economy, about 30 times in this particular application, in computer memory
consumption. It is naturally parallelizable. Thus the economy of memory
consumption is convertible into a gain in computation speed.
",['\nIvan I. Shevchenko\n'],14 pages,"Journal of Symbolic Computation, Volume 24, Issue 1, Pages 103-111
  (1997)",http://dx.doi.org/10.1006/jsco.1997.0115,cs.SC,['cs.SC'],10.1006/jsco.1997.0115,,[]
Segmentation of real algebraic plane curves,http://arxiv.org/abs/1605.06862v1,2016-05-22T23:00:47Z,2016-05-22T23:00:47Z,"  In this article we give an implementation of the standard algorithm to
segment a real algebraic plane curve defined implicitly. Our implementation is
efficient and simpler than previous. We use global information to count the
number of half-branches at a critical point.
","['\nCesar Massri\n', '\nManuel Dubinsky\n']",,,http://arxiv.org/abs/1605.06862v1,math.AG,"['math.AG', 'cs.SC', '68W30, 14Q05, 14Q20']",,,[]
The Symbolic Interior Point Method,http://arxiv.org/abs/1605.08187v3,2016-05-26T08:26:34Z,2016-06-14T18:29:14Z,"  A recent trend in probabilistic inference emphasizes the codification of
models in a formal syntax, with suitable high-level features such as
individuals, relations, and connectives, enabling descriptive clarity,
succinctness and circumventing the need for the modeler to engineer a custom
solver. Unfortunately, bringing these linguistic and pragmatic benefits to
numerical optimization has proven surprisingly challenging. In this paper, we
turn to these challenges: we introduce a rich modeling language, for which an
interior-point method computes approximate solutions in a generic way. While
logical features easily complicates the underlying model, often yielding
intricate dependencies, we exploit and cache local structure using algebraic
decision diagrams (ADDs). Indeed, standard matrix-vector algebra is efficiently
realizable in ADDs, but we argue and show that well-known optimization methods
are not ideal for ADDs. Our engine, therefore, invokes a sophisticated
matrix-free approach. We demonstrate the flexibility of the resulting
symbolic-numeric optimizer on decision making and compressed sensing tasks with
millions of non-zero entries.
","['\nMartin Mladenov\n', '\nVaishak Belle\n', '\nKristian Kersting\n']",,,http://arxiv.org/abs/1605.08187v3,cs.AI,"['cs.AI', 'cs.LO', 'cs.SC']",,,[]
"The complexity of cylindrical algebraic decomposition with respect to
  polynomial degree",http://arxiv.org/abs/1605.02494v1,2016-05-09T09:46:37Z,2016-05-09T09:46:37Z,"  Cylindrical algebraic decomposition (CAD) is an important tool for working
with polynomial systems, particularly quantifier elimination. However, it has
complexity doubly exponential in the number of variables. The base algorithm
can be improved by adapting to take advantage of any equational constraints
(ECs): equations logically implied by the input. Intuitively, we expect the
double exponent in the complexity to decrease by one for each EC. In ISSAC 2015
the present authors proved this for the factor in the complexity bound
dependent on the number of polynomials in the input. However, the other term,
that dependent on the degree of the input polynomials, remained unchanged.
  In the present paper the authors investigate how CAD in the presence of ECs
could be further refined using the technology of Groebner Bases to move towards
the intuitive bound for polynomial degree.
","['\nMatthew England\n', '\nJames H. Davenport\n']",,"V.P. Gerdt, W. Koepf, W.M. Seiler and E.V. Vorozhtsov, eds.
  Computer Algebra in Scientific Computing, pp. 172-192. (Lecture Notes in
  Computer Science, 9890). Springer International, 2016",http://dx.doi.org/10.1007/978-3-319-45641-6_12,cs.SC,"['cs.SC', '68W30, 03C10', 'I.1.2']",10.1007/978-3-319-45641-6_12,,[]
"Critical Point Computations on Smooth Varieties: Degree and Complexity
  bounds",http://arxiv.org/abs/1605.02518v1,2016-05-09T10:53:12Z,2016-05-09T10:53:12Z,"  Let V $\subset$ C n be an equidimensional algebraic set and g be an n-variate
polynomial with rational coefficients. Computing the critical points of the map
that evaluates g at the points of V is a cornerstone of several algorithms in
real algebraic geometry and optimization. Under the assumption that the
critical locus is finite and that the projective closure of V is smooth, we
provide sharp upper bounds on the degree of the critical locus which depend
only on deg(g) and the degrees of the generic polar varieties associated to V.
Hence, in some special cases where the degrees of the generic polar varieties
do not reach the worst-case bounds, this implies that the number of critical
points of the evaluation map of g is less than the currently known degree
bounds. We show that, given a lifting fiber of V , a slight variant of an
algorithm due to Bank, Giusti, Heintz, Lecerf, Matera and Solern{\'o} computes
these critical points in time which is quadratic in this bound up to
logarithmic factors, linear in the complexity of evaluating the input system
and polynomial in the number of variables and the maximum degree of the input
polynomials.
","['\nMohab Safey El Din\nPolSys\n', '\nPierre-Jean Spaenlehauer\nCARAMBA\n']",,,http://arxiv.org/abs/1605.02518v1,cs.SC,['cs.SC'],,,"['PolSys', 'CARAMBA']"
Need Polynomial Systems be Doubly-exponential?,http://arxiv.org/abs/1605.02912v1,2016-05-10T09:54:16Z,2016-05-10T09:54:16Z,"  Polynomial Systems, or at least their algorithms, have the reputation of
being doubly-exponential in the number of variables [Mayr and Mayer, 1982],
[Davenport and Heintz, 1988]. Nevertheless, the Bezout bound tells us that that
number of zeros of a zero-dimensional system is singly-exponential in the
number of variables. How should this contradiction be reconciled?
  We first note that [Mayr and Ritscher, 2013] shows that the doubly
exponential nature of Gr\""{o}bner bases is with respect to the dimension of the
ideal, not the number of variables. This inspires us to consider what can be
done for Cylindrical Algebraic Decomposition which produces a
doubly-exponential number of polynomials of doubly-exponential degree.
  We review work from ISSAC 2015 which showed the number of polynomials could
be restricted to doubly-exponential in the (complex) dimension using McCallum's
theory of reduced projection in the presence of equational constraints. We then
discuss preliminary results showing the same for the degree of those
polynomials. The results are under primitivity assumptions whose importance we
illustrate.
","['\nJames H. Davenport\n', '\nMatthew England\n']","Extended Abstract for ICMS 2016 Presentation. arXiv admin note: text
  overlap with arXiv:1605.02494","In: G.M. Greuel, T. Koch, P. Paule and A. Sommese, eds.
  Mathematical Software - ICMS 2016, pp.167-164, (Lecture Notes in Computer
  Science, 9725). Springer International Publishing, 2016",http://dx.doi.org/10.1007/978-3-319-42432-3_20,cs.SC,"['cs.SC', '68W30, 13P10, 03C10', 'I.1.2']",10.1007/978-3-319-42432-3_20,,[]
Extended Hardness Results for Approximate Gröbner Basis Computation,http://arxiv.org/abs/1605.04472v1,2016-05-14T21:44:01Z,2016-05-14T21:44:01Z,"  Two models were recently proposed to explore the robust hardness of Gr\""obner
basis computation. Given a polynomial system, both models allow an algorithm to
selectively ignore some of the polynomials: the algorithm is only responsible
for returning a Gr\""obner basis for the ideal generated by the remaining
polynomials. For the $q$-Fractional Gr\""obner Basis Problem the algorithm is
allowed to ignore a constant $(1-q)$-fraction of the polynomials (subject to
one natural structural constraint). Here we prove a new strongest-parameter
result: even if the algorithm is allowed to choose a $(3/10-\epsilon)$-fraction
of the polynomials to ignore, and need only compute a Gr\""obner basis with
respect to some lexicographic order for the remaining polynomials, this cannot
be accomplished in polynomial time (unless $P=NP$). This statement holds even
if every polynomial has maximum degree 3. Next, we prove the first robust
hardness result for polynomial systems of maximum degree 2: for the
$q$-Fractional model a $(1/5-\epsilon)$ fraction of the polynomials may be
ignored without losing provable NP-Hardness. Both theorems hold even if every
polynomial contains at most three distinct variables. Finally, for the Strong
$c$-partial Gr\""obner Basis Problem of De Loera et al. we give conditional
results that depend on famous (unresolved) conjectures of Khot and Dinur, et
al.
",['\nGwen Spencer\n'],23 pages. arXiv admin note: text overlap with arXiv:1511.06436,,http://arxiv.org/abs/1605.04472v1,cs.SC,['cs.SC'],,,[]
HLinear: Exact Dense Linear Algebra in Haskell,http://arxiv.org/abs/1605.02532v3,2016-05-09T11:24:39Z,2017-08-10T20:20:02Z,"  We present an implementation in the functional programming language Haskell
of the PLE decomposition of matrices over division rings. Our benchmarks
indicate that it is competitive with the C-based implementation provided in
Flint. Describing the guiding principles of our work, we introduce the reader
to basic ideas from high-performance functional programming.
","['\nAlexandru Ghitza\n', '\nMartin Raum\n']","13 pages, 6 tables; code available at
  https://github.com/martinra/hlinear/tree/paper-toms",,http://arxiv.org/abs/1605.02532v3,cs.MS,"['cs.MS', 'cs.SC', '68N18', 'G.4; D.1.1']",,,[]
Synthesizing Probabilistic Invariants via Doob's Decomposition,http://arxiv.org/abs/1605.02765v1,2016-05-09T20:30:20Z,2016-05-09T20:30:20Z,"  When analyzing probabilistic computations, a powerful approach is to first
find a martingale---an expression on the program variables whose expectation
remains invariant---and then apply the optional stopping theorem in order to
infer properties at termination time. One of the main challenges, then, is to
systematically find martingales.
  We propose a novel procedure to synthesize martingale expressions from an
arbitrary initial expression. Contrary to state-of-the-art approaches, we do
not rely on constraint solving. Instead, we use a symbolic construction based
on Doob's decomposition. This procedure can produce very complex martingales,
expressed in terms of conditional expectations.
  We show how to automatically generate and simplify these martingales, as well
as how to apply the optional stopping theorem to infer properties at
termination time. This last step typically involves some simplification steps,
and is usually done manually in current approaches. We implement our techniques
in a prototype tool and demonstrate our process on several classical examples.
Some of them go beyond the capability of current semi-automatic approaches.
","['\nGilles Barthe\n', '\nThomas Espitau\n', '\nLuis María Ferrer Fioriti\n', '\nJustin Hsu\n']",,,http://dx.doi.org/10.1007/978-3-319-41528-4_3,cs.PL,"['cs.PL', 'cs.SC']",10.1007/978-3-319-41528-4_3,,[]
"Theano: A Python framework for fast computation of mathematical
  expressions",http://arxiv.org/abs/1605.02688v1,2016-05-09T18:32:34Z,2016-05-09T18:32:34Z,"  Theano is a Python library that allows to define, optimize, and evaluate
mathematical expressions involving multi-dimensional arrays efficiently. Since
its introduction, it has been one of the most used CPU and GPU mathematical
compilers - especially in the machine learning community - and has shown steady
performance improvements. Theano is being actively and continuously developed
since 2008, multiple frameworks have been built on top of it and it has been
used to produce many state-of-the-art machine learning models.
  The present article is structured as follows. Section I provides an overview
of the Theano software and its community. Section II presents the principal
features of Theano and how to use them, and compares them with other similar
projects. Section III focuses on recently-introduced functionalities and
improvements. Section IV compares the performance of Theano against Torch7 and
TensorFlow on several machine learning models. Section V discusses current
limitations of Theano and potential ways of improving it.
","['\n The Theano Development Team\n', '\nRami Al-Rfou\n', '\nGuillaume Alain\n', '\nAmjad Almahairi\n', '\nChristof Angermueller\n', '\nDzmitry Bahdanau\n', '\nNicolas Ballas\n', '\nFrédéric Bastien\n', '\nJustin Bayer\n', '\nAnatoly Belikov\n', '\nAlexander Belopolsky\n', '\nYoshua Bengio\n', '\nArnaud Bergeron\n', '\nJames Bergstra\n', '\nValentin Bisson\n', '\nJosh Bleecher Snyder\n', '\nNicolas Bouchard\n', '\nNicolas Boulanger-Lewandowski\n', '\nXavier Bouthillier\n', '\nAlexandre de Brébisson\n', '\nOlivier Breuleux\n', '\nPierre-Luc Carrier\n', '\nKyunghyun Cho\n', '\nJan Chorowski\n', '\nPaul Christiano\n', '\nTim Cooijmans\n', '\nMarc-Alexandre Côté\n', '\nMyriam Côté\n', '\nAaron Courville\n', '\nYann N. Dauphin\n', '\nOlivier Delalleau\n', '\nJulien Demouth\n', '\nGuillaume Desjardins\n', '\nSander Dieleman\n', '\nLaurent Dinh\n', '\nMélanie Ducoffe\n', '\nVincent Dumoulin\n', '\nSamira Ebrahimi Kahou\n', '\nDumitru Erhan\n', '\nZiye Fan\n', '\nOrhan Firat\n', '\nMathieu Germain\n', '\nXavier Glorot\n', '\nIan Goodfellow\n', '\nMatt Graham\n', '\nCaglar Gulcehre\n', '\nPhilippe Hamel\n', '\nIban Harlouchet\n', '\nJean-Philippe Heng\n', '\nBalázs Hidasi\n', '\nSina Honari\n', '\nArjun Jain\n', '\nSébastien Jean\n', '\nKai Jia\n', '\nMikhail Korobov\n', '\nVivek Kulkarni\n', '\nAlex Lamb\n', '\nPascal Lamblin\n', '\nEric Larsen\n', '\nCésar Laurent\n', '\nSean Lee\n', '\nSimon Lefrancois\n', '\nSimon Lemieux\n', '\nNicholas Léonard\n', '\nZhouhan Lin\n', '\nJesse A. Livezey\n', '\nCory Lorenz\n', '\nJeremiah Lowin\n', '\nQianli Ma\n', '\nPierre-Antoine Manzagol\n', '\nOlivier Mastropietro\n', '\nRobert T. McGibbon\n', '\nRoland Memisevic\n', '\nBart van Merriënboer\n', '\nVincent Michalski\n', '\nMehdi Mirza\n', '\nAlberto Orlandi\n', '\nChristopher Pal\n', '\nRazvan Pascanu\n', '\nMohammad Pezeshki\n', '\nColin Raffel\n', '\nDaniel Renshaw\n', '\nMatthew Rocklin\n', '\nAdriana Romero\n', '\nMarkus Roth\n', '\nPeter Sadowski\n', '\nJohn Salvatier\n', '\nFrançois Savard\n', '\nJan Schlüter\n', '\nJohn Schulman\n', '\nGabriel Schwartz\n', '\nIulian Vlad Serban\n', '\nDmitriy Serdyuk\n', '\nSamira Shabanian\n', '\nÉtienne Simon\n', '\nSigurd Spieckermann\n', '\nS. Ramana Subramanyam\n', '\nJakub Sygnowski\n', '\nJérémie Tanguay\n', '\nGijs van Tulder\n', '\nJoseph Turian\n', '\nSebastian Urban\n', '\nPascal Vincent\n', '\nFrancesco Visin\n', '\nHarm de Vries\n', '\nDavid Warde-Farley\n', '\nDustin J. Webb\n', '\nMatthew Willson\n', '\nKelvin Xu\n', '\nLijun Xue\n', '\nLi Yao\n', '\nSaizheng Zhang\n', '\nYing Zhang\n']","19 pages, 5 figures",,http://arxiv.org/abs/1605.02688v1,cs.SC,"['cs.SC', 'cs.LG', 'cs.MS']",,,[]
"Using Two Types of Computer Algebra Systems to Solve Maxwell Optics
  Problems",http://arxiv.org/abs/1605.00832v1,2016-05-03T10:55:19Z,2016-05-03T10:55:19Z,"  To synthesize Maxwell optics systems, the mathematical apparatus of tensor
and vector analysis is generally employed. This mathematical apparatus implies
executing a great number of simple stereotyped operations, which are adequately
supported by computer algebra systems. In this paper, we distinguish between
two stages of working with a mathematical model: model development and model
usage. Each of these stages implies its own computer algebra system. As a model
problem, we consider the problem of geometrization of Maxwell's equations. Two
computer algebra systems---Cadabra and FORM---are selected for use at different
stages of investigation.
",['\nD. S. Kulyabov\n'],in Russian; in English,Programming and Computer Software 42 (2) (2016) 77--83,http://dx.doi.org/10.1134/S0361768816020043,cs.SC,['cs.SC'],10.1134/S0361768816020043,,[]
"Determinantal sets, singularities and application to optimal control in
  medical imagery",http://arxiv.org/abs/1605.00887v2,2016-05-03T13:05:59Z,2017-07-06T09:10:06Z,"  Control theory has recently been involved in the field of nuclear magnetic
resonance imagery. The goal is to control the magnetic field optimally in order
to improve the contrast between two biological matters on the pictures.
Geometric optimal control leads us here to analyze mero-morphic vector fields
depending upon physical parameters , and having their singularities defined by
a deter-minantal variety. The involved matrix has polynomial entries with
respect to both the state variables and the parameters. Taking into account the
physical constraints of the problem, one needs to classify, with respect to the
parameters, the number of real singularities lying in some prescribed
semi-algebraic set. We develop a dedicated algorithm for real root
classification of the singularities of the rank defects of a polynomial matrix,
cut with a given semi-algebraic set. The algorithm works under some genericity
assumptions which are easy to check. These assumptions are not so restrictive
and are satisfied in the aforementioned application. As more general strategies
for real root classification do, our algorithm needs to compute the critical
loci of some maps, intersections with the boundary of the semi-algebraic
domain, etc. In order to compute these objects, the determinantal structure is
exploited through a stratifi-cation by the rank of the polynomial matrix. This
speeds up the computations by a factor 100. Furthermore, our implementation is
able to solve the application in medical imagery, which was out of reach of
more general algorithms for real root classification. For instance,
computational results show that the contrast problem where one of the matters
is water is partitioned into three distinct classes.
","['\nBernard Bonnard\nIMB, McTAO\n', '\nJean-Charles Faugère\nPolSys\n', '\nAlain Jacquemard\nIMB, PolSys\n', '\nMohab Safey El Din\nPolSys\n', '\nThibaut Verron\nPolSys\n']",,"International symposium on symbolic and algebraic computations,
  Waterloo, Canada. ACM, pp.103-110 (2016)",http://dx.doi.org/10.1145/2930889.2930916,cs.SC,['cs.SC'],10.1145/2930889.2930916,,"['IMB, McTAO', 'PolSys', 'IMB, PolSys', 'PolSys', 'PolSys']"
New Bounds for Hypergeometric Creative Telescoping,http://arxiv.org/abs/1604.08059v4,2016-04-27T13:28:12Z,2016-05-13T12:48:42Z,"  Based on a modified version of Abramov-Petkov\v{s}ek reduction, a new
algorithm to compute minimal telescopers for bivariate hypergeometric terms was
developed last year. We investigate further in this paper and present a new
argument for the termination of this algorithm, which provides an independent
proof of the existence of telescopers and even enables us to derive lower as
well as upper bounds for the order of telescopers for hypergeometric terms.
Compared to the known bounds in the literature, our bounds are sometimes
better, and never worse than the known ones.
",['\nHui Huang\n'],"8 pages, ISSAC 2016 submission",,http://dx.doi.org/10.1145/2930889.2930893,cs.SC,"['cs.SC', 'math.RA', 'I.1.2']",10.1145/2930889.2930893,,[]
"On the Complexity of Solving Zero-Dimensional Polynomial Systems via
  Projection",http://arxiv.org/abs/1604.08944v1,2016-04-29T19:42:38Z,2016-04-29T19:42:38Z,"  Given a zero-dimensional polynomial system consisting of n integer
polynomials in n variables, we propose a certified and complete method to
compute all complex solutions of the system as well as a corresponding
separating linear form l with coefficients of small bit size. For computing l,
we need to project the solutions into one dimension along O(n) distinct
directions but no further algebraic manipulations. The solutions are then
directly reconstructed from the considered projections. The first step is
deterministic, whereas the second step uses randomization, thus being
Las-Vegas.
  The theoretical analysis of our approach shows that the overall cost for the
two problems considered above is dominated by the cost of carrying out the
projections. We also give bounds on the bit complexity of our algorithms that
are exclusively stated in terms of the number of variables, the total degree
and the bitsize of the input polynomials.
","['\nCornelius Brand\n', '\nMichael Sagraloff\n']",,,http://arxiv.org/abs/1604.08944v1,cs.SC,"['cs.SC', 'cs.CC']",,,[]
Symbolic-Numeric Tools for Analytic Combinatorics in Several Variables,http://arxiv.org/abs/1605.00402v1,2016-05-02T09:28:32Z,2016-05-02T09:28:32Z,"  Analytic combinatorics studies the asymptotic behaviour of sequences through
the analytic properties of their generating functions. This article provides
effective algorithms required for the study of analytic combinatorics in
several variables, together with their complexity analyses. Given a
multivariate rational function we show how to compute its smooth isolated
critical points, with respect to a polynomial map encoding asymptotic
behaviour, in complexity singly exponential in the degree of its denominator.
We introduce a numerical Kronecker representation for solutions of polynomial
systems with rational coefficients and show that it can be used to decide
several properties (0 coordinate, equal coordinates, sign conditions for real
solutions, and vanishing of a polynomial) in good bit complexity. Among the
critical points, those that are minimal---a property governed by inequalities
on the moduli of the coordinates---typically determine the dominant asymptotics
of the diagonal coefficient sequence. When the Taylor expansion at the origin
has all non-negative coefficients (known as the `combinatorial case') and under
regularity conditions, we utilize this Kronecker representation to determine
probabilistically the minimal critical points in complexity singly exponential
in the degree of the denominator, with good control over the exponent in the
bit complexity estimate. Generically in the combinatorial case, this allows one
to automatically and rigorously determine asymptotics for the diagonal
coefficient sequence. Examples obtained with a preliminary implementation show
the wide applicability of this approach.
","['\nStephen Melczer\n', '\nBruno Salvy\n']",As accepted to proceedings of ISSAC 2016,,http://dx.doi.org/10.1145/2930889.2930913,cs.SC,"['cs.SC', 'math.CO']",10.1145/2930889.2930913,,[]
Munchausen Iteration,http://arxiv.org/abs/1605.00422v1,2016-05-02T10:32:39Z,2016-05-02T10:32:39Z,"  We present a method for solving polynomial equations over idempotent
omega-continuous semirings. The idea is to iterate over the semiring of
functions rather than the semiring of interest, and only evaluate when needed.
The key operation is substitution. In the initial step, we compute a linear
completion of the system of equations that exhaustively inserts the equations
into one another. With functions as approximants, the following steps insert
the current approximant into itself. Since the iteration improves its precision
by substitution rather than computation we named it Munchausen, after the
fictional baron that pulled himself out of a swamp by his own hair. The first
result shows that an evaluation of the n-th Munchausen approximant coincides
with the 2^n-th Newton approximant. Second, we show how to compute linear
completions with standard techniques from automata theory. In particular, we
are not bound to (but can use) the notion of differentials prominent in Newton
iteration.
","['\nRoland Meyer\n', '\nSebastian Muskalla\n']",,,http://arxiv.org/abs/1605.00422v1,cs.SC,"['cs.SC', 'cs.DS']",,,[]
Verifying Buchberger's Algorithm in Reduction Rings,http://arxiv.org/abs/1604.08736v1,2016-04-29T08:51:32Z,2016-04-29T08:51:32Z,"  In this paper we present the formal, computer-supported verification of a
functional implementation of Buchberger's critical-pair/completion algorithm
for computing Gr\""obner bases in reduction rings. We describe how the algorithm
can be implemented and verified within one single software system, which in our
case is the Theorema system.
  In contrast to existing formal correctness proofs of Buchberger's algorithm
in other systems, e. g. Coq and ACL2, our work is not confined to the classical
setting of polynomial rings over fields, but considers the much more general
setting of reduction rings; this, naturally, makes the algorithm more
complicated and the verification more difficult.
  The correctness proof is essentially based on some non-trivial results from
the theory of reduction rings, which we formalized and formally proved as well.
This formalization already consists of more than 800 interactively proved
lemmas and theorems, making the elaboration an extensive example of
higher-order theory exploration in Theorema.
",['\nAlexander Maletzky\n'],"8 pages; appeared in the proceedings of PAS'2015 (Program
  Verification, Automated Debugging, and Symbolic Computation, Beijing, China,
  October 21--23, 2015)",,http://arxiv.org/abs/1604.08736v1,cs.SC,"['cs.SC', 'cs.LO', 'math.AC', '13P10', 'F.4.1; I.2.3']",,,[]
Computing Real Roots of Real Polynomials ... and now For Real!,http://arxiv.org/abs/1605.00410v1,2016-05-02T09:47:10Z,2016-05-02T09:47:10Z,"  Very recent work introduces an asymptotically fast subdivision algorithm,
denoted ANewDsc, for isolating the real roots of a univariate real polynomial.
The method combines Descartes' Rule of Signs to test intervals for the
existence of roots, Newton iteration to speed up convergence against clusters
of roots, and approximate computation to decrease the required precision. It
achieves record bounds on the worst-case complexity for the considered problem,
matching the complexity of Pan's method for computing all complex roots and
improving upon the complexity of other subdivision methods by several
magnitudes.
  In the article at hand, we report on an implementation of ANewDsc on top of
the RS root isolator. RS is a highly efficient realization of the classical
Descartes method and currently serves as the default real root solver in Maple.
We describe crucial design changes within ANewDsc and RS that led to a
high-performance implementation without harming the theoretical complexity of
the underlying algorithm.
  With an excerpt of our extensive collection of benchmarks, available online
at http://anewdsc.mpi-inf.mpg.de/, we illustrate that the theoretical gain in
performance of ANewDsc over other subdivision methods also transfers into
practice. These experiments also show that our new implementation outperforms
both RS and mature competitors by magnitudes for notoriously hard instances
with clustered roots. For all other instances, we avoid almost any overhead by
integrating additional optimizations and heuristics.
","['\nAlexander Kobel\n', '\nFabrice Rouillier\n', '\nMichael Sagraloff\n']","Accepted for presentation at the 41st International Symposium on
  Symbolic and Algebraic Computation (ISSAC), July 19--22, 2016, Waterloo,
  Ontario, Canada",,http://dx.doi.org/10.1145/2930889.2930937,cs.MS,"['cs.MS', 'cs.NA', 'cs.SC', 'math.NA', '65H04, 68N30 (Primary) 68W30 (Secondary)', 'G.1.5; G.1.0; G.4']",10.1145/2930889.2930937,,[]
Sparse Representations of Clifford and Tensor algebras in Maxima,http://arxiv.org/abs/1604.06967v1,2016-04-23T23:55:27Z,2016-04-23T23:55:27Z,"  Clifford algebras have broad applications in science and engineering. The use
of Clifford algebras can be further promoted in these fields by availability of
computational tools that automate tedious routine calculations. We offer an
extensive demonstration of the applications of Clifford algebras in
electromagnetism using the geometric algebra G3 = Cl(3,0) as a computational
model in the Maxima computer algebra system. We compare the geometric
algebra-based approach with conventional symbolic tensor calculations supported
by Maxima, based on the itensor package. The Clifford algebra functionality of
Maxima is distributed as two new packages called clifford - for basic
simplification of Clifford products, outer products, scalar products and
inverses; and cliffordan - for applications of geometric calculus.
","['\nDimiter Prodanov\n', '\nViktor T. Toth\n']","23 pages, 2 figures; accepted for publication in Advances in Applied
  Clifford Algebras, special issue AGACSE 2015","Advances in Applied Clifford Algebras, special issue AGACSE 2015
  (2016), pp. 1-23",http://dx.doi.org/10.1007/s00006-016-0682-x,cs.SC,['cs.SC'],10.1007/s00006-016-0682-x,,[]
Decoding Interleaved Gabidulin Codes using Alekhnovich's Algorithm,http://arxiv.org/abs/1604.05899v2,2016-04-20T11:24:53Z,2016-09-15T07:22:46Z,"  We prove that Alekhnovich's algorithm can be used for row reduction of skew
polynomial matrices. This yields an $O(\ell^3 n^{(\omega+1)/2} \log(n))$
decoding algorithm for $\ell$-Interleaved Gabidulin codes of length $n$, where
$\omega$ is the matrix multiplication exponent, improving in the exponent of
$n$ compared to previous results.
","['\nSven Puchinger\n', '\nSven Müelich\n', '\nDavid Mödinger\n', '\nJohan Rosenkilde né Nielsen\n', '\nMartin Bossert\n']","6 pages, presented at the International Workshop on Algebraic and
  Combinatorial Coding Theory (ACCT) 2016, submitted to Electronic Notes in
  Discrete Mathematics (volume devoted to ACCT 2016)",,http://arxiv.org/abs/1604.05899v2,cs.IT,"['cs.IT', 'cs.SC', 'math.IT']",,,[]
Toric Difference Variety,http://arxiv.org/abs/1604.01958v1,2016-04-07T11:23:29Z,2016-04-07T11:23:29Z,"  In this paper, the concept of toric difference varieties is defined and four
equivalent descriptions for toric difference varieties are presented in terms
of difference rational parametrization, difference coordinate rings, toric
difference ideals, and group actions by difference tori. Connections between
toric difference varieties and affine N[x]-semimodules are established by
proving the correspondence between the irreducible invariant difference
subvarieties and the faces of the N[x]-submodules and the orbit-face
correspondence. Finally, an algorithm is given to decide whether a binomial
difference ideal represented by a Z[x]-lattice defines a toric difference
variety.
","['\nXiao-Shan Gao\n', '\nZhang Huang\n', '\nJie Wang\n', '\nChun-Ming Yuan\n']",,,http://arxiv.org/abs/1604.01958v1,cs.SC,"['cs.SC', 'math.AG', 'Primary 12H10, 14M25, Secondary 14Q99, 68W30']",,,[]
A modified block Lanczos algorithm with fewer vectors,http://arxiv.org/abs/1604.02277v1,2016-04-08T08:49:24Z,2016-04-08T08:49:24Z,"  The block Lanczos algorithm proposed by Peter Montgomery is an efficient
means to tackle the sparse linear algebra problem which arises in the context
of the number field sieve factoring algorithm and its predecessors. We present
here a modified version of the algorithm, which incorporates several
improvements: we discuss how to efficiently handle homogeneous systems and how
to reduce the number of vectors stored in the course of the computation. We
also provide heuristic justification for the success probability of our
modified algorithm. While the overall complexity and expected number of steps
of the block Lanczos is not changed by the modifications presented in this
article, we expect these to be useful for implementations of the block Lanczos
algorithm where the storage of auxiliary vectors sometimes has a non-negligible
cost. 1 Linear systems for integer factoring For factoring a composite integer
N, algorithms based on the technique of combination of congruences look for
several pairs of integers (x, y) such that x 2 $\not\equiv$ y 2 mod N. This
equality is hoped to be non trivial for at least one of the obtained pairs,
letting gcd(x -- y, N) unveil a factor of the integer N. Several algorithms use
this strategy: the CFRAC algorithm, the quadratic sieve and its variants, and
the number field sieve. Pairs (x, y) as above are obtained by combining
relations which have been collected as a step of these algorithms. Relations
are written multiplicatively as a set of valuations. All the algorithms
considered seek a multiplicative combination of these relations which can be
rewritten as an equality of squares. This is achieved by solving a system of
linear equations defined over F 2, where equations are parity constraints on
",['\nEmmanuel Thomé\nCARAMBA\n'],"Topics in Computational Number Theory inspired by Peter L.
  Montgomery, Cambridge University Press, 2016",,http://arxiv.org/abs/1604.02277v1,cs.CR,"['cs.CR', 'cs.SC']",,,['CARAMBA']
Algorithmic computation of polynomial amoebas,http://arxiv.org/abs/1604.03603v1,2016-04-12T22:11:15Z,2016-04-12T22:11:15Z,"  We present algorithms for computation and visualization of amoebas, their
contours, compactified amoebas and sections of three-dimensional amoebas by
two-dimensional planes. We also provide method and an algorithm for the
computation of~polynomials whose amoebas exhibit the most complicated topology
among all polynomials with a fixed Newton polytope. The presented algorithms
are implemented in computer algebra systems Matlab 8 and Mathematica 9.
","['\nD. V. Bogdanov\n', '\nA. A. Kytmanov\n', '\nT. M. Sadykov\n']",,,http://arxiv.org/abs/1604.03603v1,cs.CG,"['cs.CG', 'cs.SC', '14Q10, 14Q05, 14Q15', 'G.4']",,,[]
Chordal networks of polynomial ideals,http://arxiv.org/abs/1604.02618v2,2016-04-09T23:16:04Z,2016-11-22T14:38:57Z,"  We introduce a novel representation of structured polynomial ideals, which we
refer to as chordal networks. The sparsity structure of a polynomial system is
often described by a graph that captures the interactions among the variables.
Chordal networks provide a computationally convenient decomposition into
simpler (triangular) polynomial sets, while preserving the underlying graphical
structure. We show that many interesting families of polynomial ideals admit
compact chordal network representations (of size linear in the number of
variables), even though the number of components is exponentially large.
Chordal networks can be computed for arbitrary polynomial systems using a
refinement of the chordal elimination algorithm from [Cifuentes-Parrilo-2016].
Furthermore, they can be effectively used to obtain several properties of the
variety, such as its dimension, cardinality, and equidimensional components, as
well as an efficient probabilistic test for radical ideal membership. We apply
our methods to examples from algebraic statistics and vector addition systems;
for these instances, algorithms based on chordal networks outperform existing
techniques by orders of magnitude.
","['\nDiego Cifuentes\n', '\nPablo A. Parrilo\n']","39 pages, 13 figures, 5 tables","SIAM J. Appl. Algebra Geometry, 1(1), 73-110. (38 pages), 2017",http://dx.doi.org/10.1137/16M106995X,cs.SC,"['cs.SC', 'math.AC', 'math.AG', '68W30 (Primary) 13P15, 14Q99 (Secondary)']",10.1137/16M106995X,,[]
"Solution of Interpolation Problems via the Hankel Polynomial
  Construction",http://arxiv.org/abs/1603.08752v1,2016-03-29T13:02:01Z,2016-03-29T13:02:01Z,"  We treat the interpolation problem $ \{f(x_j)=y_j\}_{j=1}^N $ for polynomial
and rational functions. Developing the approach by C.Jacobi, we represent the
interpolants by virtue of the Hankel polynomials generated by the sequences $
\{\sum_{j=1}^N x_j^ky_j/W^{\prime}(x_j) \}_{k\in \mathbb N} $ and $
\{\sum_{j=1}^N x_j^k/(y_jW^{\prime}(x_j)) \}_{k\in \mathbb N} $; here $
W(x)=\prod_{j=1}^N(x-x_j) $. The obtained results are applied for the error
correction problem, i.e. the problem of reconstructing the polynomial from a
redundant set of its values some of which are probably erroneous. The problem
of evaluation of the resultant of polynomials $ p(x) $ and $ q(x) $ from the
set of values $ \{p(x_j)/q(x_j) \}_{j=1}^N $ is also tackled within the
framework of this approach.
","['\nAlexei Yu. Uteshev\n', '\nIvan Baravy\n']","56 pages, 1 figure",,http://arxiv.org/abs/1603.08752v1,cs.SC,"['cs.SC', '68W30, 30E05, 16D05, 12Y05, 26C15', 'I.1.2; G.1.1']",,,[]
"Secure cloud computations: Description of (fully)homomorphic ciphers
  within the P-adic model of encryption",http://arxiv.org/abs/1603.07699v1,2016-03-24T18:43:49Z,2016-03-24T18:43:49Z,"  In this paper we consider the description of homomorphic and fully
homomorphic ciphers in the $p$-adic model of encryption. This model describes a
wide class of ciphers, but certainly not all. Homomorphic and fully homomorphic
ciphers are used to ensure the credibility of remote computing, including cloud
technology. The model describes all homomorphic ciphers with respect to
arithmetic and coordinate-wise logical operations in the ring of $p$-adic
integers $Z_p$. We show that there are no fully homomorphic ciphers for each
pair of the considered set of arithmetic and coordinate-wise logical operations
on $Z_p$. We formulate the problem of constructing a fully homomorphic cipher
as follows. We consider a homomorphic cipher with respect to operation ""$*$"" on
$Z_p$. Then, we describe the complete set of operations ""$G$"", for which the
cipher is homomorphic. As a result, we construct a fully homomorphic cipher
with respect to the operations ""$*$"" and ""$G$"". We give a description of all
operations ""$G$"", for which we obtain fully homomorphic ciphers with respect to
the operations ""$+$"" and ""$G$"" from the homomorphic cipher constructed with
respect to the operation ""$+$"". We also present examples of such ""new""
operations.
","['\nAndrei Khrennikov\n', '\nEkaterina Yurova\n']","submitted to Journal of Mathematical Cryptology (JMC) in July 2015,
  under review",,http://arxiv.org/abs/1603.07699v1,cs.CR,"['cs.CR', 'cs.SC']",,,[]
"Summation Theory II: Characterizations of
  $\boldsymbol{RΠΣ^*}$-extensions and algorithmic aspects",http://arxiv.org/abs/1603.04285v2,2016-03-14T14:48:09Z,2016-07-13T06:24:21Z,"  Recently, $R\Pi\Sigma^*$-extensions have been introduced which extend Karr's
$\Pi\Sigma^*$-fields substantially: one can represent expressions not only in
terms of transcendental sums and products, but one can work also with products
over primitive roots of unity. Since one can solve the parameterized
telescoping problem in such rings, covering as special cases the summation
paradigms of telescoping and creative telescoping, one obtains a rather
flexible toolbox for symbolic summation. This article is the continuation of
this work. Inspired by Singer's Galois theory of difference equations we will
work out several alternative characterizations of $R\Pi\Sigma^*$-extensions:
adjoining naively sums and products leads to an $R\Pi\Sigma^*$-extension iff
the obtained difference ring is simple iff the ring can be embedded into the
ring of sequences iff the ring can be given by the interlacing of
$\Pi\Sigma^*$-extensions. From the viewpoint of applications this leads to a
fully automatic machinery to represent indefinite nested sums and products in
such $R\Pi\Sigma^*$-rings. In addition, we work out how the parameterized
telescoping paradigm can be used to prove algebraic independence of indefinite
nested sums. Furthermore, one obtains an alternative reduction tactic to solve
the parameterized telescoping problem in basic $R\Pi\Sigma^*$-extensions
exploiting the interlacing property.
",['\nCarsten Schneider\n'],"Numerous small corrections; a corrected proof of Lemma 2.22; extra
  requirements of part (1) of Theorem 6.7 have been inserted",,http://arxiv.org/abs/1603.04285v2,cs.SC,['cs.SC'],,,[]
Binomial Difference Ideals,http://arxiv.org/abs/1603.03987v1,2016-03-13T03:49:01Z,2016-03-13T03:49:01Z,"  In this paper, binomial difference ideals are studied. Three canonical
representations for Laurent binomial difference ideals are given in terms of
the reduced Groebner basis of Z[x]-lattices, regular and coherent difference
ascending chains, and partial characters over Z[x]-lattices, respectively.
Criteria for a Laurent binomial difference ideal to be reflexive, prime,
well-mixed, and perfect are given in terms of their support lattices. The
reflexive, well-mixed, and perfect closures of a Laurent binomial difference
ideal are shown to be binomial. Most of the properties of Laurent binomial
difference ideals are extended to the case of difference binomial ideals.
Finally, algorithms are given to check whether a given Laurent binomial
difference ideal I is reflexive, prime, well-mixed, or perfect, and in the
negative case, to compute the reflexive, well-mixed, and perfect closures of I.
An algorithm is given to decompose a finitely generated perfect binomial
difference ideal as the intersection of reflexive prime binomial difference
ideals.
","['\nXiao-Shan Gao\n', '\nZhang Huang\n', '\nChun-Ming Yuan\n']",arXiv admin note: substantial text overlap with arXiv:1404.7580,,http://arxiv.org/abs/1603.03987v1,cs.SC,"['cs.SC', 'math.AC']",,,[]
Extraction of cylinders and cones from minimal point sets,http://arxiv.org/abs/1603.04582v2,2016-03-15T07:37:29Z,2016-06-21T08:36:41Z,"  We propose new algebraic methods for extracting cylinders and cones from
minimal point sets, including oriented points. More precisely, we are
interested in computing efficiently cylinders through a set of three points,
one of them being oriented, or through a set of five simple points. We are also
interested in computing efficiently cones through a set of two oriented points,
through a set of four points, one of them being oriented, or through a set of
six points. For these different interpolation problems, we give optimal bounds
on the number of solutions. Moreover, we describe algebraic methods targeted to
solve these problems efficiently.
","['\nLaurent Busé\nGALAAD2\n', '\nAndré Galligo\nJAD, GALAAD2\n', '\nJiajun Zhang\nGALAAD2\n']",,"Graphical Models, Elsevier, 2016, 86, pp.1-12",http://arxiv.org/abs/1603.04582v2,cs.CG,"['cs.CG', 'cs.SC']",,,"['GALAAD2', 'JAD, GALAAD2', 'GALAAD2']"
Algorithm for computing $μ$-bases of univariate polynomials,http://arxiv.org/abs/1603.04813v2,2016-03-15T18:46:15Z,2017-03-08T14:57:06Z,"  We present a new algorithm for computing a $\mu$-basis of the syzygy module
of $n$ polynomials in one variable over an arbitrary field $\mathbb{K}$. The
algorithm is conceptually different from the previously-developed algorithms by
Cox, Sederberg, Chen, Zheng, and Wang for $n=3$, and by Song and Goldman for an
arbitrary $n$. It involves computing a ""partial"" reduced row-echelon form of a
$ (2d+1)\times n(d+1)$ matrix over $\mathbb{K}$, where $d$ is the maximum
degree of the input polynomials. The proof of the algorithm is based on
standard linear algebra and is completely self-contained. It includes a proof
of the existence of the $\mu$-basis and as a consequence provides an
alternative proof of the freeness of the syzygy module. The theoretical (worst
case asymptotic) computational complexity of the algorithm is
$O(d^2n+d^3+n^2)$. We have implemented this algorithm (HHK) and the one
developed by Song and Goldman (SG). Experiments on random inputs indicate that
SG gets faster than HHK when $d$ gets sufficiently large for a fixed $n$, and
that HHK gets faster than SG when $n$ gets sufficiently large for a fixed $d$.
","['\nHoon Hong\n', '\nZachary Hough\n', '\nIrina A. Kogan\n']","34 pages, 6 figures","J. of Symbolic Comput., Vol. 80, No 3, (2017), 844 - 874",http://arxiv.org/abs/1603.04813v2,math.AG,"['math.AG', 'cs.SC', 'math.AC', '12Y05, 13P10, 14Q05, 68W30']",,,[]
Symbolic Tensor Calculus -- Functional and Dynamic Approach,http://arxiv.org/abs/1603.05819v1,2016-03-18T10:21:13Z,2016-03-18T10:21:13Z,"  In this paper, we briefly discuss the dynamic and functional approach to
computer symbolic tensor analysis. The ccgrg package for Wolfram
Language/Mathematica is used to illustrate this approach. Some examples of
applications are attached.
","['\nA. Woszczyna\n', '\nP. Plaszczyk\n', '\nW. Czaja\n', '\nZ. A. Golda\n']",,,http://dx.doi.org/10.4467/2353737XCT.15.110.4147,cs.SC,"['cs.SC', 'gr-qc', 'physics.comp-ph']",10.4467/2353737XCT.15.110.4147,,[]
Nearest Points on Toric Varieties,http://arxiv.org/abs/1603.06544v4,2016-03-21T19:16:25Z,2017-12-01T16:42:07Z,"  We determine the Euclidean distance degree of a projective toric variety.
This extends the formula of Matsui and Takeuchi for the degree of the
$A$-discriminant in terms of Euler obstructions. Our primary goal is the
development of reliable algorithmic tools for computing the points on a real
toric variety that are closest to a given data point.
","['\nMartin Helmer\n', '\nBernd Sturmfels\n']",20 pages,Mathematica Scandinavica 122.2 (2018): 213-238,http://dx.doi.org/10.7146/math.scand.a-101478,math.AG,"['math.AG', 'cs.SC', 'math.OC']",10.7146/math.scand.a-101478,,[]
Finding best possible constant for a polynomial inequality,http://arxiv.org/abs/1603.01338v1,2016-03-04T03:23:49Z,2016-03-04T03:23:49Z,"  Given a multi-variant polynomial inequality with a parameter, how to find the
best possible value of this parameter that satisfies the inequality? For
instance, find the greatest number $k$ that satisfies $ a^3+b^3+c^3+
k(a^2b+b^2c+c^2a)-(k+1)(ab^2+bc^2+ca^2)\geq 0 $ for all nonnegative real
numbers $ a,b,c $. Analogues problems often appeared in studies of inequalities
and were dealt with by various methods. In this paper, a general algorithm is
proposed for finding the required best possible constant. The algorithm can be
easily implemented by computer algebra tools such as Maple.
","['\nLu Yang\n', '\nJu Zhang\n']","Proceedings of the 20th Asian Technology Conference in Mathematics
  (Leshan, China, 2015) 178-187",,http://arxiv.org/abs/1603.01338v1,cs.SC,['cs.SC'],,,[]
Matrix factoring by fraction-free reduction,http://arxiv.org/abs/1603.03565v1,2016-03-11T08:46:30Z,2016-03-11T08:46:30Z,"  We consider exact matrix decomposition by Gauss-Bareiss reduction. We
investigate two aspects of the process: common row and column factors and the
influence of pivoting strategies. We identify two types of common factors:
systematic and statistical. Systematic factors depend on the process, while
statistical factors depend on the specific data. We show that existing
fraction-free QR (Gram-Schmidt) algorithms create a common factor in the last
column of Q. We relate the existence of row factors in LU decomposition to
factors appearing in the Smith normal form of the matrix. For statistical
factors, we identify mechanisms and give estimates of the frequency. Our
conclusions are tested by experimental data. For pivoting strategies, we
compare the sizes of output factors obtained by different strategies. We also
comment on timing differences.
","['\nJohannes Middeke\n', '\nDavid J. Jeffrey\n']",Submitted to ISSAC 2016,,http://arxiv.org/abs/1603.03565v1,cs.SC,['cs.SC'],,,[]
On the ring of local unitary invariants for mixed X-states of two qubits,http://arxiv.org/abs/1603.03262v3,2016-03-10T13:41:31Z,2016-04-07T13:48:27Z,"  Entangling properties of a mixed 2-qubit system can be described by the local
homogeneous unitary invariant polynomials in elements of the density matrix.
The structure of the corresponding invariant polynomial ring for the special
subclass of states, the so-called mixed X-states, is established. It is shown
that for the X-states there is an injective ring homomorphism of the quotient
ring of SU(2)xSU(2) invariant polynomials modulo its syzygy ideal and the
SO(2)xSO(2)-invariant ring freely generated by five homogeneous polynomials of
degrees 1,1,1,2,2.
","['\nV. Gerdt\n', '\nA. Khvedelidze\n', '\nYu. Palii\n']",16 pages,,http://arxiv.org/abs/1603.03262v3,quant-ph,"['quant-ph', 'cs.SC', 'math-ph', 'math.MP', '81P45']",,,[]
"On Gröbner Bases and Krull Dimension of Residue Class Rings of
  Polynomial Rings over Integral Domains",http://arxiv.org/abs/1602.04300v4,2016-02-13T08:11:02Z,2017-03-23T06:27:56Z,"  Given an ideal $\mathfrak{a}$ in $A[x_1, \ldots, x_n]$, where $A$ is a
Noetherian integral domain, we propose an approach to compute the Krull
dimension of $A[x_1,\ldots,x_n]/\mathfrak{a}$, when the residue class
polynomial ring is a free $A$-module. When $A$ is a field, the Krull dimension
of $A[x_1,\ldots,x_n]/\mathfrak{a}$ has several equivalent algorithmic
definitions by which it can be computed. But this is not true in the case of
arbitrary Noetherian rings. For a Noetherian integral domain, $A$ we introduce
the notion of combinatorial dimension of $A[x_1, \ldots,x_n]/\mathfrak{a}$ and
give a Gr\""obner basis method to compute it for residue class polynomial rings
that have a free $A$-module representation w.r.t. a lexicographic ordering. For
such $A$-algebras, we derive a relation between Krull dimension and
combinatorial dimension of $A[x_1, \ldots, x_n]/\mathfrak{a}$. An immediate
application of this relation is that it gives a uniform method, the first of
its kind, to compute the dimension of $A[x_1, \ldots, x_n]/\mathfrak{a}$
without having to consider individual properties of the ideal. For $A$-algebras
that have a free $A$-module representation w.r.t. degree compatible monomial
orderings, we introduce the concepts of Hilbert function, Hilbert series and
Hilbert polynomials and show that Gr\""obner basis methods can be used to
compute these quantities. We then proceed to show that the combinatorial
dimension of such $A$-algebras is equal to the degree of the Hilbert
polynomial. This enables us to extend the relation between Krull dimension and
combinatorial dimension to $A$-algebras with a free $A$-module representation
w.r.t. a degree compatible ordering as well.
","['\nMaria Francis\n', '\nAmbedkar Dukkipati\n']",,,http://dx.doi.org/10.1016/j.jsc.2017.03.003,cs.SC,['cs.SC'],10.1016/j.jsc.2017.03.003,,[]
An Illustrated Introduction to the Truncated Fourier Transform,http://arxiv.org/abs/1602.04562v2,2016-02-15T05:24:41Z,2016-02-17T11:12:03Z,"  The Truncated Fourier Transform (TFT) is a variation of the Discrete Fourier
Transform (DFT/FFT) that allows for input vectors that do NOT have length $2^n$
for $n$ a positive integer. We present the univariate version of the TFT,
originally due to Joris van der Hoeven, heavily illustrating the presentation
in order to make these methods accessible to a broader audience.
",['\nPaul Vrbik\n'],,,http://arxiv.org/abs/1602.04562v2,cs.SC,['cs.SC'],,,[]
Mathematical Theory Exploration in Theorema: Reduction Rings,http://arxiv.org/abs/1602.04339v1,2016-02-13T14:32:18Z,2016-02-13T14:32:18Z,"  In this paper we present the first-ever computer formalization of the theory
of Gr\""obner bases in reduction rings, which is an important theory in
computational commutative algebra, in Theorema. Not only the formalization, but
also the formal verification of all results has already been fully completed by
now; this, in particular, includes the generic implementation and correctness
proof of Buchberger's algorithm in reduction rings. Thanks to the seamless
integration of proving and computing in Theorema, this implementation can now
be used to compute Gr\""obner bases in various different domains directly within
the system. Moreover, a substantial part of our formalization is made up solely
by ""elementary theories"" such as sets, numbers and tuples that are themselves
independent of reduction rings and may therefore be used as the foundations of
future theory explorations in Theorema.
  In addition, we also report on two general-purpose Theorema tools we
developed for an efficient and convenient exploration of mathematical theories:
an interactive proving strategy and a ""theory analyzer"" that already proved
extremely useful when creating large structured knowledge bases.
",['\nAlexander Maletzky\n'],"submitted to CICM 2015 (Conference on Intelligent Computer
  Mathematics)",,http://dx.doi.org/10.1007/978-3-319-42547-4_1,cs.SC,"['cs.SC', 'cs.LO', 'math.AC', '13P10', 'F.4.1; I.2.3']",10.1007/978-3-319-42547-4_1,,[]
On p-adic differential equations with separation of variables,http://arxiv.org/abs/1602.00244v2,2016-01-31T13:26:48Z,2016-05-03T13:55:07Z,"  Several algorithms in computer algebra involve the computation of a power
series solution of a given ordinary differential equation. Over finite fields,
the problem is often lifted in an approximate $p$-adic setting to be
well-posed. This raises precision concerns: how much precision do we need on
the input to compute the output accurately? In the case of ordinary
differential equations with separation of variables, we make use of the recent
technique of differential precision to obtain optimal bounds on the stability
of the Newton iteration. The results apply, for example, to algorithms for
manipulating algebraic numbers over finite fields, for computing isogenies
between elliptic curves or for deterministically finding roots of polynomials
in finite fields. The new bounds lead to significant speedups in practice.
","['\nPierre Lairez\n', '\nTristan Vaccon\n']","ISSAC '16, July 19-22, 2016, Waterloo, ON, Canada","Proceedings of ISSAC 2016 (Waterloo, ON, Canada)",http://dx.doi.org/10.1145/2930889.2930912,cs.SC,['cs.SC'],10.1145/2930889.2930912,,[]
Reduction-Based Creative Telescoping for Algebraic Functions,http://arxiv.org/abs/1602.00424v1,2016-02-01T08:19:40Z,2016-02-01T08:19:40Z,"  Continuing a series of articles in the past few years on creative telescoping
using reductions, we develop a new algorithm to construct minimal telescopers
for algebraic functions. This algorithm is based on Trager's Hermite reduction
and on polynomial reduction, which was originally designed for hyperexponential
functions and extended to the algebraic case in this paper.
","['\nShaoshi Chen\n', '\nManuel Kauers\n', '\nChristoph Koutschan\n']",,,http://arxiv.org/abs/1602.00424v1,cs.SC,"['cs.SC', 'I.1.2']",,,[]
"Fast Computation of Minimal Interpolation Bases in Popov Form for
  Arbitrary Shifts",http://arxiv.org/abs/1602.00651v2,2016-02-01T19:32:34Z,2016-05-13T07:50:18Z,"  We compute minimal bases of solutions for a general interpolation problem,
which encompasses Hermite-Pad\'e approximation and constrained multivariate
interpolation, and has applications in coding theory and security.
  This problem asks to find univariate polynomial relations between $m$ vectors
of size $\sigma$; these relations should have small degree with respect to an
input degree shift. For an arbitrary shift, we propose an algorithm for the
computation of an interpolation basis in shifted Popov normal form with a cost
of $\mathcal{O}\tilde{~}(m^{\omega-1} \sigma)$ field operations, where $\omega$
is the exponent of matrix multiplication and the notation
$\mathcal{O}\tilde{~}(\cdot)$ indicates that logarithmic terms are omitted.
  Earlier works, in the case of Hermite-Pad\'e approximation and in the general
interpolation case, compute non-normalized bases. Since for arbitrary shifts
such bases may have size $\Theta(m^2 \sigma)$, the cost bound
$\mathcal{O}\tilde{~}(m^{\omega-1} \sigma)$ was feasible only with restrictive
assumptions on the shift that ensure small output sizes. The question of
handling arbitrary shifts with the same complexity bound was left open.
  To obtain the target cost for any shift, we strengthen the properties of the
output bases, and of those obtained during the course of the algorithm: all the
bases are computed in shifted Popov form, whose size is always $\mathcal{O}(m
\sigma)$. Then, we design a divide-and-conquer scheme. We recursively reduce
the initial interpolation problem to sub-problems with more convenient shifts
by first computing information on the degrees of the intermediate bases.
","['\nClaude-Pierre Jeannerod\n', '\nVincent Neiger\n', '\nEric Schost\n', '\nGilles Villard\n']","8 pages, sig-alternate class, 4 figures (problems and algorithms)",,http://dx.doi.org/10.1145/2930889.2930928,cs.SC,['cs.SC'],10.1145/2930889.2930928,,[]
"Fast Computation of Shifted Popov Forms of Polynomial Matrices via
  Systems of Modular Polynomial Equations",http://arxiv.org/abs/1602.00710v2,2016-02-01T21:10:31Z,2016-05-12T09:19:09Z,"  We give a Las Vegas algorithm which computes the shifted Popov form of an $m
\times m$ nonsingular polynomial matrix of degree $d$ in expected
$\widetilde{\mathcal{O}}(m^\omega d)$ field operations, where $\omega$ is the
exponent of matrix multiplication and $\widetilde{\mathcal{O}}(\cdot)$
indicates that logarithmic factors are omitted. This is the first algorithm in
$\widetilde{\mathcal{O}}(m^\omega d)$ for shifted row reduction with arbitrary
shifts.
  Using partial linearization, we reduce the problem to the case $d \le \lceil
\sigma/m \rceil$ where $\sigma$ is the generic determinant bound, with $\sigma
/ m$ bounded from above by both the average row degree and the average column
degree of the matrix. The cost above becomes $\widetilde{\mathcal{O}}(m^\omega
\lceil \sigma/m \rceil)$, improving upon the cost of the fastest previously
known algorithm for row reduction, which is deterministic.
  Our algorithm first builds a system of modular equations whose solution set
is the row space of the input matrix, and then finds the basis in shifted Popov
form of this set. We give a deterministic algorithm for this second step
supporting arbitrary moduli in $\widetilde{\mathcal{O}}(m^{\omega-1} \sigma)$
field operations, where $m$ is the number of unknowns and $\sigma$ is the sum
of the degrees of the moduli. This extends previous results with the same cost
bound in the specific cases of order basis computation and M-Pad\'e
approximation, in which the moduli are products of known linear factors.
",['\nVincent Neiger\n'],"8 pages, sig-alternate class, 5 figures (problems and algorithms)",,http://dx.doi.org/10.1145/2930889.2930936,cs.SC,['cs.SC'],10.1145/2930889.2930936,,[]
"Linear Time Interactive Certificates for the Minimal Polynomial and the
  Determinant of a Sparse Matrix",http://arxiv.org/abs/1602.00810v2,2016-02-02T07:29:28Z,2019-12-02T13:02:25Z,"  Computational problem certificates are additional data structures for each
output, which can be used by a-possibly randomized-verification algorithm that
proves the correctness of each output. In this paper, we give an algorithm that
computes a certificate for the minimal polynomial of sparse or structured nxn
matrices over an abstract field, of sufficiently large cardinality, whose Monte
Carlo verification complexity requires a single matrix-vector multiplication
and a linear number of extra field operations. We also propose a novel
preconditioner that ensures irreducibility of the characteristic polynomial of
the generically preconditioned matrix. This preconditioner takes linear time to
be applied and uses only two random entries. We then combine these two
techniques to give algorithms that compute certificates for the determinant,
and thus for the characteristic polynomial, whose Monte Carlo verification
complexity is therefore also linear.
","['\nJean-Guillaume Dumas\nLJK\n', '\nErich Kaltofen\nNCSU\n', '\nEmmanuel Thomé\nCARAMBA\n', '\nGilles Villard\nARIC, LIP\n']",,"International Symposium on Symbolic and Algebraic Computation, Jul
  2016, Waterloo, Canada. pp.199-206,
  \&\#x27E8;10.1145/2930889.2930908\&\#x27E9",http://arxiv.org/abs/1602.00810v2,cs.SC,['cs.SC'],,,"['LJK', 'NCSU', 'CARAMBA', 'ARIC, LIP']"
Algorithms for Simultaneous Padé Approximations,http://arxiv.org/abs/1602.00836v2,2016-02-02T09:06:58Z,2016-05-23T11:44:46Z,"  We describe how to solve simultaneous Pad\'e approximations over a power
series ring $K[[x]]$ for a field $K$ using $O~(n^{\omega - 1} d)$ operations in
$K$, where $d$ is the sought precision and $n$ is the number of power series to
approximate. We develop two algorithms using different approaches. Both
algorithms return a reduced sub-bases that generates the complete set of
solutions to the input approximations problem that satisfy the given degree
constraints. Our results are made possible by recent breakthroughs in fast
computations of minimal approximant bases and Hermite Pad\'e approximations.
","['\nJohan S. R. Nielsen\n', '\nArne Storjohann\n']",ISSAC 2016,,http://dx.doi.org/10.1145/2930889.2930933,cs.SC,['cs.SC'],10.1145/2930889.2930933,,[]
Computing with quasiseparable matrices,http://arxiv.org/abs/1602.01246v2,2016-02-03T10:11:19Z,2016-09-19T14:41:20Z,"  The class of quasiseparable matrices is defined by a pair of bounds, called
the quasiseparable orders, on the ranks of the maximal sub-matrices entirely
located in their strictly lower and upper triangular parts. These arise
naturally in applications, as e.g. the inverse of band matrices, and are widely
used for they admit structured representations allowing to compute with them in
time linear in the dimension and quadratic with the quasiseparable order. We
show, in this paper, the connection between the notion of quasisepa-rability
and the rank profile matrix invariant, presented in [Dumas \& al. ISSAC'15].
This allows us to propose an algorithm computing the quasiseparable orders (rL,
rU) in time O(n^2 s^($\omega$--2)) where s = max(rL, rU) and $\omega$ the
exponent of matrix multiplication. We then present two new structured
representations, a binary tree of PLUQ decompositions, and the Bruhat
generator, using respectively O(ns log n/s) and O(ns) field elements instead of
O(ns^2) for the previously known generators. We present algorithms computing
these representations in time O(n^2 s^($\omega$--2)). These representations
allow a matrix-vector product in time linear in the size of their
representation. Lastly we show how to multiply two such structured matrices in
time O(n^2 s^($\omega$--2)).
","['\nClement Pernet\nUGA, ARIC\n']","International Symposium on Symbolic and Algebraic Computation
  (ISSAC'16), Jul 2016, Waterloo, France",,http://dx.doi.org/10.1145/2930889.2930915,cs.SC,['cs.SC'],10.1145/2930889.2930915,,"['UGA, ARIC']"
"A fast, deterministic algorithm for computing a Hermite Normal Form of a
  polynomial matrix",http://arxiv.org/abs/1602.02049v1,2016-02-05T14:59:34Z,2016-02-05T14:59:34Z,"  Given a square, nonsingular matrix of univariate polynomials $\mathbf{F} \in
\mathbb{K}[x]^{n \times n}$ over a field $\mathbb{K}$, we give a fast,
deterministic algorithm for finding the Hermite normal form of $\mathbf{F}$
with complexity $O^{\sim}\left(n^{\omega}d\right)$ where $d$ is the degree of
$\mathbf{F}$. Here soft-$O$ notation is Big-$O$ with log factors removed and
$\omega$ is the exponent of matrix multiplication. The method relies of a fast
algorithm for determining the diagonal entries of its Hermite normal form,
having as cost $O^{\sim}\left(n^{\omega}s\right)$ operations with $s$ the
average of the column degrees of $\mathbf{F}$.
","['\nGeorge Labahn\n', '\nWei Zhou\n']",,,http://arxiv.org/abs/1602.02049v1,cs.SC,['cs.SC'],,,[]
Solving rank-constrained semidefinite programs in exact arithmetic,http://arxiv.org/abs/1602.00431v3,2016-02-01T08:58:49Z,2016-11-19T17:37:08Z,"  We consider the problem of minimizing a linear function over an affine
section of the cone of positive semidefinite matrices, with the additional
constraint that the feasible matrix has prescribed rank. When the rank
constraint is active, this is a non-convex optimization problem, otherwise it
is a semidefinite program. Both find numerous applications especially in
systems control theory and combinatorial optimization, but even in more general
contexts such as polynomial optimization or real algebra. While numerical
algorithms exist for solving this problem, such as interior-point or
Newton-like algorithms, in this paper we propose an approach based on symbolic
computation. We design an exact algorithm for solving rank-constrained
semidefinite programs, whose complexity is essentially quadratic on natural
degree bounds associated to the given optimization problem: for subfamilies of
the problem where the size of the feasible matrix is fixed, the complexity is
polynomial in the number of variables. The algorithm works under assumptions on
the input data: we prove that these assumptions are generically satisfied. We
also implement it in Maple and discuss practical experiments.
",['\nSimone Naldi\n'],"Published at ISSAC 2016. Extended version submitted to the Journal of
  Symbolic Computation",,http://arxiv.org/abs/1602.00431v3,cs.SY,"['cs.SY', 'cs.SC', '14Q20, 52B55', 'F.2.2; G.1.6']",,,[]
Holonomic Tools for Basic Hypergeometric Functions,http://arxiv.org/abs/1602.00454v1,2016-02-01T10:04:20Z,2016-02-01T10:04:20Z,"  With the exception of q-hypergeometric summation, the use of computer algebra
packages implementing Zeilberger's ""holonomic systems approach"" in a broader
mathematical sense is less common in the field of q-series and basic
hypergeometric functions. A major objective of this article is to popularize
the usage of such tools also in these domains. Concrete case studies showing
software in action introduce to the basic techniques. An application highlight
is a new computer-assisted proof of the celebrated Ismail-Zhang formula, an
important q-analog of a classical expansion formula of plane waves in terms of
Gegenbauer polynomials.
","['\nChristoph Koutschan\n', '\nPeter Paule\n']",,,http://arxiv.org/abs/1602.00454v1,cs.SC,"['cs.SC', 'math.CO']",,,[]
"Fast Computation of the Nth Term of an Algebraic Series over a Finite
  Prime Field",http://arxiv.org/abs/1602.00545v2,2016-02-01T14:42:39Z,2016-05-18T15:47:36Z,"  We address the question of computing one selected term of an algebraic power
series. In characteristic zero, the best algorithm currently known for
computing the $N$th coefficient of an algebraic series uses differential
equations and has arithmetic complexity quasi-linear in $\sqrt{N}$. We show
that over a prime field of positive characteristic $p$, the complexity can be
lowered to $O(\log N)$. The mathematical basis for this dramatic improvement is
a classical theorem stating that a formal power series with coefficients in a
finite field is algebraic if and only if the sequence of its coefficients can
be generated by an automaton. We revisit and enhance two constructive proofs of
this result for finite prime fields. The first proof uses Mahler equations,
whose sizes appear to be prohibitively large. The second proof relies on
diagonals of rational functions; we turn it into an efficient algorithm, of
complexity linear in $\log N$ and quasi-linear in $p$.
","['\nAlin Bostan\n', '\nGilles Christol\n', '\nPhilippe Dumas\n']","Proceedings of ISSAC 2016, to appear",,http://dx.doi.org/10.1145/2930889.2930904,cs.SC,"['cs.SC', 'math.NT', '11Y16, 68W30, 13F25, 11B85', 'I.1.2']",10.1145/2930889.2930904,,[]
"Numerically validating the completeness of the real solution set of a
  system of polynomial equations",http://arxiv.org/abs/1602.00700v1,2016-02-01T21:01:07Z,2016-02-01T21:01:07Z,"  Computing the real solutions to a system of polynomial equations is a
challenging problem, particularly verifying that all solutions have been
computed. We describe an approach that combines numerical algebraic geometry
and sums of squares programming to test whether a given set is ""complete"" with
respect to the real solution set. Specifically, we test whether the Zariski
closure of that given set is indeed equal to the solution set of the real
radical of the ideal generated by the given polynomials. Examples with finitely
and infinitely many real solutions are provided, along with an example having
polynomial inequalities.
","['\nDaniel A. Brake\n', '\nJonathan D. Hauenstein\n', '\nAlan C. Liddell\n']",,,http://arxiv.org/abs/1602.00700v1,math.NA,"['math.NA', 'cs.SC']",,,[]
On the p-adic stability of the FGLM algorithm,http://arxiv.org/abs/1602.00848v1,2016-02-02T09:30:28Z,2016-02-02T09:30:28Z,"  Nowadays, many strategies to solve polynomial systems use the computation of
a Gr{\""o}bner basis for the graded reverse lexicographical ordering, followed
by a change of ordering algorithm to obtain a Gr{\""o}bner basis for the
lexicographical ordering. The change of ordering algorithm is crucial for these
strategies. We study the p-adic stability of the main change of ordering
algorithm, FGLM. We show that FGLM is stable and give explicit upper bound on
the loss of precision occuring in its execution. The variant of FGLM designed
to pass from the grevlex ordering to a Gr{\""o}bner basis in shape position is
also stable. Our study relies on the application of Smith Normal Form
computations for linear algebra.
","['\nGuénaël Renault\nPolSys\n', '\nTristan Vaccon\n']",,,http://arxiv.org/abs/1602.00848v1,cs.SC,"['cs.SC', 'math.AC']",,,['PolSys']
Division and Slope Factorization of p-Adic Polynomials,http://arxiv.org/abs/1602.01303v1,2016-02-03T14:03:53Z,2016-02-03T14:03:53Z,"  We study two important operations on polynomials defined over complete
discrete valuation fields: Euclidean division and factorization. In particular,
we design a simple and efficient algorithm for computing slope factorizations,
based on Newton iteration. One of its main features is that we avoid working
with fractional exponents. We pay particular attention to stability, and
analyze the behavior of the algorithm using several precision models.
","['\nXavier Caruso\nIRMAR\n', '\nDavid Roe\n', '\nTristan Vaccon\n']",,,http://dx.doi.org/10.1145/1235,math.NT,"['math.NT', 'cs.SC']",10.1145/1235,,['IRMAR']
Inverse Inequality Estimates with Symbolic Computation,http://arxiv.org/abs/1602.01304v2,2016-02-03T14:04:03Z,2016-05-03T20:47:37Z,"  In the convergence analysis of numerical methods for solving partial
differential equations (such as finite element methods) one arrives at certain
generalized eigenvalue problems, whose maximal eigenvalues need to be estimated
as accurately as possible. We apply symbolic computation methods to the
situation of square elements and are able to improve the previously known upper
bound, given in ""p- and hp-finite element methods"" (Schwab, 1998), by a factor
of 8. More precisely, we try to evaluate the corresponding determinant using
the holonomic ansatz, which is a powerful tool for dealing with determinants,
proposed by Zeilberger in 2007. However, it turns out that this method does not
succeed on the problem at hand. As a solution we present a variation of the
original holonomic ansatz that is applicable to a larger class of determinants,
including the one we are dealing with here. We obtain an explicit closed form
for the determinant, whose special form enables us to derive new and tight
upper resp. lower bounds on the maximal eigenvalue, as well as its asymptotic
behaviour.
","['\nChristoph Koutschan\n', '\nMartin Neumüller\n', '\nCristian-Silviu Radu\n']",,"Advances in Applied Mathematics 80:1-23, 2016",http://dx.doi.org/10.1016/j.aam.2016.04.005,cs.SC,"['cs.SC', 'math.NA', '33F10, 65N12 (Primary) 65N30, 68W30, 65F15, 05A20, 15A15, 15A45\n  (Secondary)']",10.1016/j.aam.2016.04.005,,[]
A Factorization Algorithm for G-Algebras and Applications,http://arxiv.org/abs/1602.00296v1,2016-01-31T18:33:33Z,2016-01-31T18:33:33Z,"  It has been recently discovered by Bell, Heinle and Levandovskyy that a large
class of algebras, including the ubiquitous $G$-algebras, are finite
factorization domains (FFD for short).
  Utilizing this result, we contribute an algorithm to find all distinct
factorizations of a given element $f \in \mathcal{G}$, where $\mathcal{G}$ is
any $G$-algebra, with minor assumptions on the underlying field.
  Moreover, the property of being an FFD, in combination with the factorization
algorithm, enables us to propose an analogous description of the factorized
Gr\""obner basis algorithm for $G$-algebras. This algorithm is useful for
various applications, e.g. in analysis of solution spaces of systems of linear
partial functional equations with polynomial coefficients, coming from
$\mathcal{G}$. Additionally, it is possible to include inequality constraints
for ideals in the input.
","['\nAlbert Heinle\n', '\nViktor Levandovskyy\n']",,,http://dx.doi.org/10.1016/j.jsc.2017.06.005,math.RA,"['math.RA', 'cs.SC', 'math.OA']",10.1016/j.jsc.2017.06.005,,[]
The algebra of Kleene stars of the plane and polylogarithms,http://arxiv.org/abs/1602.02801v2,2016-02-05T20:05:04Z,2016-04-08T06:39:49Z,"  We extend the definition and study the algebraic properties of the
polylogarithm Li(T), where T is rational series over the alphabet X = {x 0, x
1} belonging to suitable subalgebras of rational series.
","['\nNgoc Hoang\nLIPN\n', '\nGérard Duchamp\nLIPN\n', '\nHoang Ngoc Minh\nLIPN\n']",,,http://dx.doi.org/10.1145/1235,math.CO,"['math.CO', 'cs.SC', 'math.AC']",10.1145/1235,,"['LIPN', 'LIPN', 'LIPN']"
"HYPERgeometric functions DIfferential REduction: Mathematica-based
  packages for the differential reduction of generalizedhypergeometric
  functions: Fc hypergeometric function of three variables",http://arxiv.org/abs/1602.00917v2,2016-02-02T13:30:53Z,2016-05-22T21:15:00Z,"  We present a further extension of the HYPERDIRE project, which is devoted to
the creation of a set of Mathematica-based program packages for manipulations
with Horn-type hypergeometric functions on the basis of differential equations.
Specifically, we present the implementation of the differential reduction for
the Lauricella function $F_C$ of three variables.
","['\nV. Bytev\n', '\nB. Kniehl\n']","15 pages, minor changes, accepted for publication in Computer Physics
  Communications",,http://dx.doi.org/10.1016/j.cpc.2016.04.016,math-ph,"['math-ph', 'cs.SC', 'hep-ph', 'hep-th', 'math.MP']",10.1016/j.cpc.2016.04.016,,[]
Descartes' Rule of Signs for Polynomial Systems supported on Circuits,http://arxiv.org/abs/1601.05826v2,2016-01-21T22:01:22Z,2016-08-29T21:05:07Z,"  We give a multivariate version of Descartes' rule of signs to bound the
number of positive real roots of a system of polynomial equations in n
variables with n+2 monomials, in terms of the sign variation of a sequence
associated both to the exponent vectors and the given coefficients. We show
that our bound is sharp and is related to the signature of the circuit.
","['\nFrédéric Bihan\n', '\nAlicia Dickenstein\n']","25 pages, 3 figures",,http://arxiv.org/abs/1601.05826v2,math.AG,"['math.AG', 'cs.SC']",,,[]
"Computing the decomposition group of a zero-dimensional ideal by
  elimination method",http://arxiv.org/abs/1601.06626v1,2016-01-22T08:59:42Z,2016-01-22T08:59:42Z,"  In this note, we show that the decomposition group $Dec(I)$ of a
zero-dimensional radical ideal $I$ in ${\bf K}[x_1,\ldots,x_n]$ can be
represented as the direct sum of several symmetric groups of polynomials based
upon using Gr\""{o}bner bases. The new method makes a theoretical contribution
to discuss the decomposition group of $I$ by using Computer Algebra without
considering the complexity. As one application, we also present an approach to
yield new triangular sets in computing triangular decomposition of polynomial
sets ${\mathbb P}$ if $Dec(<{\mathbb P}>)$ is known.
",['\nYongbin Li\n'],,,http://arxiv.org/abs/1601.06626v1,math.AC,"['math.AC', 'cs.SC']",,,[]
"Fast Computation of the Rank Profile Matrix and the Generalized Bruhat
  Decomposition",http://arxiv.org/abs/1601.01798v2,2016-01-08T09:04:59Z,2018-05-14T14:25:24Z,"  The row (resp. column) rank profile of a matrix describes the stair-case
shape of its row (resp. column) echelon form. We here propose a new matrix
invariant, the rank profile matrix, summarizing all information on the row and
column rank profiles of all the leading sub-matrices. We show that this normal
form exists and is unique over any ring, provided that the notion of McCoy's
rank is used, in the presence of zero divisors. We then explore the conditions
for a Gaussian elimination algorithm to compute all or part of this invariant,
through the corresponding PLUQ decomposition. This enlarges the set of known
Elimination variants that compute row or column rank profiles. As a consequence
a new Crout base case variant significantly improves the practical efficiency
of previously known implementations over a finite field. With matrices of very
small rank, we also generalize the techniques of Storjohann and Yang to the
computation of the rank profile matrix, achieving an $(r^\omega+mn)^{1+o(1)}$
time complexity for an $m \times n$ matrix of rank $r$, where $\omega$ is the
exponent of matrix multiplication. Finally, by give connections to the Bruhat
decomposition, and several of its variants and generalizations. Thus, our
algorithmic improvements for the PLUQ factorization, and their implementations,
directly apply to these decompositions. In particular, we show how a PLUQ
decomposition revealing the rank profile matrix also reveals both a row and a
column echelon form of the input matrix or of any of its leading sub-matrices,
by a simple post-processing made of row and column permutations.
","['\nJean-Guillaume Dumas\nCASYS\n', '\nClement Pernet\nARIC, MOAIS, LMC - IMAG\n', '\nZiad Sultan\nMOAIS\n']",arXiv admin note: substantial text overlap with arXiv:1501.05239,"Journal of Symbolic Computation, Elsevier, 2017, Special issue on
  ISSAC'15, 83, pp.187-210",http://dx.doi.org/10.1016/j.jsc.2016.11.011,cs.SC,['cs.SC'],10.1016/j.jsc.2016.11.011,,"['CASYS', 'ARIC, MOAIS, LMC - IMAG', 'MOAIS']"
Factorization of C-finite Sequences,http://arxiv.org/abs/1601.02756v1,2016-01-12T07:45:00Z,2016-01-12T07:45:00Z,"  We discuss how to decide whether a given C-finite sequence can be written
nontrivially as a product of two other C-finite sequences.
","['\nManuel Kauers\n', '\nDoron Zeilberger\n']",,,http://arxiv.org/abs/1601.02756v1,cs.SC,"['cs.SC', 'I.1.2']",,,[]
Existence Problem of Telescopers: Beyond the Bivariate Case,http://arxiv.org/abs/1601.03080v1,2016-01-12T22:00:59Z,2016-01-12T22:00:59Z,"  In this paper, we solve the existence problem of telescopers for rational
functions in three discrete variables. We reduce the problem to that of
deciding the summability of bivariate rational functions, which has been solved
recently. The existence criteria we present is needed for detecting the
termination of Zeilberger's algorithm to the function classes studied in this
paper.
","['\nShaoshi Chen\n', '\nQing-Hu Hou\n', '\nGeorge Labahn\n', '\nRong-Hua Wang\n']",19 pages,,http://arxiv.org/abs/1601.03080v1,cs.SC,"['cs.SC', 'math.CO', '33F10']",,,[]
"A toolbox to solve coupled systems of differential and difference
  equations",http://arxiv.org/abs/1601.01856v1,2016-01-08T12:34:28Z,2016-01-08T12:34:28Z,"  We present algorithms to solve coupled systems of linear differential
equations, arising in the calculation of massive Feynman diagrams with local
operator insertions at 3-loop order, which do {\it not} request special choices
of bases. Here we assume that the desired solution has a power series
representation and we seek for the coefficients in closed form. In particular,
if the coefficients depend on a small parameter $\ep$ (the dimensional
parameter), we assume that the coefficients themselves can be expanded in
formal Laurent series w.r.t.\ $\ep$ and we try to compute the first terms in
closed form. More precisely, we have a decision algorithm which solves the
following problem: if the terms can be represented by an indefinite nested
hypergeometric sum expression (covering as special cases the harmonic sums,
cyclotomic sums, generalized harmonic sums or nested binomial sums), then we
can calculate them. If the algorithm fails, we obtain a proof that the terms
cannot be represented by the class of indefinite nested hypergeometric sum
expressions. Internally, this problem is reduced by holonomic closure
properties to solving a coupled system of linear difference equations. The
underlying method in this setting relies on decoupling algorithms, difference
ring algorithms and recurrence solving. We demonstrate by a concrete example
how this algorithm can be applied with the new Mathematica package
\texttt{SolveCoupledSystem} which is based on the packages \texttt{Sigma},
\texttt{HarmonicSums} and \texttt{OreSys}. In all applications the
representation in $x$-space is obtained as an iterated integral representation
over general alphabets, generalizing Poincar\'{e} iterated integrals.
","['\nJakob Ablinger\n', '\nJohannes Bluemlein\n', '\nAbilio de Freitas\n', '\nCarsten Schneider\n']",,,http://arxiv.org/abs/1601.01856v1,cs.SC,"['cs.SC', 'hep-ph', 'hep-th', 'math-ph', 'math.MP']",,,[]
The Module Isomorphism Problem for Finite Rings and Related Results,http://arxiv.org/abs/1512.08365v1,2015-12-28T10:18:50Z,2015-12-28T10:18:50Z,"  Let $R$ be a finite ring and let $M, N$ be two finite left $R$-modules. We
present two distinct deterministic algorithms that decide in polynomial time
whether or not $M$ and $N$ are isomorphic, and if they are, exhibit an
isomorphism. As by-products, we are able to determine the largest isomorphic
common direct summand between two modules and the minimum number of generators
of a module. By not requiring $R$ to contain a field, avoiding computation of
the Jacobson radical and not distinguishing between large and small
characteristic, both algorithms constitute improvements to known results.
",['\nIuliana Ciocănea-Teodorescu\n'],"7 pages, submitted, an abstract of this paper appeared in ACM
  Communications in Computer Algebra, Volume 49, Issue 1, page 14, March 2015",,http://arxiv.org/abs/1512.08365v1,math.RA,"['math.RA', 'cs.SC']",,,[]
Computing Chebyshev knot diagrams,http://arxiv.org/abs/1512.07766v2,2015-12-24T09:23:25Z,2017-05-16T07:36:49Z,"  A Chebyshev curve $\mathcal{C}(a,b,c,\phi)$ has a parametrization of the
form$ x(t)=T\_a(t)$; \ $y(t)=T\_b(t)$; $z(t)= T\_c(t + \phi)$, where $a,b,c$are
integers, $T\_n(t)$ is the Chebyshev polynomialof degree $n$ and $\phi \in
\mathbb{R}$. When $\mathcal{C}(a,b,c,\phi)$ is nonsingular,it defines a
polynomial knot. We determine all possible knot diagrams when $\phi$ varies.
Let $a,b,c$ be integers, $a$ is odd, $(a,b)=1$, we show that one can list all
possible knots $\mathcal{C}(a,b,c,\phi)$ in$\tilde{\mathcal{O}}(n^2)$ bit
operations, with $n=abc$.
","['\nP. -V Koseleff\nOURAGAN, IMJ-PRG, UPMC\n', '\nD Pecker\nIMJ-PRG, UPMC\n', '\nFabrice Rouillier\nOURAGAN, IMJ-PRG, UPMC\n', '\nC Tran\nUPMC, IMJ-PRG\n']",,,http://dx.doi.org/10.1016/j.jsc.2017.04.001,cs.SC,['cs.SC'],10.1016/j.jsc.2017.04.001,,"['OURAGAN, IMJ-PRG, UPMC', 'IMJ-PRG, UPMC', 'OURAGAN, IMJ-PRG, UPMC', 'UPMC, IMJ-PRG']"
"Fast Operations on Linearized Polynomials and their Applications in
  Coding Theory",http://arxiv.org/abs/1512.06520v3,2015-12-21T08:26:14Z,2017-07-11T14:49:14Z,"  This paper considers fast algorithms for operations on linearized
polynomials. We propose a new multiplication algorithm for skew polynomials (a
generalization of linearized polynomials) which has sub-quadratic complexity in
the polynomial degree $s$, independent of the underlying field extension
degree~$m$. We show that our multiplication algorithm is faster than all known
ones when $s \leq m$. Using a result by Caruso and Le Borgne (2017), this
immediately implies a sub-quadratic division algorithm for linearized
polynomials for arbitrary polynomial degree $s$. Also, we propose algorithms
with sub-quadratic complexity for the $q$-transform, multi-point evaluation,
computing minimal subspace polynomials, and interpolation, whose
implementations were at least quadratic before. Using the new fast algorithm
for the $q$-transform, we show how matrix multiplication over a finite field
can be implemented by multiplying linearized polynomials of degrees at most
$s=m$ if an elliptic normal basis of extension degree $m$ exists, providing a
lower bound on the cost of the latter problem. Finally, it is shown how the new
fast operations on linearized polynomials lead to the first error and erasure
decoding algorithm for Gabidulin codes with sub-quadratic complexity.
","['\nSven Puchinger\n', '\nAntonia Wachter-Zeh\n']","25 pages, submitted to Journal of Symbolic Computation",,http://arxiv.org/abs/1512.06520v3,cs.SC,"['cs.SC', 'cs.IT', 'math.IT']",,,[]
"A Probabilistic Algorithm for Computing Data-Discriminants of Likelihood
  Equations",http://arxiv.org/abs/1512.03901v2,2015-12-12T10:48:58Z,2016-06-10T12:23:03Z,"  An algebraic approach to the maximum likelihood estimation problem is to
solve a very structured parameterized polynomial system called likelihood
equations that have finitely many complex (real or non-real) solutions. The
only solutions that are statistically meaningful are the real solutions with
positive coordinates. In order to classify the parameters (data) according to
the number of real/positive solutions, we study how to efficiently compute the
discriminants, say data-discriminants (DD), of the likelihood equations. We
develop a probabilistic algorithm with three different strategies for computing
DDs. Our implemented probabilistic algorithm based on Maple and FGb is more
efficient than our previous version presented in ISSAC2015, and is also more
efficient than the standard elimination for larger benchmarks. By applying
RAGlib to a DD we compute, we give the real root classification of 3 by 3
symmetric matrix model.
","['\nJose Israel Rodriguez\n', '\nXiaoxian Tang\n']","4 tables. arXiv admin note: substantial text overlap with
  arXiv:1501.00334. authors' note: The paper the extended and improved version
  of our paper presented in ISSAC2015 (arXiv:1501.00334) and the paper is
  accepted by Journal of Symbolic Computation Special Issue of ISSAC2015",,http://arxiv.org/abs/1512.03901v2,cs.SC,"['cs.SC', 'G.3; I.1.2']",,,[]
Similarity detection of rational space curves,http://arxiv.org/abs/1512.02620v5,2015-12-08T20:38:42Z,2017-06-10T20:14:08Z,"  We provide an algorithm to check whether two rational space curves are
related by a similarity. The algorithm exploits the relationship between the
curvatures and torsions of two similar curves, which is formulated in a
computer algebra setting. Helical curves, where curvature and torsion are
proportional, need to be distinguished as a special case. The algorithm is easy
to implement, as it involves only standard computer algebra techniques, such as
greatest common divisors and resultants, and Gr\""obner basis for the special
case of helical curves. Details on the implementation and experimentation
carried out using the computer algebra system Maple 18 are provided.
","['\nJuan Gerardo Alcázar\n', '\nCarlos Hermoso\n', '\nGeorg Muntingh\n']",,,http://arxiv.org/abs/1512.02620v5,math.AG,"['math.AG', 'cs.CG', 'cs.SC', '14Q05, 68W30']",,,[]
"An Extension of Moebius--Lie Geometry with Conformal Ensembles of Cycles
  and Its Implementation in a GiNaC Library",http://arxiv.org/abs/1512.02960v3,2015-12-09T17:32:30Z,2018-08-19T20:16:48Z,"  We propose to consider ensembles of cycles (quadrics), which are
interconnected through conformal-invariant geometric relations (e.g. ""to be
orthogonal"", ""to be tangent"", etc.), as new objects in an extended Moebius--Lie
geometry. It was recently demonstrated in several related papers, that such
ensembles of cycles naturally parameterise many other conformally-invariant
objects, e.g. loxodromes or continued fractions. The paper describes a method,
which reduces a collection of conformally invariant geometric relations to a
system of linear equations, which may be accompanied by one fixed quadratic
relation.
  To show its usefulness, the method is implemented as a C++ library. It
operates with numeric and symbolic data of cycles in spaces of arbitrary
dimensionality and metrics with any signatures. Numeric calculations can be
done in exact or approximate arithmetic. In the two- and three-dimensional
cases illustrations and animations can be produced. An interactive Python
wrapper of the library is provided as well.
",['\nVladimir V. Kisil\n'],"LaTeX 16pp+111pp of appendices, including 10 PDF graphic files and
  program code; v2: major revision of the paper, code in v3.1; v3: formal
  definition of the extended geometry, connection with integrable systems, code
  in v3.2rc1","Proc. Int. Geom. Cent. v.11 (2018), n.3, pp.45-67",http://dx.doi.org/10.15673/tmgc.v11i3.1203,cs.CG,"['cs.CG', 'cs.MS', 'cs.SC', 'math.DG', '51B25, 51N25, 68U05, 11E88, 68W30']",10.15673/tmgc.v11i3.1203,,[]
Computing minimal interpolation bases,http://arxiv.org/abs/1512.03503v2,2015-12-11T01:49:54Z,2016-06-13T14:12:18Z,"  We consider the problem of computing univariate polynomial matrices over a
field that represent minimal solution bases for a general interpolation
problem, some forms of which are the vector M-Pad\'e approximation problem in
[Van Barel and Bultheel, Numerical Algorithms 3, 1992] and the rational
interpolation problem in [Beckermann and Labahn, SIAM J. Matrix Anal. Appl. 22,
2000]. Particular instances of this problem include the bivariate interpolation
steps of Guruswami-Sudan hard-decision and K\""otter-Vardy soft-decision
decodings of Reed-Solomon codes, the multivariate interpolation step of
list-decoding of folded Reed-Solomon codes, and Hermite-Pad\'e approximation.
  In the mentioned references, the problem is solved using iterative algorithms
based on recurrence relations. Here, we discuss a fast, divide-and-conquer
version of this recurrence, taking advantage of fast matrix computations over
the scalars and over the polynomials. This new algorithm is deterministic, and
for computing shifted minimal bases of relations between $m$ vectors of size
$\sigma$ it uses $O~( m^{\omega-1} (\sigma + |s|) )$ field operations, where
$\omega$ is the exponent of matrix multiplication, and $|s|$ is the sum of the
entries of the input shift $s$, with $\min(s) = 0$. This complexity bound
improves in particular on earlier algorithms in the case of bivariate
interpolation for soft decoding, while matching fastest existing algorithms for
simultaneous Hermite-Pad\'e approximation.
","['\nClaude-Pierre Jeannerod\n', '\nVincent Neiger\n', '\nÉric Schost\n', '\nGilles Villard\n']","53 pages, 14 figures (problems and algorithms), uses elsart.cls with
  JSC style",,http://arxiv.org/abs/1512.03503v2,cs.SC,"['cs.SC', 'cs.IT', 'math.IT']",,,[]
Contraction of Ore Ideals with Applications,http://arxiv.org/abs/1511.07922v8,2015-11-25T00:15:50Z,2016-01-29T03:02:20Z,"  Ore operators form a common algebraic abstraction of linear ordinary
differential and recurrence equations. Given an Ore operator $L$ with
polynomial coefficients in $x$, it generates a left ideal $I$ in the Ore
algebra over the field $\mathbf{k}(x)$ of rational functions. We present an
algorithm for computing a basis of the contraction ideal of $I$ in the Ore
algebra over the ring $R[x]$ of polynomials, where $R$ may be either
$\mathbf{k}$ or a domain with $\mathbf{k}$ as its fraction field. This
algorithm is based on recent work on desingularization for Ore operators by
Chen, Jaroschek, Kauers and Singer. Using a basis of the contraction ideal, we
compute a completely desingularized operator for $L$ whose leading coefficient
not only has minimal degree in $x$ but also has minimal content. Completely
desingularized operators have interesting applications such as certifying
integer sequences and checking special cases of a conjecture of Krattenthaler.
",['\nYi Zhang\n'],,,http://arxiv.org/abs/1511.07922v8,cs.SC,['cs.SC'],,,[]
On the robust hardness of Gröbner basis computation,http://arxiv.org/abs/1511.06436v3,2015-11-19T22:54:21Z,2018-07-16T20:00:14Z,"  The computation of Gr\""obner bases is an established hard problem. By
contrast with many other problems, however, there has been little investigation
of whether this hardness is robust. In this paper, we frame and present results
on the problem of approximate computation of Gr\""obner bases. We show that it
is NP-hard to construct a Gr\""obner basis of the ideal generated by a set of
polynomials, even when the algorithm is allowed to discard a $(1 - \epsilon)$
fraction of the generators, and likewise when the algorithm is allowed to
discard variables (and the generators containing them). Our results shows that
computation of Gr\""obner bases is robustly hard even for simple polynomial
systems (e.g. maximum degree 2, with at most 3 variables per generator). We
conclude by greatly strengthening results for the Strong $c$-Partial Gr\""obner
problem posed by De Loera et al. Our proofs also establish interesting
connections between the robust hardness of Gr\""obner bases and that of SAT
variants and graph-coloring.
","['\nGwen Spencer\n', '\nDavid Rolnick\n']",19 pages,,http://arxiv.org/abs/1511.06436v3,cs.SC,['cs.SC'],,,[]
"Solving the Forward Position Problem of an In-Parallel Planar
  Manipulator in the Gauss Plane",http://arxiv.org/abs/1511.05060v1,2015-11-16T17:48:44Z,2015-11-16T17:48:44Z,"  We study determining the posture of an in-parallel planar manipulator, which
has three connectors composed of revolute, prismatic and revolute joints, from
specified active joint variables. We construct an ideal in the field of complex
numbers, and we introduce self inversive polynomials. We provide results for an
in-parallel planar manipulator, which has a base and moving platform in right
triangular shape. Using Sage computer algebra system, we compute its Groebner
bases. We illustrate that the single variable polynomials obtained from the
Groebner bases are self reciprocal.
",['\nSureyya Sahin\n'],"11 pages, 4 figures",,http://arxiv.org/abs/1511.05060v1,cs.RO,"['cs.RO', 'cs.SC']",,,[]
"Visualization in teaching and learning mathematics in elementary,
  secondary and higher education",http://arxiv.org/abs/1511.07087v1,2015-11-22T23:38:14Z,2015-11-22T23:38:14Z,"  In this paper we present our experience in using visualization in mathematics
education. The experience with our university courses: ""Computer tools in
matematics"" and ""Symbolic algebra"" provides the basis for mathematics teacher
education program http://vizuelizacija.etf.rs/. The program is intended for
elementary and high school teachers. The education program deals with modern
techniques of visualization by using technologies such as GeoGegebra, JAVA and
HTML.
","['\nBranko Malesevic\n', '\nIvana Jovovic\n', '\nBojan Banjac\n']",,"Bulletin of the ""Politehnica"" University of Timisoara, Romania,
  Tom 58 (72), Fascicola 1, 2013",http://arxiv.org/abs/1511.07087v1,cs.CY,"['cs.CY', 'cs.SC']",,,[]
Improved Polynomial Remainder Sequences for Ore Polynomials,http://arxiv.org/abs/1511.01128v1,2015-11-03T21:47:11Z,2015-11-03T21:47:11Z,"  Polynomial remainder sequences contain the intermediate results of the
Euclidean algorithm when applied to (non-)commutative polynomials. The running
time of the algorithm is dependent on the size of the coefficients of the
remainders. Different ways have been studied to make these as small as
possible. The subresultant sequence of two polynomials is a polynomial
remainder sequence in which the size of the coefficients is optimal in the
generic case, but when taking the input from applications, the coefficients are
often larger than necessary. We generalize two improvements of the subresultant
sequence to Ore polynomials and derive a new bound for the minimal coefficient
size. Our approach also yields a new proof for the results in the commutative
case, providing a new point of view on the origin of the extraneous factors of
the coefficients.
",['\nMaximilian Jaroschek\n'],,"Journal of Symbolic Computation (50), 64 - 76, 2013",http://dx.doi.org/10.1016/j.jsc.2013.05.012,cs.SC,['cs.SC'],10.1016/j.jsc.2013.05.012,,[]
A general framework for Noetherian well ordered polynomial reductions,http://arxiv.org/abs/1511.03234v2,2015-11-10T19:28:53Z,2018-04-05T06:17:32Z,"  Polynomial reduction is one of the main tools in computational algebra with
innumerable applications in many areas, both pure and applied. Since many years
both the theory and an efficient design of the related algorithm have been
solidly established.
  This paper presents a general definition of polynomial reduction structure,
studies its features and highlights the aspects needed in order to grant and to
efficiently test the main properties (noetherianity, confluence, ideal
membership).
  The most significant aspect of this analysis is a negative reappraisal of the
role of the notion of term order which is usually considered a central and
crucial tool in the theory. In fact, as it was already established in the
computer science context in relation with termination of algorithms, most of
the properties can be obtained simply considering a well-founded ordering,
while the classical requirement that it be preserved by multiplication is
irrelevant.
  The last part of the paper shows how the polynomial basis concepts present in
literature are interpreted in our language and their properties are
consequences of the general results established in the first part of the paper.
","['\nMichela Ceria\n', '\nTeo Mora\n', '\nMargherita Roggero\n']","36 pages. New title and substantial improvements to the presentation
  according to the comments of the reviewers",,http://arxiv.org/abs/1511.03234v2,math.AC,"['math.AC', 'cs.SC', '14C05, 14Q20, 13P10']",,,[]
"Formal Solutions of Completely Integrable Pfaffian Systems With Normal
  Crossings",http://arxiv.org/abs/1511.00180v3,2015-10-31T21:34:58Z,2016-10-05T08:17:01Z,"  In this paper, we present an algorithm for computing a fundamental matrix of
formal solutions of completely integrable Pfaffian systems with normal
crossings in several variables. This algorithm is a generalization of a method
developed for the bivariate case based on a combination of several reduction
techniques and is implemented in the computer algebra system Maple.
","['\nMoulay A. Barkatou\n', '\nMaximilian Jaroschek\n', '\nSuzy S. Maddah\n']",Final revision,,http://arxiv.org/abs/1511.00180v3,cs.SC,['cs.SC'],,,[]
Multiple binomial sums,http://arxiv.org/abs/1510.07487v2,2015-10-26T14:18:51Z,2016-06-15T06:50:04Z,"  Multiple binomial sums form a large class of multi-indexed sequences, closed
under partial summation, which contains most of the sequences obtained by
multiple summation of products of binomial coefficients and also all the
sequences with algebraic generating function. We study the representation of
the generating functions of binomial sums by integrals of rational functions.
The outcome is twofold. Firstly, we show that a univariate sequence is a
multiple binomial sum if and only if its generating function is the diagonal of
a rational function. Secondly, we propose algorithms that decide the equality
of multiple binomial sums and that compute recurrence relations for them. In
conjunction with geometric simplifications of the integral representations,
this approach behaves well in practice. The process avoids the computation of
certificates and the problem of the appearance of spurious singularities that
afflicts discrete creative telescoping, both in theory and in practice.
","['\nAlin Bostan\n', '\nPierre Lairez\n', '\nBruno Salvy\n']",,"Journal of symbolic computation (2017) 80.2, pp. 351-386",http://dx.doi.org/10.1016/j.jsc.2016.04.002,cs.SC,"['cs.SC', 'math.CO', '05A10, 33F10, 68W30']",10.1016/j.jsc.2016.04.002,,[]
Algebraic Diagonals and Walks,http://arxiv.org/abs/1510.04080v1,2015-10-14T13:20:38Z,2015-10-14T13:20:38Z,"  The diagonal of a multivariate power series F is the univariate power series
Diag(F) generated by the diagonal terms of F. Diagonals form an important class
of power series; they occur frequently in number theory, theoretical physics
and enumerative combinatorics. We study algorithmic questions related to
diagonals in the case where F is the Taylor expansion of a bivariate rational
function. It is classical that in this case Diag(F) is an algebraic function.
We propose an algorithm that computes an annihilating polynomial for Diag(F).
Generically, it is its minimal polynomial and is obtained in time quasi-linear
in its size. We show that this minimal polynomial has an exponential size with
respect to the degree of the input rational function. We then address the
related problem of enumerating directed lattice walks. The insight given by our
study leads to a new method for expanding the generating power series of
bridges, excursions and meanders. We show that their first N terms can be
computed in quasi-linear complexity in N, without first computing a very large
polynomial equation.
","['\nAlin Bostan\n', '\nLouis Dumont\n', '\nBruno Salvy\n']","The final version of this paper has been published in the proceedings
  ISSAC 2015, in ISSAC'15 International Symposium on Symbolic and Algebraic
  Computation. Bath, United Kingdom - July 06-09, 2015. ACM New York, NY, USA",,http://dx.doi.org/10.1145/2755996.2756663,cs.SC,['cs.SC'],10.1145/2755996.2756663,,[]
"Algebraic Diagonals and Walks: Algorithms, Bounds, Complexity",http://arxiv.org/abs/1510.04526v1,2015-10-15T13:34:08Z,2015-10-15T13:34:08Z,"  The diagonal of a multivariate power series F is the univariate power series
Diag(F) generated by the diagonal terms of F. Diagonals form an important class
of power series; they occur frequently in number theory, theoretical physics
and enumerative combinatorics. We study algorithmic questions related to
diagonals in the case where F is the Taylor expansion of a bivariate rational
function. It is classical that in this case Diag(F) is an algebraic function.
We propose an algorithm that computes an annihilating polynomial for Diag(F).
We give a precise bound on the size of this polynomial and show that
generically, this polynomial is the minimal polynomial and that its size
reaches the bound. The algorithm runs in time quasi-linear in this bound, which
grows exponentially with the degree of the input rational function. We then
address the related problem of enumerating directed lattice walks. The insight
given by our study leads to a new method for expanding the generating power
series of bridges, excursions and meanders. We show that their first N terms
can be computed in quasi-linear complexity in N, without first computing a very
large polynomial equation.
","['\nAlin Bostan\n', '\nLouis Dumont\n', '\nBruno Salvy\n']","This is an extended version of arXiv:1510.04080 with more precise
  results and more detailed proofs",,http://arxiv.org/abs/1510.04526v1,cs.SC,['cs.SC'],,,[]
On Termination of Polynomial Programs with Equality Conditions,http://arxiv.org/abs/1510.05201v3,2015-10-18T06:21:35Z,2020-09-01T17:06:09Z,"  We investigate the termination problem of a family of multi-path polynomial
programs (MPPs), in which all assignments to program variables are polynomials,
and test conditions of loops and conditional statements are polynomial
equalities. We show that the set of non-terminating inputs (NTI) of such a
program is algorithmically computable, thus leading to the decidability of its
termination. To the best of our knowledge, the considered family of MPPs is
hitherto the largest one for which termination is decidable. We present an
explicit recursive function which is essentially Ackermannian, to compute the
maximal length of ascending chains of polynomial ideals under a control
function, and thereby obtain a complete answer to the questions raised by
Seidenberg. This maximal length facilitates a precise complexity analysis of
our algorithms for computing the NTI and deciding termination of MPPs. We
extend our method to programs with polynomial guarded commands and show how an
incomplete procedure for MPPs with inequality guards can be obtained. An
application of our techniques to invariant generation of polynomial programs is
further presented.
","['\nYangjia Li\n', '\nNaijun Zhan\n', '\nMingshuai Chen\n', '\nHui Lu\n', '\nGuohua Wu\n', '\nJoost-Pieter Katoen\n']",,,http://arxiv.org/abs/1510.05201v3,cs.PL,"['cs.PL', 'cs.SC', '68Q60 (Primary) 68W30, 68Q17 (Secondary)']",,,[]
"Sparse Polynomial Systems with many Positive Solutions from Bipartite
  Simplicial Complexes",http://arxiv.org/abs/1510.05622v2,2015-10-19T19:01:26Z,2016-04-18T08:50:32Z,"  Consider a regular triangulation of the convex-hull $P$ of a set $\mathcal A$
of $n$ points in $\mathbb R^d$, and a real matrix $C$ of size $d \times n$. A
version of Viro's method allows to construct from these data an unmixed
polynomial system with support $\mathcal A$ and coefficient matrix $C$ whose
number of positive solutions is bounded from below by the number of
$d$-simplices which are positively decorated by $C$. We show that all the
$d$-simplices of a triangulation can be positively decorated if and only if the
triangulation is balanced, which in turn is equivalent to the fact that its
dual graph is bipartite. This allows us to identify, among classical families,
monomial supports which admit maximally positive systems, i.e. systems all
toric complex solutions of which are real and positive. These families give
some evidence in favor of a conjecture due to Bihan. We also use this technique
in order to construct fewnomial systems with many positive solutions. This is
done by considering a simplicial complex with bipartite dual graph included in
a regular triangulation of the cyclic polytope.
","['\nFrédéric Bihan\nLAMA\n', '\nPierre-Jean Spaenlehauer\nCARAMEL\n']",,,http://arxiv.org/abs/1510.05622v2,math.AG,"['math.AG', 'cs.SC']",,,"['LAMA', 'CARAMEL']"
"Algebraic independence of sequences generated by (cyclotomic) harmonic
  sums",http://arxiv.org/abs/1510.03692v2,2015-10-13T14:25:11Z,2017-04-24T09:06:00Z,"  An expression in terms of (cyclotomic) harmonic sums can be simplified by the
quasi-shuffle algebra in terms of the so-called basis sums. By construction,
these sums are algebraically independent within the quasi-shuffle algebra. In
this article we show that the basis sums can be represented within a tower of
difference ring extensions where the constants remain unchanged. This property
enables one to embed this difference ring for the (cyclotomic) harmonic sums
into the ring of sequences. This construction implies that the sequences
produced by the basis sums are algebraically independent over the rational
sequences adjoined with the alternating sequence.
","['\nJakob Ablinger\n', '\nCarsten Schneider\n']","Updated the title in order to avoid confusion; inserted Lemma 5 to
  bring in more insight and supplemented Example 3 with further comments",,http://arxiv.org/abs/1510.03692v2,cs.SC,"['cs.SC', 'math-ph', 'math.MP', 'math.NT']",,,[]
Liaison Linkages,http://arxiv.org/abs/1510.01127v1,2015-10-05T12:31:56Z,2015-10-05T12:31:56Z,"  The complete classification of hexapods - also known as Stewart Gough
platforms - of mobility one is still open. To tackle this problem, we can
associate to each hexapod of mobility one an algebraic curve, called the
configuration curve. In this paper we establish an upper bound for the degree
of this curve, assuming the hexapod is general enough. Moreover, we provide a
construction of hexapods with curves of maximal degree, which is based on
liaison, a technique used in the theory of algebraic curves.
","['\nMatteo Gallet\n', '\nGeorg Nawratil\n', '\nJosef Schicho\n']","40 pages, 6 figures","Journal of Symbolic Computation, Volume 79, Part 1, March-April
  2017, Pages 65-98",http://dx.doi.org/10.1016/j.jsc.2016.08.006,cs.RO,"['cs.RO', 'cs.SC', 'math.AG']",10.1016/j.jsc.2016.08.006,,[]
"A Near-Optimal Subdivision Algorithm for Complex Root Isolation based on
  the Pellet Test and Newton Iteration",http://arxiv.org/abs/1509.06231v4,2015-09-21T14:13:19Z,2016-11-08T09:29:38Z,"  We describe a subdivision algorithm for isolating the complex roots of a
polynomial $F\in\mathbb{C}[x]$. Given an oracle that provides approximations of
each of the coefficients of $F$ to any absolute error bound and given an
arbitrary square $\mathcal{B}$ in the complex plane containing only simple
roots of $F$, our algorithm returns disjoint isolating disks for the roots of
$F$ in $\mathcal{B}$. Our complexity analysis bounds the absolute error to
which the coefficients of $F$ have to be provided, the total number of
iterations, and the overall bit complexity. It further shows that the
complexity of our algorithm is controlled by the geometry of the roots in a
near neighborhood of the input square $\mathcal{B}$, namely, the number of
roots, their absolute values and pairwise distances. The number of subdivision
steps is near-optimal. For the \emph{benchmark problem}, namely, to isolate all
the roots of a polynomial of degree $n$ with integer coefficients of bit size
less than $\tau$, our algorithm needs $\tilde O(n^3+n^2\tau)$ bit operations,
which is comparable to the record bound of Pan (2002). It is the first time
that such a bound has been achieved using subdivision methods, and independent
of divide-and-conquer techniques such as Sch\""onhage's splitting circle
technique. Our algorithm uses the quadtree construction of Weyl (1924) with two
key ingredients: using Pellet's Theorem (1881) combined with Graeffe iteration,
we derive a ""soft-test"" to count the number of roots in a disk. Using
Schr\""oder's modified Newton operator combined with bisection, in a form
inspired by the quadratic interval method from Abbot (2006), we achieve
quadratic convergence towards root clusters. Relative to the divide-conquer
algorithms, our algorithm is quite simple with the potential of being
practical. This paper is self-contained: we provide pseudo-code for all
subroutines used by our algorithm.
","['\nRuben Becker\n', '\nMichael Sagraloff\n', '\nVikram Sharma\n', '\nChee Yap\n']",,,http://arxiv.org/abs/1509.06231v4,cs.NA,"['cs.NA', 'cs.SC', 'math.NA']",,,[]
"Calculating Three Loop Ladder and V-Topologies for Massive Operator
  Matrix Elements by Computer Algebra",http://arxiv.org/abs/1509.08324v1,2015-09-28T14:05:33Z,2015-09-28T14:05:33Z,"  Three loop ladder and $V$-topology diagrams contributing to the massive
operator matrix element $A_{Qg}$ are calculated. The corresponding objects can
all be expressed in terms of nested sums and recurrences depending on the
Mellin variable $N$ and the dimensional parameter $\varepsilon$. Given these
representations, the desired Laurent series expansions in $\varepsilon$ can be
obtained with the help of our computer algebra toolbox. Here we rely on
generalized hypergeometric functions and Mellin-Barnes representations, on
difference ring algorithms for symbolic summation, on an optimized version of
the multivariate Almkvist-Zeilberger algorithm for symbolic integration, and on
new methods to calculate Laurent series solutions of coupled systems of
differential equations. The solutions can be computed for general coefficient
matrices directly for any basis also performing the expansion in the
dimensional parameter in case it is expressible in terms of indefinite nested
product-sum expressions. This structural result is based on new results of our
difference ring theory. In the cases discussed we deal with iterative sum- and
integral-solutions over general alphabets. The final results are expressed in
terms of special sums, forming quasi-shuffle algebras, such as nested harmonic
sums, generalized harmonic sums, and nested binomially weighted (cyclotomic)
sums. Analytic continuations to complex values of $N$ are possible through the
recursion relations obeyed by these quantities and their analytic asymptotic
expansions. The latter lead to a host of new constants beyond the multiple zeta
values, the infinite generalized harmonic and cyclotomic sums in the case of
$V$-topologies.
","['\nJ. Ablinger\n', '\nA. Behring\n', '\nJ. Blümlein\n', '\nA. De Freitas\n', '\nA. von Manteuffel\n', '\nC. Schneider\n']","110 pages Latex, 4 Figures",,http://dx.doi.org/10.1016/j.cpc.2016.01.002,hep-ph,"['hep-ph', 'cs.SC', 'hep-th', 'math-ph', 'math.MP']",10.1016/j.cpc.2016.01.002,,[]
Computing isolated orbifolds in weighted flag varieties,http://arxiv.org/abs/1509.03722v2,2015-09-12T09:03:48Z,2016-02-25T09:12:56Z,"  Given a weighted flag variety $w\Sigma(\mu,u)$ corresponding to chosen fixed
parameters $\mu$ and $u$, we present an algorithm to compute lists of all
possible projectively Gorenstein $n$-folds, having canonical weight $k$ and
isolated orbifold points, appearing as weighted complete intersections in
$w\Sigma(\mu,u) $ or some projective cone(s) over $w\Sigma(\mu,u)$. We apply
our algorithm to compute lists of interesting classes of polarized 3-folds with
isolated orbifold points in the codimension 8 weighted $G_2$ variety. We also
show the existence of some families of log-terminal $\mathbb Q$-Fano 3-folds in
codimension 8 by explicitly constructing them as quasilinear sections of a
weighted $G_2$-variety.
",['\nMuhammad Imran Qureshi\n'],"Minor Changes, few one line explainations added, To Appear in Journal
  of Symbolic Computation, 22 pages, 1 figure",,http://arxiv.org/abs/1509.03722v2,math.AG,"['math.AG', 'cs.SC']",,,[]
Computing the Chow variety of quadratic space curves,http://arxiv.org/abs/1508.07219v2,2015-08-28T14:13:17Z,2015-11-27T17:07:14Z,"  Quadrics in the Grassmannian of lines in 3-space form a 19-dimensional
projective space. We study the subvariety of coisotropic hypersurfaces.
Following Gel'fand, Kapranov and Zelevinsky, it decomposes into Chow forms of
plane conics, Chow forms of pairs of lines, and Hurwitz forms of quadric
surfaces. We compute the ideals of these loci.
","['\nPeter Bürgisser\n', '\nKathlén Kohn\n', '\nPierre Lairez\n', '\nBernd Sturmfels\n']",,"Mathematical Aspects of Computer and Information Sciences (MACIS
  2015), Lecture Notes in Computer Science 9582, pp. 130-136",http://dx.doi.org/10.1007/978-3-319-32859-1_10,math.AG,"['math.AG', 'cs.SC', '14C05, 14-04']",10.1007/978-3-319-32859-1_10,,[]
"Computing explicit isomorphisms with full matrix algebras over
  $\mathbb{F}_q(x)$",http://arxiv.org/abs/1508.07755v3,2015-08-31T10:22:07Z,2017-01-02T09:36:12Z,"  We propose a polynomial time $f$-algorithm (a deterministic algorithm which
uses an oracle for factoring univariate polynomials over $\mathbb{F}_q$) for
computing an isomorphism (if there is any) of a finite dimensional
$\mathbb{F}_q(x)$-algebra $A$ given by structure constants with the algebra of
$n$ by $n$ matrices with entries from $\mathbb{F}_q(x)$. The method is based on
computing a finite $\mathbb{F}_q$-subalgebra of $A$ which is the intersection
of a maximal $\mathbb{F}_q[x]$-order and a maximal $R$-order, where $R$ is the
subring of $\mathbb{F}_q(x)$ consisting of fractions of polynomials with
denominator having degree not less than that of the numerator.
","['\nGábor Ivanyos\n', '\nPéter Kutas\n', '\nLajos Rónyai\n']","15 pages, contains updated grant numbers",,http://arxiv.org/abs/1508.07755v3,math.RA,"['math.RA', 'cs.SC', 'math.NT']",,,[]
Lagrangian Constraints and Differential Thomas Decomposition,http://arxiv.org/abs/1509.01464v2,2015-09-04T14:28:51Z,2015-09-13T08:43:23Z,"  In this paper we show how to compute algorithmically the full set of
algebraically independent constraints for singular mechanical and
field-theoretical models with polynomial Lagrangians. If a model under
consideration is not singular as a whole but has domains of dynamical (field)
variables where its Lagrangian becomes singular, then our approach allows to
detect such domains and compute the relevant constraints. In doing so, we
assume that the Lagrangian of a model is a differential polynomial and apply
the differential Thomas decomposition algorithm to the Euler-Lagrange
equations.
","['\nVladimir P. Gerdt\n', '\nDaniel Robertz\n']","21 pages, to be published in Advances in Applied Mathematics,
  Elsevier",,http://arxiv.org/abs/1509.01464v2,math.DS,"['math.DS', 'cs.SC', 'math-ph', 'math.MP', '12H05, 68W30, 70S05']",,,[]
Exact algorithms for linear matrix inequalities,http://arxiv.org/abs/1508.03715v3,2015-08-15T09:10:18Z,2016-09-19T09:39:57Z,"  Let $A(x)=A\_0+x\_1A\_1+...+x\_nA\_n$ be a linear matrix, or pencil,
generated by given symmetric matrices $A\_0,A\_1,...,A\_n$ of size $m$ with
rational entries. The set of real vectors x such that the pencil is positive
semidefinite is a convex semi-algebraic set called spectrahedron, described by
a linear matrix inequality (LMI). We design an exact algorithm that, up to
genericity assumptions on the input matrices, computes an exact algebraic
representation of at least one point in the spectrahedron, or decides that it
is empty. The algorithm does not assume the existence of an interior point, and
the computed point minimizes the rank of the pencil on the spectrahedron. The
degree $d$ of the algebraic representation of the point coincides
experimentally with the algebraic degree of a generic semidefinite program
associated to the pencil. We provide explicit bounds for the complexity of our
algorithm, proving that the maximum number of arithmetic operations that are
performed is essentially quadratic in a multilinear B\'ezout bound of $d$. When
$m$ (resp. $n$) is fixed, such a bound, and hence the complexity, is polynomial
in $n$ (resp. $m$). We conclude by providing results of experiments showing
practical improvements with respect to state-of-the-art computer algebra
algorithms.
","['\nDidier Henrion\nCTU, LAAS-MAC\n', '\nSimone Naldi\nLAAS-MAC\n', '\nMohab Safey El Din\nLIP6, PolSys\n']","SIAM Journal on Optimization, Society for Industrial and Applied
  Mathematics, 2016",,http://arxiv.org/abs/1508.03715v3,math.OC,"['math.OC', 'cs.SC']",,,"['CTU, LAAS-MAC', 'LAAS-MAC', 'LIP6, PolSys']"
Duality of Multiple Root Loci,http://arxiv.org/abs/1508.00202v2,2015-08-02T06:31:07Z,2015-10-23T12:50:15Z,"  The multiple root loci among univariate polynomials of degree $n$ are indexed
by partitions of $n$. We study these loci and their conormal varieties. The
projectively dual varieties are joins of such loci where the partitions are
hooks. Our emphasis lies on equations and parametrizations that are useful for
Euclidean distance optimization. We compute the ED degrees for hooks. Among the
dual hypersurfaces are those that demarcate the set of binary forms whose real
rank equals the generic complex rank.
","['\nHwangrae Lee\n', '\nBernd Sturmfels\n']",23 pages,,http://arxiv.org/abs/1508.00202v2,math.AG,"['math.AG', 'cs.SC', 'math.OC']",,,[]
Formulas for Continued Fractions. An Automated Guess and Prove Approach,http://arxiv.org/abs/1507.04203v1,2015-07-15T13:12:23Z,2015-07-15T13:12:23Z,"  We describe a simple method that produces automatically closed forms for the
coefficients of continued fractions expansions of a large number of special
functions. The function is specified by a non-linear differential equation and
initial conditions. This is used to generate the first few coefficients and
from there a conjectured formula. This formula is then proved automatically
thanks to a linear recurrence satisfied by some remainder terms. Extensive
experiments show that this simple approach and its straightforward
generalization to difference and $q$-difference equations capture a large part
of the formulas in the literature on continued fractions.
","['\nSébastien Maulat\n', '\nBruno Salvy\n']",Maple worksheet attached,"ISSAC 15: Proceedings of the 2015 ACM International Symposium on
  Symbolic and Algebraic Computation, pp. 275--282",http://dx.doi.org/10.1145/2755996.2756660,cs.SC,"['cs.SC', 'I.1.2']",10.1145/2755996.2756660,,[]
Proof of the Wilf-Zeilberger Conjecture for Mixed Hypergeometric Terms,http://arxiv.org/abs/1507.04840v2,2015-07-17T05:50:02Z,2018-06-08T08:43:04Z,"  In 1992, Wilf and Zeilberger conjectured that a hypergeometric term in
several discrete and continuous variables is holonomic if and only if it is
proper. Strictly speaking the conjecture does not hold, but it is true when
reformulated properly: Payne proved a piecewise interpretation in 1997, and
independently, Abramov and Petkovsek in 2002 proved a conjugate interpretation.
Both results address the pure discrete case of the conjecture. In this paper we
extend their work to hypergeometric terms in several discrete and continuous
variables and prove the conjugate interpretation of the Wilf-Zeilberger
conjecture in this mixed setting.
","['\nShaoshi Chen\n', '\nChristoph Koutschan\n']","20 pages, 1 figure","Journal of Symbolic Computation, vol. 93, pages 133-147, 2019",http://dx.doi.org/10.1016/j.jsc.2018.06.003,math.CO,"['math.CO', 'cs.SC', '33C70, 33F10']",10.1016/j.jsc.2018.06.003,,[]
Resultants and subresultants of p-adic polynomials,http://arxiv.org/abs/1507.06502v1,2015-07-23T14:07:54Z,2015-07-23T14:07:54Z,"  We address the problem of the stability of the computations of resultants and
subresultants of polynomials defined over complete discrete valuation rings
(e.g. Zp or k[[t]] where k is a field). We prove that Euclide-like algorithms
are highly unstable on average and we explain, in many cases, how one can
stabilize them without sacrifying the complexity. On the way, we completely
determine the distribution of the valuation of the principal subresultants of
two random monic p-adic polynomials having the same degree.
",['\nXavier Caruso\nIRMAR\n'],,,http://arxiv.org/abs/1507.06502v1,math.NT,"['math.NT', 'cs.SC']",,,['IRMAR']
An algorithm for computing Grobner basis and the complexity evaluation,http://arxiv.org/abs/1507.03217v1,2015-07-12T11:12:42Z,2015-07-12T11:12:42Z,"  In this paper, we suggest a new efficient algorithm in order to compute
S-polynomial reduction rapidly in the known algorithm for computing Grobner
bases, and compare the complexity with others.
","['\nYong-Jin Kim\n', '\nHyon-Song Paek\n', '\nNam-Chol Kim\n', '\nChong-Il Byon\n']",,,http://arxiv.org/abs/1507.03217v1,cs.SC,['cs.SC'],,,[]
"Middle-Solving Grobner bases algorithm for cryptanalysis over finite
  fields",http://arxiv.org/abs/1507.03480v1,2015-07-13T14:46:05Z,2015-07-13T14:46:05Z,"  Algebraic cryptanalysis usually requires to recover the secret key by solving
polynomial equations. Grobner bases algorithm is a well-known method to solve
this problem. However, a serious drawback exists in the Grobner bases based
algebraic attacks, namely, any information won't be got if we couldn't work out
the Grobner bases of the polynomial equations system. In this paper, firstly, a
generalized model of Grobner basis algorithms is presented, which provides us a
platform to analyze and solve common problems of the algorithms. Secondly, we
give and prove the degree bound of the polynomials appeared during the
computation of Grobner basis after field polynomials is added. Finally, by
detecting the temporary basis during the computation of Grobner bases and then
extracting the univariate polynomials contained unique solution in the
temporary basis, a heuristic strategy named Middle-Solving is presented to
solve these polynomials at each iteration of the algorithm. Farther, two
specific application mode of Middle-Solving strategy for the incremental and
non-incremental Grobner bases algorithms are presented respectively. By using
the Middle-Solving strategy, even though we couldn't work out the final Grobner
bases, some information of the variables still leak during the computational
process.
","['\nWansu Bao\n', '\nHeliang Huang\n']",arXiv admin note: text overlap with arXiv:1310.2332,,http://arxiv.org/abs/1507.03480v1,cs.CR,"['cs.CR', 'cs.SC']",,,[]
"Interactive certificate for the verification of Wiedemann's Krylov
  sequence: application to the certification of the determinant, the minimal
  and the characteristic polynomials of sparse matrices",http://arxiv.org/abs/1507.01083v1,2015-07-04T09:02:37Z,2015-07-04T09:02:37Z,"  Certificates to a linear algebra computation are additional data structures
for each output, which can be used by a-possibly randomized- verification
algorithm that proves the correctness of each output. Wiede-mann's algorithm
projects the Krylov sequence obtained by repeatedly multiplying a vector by a
matrix to obtain a linearly recurrent sequence. The minimal polynomial of this
sequence divides the minimal polynomial of the matrix. For instance, if the
$n\times n$ input matrix is sparse with n 1+o(1) non-zero entries, the
computation of the sequence is quadratic in the dimension of the matrix while
the computation of the minimal polynomial is n 1+o(1), once that projected
Krylov sequence is obtained. In this paper we give algorithms that compute
certificates for the Krylov sequence of sparse or structured $n\times n$
matrices over an abstract field, whose Monte Carlo verification complexity can
be made essentially linear. As an application this gives certificates for the
determinant, the minimal and characteristic polynomials of sparse or structured
matrices at the same cost.
","['\nJean-Guillaume Dumas\nNCSU\n', '\nErich Kaltofen\nNCSU\n', '\nEmmanuel Thomé\nCARAMEL\n']",,,http://arxiv.org/abs/1507.01083v1,cs.SC,"['cs.SC', 'cs.CC', 'cs.CR']",,,"['NCSU', 'NCSU', 'CARAMEL']"
Discovering and Proving Infinite Binomial Sums Identities,http://arxiv.org/abs/1507.01703v3,2015-07-07T08:32:20Z,2015-10-29T10:18:41Z,"  We consider binomial and inverse binomial sums at infinity and rewrite them
in terms of a small set of constants, such as powers of $\pi$ or $\log(2)$. In
order to perform these simplifications, we view the series as specializations
of generating series. For these generating series, we derive integral
representations in terms of root-valued iterated integrals. Using
substitutions, we express the interated integrals as cyclotomic harmonic
polylogarithms. Finally, by applying known relations among the cyclotomic
harmonic polylogarithms, we derive expressions in terms of several constants.
",['\nJakob Ablinger\n'],25 pages,,http://arxiv.org/abs/1507.01703v3,math.NT,"['math.NT', 'cs.SC', 'math.CO', '05A10, 68W30, 11M32, 33F05']",,,[]
(Pure) transcendence bases in $φ$-deformed shuffle bialgebras,http://arxiv.org/abs/1507.01089v2,2015-07-04T09:25:07Z,2018-06-20T08:31:50Z,"  Computations with integro-differential operators are often carried out in an
associative algebra with unit, and they are essentially non-commutative
computations. By adjoining a cocommutative co-product, one can have those
operators perform act on a bialgebra isomorphic to an enveloping algebra. That
gives an adequate framework for a computer-algebra implementation via monoidal
factorization, (pure) transcendence bases and Poincar\'e--Birkhoff--Witt bases.
In this paper, we systematically study these deformations, obtaining necessary
and sufficient conditions for the operators to exist, and we give the most
general cocommutative deformations of the shuffle co-product and an effective
construction of pairs of bases in duality. The paper ends by the combinatorial
setting of local systems of coordinates on the group of group-like series.
","['\nVan Chiên Bui\nLIPN\n', '\nGérard H. E. Duchamp\nLIPN\n', '\nQuoc Hoan Ngô\nLIPN\n', '\nVincel Hoang Ngoc Minh\nLIPN\n', '\nChristophe Tollu\nLIPN\n']","The present work is part of a series of papers devoted to the study
  of the renormalization of divergent polyzetas (at positive and at
  non-positive indices) via the factorization of the non commutative generating
  series of polylogarithms and of harmonic sums and via the effective
  construction of pairs of dual bases in duality in $\phi$ deformed shuffle
  algebras. It is a sequel to [3] and its content was presented in several
  seminars and meetings, including the 74th S\'eminaire Lotharingien de
  Combinatoire","Seminaire Lotharingien de Combinatoire, Universit{\'e} Louis
  Pasteur, 2015, 74, pp.1-31",http://arxiv.org/abs/1507.01089v2,cs.SC,"['cs.SC', 'math-ph', 'math.CO', 'math.GR', 'math.MP']",,,"['LIPN', 'LIPN', 'LIPN', 'LIPN', 'LIPN']"
"On the Connection Between Ritt Characteristic Sets and
  Buchberger-Gröbner Bases",http://arxiv.org/abs/1506.08994v1,2015-06-30T08:43:44Z,2015-06-30T08:43:44Z,"  For any polynomial ideal $I$, let the minimal triangular set contained in the
reduced Buchberger-Gr\""obner basis of $I$ with respect to the purely
lexicographical term order be called the W-characteristic set of $I$. In this
paper, we establish a strong connection between Ritt's characteristic sets and
Buchberger's Gr\""obner bases of polynomial ideals by showing that the
W-characteristic set $C$ of $I$ is a Ritt characteristic set of $I$ whenever
$C$ is an ascending set, and a Ritt characteristic set of $I$ can always be
computed from $C$ with simple pseudo-division when $C$ is regular. We also
prove that under certain variable ordering, either the W-characteristic set of
$I$ is normal, or irregularity occurs for the $j$th, but not the $(j+1)$th,
elimination ideal of $I$ for some $j$. In the latter case, we provide explicit
pseudo-divisibility relations, which lead to nontrivial factorizations of
certain polynomials in the Buchberger-Gr\""obner basis and thus reveal the
structure of such polynomials. The pseudo-divisibility relations may be used to
devise an algorithm to decompose arbitrary polynomial sets into normal
triangular sets based on Buchberger-Gr\""obner bases computation.
",['\nDongming Wang\n'],15 pages,,http://arxiv.org/abs/1506.08994v1,math.AC,"['math.AC', 'cs.SC', '13P10']",,,[]
Symbolic Derivation of Mean-Field PDEs from Lattice-Based Models,http://arxiv.org/abs/1506.08527v3,2015-06-29T07:50:21Z,2016-03-21T12:54:00Z,"  Transportation processes, which play a prominent role in the life and social
sciences, are typically described by discrete models on lattices. For studying
their dynamics a continuous formulation of the problem via partial differential
equations (PDE) is employed. In this paper we propose a symbolic computation
approach to derive mean-field PDEs from a lattice-based model. We start with
the microscopic equations, which state the probability to find a particle at a
given lattice site. Then the PDEs are formally derived by Taylor expansions of
the probability densities and by passing to an appropriate limit as the time
steps and the distances between lattice sites tend to zero. We present an
implementation in a computer algebra system that performs this transition for a
general class of models. In order to rewrite the mean-field PDEs in a
conservative formulation, we adapt and implement symbolic integration methods
that can handle unspecified functions in several variables. To illustrate our
approach, we consider an application in crowd motion analysis where the
dynamics of bidirectional flows are studied. However, the presented approach
can be applied to various transportation processes of multiple species with
variable size in any dimension, for example, to confirm several proposed
mean-field models for cell motility.
","['\nChristoph Koutschan\n', '\nHelene Ranetbauer\n', '\nGeorg Regensburger\n', '\nMarie-Therese Wolfram\n']",,"Proceedings of the 17th International Symposium on Symbolic and
  Numeric Algorithms for Scientific Computing (SYNASC), pp. 27-33, 2015. ISBN
  978-1-5090-0461-4",http://dx.doi.org/10.1109/SYNASC.2015.14,cs.SC,"['cs.SC', 'cs.CE', 'math.AP', 'nlin.AO']",10.1109/SYNASC.2015.14,,[]
A Fast Algorithm for Computing the p-Curvature,http://arxiv.org/abs/1506.05645v1,2015-06-18T12:14:55Z,2015-06-18T12:14:55Z,"  We design an algorithm for computing the $p$-curvature of a differential
system in positive characteristic $p$. For a system of dimension $r$ with
coefficients of degree at most $d$, its complexity is $\softO (p d r^\omega)$
operations in the ground field (where $\omega$ denotes the exponent of matrix
multiplication), whereas the size of the output is about $p d r^2$. Our
algorithm is then quasi-optimal assuming that matrix multiplication is
(\emph{i.e.} $\omega = 2$). The main theoretical input we are using is the
existence of a well-suited ring of series with divided powers for which an
analogue of the Cauchy--Lipschitz Theorem holds.
","['\nAlin Bostan\nSPECFUN\n', '\nXavier Caruso\nIRMAR\n', '\nÉric Schost\n']","ISSAC 2015, Jul 2015, Bath, United Kingdom",,http://dx.doi.org/10.1145/2755996.2756674,cs.SC,['cs.SC'],10.1145/2755996.2756674,,"['SPECFUN', 'IRMAR']"
Parallel sparse interpolation using small primes,http://arxiv.org/abs/1506.04215v1,2015-06-13T03:41:43Z,2015-06-13T03:41:43Z,"  To interpolate a supersparse polynomial with integer coefficients, two
alternative approaches are the Prony-based ""big prime"" technique, which acts
over a single large finite field, or the more recently-proposed ""small primes""
technique, which reduces the unknown sparse polynomial to many low-degree dense
polynomials. While the latter technique has not yet reached the same
theoretical efficiency as Prony-based methods, it has an obvious potential for
parallelization. We present a heuristic ""small primes"" interpolation algorithm
and report on a low-level C implementation using FLINT and MPI.
","['\nMohamed Khochtali\n', '\nDaniel S. Roche\n', '\nXisen Tian\n']",Accepted to PASCO 2015,,http://arxiv.org/abs/1506.04215v1,cs.SC,"['cs.SC', 'cs.DC', '68W30 (Primary) 68W10, 68Q10, 12Y05 (Secondary)', 'F.2.1; G.3; G.4; I.1.2']",,,[]
p-Adic Stability In Linear Algebra,http://arxiv.org/abs/1506.05644v1,2015-06-18T12:13:41Z,2015-06-18T12:13:41Z,"  Using the differential precision methods developed previously by the same
authors, we study the p-adic stability of standard operations on matrices and
vector spaces. We demonstrate that lattice-based methods surpass naive methods
in many applications, such as matrix multiplication and sums and intersections
of subspaces. We also analyze determinants , characteristic polynomials and LU
factorization using these differential methods. We supplement our observations
with numerical experiments.
","['\nXavier Caruso\nIRMAR\n', '\nDavid Roe\nIRMAR\n', '\nTristan Vaccon\nIRMAR\n']","ISSAC 2015, Jul 2015, Bath, United Kingdom. 2015",,http://dx.doi.org/10.1145/2755996.2756655,math.NT,"['math.NT', 'cs.SC']",10.1145/2755996.2756655,,"['IRMAR', 'IRMAR', 'IRMAR']"
Real root finding for low rank linear matrices,http://arxiv.org/abs/1506.05897v3,2015-06-19T07:59:36Z,2019-07-18T05:43:36Z,"  We consider $m \times s$ matrices (with $m\geq s$) in a real affine subspace
of dimension $n$. The problem of finding elements of low rank in such spaces
finds many applications in information and systems theory, where low rank is
synonymous of structure and parsimony. We design computer algebra algorithms,
based on advanced methods for polynomial system solving, to solve this problem
efficiently and exactly: the input are the rational coefficients of the
matrices spanning the affine subspace as well as the expected maximum rank, and
the output is a rational parametrization encoding a finite set of points that
intersects each connected component of the low rank real algebraic set. The
complexity of our algorithm is studied thoroughly. It is polynomial in
$\binom{n+m(s-r)}{n}$. It improves on the state-of-the-art in computer algebra
and effective real algebraic geometry. Moreover, computer experiments show the
practical efficiency of our approach.
","['\nDidier Henrion\nLAAS-MAC\n', '\nSimone Naldi\nXLIM-MATHIS\n', '\nMohab Safey El Din\nPolSys, LIP6\n']",Final published version in Appl. Algebr. Eng. Comm,,http://dx.doi.org/10.1007/s00200-019-00396-w,cs.SC,"['cs.SC', 'math.AG', '13-XX, 14Q20, 12Y05, 68W30']",10.1007/s00200-019-00396-w,,"['LAAS-MAC', 'XLIM-MATHIS', 'PolSys, LIP6']"
An Elimination Method to Solve Interval Polynomial Systems,http://arxiv.org/abs/1506.02423v1,2015-06-08T09:51:35Z,2015-06-08T09:51:35Z,"  There are several efficient methods to solve linear interval polynomial
systems in the context of interval computations, however, the general case of
interval polynomial systems is not yet covered as well. In this paper we
introduce a new elimination method to solve and analyse interval polynomial
systems, in general case. This method is based on computational algebraic
geometry concepts such as polynomial ideals and Groebner basis computation.
Specially, we use the comprehensive Groebner system concept to keep the
dependencies between interval coefficients. At the end of paper, we will state
some applications of our method to evaluate its performance.
","['\nSajjad Rahmany\n', '\nAbdolali Basiri\n', '\nBenyamin M. -Alizadeh\n']",,,http://arxiv.org/abs/1506.02423v1,cs.SC,['cs.SC'],,,[]
On the Skolem Problem for Continuous Linear Dynamical Systems,http://arxiv.org/abs/1506.00695v3,2015-06-01T22:30:59Z,2016-05-10T17:35:05Z,"  The Continuous Skolem Problem asks whether a real-valued function satisfying
a linear differential equation has a zero in a given interval of real numbers.
This is a fundamental reachability problem for continuous linear dynamical
systems, such as linear hybrid automata and continuous-time Markov chains.
Decidability of the problem is currently open---indeed decidability is open
even for the sub-problem in which a zero is sought in a bounded interval. In
this paper we show decidability of the bounded problem subject to Schanuel's
Conjecture, a unifying conjecture in transcendental number theory. We
furthermore analyse the unbounded problem in terms of the frequencies of the
differential equation, that is, the imaginary parts of the characteristic
roots. We show that the unbounded problem can be reduced to the bounded problem
if there is at most one rationally linearly independent frequency, or if there
are two rationally linearly independent frequencies and all characteristic
roots are simple. We complete the picture by showing that decidability of the
unbounded problem in the case of two (or more) rationally linearly independent
frequencies would entail a major new effectiveness result in Diophantine
approximation, namely computability of the Diophantine-approximation types of
all real algebraic numbers.
","['\nVentsislav Chonev\n', '\nJoel Ouaknine\n', '\nJames Worrell\n']",Full version of paper at ICALP'16,,http://arxiv.org/abs/1506.00695v3,cs.SY,"['cs.SY', 'cs.SC', 'F.1.1; F.2.1']",,,[]
"Solving Polynomial Systems in the Cloud with Polynomial Homotopy
  Continuation",http://arxiv.org/abs/1506.02618v1,2015-06-08T19:05:49Z,2015-06-08T19:05:49Z,"  Polynomial systems occur in many fields of science and engineering.
Polynomial homotopy continuation methods apply symbolic-numeric algorithms to
solve polynomial systems. We describe the design and implementation of our web
interface and reflect on the application of polynomial homotopy continuation
methods to solve polynomial systems in the cloud. Via the graph isomorphism
problem we organize and classify the polynomial systems we solved. The
classification with the canonical form of a graph identifies newly submitted
systems with systems that have already been solved.
","['\nNathan Bliss\n', '\nJeff Sommars\n', '\nJan Verschelde\n', '\nXiangcheng Yu\n']",Accepted for publication in the Proceedings of CASC 2015,,http://arxiv.org/abs/1506.02618v1,cs.MS,"['cs.MS', 'cs.NA', 'cs.SC', 'math.AG', 'math.NA']",,,[]
On the last fall degree of zero-dimensional Weil descent systems,http://arxiv.org/abs/1505.02532v2,2015-05-11T09:20:25Z,2015-06-18T03:02:02Z,"  In this article we will discuss a new, mostly theoretical, method for solving
(zero-dimensional) polynomial systems, which lies in between Gr\""obner basis
computations and the heuristic first fall degree assumption and is not based on
any heuristic. This method relies on the new concept of last fall degree.
  Let $k$ be a finite field of cardinality $q^n$ and let $k'$ be its subfield
of cardinality $q$. Let $\mathcal{F} \subset k[X_0,\ldots,X_{m-1}]$ be a finite
subset generating a zero-dimensional ideal. We give an upper bound of the last
fall degree of the Weil descent system of $\mathcal{F}$, which depends on $q$,
$m$, the last fall degree of $\mathcal{F}$, the degree of $\mathcal{F}$ and the
number of solutions of $\mathcal{F}$, but not on $n$. This shows that such Weil
descent systems can be solved efficiently if $n$ grows. In particular, we apply
these results for multi-HFE and essentially show that multi-HFE is insecure.
  Finally, we discuss that the degree of regularity (or last fall degree) of
Weil descent systems coming from summation polynomials to solve the elliptic
curve discrete logarithm problem might depend on $n$, since such systems
without field equations are not zero-dimensional.
","['\nMing-Deh A. Huang\n', '\nMichiel Kosters\n', '\nYun Yang\n', '\nSze Ling Yeo\n']","16 pages, changed definition of tau and revised Section 5",,http://arxiv.org/abs/1505.02532v2,math.AC,"['math.AC', 'cs.SC', '13P10, 13P15']",,,[]
"Pfaffian Systems of A-Hypergeometric Systems II --- Holonomic Gradient
  Method",http://arxiv.org/abs/1505.02947v1,2015-05-12T10:26:10Z,2015-05-12T10:26:10Z,"  We give two efficient methods to derive Pfaffian systems for A-hypergeometric
systems for the application to the holonomic gradient method for statistics. We
utilize the Hilbert driven Buchberger algorithm and Macaulay type matrices in
the two methods.
","['\nKatsuyoshi Ohara\n', '\nNobuki Takayama\n']",,,http://arxiv.org/abs/1505.02947v1,cs.SC,"['cs.SC', 'math.CA']",,,[]
"Symbolic-numeric methods for improving structural analysis of
  differential-algebraic equation systems",http://arxiv.org/abs/1505.03445v1,2015-05-13T16:11:42Z,2015-05-13T16:11:42Z,"  Systems of differential-algebraic equations (DAEs) are generated routinely by
simulation and modeling environments such as Modelica and MapleSim. Before a
simulation starts and a numerical solution method is applied, some kind of
structural analysis is performed to determine the structure and the index of a
DAE. Structural analysis methods serve as a necessary preprocessing stage, and
among them, Pantelides's algorithm is widely used.
  Recently Pryce's $\Sigma$-method is becoming increasingly popular, owing to
its straightforward approach and capability of analyzing high-order systems.
Both methods are equivalent in the sense that when one succeeds, producing a
nonsingular system Jacobian, the other also succeeds, and the two give the same
structural index.
  Although provably successful on fairly many problems of interest, the
structural analysis methods can fail on some simple, solvable DAEs and give
incorrect structural information including the index. In this report, we focus
on the $\Sigma$-method. We investigate its failures, and develop two
symbolic-numeric conversion methods for converting a DAE, on which the
$\Sigma$-method fails, to an equivalent problem on which this method succeeds.
Aimed at making structural analysis methods more reliable, our conversion
methods exploit structural information of a DAE, and require a symbolic tool
for their implementation.
","['\nGuangning Tan\n', '\nNed S. Nedialkov\n', '\nJohn D. Pryce\n']","technical report, 84 pages",,http://arxiv.org/abs/1505.03445v1,cs.SC,"['cs.SC', 'cs.NA']",,,[]
"Index reduction of differential algebraic equations by differential
  algebraic elimination",http://arxiv.org/abs/1504.04977v1,2015-04-20T09:02:05Z,2015-04-20T09:02:05Z,"  High index differential algebraic equations (DAEs) are ordinary differential
equations (ODEs) with constraints and arise frequently from many mathematical
models of physical phenomenons and engineering fields. In this paper, we
generalize the idea of differential elimination with Dixon resultant to
polynomially nonlinear DAEs. We propose a new algorithm for index reduction of
DAEs and establish the notion of differential algebraic elimination, which can
provide the differential algebraic resultant of the enlarged system of original
equations. To make use of structure of DAEs, variable pencil technique is given
to determine the termination of differentiation. Moreover, we also provide a
heuristics method for removing the extraneous factors from differential
algebraic resultant. The experimentation shows that the proposed algorithm
outperforms existing ones for many examples taken from the literature.
","['\nXiaolin Qin\n', '\nLu Yang\n', '\nYong Feng\n', '\nBernhard Bachmann\n', '\nPeter Fritzson\n']","19 pages, 1figure",,http://arxiv.org/abs/1504.04977v1,cs.SC,['cs.SC'],,,[]
Recent Advances in Real Geometric Reasoning,http://arxiv.org/abs/1504.06484v1,2015-04-24T12:23:32Z,2015-04-24T12:23:32Z,"  In the 1930s Tarski showed that real quantifier elimination was possible, and
in 1975 Collins gave a remotely practicable method, albeit with
doubly-exponential complexity, which was later shown to be inherent. We discuss
some of the recent major advances in Collins method: such as an alternative
approach based on passing via the complexes, and advances which come closer to
""solving the question asked"" rather than ""solving all problems to do with these
polynomials"".
","['\nJames H. Davenport\n', '\nMatthew England\n']",,"Automated Deduction in Geometry (LNCS 9201), pp. 37-52. Springer
  International, 2015",http://dx.doi.org/10.1007/978-3-319-21362-0_3,cs.SC,"['cs.SC', 'cs.CG', '68W30, 03C10', 'I.1.2; F.2.2; G.4']",10.1007/978-3-319-21362-0_3,,[]
Timed Orchestration for Component-based Systems,http://arxiv.org/abs/1504.05513v3,2015-04-21T17:14:37Z,2016-05-20T11:55:40Z,"  Individual machines in flexible production lines explicitly expose
capabilities at their interfaces by means of parametric skills. Given such a
set of configurable machines, a line integrator is faced with the problem of
finding and tuning parameters for each machine such that the overall production
line implements given safety and temporal requirements in an optimized and
robust fashion. We formalize this problem of configuring and orchestrating
flexible production lines as a parameter synthesis problem for systems of
parametric timed automata, where interactions are based on skills. Parameter
synthesis problems for interaction-level LTL properties are translated to
parameter synthesis problems for state-based safety properties. For safety
properties, synthesis problems are solved by checking satisfiability of
$\exists\forall$SMT constraints. For constraint generation, we provide a set of
computationally cheap over-approximations of the set of reachable states,
together with fence constructions as sufficient conditions for safety formulas.
We demonstrate the feasibility of our approach by solving typical machine
configuration problems as encountered in industrial automation.
","['\nChih-Hong Cheng\n', '\nLacramioara Astefanoaei\n', '\nHarald Ruess\n', '\nSouha Ben Rayana\n', '\nSaddek Bensalem\n']","Timestamp of the work, with evaluation added by creating MES
  orchestration examples. (v3): typo fix and bring back definition and citation
  in en^t as in v1",,http://arxiv.org/abs/1504.05513v3,cs.FL,"['cs.FL', 'cs.LO', 'cs.SC']",,,[]
An implementation of Sub-CAD in Maple,http://arxiv.org/abs/1503.06599v1,2015-03-23T11:21:37Z,2015-03-23T11:21:37Z,"  Cylindrical algebraic decomposition (CAD) is an important tool for the
investigation of semi-algebraic sets, with applications in algebraic geometry
and beyond. We have previously reported on an implementation of CAD in Maple
which offers the original projection and lifting algorithm of Collins along
with subsequent improvements.
  Here we report on new functionality: specifically the ability to build
cylindrical algebraic sub-decompositions (sub-CADs) where only certain cells
are returned. We have implemented algorithms to return cells of a prescribed
dimensions or higher (layered {\scad}s), and an algorithm to return only those
cells on which given polynomials are zero (variety {\scad}s). These offer
substantial savings in output size and computation time.
  The code described and an introductory Maple worksheet / pdf demonstrating
the full functionality of the package are freely available online at
http://opus.bath.ac.uk/43911/.
","['\nMatthew England\n', '\nDavid Wilson\n']",9 pages,,http://arxiv.org/abs/1503.06599v1,cs.SC,"['cs.SC', '68W30', 'I.1.2']",,,[]
One-Step Stochastic Processes Simulation Software Package,http://arxiv.org/abs/1503.07342v1,2015-03-25T11:34:51Z,2015-03-25T11:34:51Z,"  Background. It is assumed that the introduction of stochastic in mathematical
model makes it more adequate. But there is virtually no methods of coordinated
(depended on structure of the system) stochastic introduction into
deterministic models. Authors have improved the method of stochastic models
construction for the class of one-step processes and illustrated by models of
population dynamics. Population dynamics was chosen for study because its
deterministic models were sufficiently well explored that allows to compare the
results with already known ones.
  Purpose. To optimize the models creation as much as possible some routine
operations should be automated. In this case, the process of drawing up the
model equations can be algorithmized and implemented in the computer algebra
system. Furthermore, on the basis of these results a set of programs for
numerical experiment can be obtained.
  Method. The computer algebra system Axiom is used for analytical calculations
implementation. To perform the numerical experiment FORTRAN and Julia languages
are used. The method Runge--Kutta method for stochastic differential equations
is used as numerical method.
  Results. The program compex for creating stochastic one-step processes models
is constructed. Its application is illustrated by the predator-prey population
dynamic system.
  Conclusions. Computer algebra systems are very convenient for the purposes of
rapid prototyping in mathematical models design and analysis.
","['\nE. G. Eferina\n', '\nA. V. Korolkova\n', '\nM. N. Gevorkyan\n', '\nD. S. Kulyabov\n', '\nL. A. Sevastyanov\n']",in Russian; in English,"Bulletin of PFUR. Series Mathematics. Information Sciences.
  Physics (3) (2014) 46-59",http://arxiv.org/abs/1503.07342v1,cs.SC,['cs.SC'],,,[]
Polynomial complexity recognizing a tropical linear variety,http://arxiv.org/abs/1503.06126v1,2015-03-20T15:58:25Z,2015-03-20T15:58:25Z,"  A polynomial complexity algorithm is designed which tests whether a point
belongs to a given tropical linear variety.
",['\nDima Grigoriev\n'],,"Lect. Notes Comput. Sci., 2015, vol. 9301, p. 152-157",http://arxiv.org/abs/1503.06126v1,cs.SC,"['cs.SC', 'math.AG', '15T05', 'I.1.2']",,,[]
On the Computation of the Galois Group of Linear Difference Equations,http://arxiv.org/abs/1503.02239v1,2015-03-08T02:20:14Z,2015-03-08T02:20:14Z,"  We present an algorithm that determines the Galois group of linear difference
equations with rational function coefficients.
",['\nRuyong Feng\n'],23 pages,,http://arxiv.org/abs/1503.02239v1,cs.SC,"['cs.SC', '39A06, 68W30', 'I.1.2']",,,[]
Groebner basis in Boolean rings is not polynomial-space,http://arxiv.org/abs/1502.07220v2,2015-02-25T16:09:01Z,2015-02-26T20:38:32Z,"  We give an example where the number of elements of a Groebner basis in a
Boolean ring is not polynomially bounded in terms of the bitsize and degrees of
the input.
",['\nMark van Hoeij\n'],3 pages,,http://arxiv.org/abs/1502.07220v2,cs.SC,['cs.SC'],,,[]
Factorization of Motion Polynomials,http://arxiv.org/abs/1502.07600v1,2015-02-26T15:42:46Z,2015-02-26T15:42:46Z,"  In this paper, we consider the existence of a factorization of a monic,
bounded motion polynomial. We prove existence of factorizations, possibly after
multiplication with a real polynomial and provide algorithms for computing
polynomial factor and factorizations. The first algorithm is conceptually
simpler but may require a high degree of the polynomial factor. The second
algorithm gives an optimal degree.
","['\nZijia Li\n', '\nJosef Schicho\n', '\nHans-Peter Schröcker\n']",,,http://arxiv.org/abs/1502.07600v1,cs.SC,"['cs.SC', 'cs.RO']",,,[]
Tropical differential equations,http://arxiv.org/abs/1502.08010v1,2015-02-27T19:28:44Z,2015-02-27T19:28:44Z,"  Tropical differential equations are introduced and an algorithm is designed
which tests solvability of a system of tropical linear differential equations
within the complexity polynomial in the size of the system and in its
coefficients. Moreover, we show that there exists a minimal solution, and the
algorithm constructs it (in case of solvability). This extends a similar
complexity bound established for tropical linear systems. In case of tropical
linear differential systems in one variable a polynomial complexity algorithm
for testing its solvability is designed.
  We prove also that the problem of solvability of a system of tropical
non-linear differential equations in one variable is $NP$-hard, and this
problem for arbitrary number of variables belongs to $NP$. Similar to tropical
algebraic equations, a tropical differential equation expresses the (necessary)
condition on the dominant term in the issue of solvability of a differential
equation in power series.
",['\nDima Grigoriev\n'],,"Adv. Appl. Math., 2017, vol. 82, p. 120-128",http://arxiv.org/abs/1502.08010v1,cs.SC,"['cs.SC', 'math.AG', '14T05', 'I.1.2']",,,[]
"Extractions: Computable and Visible Analogues of Localizations for
  Polynomial Ideals",http://arxiv.org/abs/1502.03967v1,2015-02-13T12:52:45Z,2015-02-13T12:52:45Z,"  When studying local properties of a polynomial ideal, one usually needs a
theoretic technique called localization. For most cases, in spite of its
importance, the computation in a localized ring cannot be algorithmically
preformed. On the other hand, the standard basis method is very effective for
the computation in a special kind of localized rings, but for a general
semigroup order the geometry of the localization of a positive-dimensional
ideal is difficult to interpret.
  In this paper, we introduce a new ideal operation called extraction. For an
ideal $I$ in a polynomial ring $K[x_1,\ldots,x_n]$ over a field $K$, we use
another ideal $J$ to control the primary components of $I$ and the result
$\beta(I,J)$ is called the extraction of $I$ by $J$. It is still a polynomial
ideal and has a concrete geometric meaning in $\bar{K}^n$, i.e., we keep the
branches of $\textbf{V}(I) \subset \bar{K}^n$ that intersect with
$\textbf{V}(J) \subset \bar{K}^n$ and delete others, where $\bar{K}$ is the
algebraic closure of $K$. This is what we mean by visible. On the other hand,
we can use the standard basis method to compute a localized ideal corresponding
to $\beta(I,J)$ without a complete primary decomposition, and can do further
computation in the localized ring such as determining the membership problem of
$\beta(I,J)$. Moreover, we prove that extractions are as powerful as
localizations in the sense that for any multiplicatively closed subset $S$ of
$K[x_1,\ldots,x_n]$ and any polynomial ideal $I$, there always exists a
polynomial ideal $J$ such that $\beta(I,J)=(S^{-1}I)^c$.
",['\nYe Liang\n'],,,http://arxiv.org/abs/1502.03967v1,cs.SC,"['cs.SC', 'math.AC']",,,[]
Automatic differentiation in machine learning: a survey,http://arxiv.org/abs/1502.05767v4,2015-02-20T04:20:47Z,2018-02-05T15:57:57Z,"  Derivatives, mostly in the form of gradients and Hessians, are ubiquitous in
machine learning. Automatic differentiation (AD), also called algorithmic
differentiation or simply ""autodiff"", is a family of techniques similar to but
more general than backpropagation for efficiently and accurately evaluating
derivatives of numeric functions expressed as computer programs. AD is a small
but established field with applications in areas including computational fluid
dynamics, atmospheric sciences, and engineering design optimization. Until very
recently, the fields of machine learning and AD have largely been unaware of
each other and, in some cases, have independently discovered each other's
results. Despite its relevance, general-purpose AD has been missing from the
machine learning toolbox, a situation slowly changing with its ongoing adoption
under the names ""dynamic computational graphs"" and ""differentiable
programming"". We survey the intersection of AD and machine learning, cover
applications where AD has direct relevance, and address the main implementation
techniques. By precisely defining the main differentiation techniques and their
interrelationships, we aim to bring clarity to the usage of the terms
""autodiff"", ""automatic differentiation"", and ""symbolic differentiation"" as
these are encountered more and more in machine learning settings.
","['\nAtilim Gunes Baydin\n', '\nBarak A. Pearlmutter\n', '\nAlexey Andreyevich Radul\n', '\nJeffrey Mark Siskind\n']","43 pages, 5 figures","Atilim Gunes Baydin, Barak A. Pearlmutter, Alexey Andreyevich
  Radul, Jeffrey Mark Siskind. Automatic differentiation in machine learning: a
  survey. The Journal of Machine Learning Research, 18(153):1--43, 2018",http://arxiv.org/abs/1502.05767v4,cs.SC,"['cs.SC', 'cs.LG', 'stat.ML', '68W30, 65D25, 68T05', 'G.1.4; I.2.6']",,,[]
Planar Linkages Following a Prescribed Motion,http://arxiv.org/abs/1502.05623v2,2015-02-19T16:52:41Z,2016-04-05T12:48:05Z,"  Designing mechanical devices, called linkages, that draw a given plane curve
has been a topic that interested engineers and mathematicians for hundreds of
years, and recently also computer scientists. Already in 1876, Kempe proposed a
procedure for solving the problem in full generality, but his constructions
tend to be extremely complicated. We provide a novel algorithm that produces
much simpler linkages, but works only for parametric curves. Our approach is to
transform the problem into a factorization task over some noncommutative
algebra. We show how to compute such a factorization, and how to use it to
construct a linkage tracing a given curve.
","['\nMatteo Gallet\n', '\nChristoph Koutschan\n', '\nZijia Li\n', '\nGeorg Regensburger\n', '\nJosef Schicho\n', '\nNelly Villamizar\n']","33 pages, 12 figures","Mathematics of Computation 86 (2017), pages 473-506",http://dx.doi.org/10.1090/mcom/3120,cs.SC,"['cs.SC', 'cs.CG', 'cs.RO', 'math.AG', 'math.RA', '70B15, 68W30, 70G55, 20G20, 16Z05, 14P05, 12Y05']",10.1090/mcom/3120,,[]
Real root finding for rank defects in linear Hankel matrices,http://arxiv.org/abs/1502.02473v1,2015-02-09T13:12:40Z,2015-02-09T13:12:40Z,"  Let $H\_0, ..., H\_n$ be $m \times m$ matrices with entries in $\QQ$ and
Hankel structure, i.e. constant skew diagonals. We consider the linear Hankel
matrix $H(\vecx)=H\_0+\X\_1H\_1+...+\X\_nH\_n$ and the problem of computing
sample points in each connected component of the real algebraic set defined by
the rank constraint ${\sf rank}(H(\vecx))\leq r$, for a given integer $r \leq
m-1$. Computing sample points in real algebraic sets defined by rank defects in
linear matrices is a general problem that finds applications in many areas such
as control theory, computational geometry, optimization, etc. Moreover, Hankel
matrices appear in many areas of engineering sciences. Also, since Hankel
matrices are symmetric, any algorithmic development for this problem can be
seen as a first step towards a dedicated exact algorithm for solving
semi-definite programming problems, i.e. linear matrix inequalities. Under some
genericity assumptions on the input (such as smoothness of an incidence
variety), we design a probabilistic algorithm for tackling this problem. It is
an adaptation of the so-called critical point method that takes advantage of
the special structure of the problem. Its complexity reflects this: it is
essentially quadratic in specific degree bounds on an incidence variety. We
report on practical experiments and analyze how the algorithm takes advantage
of this special structure. A first implementation outperforms existing
implementations for computing sample points in general real algebraic sets: it
tackles examples that are out of reach of the state-of-the-art.
","['\nDidier Henrion\nLAAS\n', '\nSimone Naldi\nLAAS\n', '\nMohab Safey El Din\nSystèmes Polynomiaux, LIP6\n']",,,http://arxiv.org/abs/1502.02473v1,cs.SC,['cs.SC'],,,"['LAAS', 'LAAS', 'Systèmes Polynomiaux, LIP6']"
Subtropical Real Root Finding,http://arxiv.org/abs/1501.04836v1,2015-01-20T15:01:11Z,2015-01-20T15:01:11Z,"  We describe a new incomplete but terminating method for real root finding for
large multivariate polynomials. We take an abstract view of the polynomial as
the set of exponent vectors associated with sign information on the
coefficients. Then we employ linear programming to heuristically find roots.
There is a specialized variant for roots with exclusively positive coordinates,
which is of considerable interest for applications in chemistry and systems
biology. An implementation of our method combining the computer algebra system
Reduce with the linear programming solver Gurobi has been successfully applied
to input data originating from established mathematical models used in these
areas. We have solved several hundred problems with up to more than 800000
monomials in up to 10 variables with degrees up to 12. Our method has failed
due to its incompleteness in less than 8 percent of the cases.
",['\nThomas Sturm\n'],,"Proc. ISSAC 2015, pp.347-354, ACM 2015",http://dx.doi.org/10.1145/2755996.2756677,cs.SC,['cs.SC'],10.1145/2755996.2756677,,[]
Computing the Rank Profile Matrix,http://arxiv.org/abs/1501.05239v2,2015-01-21T17:31:13Z,2015-08-20T06:56:41Z,"  The row (resp. column) rank profile of a matrix describes the staircase shape
of its row (resp. column) echelon form. In an ISSAC'13 paper, we proposed a
recursive Gaussian elimination that can compute simultaneously the row and
column rank profiles of a matrix as well as those of all of its leading
sub-matrices, in the same time as state of the art Gaussian elimination
algorithms. Here we first study the conditions making a Gaus-sian elimination
algorithm reveal this information. Therefore, we propose the definition of a
new matrix invariant, the rank profile matrix, summarizing all information on
the row and column rank profiles of all the leading sub-matrices. We also
explore the conditions for a Gaussian elimination algorithm to compute all or
part of this invariant, through the corresponding PLUQ decomposition. As a
consequence, we show that the classical iterative CUP decomposition algorithm
can actually be adapted to compute the rank profile matrix. Used, in a Crout
variant, as a base-case to our ISSAC'13 implementation, it delivers a
significant improvement in efficiency. Second, the row (resp. column) echelon
form of a matrix are usually computed via different dedicated triangular
decompositions. We show here that, from some PLUQ decompositions, it is
possible to recover the row and column echelon forms of a matrix and of any of
its leading sub-matrices thanks to an elementary post-processing algorithm.
","['\nJean-Guillaume Dumas\nLJK\n', '\nClément Pernet\nMOAIS, ARIC\n', '\nZiad Sultan\nMOAIS, LJK\n']",,"ACM International Symposium on Symbolic and Algebraic
  Computations, pp.146--153, 2015, ISSAC 2015",http://dx.doi.org/10.1145/2755996.2756682,cs.SC,['cs.SC'],10.1145/2755996.2756682,,"['LJK', 'MOAIS, ARIC', 'MOAIS, LJK']"
Numerically Safe Gaussian Elimination with No Pivoting,http://arxiv.org/abs/1501.05385v11,2015-01-22T04:07:57Z,2017-04-17T20:21:50Z,"  Gaussian elimination with no pivoting and block Gaussian elimination are
attractive alternatives to the customary but communication intensive Gaussian
elimination with partial pivoting (hereafter we use the acronyms GENP, BGE, and
GEPP} provided that the computations proceed safely and numerically safely},
that is, run into neither division by 0 nor numerical problems. Empirically,
safety and numerical safety of GENP have been consistently observed in a number
of papers where an input matrix was pre-processed with various structured
multipliers chosen ad hoc. Our present paper provides missing formal support
for this empirical observation and explains why it was elusive so far. Namely
we prove that GENP is numerically unsafe for a specific class of input matrices
in spite of its pre-processing with some well-known and well-tested structured
multipliers, but we also prove that GENP and BGE are safe and numerically safe
for the average input matrix pre-processed with any nonsingular and
well-conditioned multiplier. This should embolden search for sparse and
structured multipliers, and we list and test some new classes of them. We also
seek randomized pre-processing that universally (that is, for all input
matrices) supports (i) safe GENP and BGE with probability 1 and/or (ii)
numerically safe GENP and BGE with a probability close to 1.We achieve goal (i)
with a Gaussian structured multiplier and goal (ii) with a Gaussian
unstructured multiplier and alternatively with Gaussian structured
augmentation. We consistently confirm all these formal results with our tests
of GENP for benchmark inputs. We have extended our approach to other
fundamental matrix computations and keep working on further extensions.
","['\nVictor Pan\n', '\nLiang Zhao\n']","27 pages, 7 figures, the paper is in both areas of computer science
  (randomized algorithms) and numerical matrix computations",,http://arxiv.org/abs/1501.05385v11,cs.SC,['cs.SC'],,,[]
"Real Polynomial Root-finding by Means of Matrix and Polynomial
  Iterations",http://arxiv.org/abs/1501.05390v3,2015-01-22T04:30:09Z,2017-04-13T15:56:28Z,"  Univariate polynomial root-finding is a classical subject, still important
for modern computing. Frequently one seeks just the real roots of a polynomial
with real coefficients. They can be approximated at a low computational cost if
the polynomial has no nonreal roots, but for high degree polynomials, nonreal
roots are typically much more numerous than the real ones. The challenge is
known for a long time, and the subject has been intensively studied.
Nevertheless, we produce some novel ideas and techniques and obtain dramatic
acceleration of the known algorithms. In order to achieve our progress we
exploit the correlation between the computations with matrices and polynomials,
randomized matrix computations, and complex plane geometry, extend the
techniques of the matrix sign iterations, and use the structure of the
companion matrix of the input polynomial. The results of our extensive tests
with benchmark polynomials and random matrices are quite encouraging. In
particular in our tests the number of iterations required for convergence of
our algorithms grew very slowly (if at all) as we increased the degree of the
univariate input polynomials and the dimension of the input matrices from 64 to
1024.
","['\nVictor Y. Pan\n', '\nLiang Zhao\n']","24 pages 12 tables. arXiv admin note: substantial text overlap with
  arXiv:1404.6817",,http://arxiv.org/abs/1501.05390v3,cs.SC,['cs.SC'],,,[]
"Accelerated Approximation of the Complex Roots and Factors of a
  Univariate Polynomial",http://arxiv.org/abs/1501.05392v3,2015-01-22T04:39:26Z,2016-11-09T04:45:07Z,"  The algorithms of Pan (1995) and(2002) approximate the roots of a complex
univariate polynomial in nearly optimal arithmetic and Boolean time but require
precision of computing that exceeds the degree of the polynomial. This causes
numerical stability problems when the degree is large. We observe, however,
that such a difficulty disappears at the initial stage of the algorithms, and
in our present paper we extend this stage to root-finding within a nearly
optimal arithmetic and Boolean complexity bounds provided that some mild
initial isolation of the roots of the input polynomial has been ensured.
Furthermore our algorithm is nearly optimal for the approximation of the roots
isolated in a fixed disc, square or another region on the complex plane rather
than all complex roots of a polynomial. Moreover the algorithm can be applied
to a polynomial given by a black box for its evaluation (even if its
coefficients are not known); it promises to be of practical value for
polynomial root-finding and factorization, the latter task being of interest on
its own right. We also provide a new support for a winding number algorithm,
which enables extension of our progress to obtaining mild initial
approximations to the roots. We conclude with summarizing our algorithms and
their extension to the approximation of isolated multiple roots and root
clusters.
","['\nVictor Y. Pan\n', '\nElias P. Tsigaridas\n', '\nVitaly Zaderman\n', '\nLiang Zhao\n']",16 pages,,http://arxiv.org/abs/1501.05392v3,cs.SC,['cs.SC'],,,[]
Better Answers to Real Questions,http://arxiv.org/abs/1501.05098v1,2015-01-21T09:14:26Z,2015-01-21T09:14:26Z,"  We consider existential problems over the reals. Extended quantifier
elimination generalizes the concept of regular quantifier elimination by
providing in addition answers, which are descriptions of possible assignments
for the quantified variables. Implementations of extended quantifier
elimination via virtual substitution have been successfully applied to various
problems in science and engineering. So far, the answers produced by these
implementations included infinitesimal and infinite numbers, which are hard to
interpret in practice. We introduce here a post-processing procedure to
convert, for fixed parameters, all answers into standard real numbers. The
relevance of our procedure is demonstrated by application of our implementation
to various examples from the literature, where it significantly improves the
quality of the results.
","['\nMarek Kosta\n', '\nThomas Sturm\n', '\nAndreas Dolzmann\n']",,"J. Symb. Comput., 74:255-275, 2016",http://dx.doi.org/10.1016/j.jsc.2015.07.002,cs.SC,"['cs.SC', 'cs.LO']",10.1016/j.jsc.2015.07.002,,[]
A Generalized Framework for Virtual Substitution,http://arxiv.org/abs/1501.05826v1,2015-01-23T15:13:43Z,2015-01-23T15:13:43Z,"  We generalize the framework of virtual substitution for real quantifier
elimination to arbitrary but bounded degrees. We make explicit the
representation of test points in elimination sets using roots of parametric
univariate polynomials described by Thom codes. Our approach follows an early
suggestion by Weispfenning, which has never been carried out explicitly.
Inspired by virtual substitution for linear formulas, we show how to
systematically construct elimination sets containing only test points
representing lower bounds.
","['\nMarek Kosta\n', '\nThomas Sturm\n']",,,http://arxiv.org/abs/1501.05826v1,cs.SC,"['cs.SC', 'cs.LO']",,,[]
"Output-sensitive algorithms for sumset and sparse polynomial
  multiplication",http://arxiv.org/abs/1501.05296v3,2015-01-21T04:43:58Z,2015-04-24T11:10:00Z,"  We present randomized algorithms to compute the sumset (Minkowski sum) of two
integer sets, and to multiply two univariate integer polynomials given by
sparse representations. Our algorithm for sumset has cost softly linear in the
combined size of the inputs and output. This is used as part of our sparse
multiplication algorithm, whose cost is softly linear in the combined size of
the inputs, output, and the sumset of the supports of the inputs. As a
subroutine, we present a new method for computing the coefficients of a sparse
polynomial, given a set containing its support. Our multiplication algorithm
extends to multivariate Laurent polynomials over finite fields and rational
numbers. Our techniques are based on sparse interpolation algorithms and
results from analytic number theory.
","['\nAndrew Arnold\n', '\nDaniel S. Roche\n']",Submitted to ISSAC 2015,,http://arxiv.org/abs/1501.05296v3,cs.SC,"['cs.SC', 'cs.CC', 'cs.DS', '68W20, 68W30, 68Q17, 12Y05', 'F.2.1; F.2.3; G.3; G.4; I.1.2']",,,[]
Computation of Differential Chow Forms for Prime Differential Ideals,http://arxiv.org/abs/1501.02755v1,2015-01-12T19:14:47Z,2015-01-12T19:14:47Z,"  In this paper, we propose algorithms to compute differential Chow forms for
prime differential ideals which are given by their characteristic sets. The
main algorithm is based on an optimal bound for the order of a prime
differential ideal in terms of its characteristic set under an arbitrary
ranking, which shows the Jacobi bound conjecture holds in this case. Apart from
the order bound, we also give a degree bound for the differential Chow form. In
addition, for prime differential ideals given by their characteristic sets
under an orderly ranking, a much more simpler algorithm is given to compute its
differential Chow form. The computational complexity of both is single
exponential in terms of the Jacobi number, the maximal degree of the
differential polynomials in the characteristic set and the number of variables.
","['\nWei Li\n', '\nYinghong Li\n']",24 pages,,http://arxiv.org/abs/1501.02755v1,math.AG,"['math.AG', 'cs.SC', '12H05, 14C05']",,,[]
Integral D-Finite Functions,http://arxiv.org/abs/1501.03691v2,2015-01-15T14:29:19Z,2015-06-30T07:27:48Z,"  We propose a differential analog of the notion of integral closure of
algebraic function fields. We present an algorithm for computing the integral
closure of the algebra defined by a linear differential operator. Our algorithm
is a direct analog of van Hoeij's algorithm for computing integral bases of
algebraic function fields.
","['\nManuel Kauers\n', '\nChristoph Koutschan\n']",,"Proceedings of the International Symposium on Symbolic and
  Algebraic Computation (ISSAC 2015), pages 251-258, 2015. ACM, New York, USA,
  ISBN 978-1-4503-3435-8",http://dx.doi.org/10.1145/2755996.2756658,cs.SC,"['cs.SC', 'math.RA', 'I.1.2']",10.1145/2755996.2756658,,[]
"Parallel degree computation for solution space of binomial systems with
  an application to the master space of $\mathcal{N}=1$ gauge theories",http://arxiv.org/abs/1501.02237v2,2015-01-09T19:27:04Z,2015-03-01T18:53:19Z,"  The problem of solving a system of polynomial equations is one of the most
fundamental problems in applied mathematics. Among them, the problem of solving
a system of binomial equations form a important subclass for which specialized
techniques exist. For both theoretic and applied purposes, the degree of the
solution set of a system of binomial equations often plays an important role in
understanding the geometric structure of the solution set. Its computation,
however, is computationally intensive. This paper proposes a specialized
parallel algorithm for computing the degree on GPUs that takes advantage of the
massively parallel nature of GPU devices. The preliminary implementation shows
remarkable efficiency and scalability when compared to the closest CPU-based
counterpart. Applied to the ""master space problem of $\mathcal{N}=1$ gauge
theories"" the GPU-based implementation achieves nearly 30 fold speedup over its
CPU-only counterpart enabling the discovery of previously unknown results.
Equally important to note is the far superior scalability: with merely 3 GPU
devices on a single workstation, the GPU-based implementation shows better
performance, on certain problems, than a small cluster totaling 100 CPU cores.
","['\nTianran Chen\n', '\nDhagash Mehta\n']","27 pages, 5 figures. Improved data with further computation",,http://arxiv.org/abs/1501.02237v2,math.AG,"['math.AG', 'cs.SC', 'hep-th', 'math-ph', 'math.MP']",,,[]
Data-Discriminants of Likelihood Equations,http://arxiv.org/abs/1501.00334v5,2015-01-02T01:55:37Z,2015-05-06T00:45:37Z,"  Maximum likelihood estimation (MLE) is a fundamental computational problem in
statistics. The problem is to maximize the likelihood function with respect to
given data on a statistical model. An algebraic approach to this problem is to
solve a very structured parameterized polynomial system called likelihood
equations. For general choices of data, the number of complex solutions to the
likelihood equations is finite and called the ML-degree of the model. The only
solutions to the likelihood equations that are statistically meaningful are the
real/positive solutions. However, the number of real/positive solutions is not
characterized by the ML-degree. We use discriminants to classify data according
to the number of real/positive solutions of the likelihood equations. We call
these discriminants data-discriminants (DD). We develop a probabilistic
algorithm for computing DDs. Experimental results show that, for the benchmarks
we have tried, the probabilistic algorithm is more efficient than the standard
elimination algorithm. Based on the computational results, we discuss the real
root classification problem for the 3 by 3 symmetric matrix~model.
","['\nJose Israel Rodriguez\n', '\nXiaoxian Tang\n']",2 tables,,http://arxiv.org/abs/1501.00334v5,cs.SC,"['cs.SC', 'G.3; I.1.2']",,,[]
"Accurate solution of near-colliding Prony systems via decimation and
  homotopy continuation",http://arxiv.org/abs/1501.00160v3,2014-12-31T15:56:37Z,2016-10-21T17:08:03Z,"  We consider polynomial systems of Prony type, appearing in many areas of
mathematics. Their robust numerical solution is considered to be difficult,
especially in ""near-colliding"" situations. We consider a case when the
structure of the system is a-priori fixed. We transform the nonlinear part of
the Prony system into a Hankel-type polynomial system. Combining this
representation with a recently discovered ""decimation"" technique, we present an
algorithm which applies homotopy continuation to an appropriately chosen
Hankel-type system as above. In this way, we are able to solve for the
nonlinear variables of the original system with high accuracy when the data is
perturbed.
",['\nDmitry Batenkov\n'],,,http://arxiv.org/abs/1501.00160v3,cs.NA,"['cs.NA', 'cs.SC', 'math.NA']",,,[]
"On the complexity of computing Gröbner bases for weighted homogeneous
  systems",http://arxiv.org/abs/1412.7547v2,2014-12-23T21:15:05Z,2015-12-21T09:08:47Z,"  Solving polynomial systems arising from applications is frequently made
easier by the structure of the systems. Weighted homogeneity (or
quasi-homogeneity) is one example of such a structure: given a system of
weights $W=(w\_{1},\dots,w\_{n})$, $W$-homogeneous polynomials are polynomials
which are homogeneous w.r.t the weighted degree
$\deg\_{W}(X\_{1}^{\alpha\_{1}},\dots,X\_{n}^{\alpha\_{n}}) = \sum
w\_{i}\alpha\_{i}$. Gr\""obner bases for weighted homogeneous systems can be
computed by adapting existing algorithms for homogeneous systems to the
weighted homogeneous case. We show that in this case, the complexity estimate
for Algorithm~\F5 $\left(\binom{n+\dmax-1}{\dmax}^{\omega}\right)$ can be
divided by a factor $\left(\prod w\_{i} \right)^{\omega}$. For zero-dimensional
systems, the complexity of Algorithm~\FGLM $nD^{\omega}$ (where $D$ is the
number of solutions of the system) can be divided by the same factor
$\left(\prod w\_{i} \right)^{\omega}$. Under genericity assumptions, for
zero-dimensional weighted homogeneous systems of $W$-degree
$(d\_{1},\dots,d\_{n})$, these complexity estimates are polynomial in the
weighted B\'ezout bound $\prod\_{i=1}^{n}d\_{i} / \prod\_{i=1}^{n}w\_{i}$.
Furthermore, the maximum degree reached in a run of Algorithm \F5 is bounded by
the weighted Macaulay bound $\sum (d\_{i}-w\_{i}) + w\_{n}$, and this bound is
sharp if we can order the weights so that $w\_{n}=1$. For overdetermined
semi-regular systems, estimates from the homogeneous case can be adapted to the
weighted case. We provide some experimental results based on systems arising
from a cryptography problem and from polynomial inversion problems. They show
that taking advantage of the weighted homogeneous structure yields substantial
speed-ups, and allows us to solve systems which were otherwise out of reach.
","['\nJean-Charles Faugère\nPolSys\n', '\nMohab Safey El Din\nPolSys\n', '\nThibaut Verron\nLIP6, PolSys\n']",,,http://dx.doi.org/10.1016/j.jsc.2015.12.001,cs.SC,['cs.SC'],10.1016/j.jsc.2015.12.001,,"['PolSys', 'PolSys', 'LIP6, PolSys']"
Real root finding for determinants of linear matrices,http://arxiv.org/abs/1412.5873v1,2014-12-18T14:28:36Z,2014-12-18T14:28:36Z,"  Let $\A_0, \A_1, \ldots, \A_n$ be given square matrices of size $m$ with
rational coefficients. The paper focuses on the exact computation of one point
in each connected component of the real determinantal variety $\{\X \in\RR^n \:
:\: \det(\A_0+x_1\A_1+\cdots+x_n\A_n)=0\}$. Such a problem finds applications
in many areas such as control theory, computational geometry, optimization,
etc. Using standard complexity results this problem can be solved using
$m^{O(n)}$ arithmetic operations. Under some genericity assumptions on the
coefficients of the matrices, we provide an algorithm solving this problem
whose runtime is essentially quadratic in ${{n+m}\choose{n}}^{3}$. We also
report on experiments with a computer implementation of this algorithm. Its
practical performance illustrates the complexity estimates. In particular, we
emphasize that for subfamilies of this problem where $m$ is fixed, the
complexity is polynomial in $n$.
","['\nDidier Henrion\n', '\nSimone Naldi\n', '\nMohab Safey El Din\n']",,,http://arxiv.org/abs/1412.5873v1,cs.SC,"['cs.SC', 'math.AG', 'I.1; F.2']",,,[]
"A streamlined difference ring theory: Indefinite nested sums, the
  alternating sign and the parameterized telescoping problem",http://arxiv.org/abs/1412.2782v2,2014-12-08T21:35:44Z,2015-01-30T13:56:02Z,"  We present an algebraic framework to represent indefinite nested sums over
hypergeometric expressions in difference rings. In order to accomplish this
task, parts of Karr's difference field theory have been extended to a ring
theory in which also the alternating sign can be expressed. The underlying
machinery relies on algorithms that compute all solutions of a given
parameterized telescoping equation. As a consequence, we can solve the
telescoping and creative telescoping problem in such difference rings.
",['\nCarsten Schneider\n'],Some typos are removed,"Proceedings of the 16th International Symposium on Symbolic and
  Numeric Algorithms for Scientific Computing (SYNASC '14), pp. 26-33. IEEE,
  2014",http://dx.doi.org/10.1109/SYNASC.2014.12,cs.SC,['cs.SC'],10.1109/SYNASC.2014.12,,[]
"A Successive Resultant Projection for Cylindrical Algebraic
  Decomposition",http://arxiv.org/abs/1412.4861v1,2014-12-16T02:39:44Z,2014-12-16T02:39:44Z,"  This note shows the equivalence of two projection operators which both can be
used in cylindrical algebraic decomposition (CAD) . One is known as Brown's
Projection (C. W. Brown (2001)); the other was proposed by Lu Yang in his
earlier work (L.Yang and S.~H. Xia (2000)) that is sketched as follows: given a
polynomial $f$ in $x_1,\,x_2,\,\cdots$, by $f_1$ denote the resultant of $f$
and its partial derivative with respect to $x_1$ (removing the multiple
factors), by $f_2$ denote the resultant of $f_1$ and its partial derivative
with respect to $x_2$, (removing the multiple factors), $\cdots$, repeat this
procedure successively until the last resultant becomes a univariate
polynomial. Making use of an identity, the equivalence of these two projection
operators is evident.
","['\nYong Yao\n', '\nJia Xu\n', '\nLu Yang\n']",6 pages,,http://arxiv.org/abs/1412.4861v1,cs.SC,"['cs.SC', '12Y05, 13P15, 14P10, 68W30', 'G.1.5']",,,[]
"Probabilistic analysis of Wiedemann's algorithm for minimal polynomial
  computation",http://arxiv.org/abs/1412.5071v3,2014-12-16T16:41:27Z,2015-06-17T13:36:03Z,"  Blackbox algorithms for linear algebra problems start with projection of the
sequence of powers of a matrix to a sequence of vectors (Lanczos), a sequence
of scalars (Wiedemann) or a sequence of smaller matrices (block methods). Such
algorithms usually depend on the minimal polynomial of the resulting sequence
being that of the given matrix. Here exact formulas are given for the
probability that this occurs. They are based on the generalized Jordan normal
form (direct sum of companion matrices of the elementary divisors) of the
matrix. Sharp bounds follow from this for matrices of unknown elementary
divisors. The bounds are valid for all finite field sizes and show that a small
blocking factor can give high probability of success for all cardinalities and
matrix dimensions.
","['\nGavin Harrison\n', '\nJeremy Johnson\n', '\nB. David Saunders\n']","19 pages. To be published in the Journal of Symbolic Computation,
  please cite as ""Harrison, G., et al. Probabilistic analysis of Wiedemann's
  algorithm for minimal polynomial computation. J. Symb. Comput. (2015),
  http://dx.doi.org/10.1016/j.jsc.2015.06.005""",,http://dx.doi.org/10.1016/j.jsc.2015.06.005,cs.SC,"['cs.SC', 'F.2.1; I.1.2']",10.1016/j.jsc.2015.06.005,,[]
"Faster Sparse Multivariate Polynomial Interpolation of Straight-Line
  Programs",http://arxiv.org/abs/1412.4088v2,2014-12-12T19:12:15Z,2014-12-15T18:44:05Z,"  Given a straight-line program whose output is a polynomial function of the
inputs, we present a new algorithm to compute a concise representation of that
unknown function. Our algorithm can handle any case where the unknown function
is a multivariate polynomial, with coefficients in an arbitrary finite field,
and with a reasonable number of nonzero terms but possibly very large degree.
It is competitive with previously known sparse interpolation algorithms that
work over an arbitrary finite field, and provides an improvement when there are
a large number of variables.
","['\nAndrew Arnold\n', '\nMark Giesbrecht\n', '\nDaniel S. Roche\n']",33 pages. Submitted for publication,,http://arxiv.org/abs/1412.4088v2,cs.SC,"['cs.SC', 'cs.DS', '68W30 (Primary), 12Y05, 68W20 (Secondary)', 'F.2.1; I.1.2']",,,[]
On the Inverting of A General Heptadiagonal Matrix,http://arxiv.org/abs/1412.4155v2,2014-12-12T22:41:35Z,2014-12-18T10:34:26Z,"  In this paper, we developed new numeric and symbolic algorithms to find the
inverse of any nonsingular heptadiagonal matrix. Symbolic algorithm will not
break and it is without setting any restrictive conditions. The computational
cost of our algorithms is $O(n)$. The algorithms are suitable for
implementation using computer algebra system such as MAPLE, MATLAB and
MATHEMATICA. Examples are given to illustrate the efficiency of the algorithms.
",['\nA. A. Karawia\n'],,,http://arxiv.org/abs/1412.4155v2,math.NA,"['math.NA', 'cs.SC', '15A15, 15A23, 68W30, 11Y05, 33F10', 'F.2.1; G.1.0']",,,[]
"Numeric certified algorithm for the topology of resultant and
  discriminant curves",http://arxiv.org/abs/1412.3290v2,2014-12-10T12:57:08Z,2015-05-23T10:22:21Z,"  Let $\mathcal C$ be a real plane algebraic curve defined by the resultant of
two polynomials (resp. by the discriminant of a polynomial). Geometrically such
a curve is the projection of the intersection of the surfaces
$P(x,y,z)=Q(x,y,z)=0$ (resp. $P(x,y,z)=\frac{\partial P}{\partial
z}(x,y,z)=0$), and generically its singularities are nodes (resp. nodes and
ordinary cusps). State-of-the-art numerical algorithms compute the topology of
smooth curves but usually fail to certify the topology of singular ones. The
main challenge is to find practical numerical criteria that guarantee the
existence and the uniqueness of a singularity inside a given box $B$, while
ensuring that $B$ does not contain any closed loop of $\mathcal{C}$. We solve
this problem by first providing a square deflation system, based on
subresultants, that can be used to certify numerically whether $B$ contains a
unique singularity $p$ or not. Then we introduce a numeric adaptive separation
criterion based on interval arithmetic to ensure that the topology of $\mathcal
C$ in $B$ is homeomorphic to the local topology at $p$. Our algorithms are
implemented and experiments show their efficiency compared to state-of-the-art
symbolic or homotopic methods.
","['\nRémi Imbach\nINRIA Nancy - Grand Est / LORIA\n', '\nGuillaume Moroz\nINRIA Nancy - Grand Est / LORIA\n', '\nMarc Pouget\nINRIA Nancy - Grand Est / LORIA\n']",,,http://arxiv.org/abs/1412.3290v2,cs.CG,"['cs.CG', 'cs.DS', 'cs.SC', 'math.AG']",,,"['INRIA Nancy - Grand Est / LORIA', 'INRIA Nancy - Grand Est / LORIA', 'INRIA Nancy - Grand Est / LORIA']"
Bounded-degree factors of lacunary multivariate polynomials,http://arxiv.org/abs/1412.3570v2,2014-12-11T08:41:03Z,2016-01-29T10:45:16Z,"  In this paper, we present a new method for computing bounded-degree factors
of lacunary multivariate polynomials. In particular for polynomials over number
fields, we give a new algorithm that takes as input a multivariate polynomial f
in lacunary representation and a degree bound d and computes the irreducible
factors of degree at most d of f in time polynomial in the lacunary size of f
and in d. Our algorithm, which is valid for any field of zero characteristic,
is based on a new gap theorem that enables reducing the problem to several
instances of (a) the univariate case and (b) low-degree multivariate
factorization.
  The reduction algorithms we propose are elementary in that they only
manipulate the exponent vectors of the input polynomial. The proof of
correctness and the complexity bounds rely on the Newton polytope of the
polynomial, where the underlying valued field consists of Puiseux series in a
single variable.
",['\nBruno Grenet\n'],31 pages; Long version of arXiv:1401.4720 with simplified proofs,"Journal of Symbolic Computation 75, pages 171-192, 2016",http://dx.doi.org/10.1016/j.jsc.2015.11.013,cs.SC,"['cs.SC', 'cs.CC', 'cs.DS']",10.1016/j.jsc.2015.11.013,,[]
"Four Random Permutations Conjugated by an Adversary Generate $S_n$ with
  High Probability",http://arxiv.org/abs/1412.3781v1,2014-12-11T19:59:25Z,2014-12-11T19:59:25Z,"  We prove a conjecture dating back to a 1978 paper of D.R.\
Musser~\cite{musserirred}, namely that four random permutations in the
symmetric group $\mathcal{S}_n$ generate a transitive subgroup with probability
$p_n > \epsilon$ for some $\epsilon > 0$ independent of $n$, even when an
adversary is allowed to conjugate each of the four by a possibly different
element of $\S_n$ (in other words, the cycle types already guarantee generation
of $\mathcal{S}_n$). This is closely related to the following random set model.
A random set $M \subseteq \mathbb{Z}^+$ is generated by including each $n \geq
1$ independently with probability $1/n$. The sumset $\text{sumset}(M)$ is
formed. Then at most four independent copies of $\text{sumset}(M)$ are needed
before their mutual intersection is no longer infinite.
","['\nRobin Pemantle\n', '\nYuval Peres\n', '\nIgor Rivin\n']","19pages, 1 figure",,http://arxiv.org/abs/1412.3781v1,math.PR,"['math.PR', 'cs.SC', 'math-ph', 'math.MP', '60C05, 12Y05, 68W20, 68W30, 68W40']",,,[]
Efficient edge-skeleton computation for polytopes defined by oracles,http://arxiv.org/abs/1412.3987v1,2014-12-12T13:34:22Z,2014-12-12T13:34:22Z,"  In general dimension, there is no known total polynomial algorithm for either
convex hull or vertex enumeration, i.e. an algorithm whose complexity depends
polynomially on the input and output sizes. It is thus important to identify
problems (and polytope representations) for which total polynomial-time
algorithms can be obtained. We offer the first total polynomial-time algorithm
for computing the edge-skeleton (including vertex enumeration) of a polytope
given by an optimization or separation oracle, where we are also given a
superset of its edge directions. We also offer a space-efficient variant of our
algorithm by employing reverse search. All complexity bounds refer to the
(oracle) Turing machine model. There is a number of polytope classes naturally
defined by oracles; for some of them neither vertex nor facet representation is
obvious. We consider two main applications, where we obtain (weakly) total
polynomial-time algorithms: Signed Minkowski sums of convex polytopes, where
polytopes can be subtracted provided the signed sum is a convex polytope, and
computation of secondary, resultant, and discriminant polytopes. Further
applications include convex combinatorial optimization and convex integer
programming, where we offer a new approach, thus removing the complexity's
exponential dependence in the dimension.
","['\nIoannis Z. Emiris\n', '\nVissarion Fisikopoulos\n', '\nBernd Gärtner\n']","22 pages, 2 figures",Journal of Symbolic Computation 2016,http://dx.doi.org/10.1016/j.jsc.2015.06.001,cs.CG,"['cs.CG', 'cs.SC', 'math.OC', '52-XX, 14Qxx']",10.1016/j.jsc.2015.06.001,,[]
Generalization of Gabidulin Codes over Fields of Rational Functions,http://arxiv.org/abs/1412.6080v1,2014-12-15T15:12:13Z,2014-12-15T15:12:13Z,"  We transpose the theory of rank metric and Gabidulin codes to the case of
fields which are not finite fields. The Frobenius automorphism is replaced by
any element of the Galois group of a cyclic algebraic extension of a base
field. We use our framework to define Gabidulin codes over the field of
rational functions using algebraic function fields with a cyclic Galois group.
This gives a linear subspace of matrices whose coefficients are rational
function, such that the rank of each of this matrix is lower bounded, where the
rank is comprised in term of linear combination with rational functions. We
provide two examples based on Kummer and Artin-Schreier extensions.The matrices
that we obtain may be interpreted as generating matrices of convolutional
codes.
",['\nDaniel Augot\nLIX\n'],"21st International Symposium on Mathematical Theory of Networks and
  Systems (MTNS 2014), Jul 2014, Groningen, Netherlands.
  https://fwn06.housing.rug.nl/mtns2014/",,http://arxiv.org/abs/1412.6080v1,cs.SC,"['cs.SC', 'cs.IT', 'math.IT']",,,['LIX']
"Finding Semi-Analytic Solutions of Power System Differential-Algebraic
  Equations for Fast Transient Stability Simulation",http://arxiv.org/abs/1412.0904v2,2014-12-02T13:28:12Z,2017-02-08T14:00:56Z,"  This paper studies the semi-analytic solution (SAS) of a power system's
differential-algebraic equation. A SAS is a closed-form function of symbolic
variables including time, the initial state and the parameters on system
operating conditions, and hence able to directly give trajectories on system
state variables, which are accurate for at least a certain time window. A
two-stage SAS-based approach for fast transient stability simulation is
proposed, which offline derives the SAS by the Adomian Decomposition Method and
online evaluates the SAS for each of sequential time windows until making up a
desired simulation period. When applied to fault simulation, the new approach
employs numerical integration only for the fault-on period to determine the
post-disturbance initial state of the SAS. The paper further analyzes the
maximum length of a time window for a SAS to keep its accuracy, and
accordingly, introduces a divergence indicator for adaptive time windows. The
proposed SAS-based new approach is validated on the IEEE 10-machine, 39-bus
system.
","['\nNan Duan\n', '\nKai Sun\n']","An extension of this work has been published as:Nan Duan, Kai Sun,
  ""Power System Simulation Using the Multi-stage Adomian Decomposition Method,
  IEEE Transactions on Power Systems,"" vol. 32, no. 1, pp. 430-441, January
  2017",,http://arxiv.org/abs/1412.0904v2,math.DS,"['math.DS', 'cs.NA', 'cs.SC', 'cs.SY', 'math.CA', '93D99, 65P99', 'G.1.0; G.1.1; G.1.7']",,,[]
Sparse Univariate Polynomials with Many Roots Over Finite Fields,http://arxiv.org/abs/1411.6346v3,2014-11-24T04:20:25Z,2016-07-06T18:17:24Z,"  Suppose $q$ is a prime power and $f\in\mathbb{F}_q[x]$ is a univariate
polynomial with exactly $t$ monomial terms and degree $<q-1$. To establish a
finite field analogue of Descartes' Rule, Bi, Cheng, and Rojas (2013) proved an
upper bound of $2(q-1)^{\frac{t-2}{t-1}}$ on the number of cosets in
$\mathbb{F}^*_q$ needed to cover the roots of $f$ in $\mathbb{F}^*_q$. Here, we
give explicit $f$ with root structure approaching this bound: For $q$ a
$(t-1)$-st power of a prime we give an explicit $t$-nomial vanishing on
$q^{\frac{t-2}{t-1}}$ distinct cosets of $\mathbb{F}^*_q$. Over prime fields
$\mathbb{F}_p$, computational data we provide suggests that it is harder to
construct explicit sparse polynomials with many roots. Nevertheless, assuming
the Generalized Riemann Hypothesis, we find explicit trinomials having
$\Omega\left(\frac{\log p}{\log \log p}\right)$ distinct roots in
$\mathbb{F}_p$.
","['\nQi Cheng\n', '\nShuhong Gao\n', '\nJ. Maurice Rojas\n', '\nDaqing Wan\n']","9 pages, 1 figure, presented at MEGA 2015. This is the journal
  version, and includes new extremal examples and additional references,
  including pointers to recent advances by Kelley and Owen. Comments and
  questions welcome",,http://arxiv.org/abs/1411.6346v3,math.NT,"['math.NT', 'cs.SC']",,,[]
"Sparse implicitization by interpolation: Geometric computations using
  matrix representations",http://arxiv.org/abs/1411.2846v1,2014-11-11T15:13:03Z,2014-11-11T15:13:03Z,"  Based on the computation of a superset of the implicit support,
implicitization of a parametrically given hyper-surface is reduced to computing
the nullspace of a numeric matrix. Our approach exploits the sparseness of the
given parametric equations and of the implicit polynomial. In this work, we
study how this interpolation matrix can be used to reduce some key geometric
predicates on the hyper-surface to simple numerical operations on the matrix,
namely membership and sidedness for given query points. We illustrate our
results with examples based on our Maple implementation.
","['\nIoannis Emiris\n', '\nTatjana Kalinka\n', '\nChristos Konaxis\n']",,,http://arxiv.org/abs/1411.2846v1,math.AG,"['math.AG', 'cs.SC']",,,[]
New effective differential Nullstellensatz,http://arxiv.org/abs/1411.1000v1,2014-11-04T18:53:23Z,2014-11-04T18:53:23Z,"  We show new upper and lower bounds for the effective differential
Nullstellensatz for differential fields of characteristic zero with several
commuting derivations. Seidenberg was the first to address this problem in
1956, without giving a complete solution. The first explicit bounds appeared in
2009 in a paper by Golubitsky, Kondratieva, Szanto, and Ovchinnikov, with the
upper bound expressed in terms of the Ackermann function. D'Alfonso, Jeronimo,
and Solern\'o, using novel ideas, obtained in 2014 a new bound if restricted to
the case of one derivation and constant coefficients. To obtain the bound in
the present paper without this restriction, we extend this approach and use the
new methods of Freitag and Le\'on S\'anchez and of Pierce from 2014, which
represent a model-theoretic approach to differential algebraic geometry.
","['\nRichard Gustavson\n', '\nMarina Kondratieva\n', '\nAlexey Ovchinnikov\n']",,Advances in Mathematics 290 (2016) 1138-1158,http://dx.doi.org/10.1016/j.aim.2015.12.021,math.AC,"['math.AC', 'cs.SC', 'math.AG', 'math.CA']",10.1016/j.aim.2015.12.021,,[]
"Exploiting chordal structure in polynomial ideals: a Gröbner bases
  approach",http://arxiv.org/abs/1411.1745v2,2014-11-06T20:54:53Z,2016-05-09T23:15:24Z,"  Chordal structure and bounded treewidth allow for efficient computation in
numerical linear algebra, graphical models, constraint satisfaction and many
other areas. In this paper, we begin the study of how to exploit chordal
structure in computational algebraic geometry, and in particular, for solving
polynomial systems. The structure of a system of polynomial equations can be
described in terms of a graph. By carefully exploiting the properties of this
graph (in particular, its chordal completions), more efficient algorithms can
be developed. To this end, we develop a new technique, which we refer to as
chordal elimination, that relies on elimination theory and Gr\""obner bases. By
maintaining graph structure throughout the process, chordal elimination can
outperform standard Gr\""obner basis algorithms in many cases. The reason is
that all computations are done on ""smaller"" rings, of size equal to the
treewidth of the graph. In particular, for a restricted class of ideals, the
computational complexity is linear in the number of variables. Chordal
structure arises in many relevant applications. We demonstrate the suitability
of our methods in examples from graph colorings, cryptography, sensor
localization and differential equations.
","['\nDiego Cifuentes\n', '\nPablo Parrilo\n']","40 pages, 5 figures","SIAM Journal on Discrete Mathematics, Vol. 30, No. 3 : pp.
  1534-1570, 2016",http://dx.doi.org/10.1137/151002666,cs.SC,"['cs.SC', 'math.AC', 'math.AG', 'math.OC', '68W30 (Primary), 13P10 (Secondary)']",10.1137/151002666,,[]
Gröbner Bases and Nullstellensätze for Graph-Coloring Ideals,http://arxiv.org/abs/1410.6806v1,2014-10-24T19:25:05Z,2014-10-24T19:25:05Z,"  We revisit a well-known family of polynomial ideals encoding the problem of
graph-$k$-colorability. Our paper describes how the inherent combinatorial
structure of the ideals implies several interesting algebraic properties.
Specifically, we provide lower bounds on the difficulty of computing Gr\""obner
bases and Nullstellensatz certificates for the coloring ideals of general
graphs. For chordal graphs, however, we explicitly describe a Gr\""obner basis
for the coloring ideal, and provide a polynomial-time algorithm.
","['\nJesús A. De Loera\n', '\nSusan Margulies\n', '\nMichael Pernpeintner\n', '\nEric Riedl\n', '\nDavid Rolnick\n', '\nGwen Spencer\n', '\nDespina Stasi\n', '\nJon Swenson\n']",16 pages,,http://arxiv.org/abs/1410.6806v1,cs.SC,"['cs.SC', 'cs.CC', 'math.AC', 'math.AG', 'math.CO']",,,[]
Recognizing implicitly given rational canal surfaces,http://arxiv.org/abs/1410.3628v1,2014-10-14T09:46:44Z,2014-10-14T09:46:44Z,"  It is still a challenging task of today to recognize the type of a given
algebraic surface which is described only by its implicit representation.
In~this paper we will investigate in more detail the case of canal surfaces
that are often used in geometric modelling, Computer-Aided Design and technical
practice (e.g. as blending surfaces smoothly joining two parts with circular
ends). It is known that if the squared medial axis transform is a rational
curve then so is also the corresponding surface. However, starting from a
polynomial it is not known how to decide if the corresponding algebraic surface
is rational canal surface or not. Our goal is to formulate a simple and
efficient algorithm whose input is a~polynomial with the coefficients from some
subfield of real numbers and the output is the answer whether the surface is a
rational canal surface. In the affirmative case we also compute a rational
parameterization of the squared medial axis transform which can be then used
for finding a rational parameterization of the implicitly given canal surface.
","['\nJan Vršek\n', '\nMiroslav Lávička\n']","10 pages, 1 figure",,http://arxiv.org/abs/1410.3628v1,cs.SC,"['cs.SC', 'math.AG']",,,[]
"A Direct Algorithm to Compute the Topological Euler Characteristic and
  Chern-Schwartz-MacPherson Class of Projective Complete Intersection Varieties",http://arxiv.org/abs/1410.4113v2,2014-10-15T16:15:52Z,2016-07-26T22:23:39Z,"  Let $V$ be a possibly singular scheme-theoretic complete intersection
subscheme of $\mathbb{P}^n$ over an algebraically closed field of
characteristic zero. Using a recent result of Fullwood (""On Milnor classes via
invariants of singular subschemes"", Journal of Singularities) we develop an
algorithm to compute the Chern-Schwartz-MacPherson class and Euler
characteristic of $V$. This algorithm complements existing algorithms by
providing performance improvements in the computation of the
Chern-Schwartz-MacPherson class and Euler characteristic for certain types of
complete intersection subschemes of $\mathbb{P}^n$.
",['\nMartin Helmer\n'],"47 pages, 3 tables, with Appendix by Martin Helmer and Eric Schost",,http://dx.doi.org/10.1016/j.tcs.2017.03.029,math.AG,"['math.AG', 'cs.SC', '14Qxx, 68W30, 14C17, 13P15, 65H10', 'F.2.2; I.1.2; I.1.1']",10.1016/j.tcs.2017.03.029,,[]
Missing sets in rational parametrizations of surfaces of revolution,http://arxiv.org/abs/1410.5458v1,2014-10-20T20:42:59Z,2014-10-20T20:42:59Z,"  Parametric representations do not cover, in general, the whole geometric
object that they parametrize. This can be a problem in practical applications.
In this paper we analyze the question for surfaces of revolution generated by
real rational profile curves, and we describe a simple small superset of the
real zone of the surface not covered by the parametrization. This superset
consists, in the worst case, of the union of a circle and the mirror curve of
the profile curve.
","['\nJ. Rafael Sendra\n', '\nDavid Sevilla\n', '\nCarlos Villarino\n']","13 pages, 8 jpg figures",,http://arxiv.org/abs/1410.5458v1,math.AG,"['math.AG', 'cs.SC']",,,[]
Automatic Generation of Loop-Invariants for Matrix Operations,http://arxiv.org/abs/1410.0564v1,2014-10-02T14:27:17Z,2014-10-02T14:27:17Z,"  In recent years it has been shown that for many linear algebra operations it
is possible to create families of algorithms following a very systematic
procedure. We do not refer to the fine tuning of a known algorithm, but to a
methodology for the actual generation of both algorithms and routines to solve
a given target matrix equation. Although systematic, the methodology relies on
complex algebraic manipulations and non-obvious pattern matching, making the
procedure challenging to be performed by hand, our goal is the development of a
fully automated system that from the sole description of a target equation
creates multiple algorithms and routines. We present CL1ck, a symbolic system
written in Mathematica, that starts with an equation, decomposes it into
multiple equations, and returns a set of loop-invariants for the algorithms --
yet to be generated -- that will solve the equation. In a successive step each
loop-invariant is then mapped to its corresponding algorithm and routine. For a
large class of equations, the methodology generates known algorithms as well as
many previously unknown ones. Most interestingly, the methodology unifies
algorithms traditionally developed in isolation. As an example, the five well
known algorithms for the LU factorization are for the first time unified under
a common root.
","['\nDiego Fabregat-Traver\n', '\nPaolo Bientinesi\n']",,,http://arxiv.org/abs/1410.0564v1,cs.MS,"['cs.MS', 'cs.SC']",,,[]
Knowledge-Based Automatic Generation of Partitioned Matrix Expressions,http://arxiv.org/abs/1410.0567v1,2014-10-02T14:33:43Z,2014-10-02T14:33:43Z,"  In a series of papers it has been shown that for many linear algebra
operations it is possible to generate families of algorithms by following a
systematic procedure. Although powerful, such a methodology involves complex
algebraic manipulation, symbolic computations and pattern matching, making the
generation a process challenging to be performed by hand. We aim for a fully
automated system that from the sole description of a target operation creates
multiple algorithms without any human intervention. Our approach consists of
three main stages. The first stage yields the core object for the entire
process, the Partitioned Matrix Expression (PME), which establishes how the
target problem may be decomposed in terms of simpler sub-problems. In the
second stage the PME is inspected to identify predicates, the Loop-Invariants,
to be used to set up the skeleton of a family of proofs of correctness. In the
third and last stage the actual algorithms are constructed so that each of them
satisfies its corresponding proof of correctness. In this paper we focus on the
first stage of the process, the automatic generation of Partitioned Matrix
Expressions. In particular, we discuss the steps leading to a PME and the
knowledge necessary for a symbolic system to perform such steps. We also
introduce Cl1ck, a prototype system written in Mathematica that generates PMEs
automatically.
","['\nDiego Fabregat-Traver\n', '\nPaolo Bientinesi\n']",,,http://arxiv.org/abs/1410.0567v1,cs.MS,"['cs.MS', 'cs.SC']",,,[]
"On Ideal Lattices, Gröbner Bases and Generalized Hash Functions",http://arxiv.org/abs/1410.2011v3,2014-10-08T08:04:56Z,2016-09-08T11:20:40Z,"  In this paper, we draw connections between ideal lattices and multivariate
polynomial rings over integers using Gr\""obner bases. Ideal lattices are ideals
in the residue class ring, $\mathbb{Z}[x]/\langle f \rangle$ (here $f$ is a
monic polynomial), and cryptographic primitives have been built based on these
objects. As ideal lattices in the univariate case are generalizations of cyclic
lattices, we introduce the notion of multivariate cyclic lattices and show that
multivariate ideal lattices are indeed a generalization of them. Based on
multivariate ideal lattices, we establish the existence of collision resistant
hash functions using Gr\""obner basis techniques. For the construction of hash
functions, we define a worst case problem, shortest substitution problem w.r.t.
an ideal in $\mathbb{Z}[x_1,\ldots, x_n]$, and establish hardness results using
functional fields.
","['\nMaria Francis\n', '\nAmbedkar Dukkipati\n']",arXiv admin note: text overlap with arXiv:1409.7788,,http://dx.doi.org/10.1142/S0219498818501128,cs.SC,"['cs.SC', 'cs.CR']",10.1142/S0219498818501128,,[]
On Ideal Lattices and Gröbner Bases,http://arxiv.org/abs/1409.7788v2,2014-09-27T10:01:19Z,2017-10-09T11:31:15Z,"  In this paper, we draw a connection between ideal lattices and Gr\""{o}bner
bases in the multivariate polynomial rings over integers. We study extension of
ideal lattices in $\mathbb{Z}[x]/\langle f \rangle$ (Lyubashevsky \&
Micciancio, 2006) to ideal lattices in
$\mathbb{Z}[x_1,\ldots,x_n]/\mathfrak{a}$, the multivariate case, where $f$ is
a polynomial in $\mathbb{Z}[X]$ and $\mathfrak{a}$ is an ideal in
$\mathbb{Z}[x_1,\ldots,x_n]$. Ideal lattices in univariate case are interpreted
as generalizations of cyclic lattices. We introduce a notion of multivariate
cyclic lattices and we show that multivariate ideal lattices are indeed a
generalization of them. We show that the fact that existence of ideal lattice
in univariate case if and only if $f$ is monic translates to short reduced
Gr\""obner basis (Francis \& Dukkipati, 2014) of $\mathfrak{a}$ is monic in
multivariate case. We, thereby, give a necessary and sufficient condition for
residue class polynomial rings over $\mathbb{Z}$ to have ideal lattices. We
also characterize ideals in $\mathbb{Z}[x_1,\ldots,x_n]$ that give rise to full
rank lattices.
","['\nMaria Francis\n', '\nAmbedkar Dukkipati\n']","The following paper in the arxiv is the longer version of the same
  paper: arXiv:1410.2011",,http://arxiv.org/abs/1409.7788v2,cs.SC,['cs.SC'],,,[]
Tropical Effective Primary and Dual Nullstellensätze,http://arxiv.org/abs/1409.6215v2,2014-09-22T15:56:27Z,2015-06-04T15:45:21Z,"  Tropical algebra is an emerging field with a number of applications in
various areas of mathematics. In many of these applications appeal to tropical
polynomials allows to study properties of mathematical objects such as
algebraic varieties and algebraic curves from the computational point of view.
This makes it important to study both mathematical and computational aspects of
tropical polynomials.
  In this paper we prove a tropical Nullstellensatz and moreover we show an
effective formulation of this theorem. Nullstellensatz is a natural step in
building algebraic theory of tropical polynomials and its effective version is
relevant for computational aspects of this field.
  On our way we establish a simple formulation of min-plus and tropical linear
dualities. We also observe a close connection between tropical and min-plus
polynomial systems.
","['\nDima Grigoriev\n', '\nVladimir V. Podolskii\n']",,,http://arxiv.org/abs/1409.6215v2,math.AG,"['math.AG', 'cs.SC']",,,[]
"Fast and deterministic computation of the determinant of a polynomial
  matrix",http://arxiv.org/abs/1409.5462v1,2014-09-18T20:55:37Z,2014-09-18T20:55:37Z,"  Given a square, nonsingular matrix of univariate polynomials
$\mathbf{F}\in\mathbb{K}[x]^{n\times n}$ over a field $\mathbb{K}$, we give a
deterministic algorithm for finding the determinant of $\mathbf{F}$. The
complexity of the algorithm is $\bigO \left(n^{\omega}s\right)$ field
operations where $s$ is the average column degree or the average row degree of
$\mathbf{F}$. Here $\bigO$ notation is Big-$O$ with log factors omitted and
$\omega$ is the exponent of matrix multiplication.
","['\nWei Zhou\n', '\nGeorge Labahn\n']",10 pages,,http://arxiv.org/abs/1409.5462v1,cs.SC,"['cs.SC', '15A15']",,,[]
On Solving Pentadiagonal Linear Systems via Transformations,http://arxiv.org/abs/1409.4802v3,2014-09-16T20:59:27Z,2015-01-01T19:05:59Z,"  Many authors studied numeric algorithms for solving the linear systems of the
pentadiagonal type. The well-known Fast Pentadiagonal System Solver algorithm
is an example of such algorithms. The current article are described new numeric
and symbolic algorithms for solving pentadiagonal linear systems via
transformations. New algorithms are natural generalization of the work
presented in [Moawwad El- Mikkawy and Faiz Atlan, Algorithms for Solving Linear
Systems of Equations of Tridiagonal Type via Transformations, Applied
Mathematics, 2014, 5, 413-422]. The symbolic algorithms remove the cases where
the numeric algorithms fail. The computational cost of our algorithms is given.
Some examples are given in order to illustrate the effectiveness of the
proposed algorithms. All of the experiments are performed on a computer with
the aid of programs written in MATLAB.
",['\nA. A. Karawia\n'],,,http://arxiv.org/abs/1409.4802v3,math.NA,"['math.NA', 'cs.NA', 'cs.SC', '15A15, 15A23, 68W30, 11Y05, 33F10', 'F.2.1; G.1.0']",,,[]
"Using the distribution of cells by dimension in a cylindrical algebraic
  decomposition",http://arxiv.org/abs/1409.1781v1,2014-09-05T13:11:17Z,2014-09-05T13:11:17Z,"  We investigate the distribution of cells by dimension in cylindrical
algebraic decompositions (CADs). We find that they follow a standard
distribution which seems largely independent of the underlying problem or CAD
algorithm used. Rather, the distribution is inherent to the cylindrical
structure and determined mostly by the number of variables.
  This insight is then combined with an algorithm that produces only
full-dimensional cells to give an accurate method of predicting the number of
cells in a complete CAD. Since constructing only full-dimensional cells is
relatively inexpensive (involving no costly algebraic number calculations) this
leads to heuristics for helping with various questions of problem formulation
for CAD, such as choosing an optimal variable ordering. Our experiments
demonstrate that this approach can be highly effective.
","['\nDavid Wilson\n', '\nMatthew England\n', '\nRussell Bradford\n', '\nJames H. Davenport\n']",8 pages,"Proceedings of the 16th International Symposium on Symbolic and
  Numeric Algorithms for Scientific Computing (SYNASC '14), pp. 53--60. IEEE,
  2014",http://dx.doi.org/10.1109/SYNASC.2014.15,cs.SC,"['cs.SC', '68W30', 'I.1.2']",10.1109/SYNASC.2014.15,,[]
Algorithms in Real Algebraic Geometry: A Survey,http://arxiv.org/abs/1409.1534v1,2014-09-04T19:00:00Z,2014-09-04T19:00:00Z,"  We survey both old and new developments in the theory of algorithms in real
algebraic geometry -- starting from effective quantifier elimination in the
first order theory of reals due to Tarski and Seidenberg, to more recent
algorithms for computing topological invariants of semi-algebraic sets. We
emphasize throughout the complexity aspects of these algorithms and also
discuss the computational hardness of the underlying problems. We also describe
some recent results linking the computational hardness of decision problems in
the first order theory of the reals, with that of computing certain topological
invariants of semi-algebraic sets. Even though we mostly concentrate on exact
algorithms, we also discuss some numerical approaches involving semi-definite
programming that have gained popularity in recent times.
",['\nSaugata Basu\n'],"41 pages, 4 figures. Based on survey talk given at the Real Algebraic
  Geometry Conference, Rennes, June 20-24, 2011. Some references updated and
  some newer material added",,http://arxiv.org/abs/1409.1534v1,math.AG,"['math.AG', 'cs.CC', 'cs.CG', 'cs.SC', 'Primary 14P10, 14P25, Secondary 68W30']",,,[]
"Computing the determinant of a matrix with polynomial entries by
  approximation",http://arxiv.org/abs/1408.5879v2,2014-08-25T19:45:05Z,2015-04-12T14:31:06Z,"  Computing the determinant of a matrix with the univariate and multivariate
polynomial entries arises frequently in the scientific computing and
engineering fields. In this paper, an effective algorithm is presented for
computing the determinant of a matrix with polynomial entries using hybrid
symbolic and numerical computation. The algorithm relies on the Newton's
interpolation method with error control for solving Vandermonde systems. It is
also based on a novel approach for estimating the degree of variables, and the
degree homomorphism method for dimension reduction. Furthermore, the
parallelization of the method arises naturally.
","['\nXiaolin Qin\n', '\nZhi Sun\n', '\nTuo Leng\n', '\nYong Feng\n']","17 pages, 2 figures",,http://arxiv.org/abs/1408.5879v2,cs.SC,"['cs.SC', '15A15, 41A05, 65Y10, 68W30']",,,[]
Computing Multiplicative Order and Primitive Root in Finite Cyclic Group,http://arxiv.org/abs/1408.4942v1,2014-08-21T10:36:15Z,2014-08-21T10:36:15Z,"  Multiplicative order of an element $a$ of group $G$ is the least positive
integer $n$ such that $a^n=e$, where $e$ is the identity element of $G$. If the
order of an element is equal to $|G|$, it is called generator or primitive
root. This paper describes the algorithms for computing multiplicative order
and primitive root in $\mathbb{Z}^*_{p}$, we also present a logarithmic
improvement over classical algorithms.
",['\nShri Prakash Dwivedi\n'],8 pages,,http://dx.doi.org/10.1109/IC3.2014.6897161,cs.SC,"['cs.SC', 'cs.DS']",10.1109/IC3.2014.6897161,,[]
Desingularization of Ore Operators,http://arxiv.org/abs/1408.5512v1,2014-08-23T16:52:18Z,2014-08-23T16:52:18Z,"  We show that Ore operators can be desingularized by calculating a least
common left multiple with a random operator of appropriate order. Our result
generalizes a classical result about apparent singularities of linear
differential equations, and it gives rise to a surprisingly simple
desingularization algorithm.
","['\nShaoshi Chen\n', '\nManuel Kauers\n', '\nMichael F. Singer\n']",,,http://arxiv.org/abs/1408.5512v1,cs.SC,"['cs.SC', 'math.AC']",,,[]
Bounds for D-finite closure properties,http://arxiv.org/abs/1408.5514v1,2014-08-23T16:56:26Z,2014-08-23T16:56:26Z,"  We provide bounds on the size of operators obtained by algorithms for
executing D-finite closure properties. For operators of small order, we give
bounds on the degree and on the height (bit-size). For higher order operators,
we give degree bounds that are parameterized with respect to the order and
reflect the phenomenon that higher order operators may have lower degrees
(order-degree curves).
",['\nManuel Kauers\n'],,,http://arxiv.org/abs/1408.5514v1,cs.SC,"['cs.SC', 'math.CO', 'I.1.2']",,,[]
A Difference Ring Theory for Symbolic Summation,http://arxiv.org/abs/1408.2776v2,2014-08-12T16:54:37Z,2015-02-03T13:30:47Z,"  A summation framework is developed that enhances Karr's difference field
approach. It covers not only indefinite nested sums and products in terms of
transcendental extensions, but it can treat, e.g., nested products defined over
roots of unity. The theory of the so-called $R\Pi\Sigma^*$-extensions is
supplemented by algorithms that support the construction of such difference
rings automatically and that assist in the task to tackle symbolic summation
problems. Algorithms are presented that solve parameterized telescoping
equations, and more generally parameterized first-order difference equations,
in the given difference ring. As a consequence, one obtains algorithms for the
summation paradigms of telescoping and Zeilberger's creative telescoping. With
this difference ring theory one obtains a rigorous summation machinery that has
been applied to numerous challenging problems coming, e.g., from combinatorics
and particle physics.
",['\nCarsten Schneider\n'],"The environments are labelled differently, some parts are
  restructured, an index has been inserted, various typos are removed",,http://arxiv.org/abs/1408.2776v2,cs.SC,['cs.SC'],,,[]
"Solving Polynomial Equations with Equation Constraints: the
  Zero-dimensional Case",http://arxiv.org/abs/1408.3639v1,2014-08-15T20:03:21Z,2014-08-15T20:03:21Z,"  A zero-dimensional polynomial ideal may have a lot of complex zeros. But
sometimes, only some of them are needed. In this paper, for a zero-dimensional
ideal $I$, we study its complex zeros that locate in another variety
$\textbf{V}(J)$ where $J$ is an arbitrary ideal.
  The main problem is that for a point in $\textbf{V}(I) \cap
\textbf{V}(J)=\textbf{V}(I+J)$, its multiplicities w.r.t. $I$ and $I+J$ may be
different. Therefore, we cannot get the multiplicity of this point w.r.t. $I$
by studying $I + J$. A straightforward way is that first compute the points of
$\textbf{V}(I + J)$, then study their multiplicities w.r.t. $I$. But the former
step is difficult to realize exactly.
  In this paper, we propose a natural geometric explanation of the localization
of a polynomial ring corresponding to a semigroup order. Then, based on this
view, using the standard basis method and the border basis method, we introduce
a way to compute the complex zeros of $I$ in $\textbf{V}(J)$ with their
multiplicities w.r.t. $I$. As an application, we compute the sum of Milnor
numbers of the singular points on a polynomial hypersurface and work out all
the singular points on the hypersurface with their Milnor numbers.
",['\nYe Liang\n'],,,http://arxiv.org/abs/1408.3639v1,cs.SC,['cs.SC'],,,[]
"Certifying solutions to overdetermined and singular polynomial systems
  over Q",http://arxiv.org/abs/1408.2721v1,2014-08-12T14:18:37Z,2014-08-12T14:18:37Z,"  This paper is concerned with certifying that a given point is near an exact
root of an overdetermined or singular polynomial system with rational
coefficients. The difficulty lies in the fact that consistency of
overdetermined systems is not a continuous property. Our certification is based
on hybrid symbolic-numeric methods to compute the exact ""rational univariate
representation"" (RUR) of a component of the input system from approximate
roots. For overdetermined polynomial systems with simple roots, we compute an
initial RUR from approximate roots. The accuracy of the RUR is increased via
Newton iterations until the exact RUR is found, which we certify using exact
arithmetic. Since the RUR is well-constrained, we can use it to certify the
given approximate roots using alpha-theory. To certify isolated singular roots,
we use a determinantal form of the ""isosingular deflation"", which adds new
polynomials to the original system without introducing new variables. The
resulting polynomial system is overdetermined, but the roots are now simple,
thereby reducing the problem to the overdetermined case. We prove that our
algorithms have complexity that are polynomial in the input plus the output
size upon successful convergence, and we use worst case upper bounds for
termination when our iteration does not converge to an exact RUR. Examples are
included to demonstrate the approach.
","['\nTulay Ayyildiz Akoglu\n', '\nJonathan D. Hauenstein\n', '\nAgnes Szanto\n']",,,http://arxiv.org/abs/1408.2721v1,cs.SC,"['cs.SC', 'math.AG', 'math.NA']",,,[]
"Nested (inverse) binomial sums and new iterated integrals for massive
  Feynman diagrams",http://arxiv.org/abs/1407.4721v1,2014-07-17T16:11:42Z,2014-07-17T16:11:42Z,"  Nested sums containing binomial coefficients occur in the computation of
massive operator matrix elements. Their associated iterated integrals lead to
alphabets including radicals, for which we determined a suitable basis. We
discuss algorithms for converting between sum and integral representations,
mainly relying on the Mellin transform. To aid the conversion we worked out
dedicated rewrite rules, based on which also some general patterns emerging in
the process can be obtained.
","['\nJakob Ablinger\n', '\nJohannes Blümlein\n', '\nClemens G. Raab\n', '\nCarsten Schneider\n']","13 pages LATEX, one style file, Proceedings of Loops and Legs in
  Quantum Field Theory -- LL2014,27 April 2014 -- 02 May 2014 Weimar, Germany",,http://arxiv.org/abs/1407.4721v1,hep-th,"['hep-th', 'cs.SC']",,,[]
"The package HarmonicSums: Computer Algebra and Analytic aspects of
  Nested Sums",http://arxiv.org/abs/1407.6180v1,2014-07-23T11:33:32Z,2014-07-23T11:33:32Z,"  This paper summarizes the essential functionality of the computer algebra
package HarmonicSums. On the one hand HarmonicSums can work with nested sums
such as harmonic sums and their generalizations and on the other hand it can
treat iterated integrals of the Poincare and Chen-type, such as harmonic
polylogarithms and their generalizations. The interplay of these
representations and the analytic aspects are illustrated by concrete examples.
",['\nJakob Ablinger\n'],10 pages,PoS(LL2014)019,http://arxiv.org/abs/1407.6180v1,cs.SC,"['cs.SC', 'hep-ph']",,,[]
GCD Computation of n Integers,http://arxiv.org/abs/1407.6794v1,2014-07-25T06:55:58Z,2014-07-25T06:55:58Z,"  Greatest Common Divisor (GCD) computation is one of the most important
operation of algorithmic number theory. In this paper we present the algorithms
for GCD computation of $n$ integers. We extend the Euclid's algorithm and
binary GCD algorithm to compute the GCD of more than two integers.
",['\nShri Prakash Dwivedi\n'],RAECS 2014,,http://dx.doi.org/10.1109/RAECS.2014.6799612,cs.DS,"['cs.DS', 'cs.SC']",10.1109/RAECS.2014.6799612,,[]
"Resultant of an equivariant polynomial system with respect to the
  symmetric group",http://arxiv.org/abs/1407.2799v1,2014-07-10T14:23:28Z,2014-07-10T14:23:28Z,"  Given a system of n homogeneous polynomials in n variables which is
equivariant with respect to the canonical actions of the symmetric group of n
symbols on the variables and on the polynomials, it is proved that its
resultant can be decomposed into a product of several smaller resultants that
are given in terms of some divided differences. As an application, we obtain a
decomposition formula for the discriminant of a multivariate homogeneous
symmetric polynomial.
","['\nLaurent Busé\nINRIA Sophia Antipolis\n', '\nAnna Karasoulou\nAthens\n']",,,http://arxiv.org/abs/1407.2799v1,math.AC,"['math.AC', 'cs.SC']",,,"['INRIA Sophia Antipolis', 'Athens']"
"Rigorous uniform approximation of D-finite functions using Chebyshev
  expansions",http://arxiv.org/abs/1407.2802v1,2014-07-10T14:27:23Z,2014-07-10T14:27:23Z,"  A wide range of numerical methods exists for computing polynomial
approximations of solutions of ordinary differential equations based on
Chebyshev series expansions or Chebyshev interpolation polynomials. We consider
the application of such methods in the context of rigorous computing (where we
need guarantees on the accuracy of the result), and from the complexity point
of view. It is well-known that the order-n truncation of the Chebyshev
expansion of a function over a given interval is a near-best uniform polynomial
approximation of the function on that interval. In the case of solutions of
linear differential equations with polynomial coefficients, the coefficients of
the expansions obey linear recurrence relations with polynomial coefficients.
Unfortunately, these recurrences do not lend themselves to a direct recursive
computation of the coefficients, owing among other things to a lack of initial
conditions. We show how they can nevertheless be used, as part of a validated
process, to compute good uniform approximations of D-finite functions together
with rigorous error bounds, and we study the complexity of the resulting
algorithms. Our approach is based on a new view of a classical numerical method
going back to Clenshaw, combined with a functional enclosure method.
","['\nAlexandre Benoit\nLAAS\n', '\nMioara Joldes\nLAAS\n', '\nMarc Mezzarobba\nLIP6\n']",,,http://arxiv.org/abs/1407.2802v1,cs.SC,"['cs.SC', 'cs.NA']",,,"['LAAS', 'LAAS', 'LIP6']"
Survey on counting special types of polynomials,http://arxiv.org/abs/1407.2970v1,2014-07-10T21:25:48Z,2014-07-10T21:25:48Z,"  Most integers are composite and most univariate polynomials over a finite
field are reducible. The Prime Number Theorem and a classical result of
Gau{\ss} count the remaining ones, approximately and exactly.
  For polynomials in two or more variables, the situation changes dramatically.
Most multivariate polynomials are irreducible. This survey presents counting
results for some special classes of multivariate polynomials over a finite
field, namely the the reducible ones, the s-powerful ones (divisible by the
s-th power of a nonconstant polynomial), the relatively irreducible ones
(irreducible but reducible over an extension field), the decomposable ones, and
also for reducible space curves. These come as exact formulas and as
approximations with relative errors that essentially decrease exponentially in
the input size.
  Furthermore, a univariate polynomial f is decomposable if f = g o h for some
nonlinear polynomials g and h. It is intuitively clear that the decomposable
polynomials form a small minority among all polynomials. The tame case, where
the characteristic p of Fq does not divide n = deg f, is fairly
well-understood, and we obtain closely matching upper and lower bounds on the
number of decomposable polynomials. In the wild case, where p does divide n,
the bounds are less satisfactory, in particular when p is the smallest prime
divisor of n and divides n exactly twice. The crux of the matter is to count
the number of collisions, where essentially different (g, h) yield the same f.
We present a classification of all collisions at degree n = p^2 which yields an
exact count of those decomposable polynomials.
","['\nJoachim von zur Gathen\n', '\nKonstantin Ziegler\n']","to appear in Jaime Gutierrez, Josef Schicho & Martin Weimann
  (editors), Computer Algebra and Polynomials, Lecture Notes in Computer
  Science",,http://arxiv.org/abs/1407.2970v1,math.AC,"['math.AC', 'cs.SC', '11T06 (Primary), 05A15, 12Y05 (Secondary)', 'F.2.1']",,,[]
"Recent Symbolic Summation Methods to Solve Coupled Systems of
  Differential and Difference Equations",http://arxiv.org/abs/1407.2537v1,2014-07-09T15:51:47Z,2014-07-09T15:51:47Z,"  We outline a new algorithm to solve coupled systems of differential equations
in one continuous variable $x$ (resp. coupled difference equations in one
discrete variable $N$) depending on a small parameter $\epsilon$: given such a
system and given sufficiently many initial values, we can determine the first
coefficients of the Laurent-series solutions in $\epsilon$ if they are
expressible in terms of indefinite nested sums and products. This systematic
approach is based on symbolic summation algorithms in the context of difference
rings/fields and uncoupling algorithms. The proposed method gives rise to new
interesting applications in connection with integration by parts (IBP) methods.
As an illustrative example, we will demonstrate how one can calculate the
$\epsilon$-expansion of a ladder graph with 6 massive fermion lines.
","['\nJohannes Bluemlein\n', '\nAbilio De Freitas\n', '\nCarsten Schneider\n']",,,http://arxiv.org/abs/1407.2537v1,cs.SC,"['cs.SC', 'hep-ph', 'math-ph', 'math.MP']",,,[]
Determining surfaces of revolution from their implicit equations,http://arxiv.org/abs/1407.2723v1,2014-07-10T08:24:59Z,2014-07-10T08:24:59Z,"  Results of number of geometric operations (often used in technical practise,
as e.g. the operation of blending) are in many cases surfaces described
implicitly. Then it is a challenging task to recognize the type of the obtained
surface, find its characteristics and for the rational surfaces compute also
their parameterizations. In this contribution we will focus on surfaces of
revolution. These objects, widely used in geometric modelling, are generated by
rotating a generatrix around a given axis. If the generatrix is an algebraic
curve then so is also the resulting surface, described uniquely by a polynomial
which can be found by some well-established implicitation technique. However,
starting from a polynomial it is not known how to decide if the corresponding
algebraic surface is rotational or not. Motivated by this, our goal is to
formulate a simple and efficient algorithm whose input is a polynomial with the
coefficients from some subfield of $\mathbb{R}$ and the output is the answer
whether the shape is a surface of revolution. In the affirmative case we also
find the equations of its axis and generatrix. Furthermore, we investigate the
problem of rationality and unirationality of surfaces of revolution and show
that this question can be efficiently answered discussing the rationality of a
certain associated planar curve.
","['\nJan Vršek\nNTIS -- New Technologies for the Information Society, Faculty of Applied Sciences, University of West Bohemia, Plzeň, Czech Republic\n', '\nMiroslav Lávička\nNTIS -- New Technologies for the Information Society, Faculty of Applied Sciences, University of West Bohemia, Plzeň, Czech Republic\nDepartment of Mathematics, Faculty of Applied Sciences, University of West Bohemia, Plzeň, Czech Republic\n']",,,http://arxiv.org/abs/1407.2723v1,cs.SC,"['cs.SC', 'cs.GR', 'math.AG']",,,"['NTIS -- New Technologies for the Information Society, Faculty of Applied Sciences, University of West Bohemia, Plzeň, Czech Republic', 'NTIS -- New Technologies for the Information Society, Faculty of Applied Sciences, University of West Bohemia, Plzeň, Czech Republic', 'Department of Mathematics, Faculty of Applied Sciences, University of West Bohemia, Plzeň, Czech Republic']"
Even faster integer multiplication,http://arxiv.org/abs/1407.3360v1,2014-07-12T07:54:52Z,2014-07-12T07:54:52Z,"  We give a new proof of F\""urer's bound for the cost of multiplying n-bit
integers in the bit complexity model. Unlike F\""urer, our method does not
require constructing special coefficient rings with ""fast"" roots of unity.
Moreover, we prove the more explicit bound O(n log n K^(log^* n))$ with K = 8.
We show that an optimised variant of F\""urer's algorithm achieves only K = 16,
suggesting that the new algorithm is faster than F\""urer's by a factor of
2^(log^* n). Assuming standard conjectures about the distribution of Mersenne
primes, we give yet another algorithm that achieves K = 4.
","['\nDavid Harvey\n', '\nJoris van der Hoeven\n', '\nGrégoire Lecerf\n']",,,http://arxiv.org/abs/1407.3360v1,cs.CC,"['cs.CC', 'cs.SC', 'math.NT', '68W30, 68Q17, 68W40', 'G.1.0; F.2.1']",,,[]
Faster polynomial multiplication over finite fields,http://arxiv.org/abs/1407.3361v1,2014-07-12T07:57:14Z,2014-07-12T07:57:14Z,"  Let p be a prime, and let M_p(n) denote the bit complexity of multiplying two
polynomials in F_p[X] of degree less than n. For n large compared to p, we
establish the bound M_p(n) = O(n log n 8^(log^* n) log p), where log^* is the
iterated logarithm. This is the first known F\""urer-type complexity bound for
F_p[X], and improves on the previously best known bound M_p(n) = O(n log n log
log n log p).
","['\nDavid Harvey\n', '\nJoris van der Hoeven\n', '\nGrégoire Lecerf\n']",,,http://arxiv.org/abs/1407.3361v1,cs.CC,"['cs.CC', 'cs.SC', 'math.NT', '68W30, 68Q17, 68W40', 'G.1.0; F.2.1']",,,[]
"An Algorithm for Deciding the Summability of Bivariate Rational
  Functions",http://arxiv.org/abs/1408.2473v1,2014-06-26T02:38:19Z,2014-06-26T02:38:19Z,"  Let $\Delta_x f(x,y)=f(x+1,y)-f(x,y)$ and $\Delta_y f(x,y)=f(x,y+1)-f(x,y)$
be the difference operators with respect to $x$ and $y$. A rational function
$f(x,y)$ is called summable if there exist rational functions $g(x,y)$ and
$h(x,y)$ such that $f(x,y)=\Delta_x g(x,y) + \Delta_y h(x,y)$. Recently, Chen
and Singer presented a method for deciding whether a rational function is
summable. To implement their method in the sense of algorithms, we need to
solve two problems. The first is to determine the shift equivalence of two
bivariate polynomials. We solve this problem by presenting an algorithm for
computing the dispersion sets of any two bivariate polynomials. The second is
to solve a univariate difference equation in an algebraically closed field. By
considering the irreducible factorization of the denominator of $f(x,y)$ in a
general field, we present a new criterion which requires only finding a
rational solution of a bivariate difference equation. This goal can be achieved
by deriving a universal denominator of the rational solutions and a degree
bound on the numerator. Combining these two algorithms, we can decide the
summability of a bivariate rational function.
","['\nQing-Hu Hou\n', '\nRong-Hua Wang\n']",18 pages,,http://arxiv.org/abs/1408.2473v1,cs.SC,"['cs.SC', 'math.CA', '33F10, 39A04, 68W30']",,,[]
"A New Primitive for a Diffie-Hellman-like Key Exchange Protocol Based on
  Multivariate Ore Polynomials",http://arxiv.org/abs/1407.1270v4,2014-07-04T16:15:50Z,2015-05-19T18:32:37Z,"  In this paper we present a new primitive for a key exchange protocol based on
multivariate non-commutative polynomial rings, analogous to the classic
Diffie-Hellman method. Our technique extends the proposed scheme of Boucher et
al. from 2010. Their method was broken by Dubois and Kammerer in 2011, who
exploited the Euclidean domain structure of the chosen ring. However, our
proposal is immune against such attacks, without losing the advantages of
non-commutative polynomial rings as outlined by Boucher et al. Moreover, our
extension is not restricted to any particular ring, but is designed to allow
users to readily choose from a large class of rings when applying the protocol.
Our primitive can also be applied to other cryptographic paradigms. In
particular, we develop a three-pass protocol, a public key cryptosystem, a
digital signature scheme and a zero-knowledge proof protocol.
","['\nReinhold Burger\n', '\nAlbert Heinle\n']",,,http://arxiv.org/abs/1407.1270v4,cs.CR,"['cs.CR', 'cs.SC', 'math.RA', '94A60, 68P25, 47N99']",,,[]
Elements of Design for Containers and Solutions in the LinBox Library,http://arxiv.org/abs/1407.3262v1,2014-06-25T19:38:09Z,2014-06-25T19:38:09Z,"  We describe in this paper new design techniques used in the \cpp exact linear
algebra library \linbox, intended to make the library safer and easier to use,
while keeping it generic and efficient. First, we review the new simplified
structure for containers, based on our \emph{founding scope allocation} model.
We explain design choices and their impact on coding: unification of our matrix
classes, clearer model for matrices and submatrices, \etc Then we present a
variation of the \emph{strategy} design pattern that is comprised of a
controller--plugin system: the controller (solution) chooses among plug-ins
(algorithms) that always call back the controllers for subtasks. We give
examples using the solution \mul. Finally we present a benchmark architecture
that serves two purposes: Providing the user with easier ways to produce
graphs; Creating a framework for automatically tuning the library and
supporting regression testing.
","['\nBrice Boyer\nLJK\n', '\nJean-Guillaume Dumas\nLJK\n', '\nPascal Giorgi\nLIRMM\n', ""\nClément Pernet\nINRIA Grenoble Rhône-Alpes / LIG Laboratoire d'Informatique de Grenoble\n"", '\nB. David Saunders\nCIS\n']","8 pages, 4th International Congress on Mathematical Software, Seoul :
  Korea, Republic Of (2014)",,http://arxiv.org/abs/1407.3262v1,cs.MS,"['cs.MS', 'cs.SC', 'cs.SE']",,,"['LJK', 'LJK', 'LIRMM', ""INRIA Grenoble Rhône-Alpes / LIG Laboratoire d'Informatique de Grenoble"", 'CIS']"
Strongly stable ideals and Hilbert polynomials,http://arxiv.org/abs/1406.6924v2,2014-06-26T15:35:00Z,2018-11-05T07:59:45Z,"  The \texttt{StronglyStableIdeals} package for \textit{Macaulay2} provides a
method to compute all saturated strongly stable ideals in a given polynomial
ring with a fixed Hilbert polynomial. A description of the main method and
auxiliary tools is given.
","['\nDavide Alberelli\n', '\nPaolo Lella\n']",Source code available as an ancillary file. Final version,J. Softw. Alg. Geom. 9 (2019) 1-9,http://dx.doi.org/10.2140/jsag.2019.9.1,cs.SC,"['cs.SC', 'cs.MS', 'math.AC', 'math.AG', 'math.CO', '13P10, 13P99']",10.2140/jsag.2019.9.1,,[]
A novel approach to integration by parts reduction,http://arxiv.org/abs/1406.4513v2,2014-06-17T20:00:24Z,2015-09-21T12:36:02Z,"  Integration by parts reduction is a standard component of most modern
multi-loop calculations in quantum field theory. We present a novel strategy
constructed to overcome the limitations of currently available reduction
programs based on Laporta's algorithm. The key idea is to construct algebraic
identities from numerical samples obtained from reductions over finite fields.
We expect the method to be highly amenable to parallelization, show a low
memory footprint during the reduction step, and allow for significantly better
run-times.
","['\nAndreas von Manteuffel\n', '\nRobert M. Schabinger\n']","4 pages. Version 2 is the final, published version of this article",,http://dx.doi.org/10.1016/j.physletb.2015.03.029,hep-ph,"['hep-ph', 'cs.SC', 'hep-th']",10.1016/j.physletb.2015.03.029,,[]
Gröbner Bases for Linearized Polynomials,http://arxiv.org/abs/1406.4600v1,2014-06-18T05:25:46Z,2014-06-18T05:25:46Z,"  In this work we develop the theory of Gr\""obner bases for modules over the
ring of univariate linearized polynomials with coefficients from a finite
field.
","['\nMargreta Kuijper\n', '\nAnna-Lena Trautmann\n']",,,http://arxiv.org/abs/1406.4600v1,cs.SC,"['cs.SC', 'cs.IT', 'math.IT', 'math.RA']",,,[]
Hierarchical Comprehensive Triangular Decomposition,http://arxiv.org/abs/1406.0599v1,2014-06-03T07:13:17Z,2014-06-03T07:13:17Z,"  The concept of comprehensive triangular decomposition (CTD) was first
introduced by Chen et al. in their CASC'2007 paper and could be viewed as an
analogue of comprehensive Grobner systems for parametric polynomial systems.
The first complete algorithm for computing CTD was also proposed in that paper
and implemented in the RegularChains library in Maple. Following our previous
work on generic regular decomposition for parametric polynomial systems, we
introduce in this paper a so-called hierarchical strategy for computing CTDs.
Roughly speaking, for a given parametric system, the parametric space is
divided into several sub-spaces of different dimensions and we compute CTDs
over those sub-spaces one by one. So, it is possible that, for some benchmarks,
it is difficult to compute CTDs in reasonable time while this strategy can
obtain some ""partial"" solutions over some parametric sub-spaces. The program
based on this strategy has been tested on a number of benchmarks from the
literature. Experimental results on these benchmarks with comparison to
RegularChains are reported and may be valuable for developing more efficient
triangularization tools.
","['\nZhenghong Chen\n', '\nXiaoxian Tang\n', '\nBican Xia\n']",,,http://arxiv.org/abs/1406.0599v1,cs.SC,['cs.SC'],,,[]
Computing GCRDs of Approximate Differential Polynomials,http://arxiv.org/abs/1406.0907v2,2014-06-03T23:52:08Z,2014-07-23T23:23:11Z,"  Differential (Ore) type polynomials with approximate polynomial coefficients
are introduced. These provide a useful representation of approximate
differential operators with a strong algebraic structure, which has been used
successfully in the exact, symbolic, setting. We then present an algorithm for
the approximate Greatest Common Right Divisor (GCRD) of two approximate
differential polynomials, which intuitively is the differential operator whose
solutions are those common to the two inputs operators. More formally, given
approximate differential polynomials $f$ and $g$, we show how to find ""nearby""
polynomials $\widetilde f$ and $\widetilde g$ which have a non-trivial GCRD.
Here ""nearby"" is under a suitably defined norm. The algorithm is a
generalization of the SVD-based method of Corless et al. (1995) for the
approximate GCD of regular polynomials. We work on an appropriately
""linearized"" differential Sylvester matrix, to which we apply a block SVD. The
algorithm has been implemented in Maple and a demonstration of its robustness
is presented.
","['\nMark Giesbrecht\n', '\nJoseph Haraldson\n']","To appear, Workshop on Symbolic-Numeric Computing (SNC'14) July 2014",,http://arxiv.org/abs/1406.0907v2,cs.SC,"['cs.SC', 'cs.NA', 'I.1']",,,[]
Covering Rational Ruled Surfaces,http://arxiv.org/abs/1406.2140v3,2014-06-09T11:22:34Z,2014-10-07T09:35:07Z,"  We present an algorithm that covers any given rational ruled surface with two
rational parametrizations. In addition, we present an algorithm that transforms
any rational surface parametrization into a new rational surface
parametrization without affine base points and such that the degree of the
corresponding maps is preserved.
","['\nJ. Rafael Sendra\n', '\nDavid Sevilla\n', '\nCarlos Villarino\n']","19 pages, 2 figures in jpg. v2: minor correction of Example 1. v3:
  updated acknowledgements",,http://arxiv.org/abs/1406.2140v3,math.AG,"['math.AG', 'cs.SC']",,,[]
"Solving the ""Isomorphism of Polynomials with Two Secrets"" Problem for
  all Pairs of Quadratic Forms",http://arxiv.org/abs/1406.3163v3,2014-06-12T09:26:31Z,2014-12-22T07:49:56Z,"  We study the Isomorphism of Polynomial (IP2S) problem with m=2 homogeneous
quadratic polynomials of n variables over a finite field of odd characteristic:
given two quadratic polynomials (a, b) on n variables, we find two bijective
linear maps (s,t) such that b=t . a . s. We give an algorithm computing s and t
in time complexity O~(n^4) for all instances, and O~(n^3) in a dominant set of
instances.
  The IP2S problem was introduced in cryptography by Patarin back in 1996. The
special case of this problem when t is the identity is called the isomorphism
with one secret (IP1S) problem. Generic algebraic equation solvers (for example
using Gr\""obner bases) solve quite well random instances of the IP1S problem.
For the particular cyclic instances of IP1S, a cubic-time algorithm was later
given and explained in terms of pencils of quadratic forms over all finite
fields; in particular, the cyclic IP1S problem in odd characteristic reduces to
the computation of the square root of a matrix.
  We give here an algorithm solving all cases of the IP1S problem in odd
characteristic using two new tools, the Kronecker form for a singular quadratic
pencil, and the reduction of bilinear forms over a non-commutative algebra.
Finally, we show that the second secret in the IP2S problem may be recovered in
cubic time.
","['\nJérôme Plût\n', '\nPierre-Alain Fouque\n', '\nGilles Macario-Rat\n']",,,http://arxiv.org/abs/1406.3163v3,cs.SC,"['cs.SC', 'cs.CR', '15A22 (Primary), 15A21, 15A63, 11T71 (Secondary)', 'F.2.1']",,,[]
A comparison of three heuristics to choose the variable ordering for CAD,http://arxiv.org/abs/1405.6082v1,2014-05-23T14:50:14Z,2014-05-23T14:50:14Z,"  Cylindrical algebraic decomposition (CAD) is a key tool for problems in real
algebraic geometry and beyond. When using CAD there is often a choice over the
variable ordering to use, with some problems infeasible in one ordering but
simple in another. Here we discuss a recent experiment comparing three
heuristics for making this choice on thousands of examples.
","['\nZongyan Huang\n', '\nMatthew England\n', '\nDavid Wilson\n', '\nJames H. Davenport\n', '\nLawrence C. Paulson\n']",,"ACM Communications in Computer Algebra 48:3 (issue 189), pp.
  121-123, ACM, 2014",http://dx.doi.org/10.1145/2733693.2733706,cs.SC,"['cs.SC', '68W30, O3C10', 'I.1.2']",10.1145/2733693.2733706,,[]
"Using the Regular Chains Library to build cylindrical algebraic
  decompositions by projecting and lifting",http://arxiv.org/abs/1405.6090v1,2014-05-23T15:05:37Z,2014-05-23T15:05:37Z,"  Cylindrical algebraic decomposition (CAD) is an important tool, both for
quantifier elimination over the reals and a range of other applications.
Traditionally, a CAD is built through a process of projection and lifting to
move the problem within Euclidean spaces of changing dimension. Recently, an
alternative approach which first decomposes complex space using triangular
decomposition before refining to real space has been introduced and implemented
within the RegularChains Library of Maple. We here describe a freely available
package ProjectionCAD which utilises the routines within the RegularChains
Library to build CADs by projection and lifting. We detail how the projection
and lifting algorithms were modified to allow this, discuss the motivation and
survey the functionality of the package.
","['\nMatthew England\n', '\nDavid Wilson\n', '\nRussell Bradford\n', '\nJames H. Davenport\n']",,"H. Hong and C. Yap, eds. Mathematical Software - ICMS 2014, pp.
  458-465. (Lecture Notes in Computer Science, 8592). Springer, 2014",http://dx.doi.org/10.1007/978-3-662-44199-2_69,cs.SC,"['cs.SC', '68W30, 03C10', 'G.4; I.1.2']",10.1007/978-3-662-44199-2_69,,[]
"Choosing a variable ordering for truth-table invariant cylindrical
  algebraic decomposition by incremental triangular decomposition",http://arxiv.org/abs/1405.6094v1,2014-05-23T15:18:19Z,2014-05-23T15:18:19Z,"  Cylindrical algebraic decomposition (CAD) is a key tool for solving problems
in real algebraic geometry and beyond. In recent years a new approach has been
developed, where regular chains technology is used to first build a
decomposition in complex space. We consider the latest variant of this which
builds the complex decomposition incrementally by polynomial and produces CADs
on whose cells a sequence of formulae are truth-invariant. Like all CAD
algorithms the user must provide a variable ordering which can have a profound
impact on the tractability of a problem. We evaluate existing heuristics to
help with the choice for this algorithm, suggest improvements and then derive a
new heuristic more closely aligned with the mechanics of the new algorithm.
","['\nMatthew England\n', '\nRussell Bradford\n', '\nJames H. Davenport\n', '\nDavid Wilson\n']",,"H. Hong and C. Yap, eds. Mathematical Software - ICMS 2014, pp.
  450-457. (Lecture Notes in Computer Science, 8592). Springer, 2014",http://dx.doi.org/10.1007/978-3-662-44199-2_68,cs.SC,"['cs.SC', '68W30, 03C10', 'I.1.2']",10.1007/978-3-662-44199-2_68,,[]
On the design of an expert help system for computer algebra systems,http://arxiv.org/abs/1405.6885v2,2014-05-27T12:33:27Z,2014-05-28T21:30:46Z,"  It is our intention here only to discuss the nature, complexity and tools
concerning the design of Smart Help, an expert help facility for aiding users
of Computer Algebra Systems. Although the expert help system presented here has
been particularly oriented to REDUCE (as a consequence of our former experience
with this system), we point out that the concept of Smart Help can be extended
to other Computer Algebra Systems. Technically, Smart Help is a Production
System on the top of a particular implementation of MANTRA, a hybrid knowledge
representation system, which has REDUCE integrated as an additional knowledge
representation module. Since the heuristic level of MANTRA has not yet been
implemented, being presently represented by the Lisp language itself, Smart
Help is coded in Lisp and resides in the same Lisp session of MANTRA. A
prototype of Smart Help is now running on a SUN work-station on an experimental
basis.
","['\nRenato P. dos Santos\n', '\nWaldir L. Roque\n']",4 pages; fixed author names,ACM SIGSAM Bulletin (1990) 24(4):22-25,http://dx.doi.org/10.1145/101108.101110,cs.SC,"['cs.SC', 'gr-qc', 'I.1; J.2']",10.1145/101108.101110,,[]
"Geometric involutive bases for positive dimensional polynomial ideals
  and SDP methods",http://arxiv.org/abs/1405.7115v1,2014-05-28T04:31:43Z,2014-05-28T04:31:43Z,"  Geometric involutive bases for polynomial systems of equations have their
origin in the prolongation and projection methods of the geometers Cartan and
Kuranishi for systems of PDE. They are useful for numerical ideal membership
testing and the solution of polynomial systems. In this paper we further
develop our symbolic-numeric methods for such bases. We give methods to
explicitly extract and decrease the degree of intermediate systems and the
output basis. Algorithms for the numerical computation of involutivity criteria
for positive dimensional ideals are also discussed.
  We were also motivated by some remarkable recent work by Lasserre and
collaborators who employed our prolongation projection involutive criteria as a
part of their semi-definite based programming (SDP) method for identifying the
real radical of zero dimensional polynomial ideals. Consequently in this paper
we begin an exploration of the interaction between geometric involutive bases
and these methods particularly in the positive dimensional case. Motivated by
the extension of these methods to the positive dimensional case we explore the
interplay between geometric involutive bases and the new SDP methods.
","['\nGreg Reid\n', '\nFei Wang\n', '\nWenyuan Wu\n']",,,http://arxiv.org/abs/1405.7115v1,math.AG,"['math.AG', 'cs.SC']",,,[]
"Fast Kötter-Nielsen-Høholdt Interpolation in the Guruswami-Sudan
  Algorithm",http://arxiv.org/abs/1406.0053v1,2014-05-31T07:57:56Z,2014-05-31T07:57:56Z,"  The K\""otter-Nielsen-H{\o}holdt algorithm is a popular way to construct the
bivariate interpolation polynomial in the Guruswami-Sudan decoding algorithm
for Reed-Solomon codes. In this paper, we show how one can use Divide & Conquer
techniques to provide an asymptotic speed-up of the algorithm, rendering its
complexity quasi-linear in n. Several of our observations can also provide a
practical speed-up to the classical version of the algorithm.
",['\nJohan S. R. Nielsen\n'],Submitted to ACCT-14,,http://arxiv.org/abs/1406.0053v1,cs.IT,"['cs.IT', 'cs.SC', 'math.IT']",,,[]
Formulating problems for real algebraic geometry,http://arxiv.org/abs/1405.3461v1,2014-05-14T11:43:13Z,2014-05-14T11:43:13Z,"  We discuss issues of problem formulation for algorithms in real algebraic
geometry, focussing on quantifier elimination by cylindrical algebraic
decomposition. We recall how the variable ordering used can have a profound
effect on both performance and output and summarise what may be done to assist
with this choice. We then survey other questions of problem formulation and
algorithm optimisation that have become pertinent following advances in CAD
theory, including both work that is already published and work that is
currently underway. With implementations now in reach of real world
applications and new theory meaning algorithms are far more sensitive to the
input, our thesis is that intelligently formulating problems for algorithms,
and indeed choosing the correct algorithm variant for a problem, is key to
improving the practical use of both quantifier elimination and symbolic real
algebraic geometry in general.
",['\nMatthew England\n'],"To be presented at The ""Encuentros de \'Algebra Computacional y
  Aplicaciones, EACA 2014"" (Meetings on Computer Algebra and Applications) in
  Barcelona","Proceedings XIV Encuentros de \'Algebra Computacional y
  Aplicaciones (EACA '14), pp. 107-110, 2014",http://arxiv.org/abs/1405.3461v1,cs.SC,"['cs.SC', '68W30, 03C10', 'I.1.2']",,,[]
"On the Efficiency of Solving Boolean Polynomial Systems with the
  Characteristic Set Method",http://arxiv.org/abs/1405.4596v4,2014-05-19T04:23:42Z,2019-11-11T01:35:13Z,"  An improved characteristic set algorithm for solving Boolean polynomial
systems is proposed. This algorithm is based on the idea of converting all the
polynomials into monic ones by zero decomposition, and using additions to
obtain pseudo-remainders. Three important techniques are applied in the
algorithm. The first one is eliminating variables by new generated linear
polynomials. The second one is optimizing the strategy of choosing polynomial
for zero decomposition. The third one is to compute add-remainders to eliminate
the leading variable of new generated monic polynomials. By analyzing the depth
of the zero decomposition tree, we present some complexity bounds of this
algorithm, which are lower than the complexity bounds of previous
characteristic set algorithms. Extensive experimental results show that this
new algorithm is more efficient than previous characteristic set algorithms for
solving Boolean polynomial systems.
","['\nZhenyu Huang\n', '\nYao Sun\n', '\nDongdai Lin\n']",,,http://dx.doi.org/10.1016/j.jsc.2019.11.001,cs.SC,['cs.SC'],10.1016/j.jsc.2019.11.001,,[]
Una metodología para realizar Diferenciación Automática Anidada,http://arxiv.org/abs/1405.4779v1,2014-05-19T16:01:18Z,2014-05-19T16:01:18Z,"  En este trabajo se presenta una propuesta para realizar Diferenciaci\'on
Autom\'atica Anidada utilizando cualquier biblioteca de Diferenciaci\'on
Autom\'atica que permita sobrecarga de operadores. Para calcular las derivadas
anidadas en una misma evaluaci\'on de la funci\'on, la cual se asume que sea
anal\'itica, se trabaja con el modo forward utilizando una nueva estructura
llamada SuperAdouble, que garantiza que se aplique correctamente la
Diferenciaci\'on Autom\'atica y se calculen el valor y la derivada que se
requiera.
  This paper proposes a framework to apply Nested Automatic Differentiation
using any library of Automatic Differentiation which allows operator
overloading. To compute nested derivatives of a function while it is being
evaluated, which is assumed to be analytic, a new structure called SuperAdouble
is used in the forward mode. This new class guarantees the correct application
of Automatic Differentiation to calculate the value and derivative of a
function where is required.
","['\nJuan Luis Valerdi\n', '\nFernando Raul Rodriguez\n']","11 pages, in Spanish",,http://arxiv.org/abs/1405.4779v1,cs.SC,"['cs.SC', 'G.1.4']",,,[]
Cylindrical Algebraic Decomposition Using Local Projections,http://arxiv.org/abs/1405.4925v1,2014-05-20T00:49:15Z,2014-05-20T00:49:15Z,"  We present an algorithm which computes a cylindrical algebraic decomposition
of a semialgebraic set using projection sets computed for each cell separately.
Such local projection sets can be significantly smaller than the global
projection set used by the Cylindrical Algebraic Decomposition (CAD) algorithm.
This leads to reduction in the number of cells the algorithm needs to
construct. We give an empirical comparison of our algorithm and the classical
CAD algorithm.
",['\nAdam Strzebonski\n'],,,http://arxiv.org/abs/1405.4925v1,cs.SC,"['cs.SC', 'I.1.2; G.4']",,,[]
"A fast algorithm for computing the characteristic polynomial of the
  p-curvature",http://arxiv.org/abs/1405.5341v1,2014-05-21T09:05:33Z,2014-05-21T09:05:33Z,"  We discuss theoretical and algorithmic questions related to the $p$-curvature
of differential operators in characteristic $p$. Given such an operator $L$,
and denoting by $\Chi(L)$ the characteristic polynomial of its $p$-curvature,
we first prove a new, alternative, description of $\Chi(L)$. This description
turns out to be particularly well suited to the fast computation of $\Chi(L)$
when $p$ is large: based on it, we design a new algorithm for computing
$\Chi(L)$, whose cost with respect to $p$ is $\softO(p^{0.5})$ operations in
the ground field. This is remarkable since, prior to this work, the fastest
algorithms for this task, and even for the subtask of deciding nilpotency of
the $p$-curvature, had merely slightly subquadratic complexity
$\softO(p^{1.79})$.
","['\nAlin Bostan\nINRIA Saclay - Ile de France\n', '\nXavier Caruso\nIRMAR\n', '\nÉric Schost\n']","ISSAC - 39th International Symposium on Symbolic and Algebraic
  Computation (2014)",,http://dx.doi.org/10.1145/2608628.2608650,cs.SC,['cs.SC'],10.1145/2608628.2608650,,"['INRIA Saclay - Ile de France', 'IRMAR']"
"Computing necessary integrability conditions for planar parametrized
  homogeneous potentials",http://arxiv.org/abs/1405.5342v1,2014-05-21T09:07:15Z,2014-05-21T09:07:15Z,"  Let $V\in\mathbb{Q}(i)(\a_1,\dots,\a_n)(\q_1,\q_2)$ be a rationally
parametrized planar homogeneous potential of homogeneity degree $k\neq -2, 0,
2$. We design an algorithm that computes polynomial \emph{necessary} conditions
on the parameters $(\a_1,\dots,\a_n)$ such that the dynamical system associated
to the potential $V$ is integrable. These conditions originate from those of
the Morales-Ramis-Sim\'o integrability criterion near all Darboux points. The
implementation of the algorithm allows to treat applications that were out of
reach before, for instance concerning the non-integrability of polynomial
potentials up to degree $9$. Another striking application is the first complete
proof of the non-integrability of the \emph{collinear three body problem}.
","['\nAlin Bostan\nINRIA Saclay - Ile de France\n', '\nThierry Combot\nIMB\n', '\nSafey El Din Mohab\nUPMC\n']","ISSAC'14 - International Symposium on Symbolic and Algebraic
  Computation (2014)",,http://dx.doi.org/10.1145/2608628.2608662,cs.SC,['cs.SC'],10.1145/2608628.2608662,,"['INRIA Saclay - Ile de France', 'IMB', 'UPMC']"
"Improved algorithm for computing separating linear forms for bivariate
  systems",http://arxiv.org/abs/1405.4740v1,2014-05-19T14:31:22Z,2014-05-19T14:31:22Z,"  We address the problem of computing a linear separating form of a system of
two bivariate polynomials with integer coefficients, that is a linear
combination of the variables that takes different values when evaluated at the
distinct solutions of the system. The computation of such linear forms is at
the core of most algorithms that solve algebraic systems by computing rational
parameterizations of the solutions and this is the bottleneck of these
algorithms in terms of worst-case bit complexity. We present for this problem a
new algorithm of worst-case bit complexity $\sOB(d^7+d^6\tau)$ where $d$ and
$\tau$ denote respectively the maximum degree and bitsize of the input (and
where $\sO$ refers to the complexity where polylogarithmic factors are omitted
and $O_B$ refers to the bit complexity). This algorithm simplifies and
decreases by a factor $d$ the worst-case bit complexity presented for this
problem by Bouzidi et al. \cite{bouzidiJSC2014a}. This algorithm also yields,
for this problem, a probabilistic Las-Vegas algorithm of expected bit
complexity $\sOB(d^5+d^4\tau)$.
","['\nYacine Bouzidi\nINRIA Nancy - Grand Est / LORIA\n', '\nSylvain Lazard\nINRIA Nancy - Grand Est / LORIA\n', '\nGuillaume Moroz\nINRIA Nancy - Grand Est / LORIA\n', '\nMarc Pouget\nINRIA Sophia Antipolis\n', '\nFabrice Rouillier\nIMJ, INRIA Paris-Rocquencourt\n']","ISSAC - 39th International Symposium on Symbolic and Algebraic
  Computation (2014)",,http://arxiv.org/abs/1405.4740v1,cs.CG,"['cs.CG', 'cs.SC']",,,"['INRIA Nancy - Grand Est / LORIA', 'INRIA Nancy - Grand Est / LORIA', 'INRIA Nancy - Grand Est / LORIA', 'INRIA Sophia Antipolis', 'IMJ, INRIA Paris-Rocquencourt']"
Numerical Hilbert functions for Macaulay2,http://arxiv.org/abs/1405.5293v1,2014-05-21T04:17:25Z,2014-05-21T04:17:25Z,"  The NumericalHilbert package for Macaulay2 includes algorithms for computing
local dual spaces of polynomial ideals, and related local combinatorial data
about its scheme structure. These techniques are numerically stable, and can be
used with floating point arithmetic over the complex numbers. They provide a
viable alternative in this setting to purely symbolic methods such as standard
bases. In particular, these methods can be used to compute initial ideals,
local Hilbert functions and Hilbert regularity.
",['\nRobert Krone\n'],5 pages,,http://arxiv.org/abs/1405.5293v1,math.AC,"['math.AC', 'cs.SC', 'math.AG', '14Q99, 68N01']",,,[]
Border Bases for Polynomial Rings over Noetherian Rings,http://arxiv.org/abs/1405.0472v5,2014-05-02T18:49:07Z,2017-02-02T16:04:01Z,"  The theory of border bases for zero-dimensional ideals has attracted several
researchers in symbolic computation due to their numerical stability and
mathematical elegance. As shown in (Francis & Dukkipati, J. Symb. Comp., 2014),
one can extend the concept of border bases over Noetherian rings whenever the
corresponding residue class ring is finitely generated and free. In this paper
we address the following problem: Can the concept of border basis over
Noetherian rings exists for ideals when the corresponding residue class rings
are finitely generated but need not necessarily be free modules? We present a
border division algorithm and prove the termination of the algorithm for a
special class of border bases. We show the existence of such border bases over
Noetherian rings and present some characterizations in this regard. We also
show that certain reduced Gr\""{o}bner bases over Noetherian rings are contained
in this class of border bases.
","['\nAmbedkar Dukkipati\n', '\nNithish Pai\n', '\nMaria Francis\n']",,,http://arxiv.org/abs/1405.0472v5,cs.SC,"['cs.SC', 'math.AC']",,,[]
Computing all Affine Solution Sets of Binomial Systems,http://arxiv.org/abs/1405.0320v1,2014-05-01T23:00:24Z,2014-05-01T23:00:24Z,"  To compute solutions of sparse polynomial systems efficiently we have to
exploit the structure of their Newton polytopes. While the application of
polyhedral methods naturally excludes solutions with zero components, an
irreducible decomposition of a variety is typically understood in affine space,
including also those components with zero coordinates. For the problem of
computing solution sets in the intersection of some coordinate planes, the
direct application of a polyhedral method fails, because the original facial
structure of the Newton polytopes may alter completely when selected variables
become zero. Our new proposed method enumerates all factors contributing to a
generalized permanent and toric solutions as a special case of this
enumeration. For benchmark problems such as the adjacent 2-by-2 minors of a
general matrix, our methods scale much better than the witness set
representations of numerical algebraic geometry.
","['\nDanko Adrovic\n', '\nJan Verschelde\n']","4 page extended abstract accepted by EACA 2014, a conference on
  Computer Algebra and its Applications",,http://arxiv.org/abs/1405.0320v1,cs.SC,"['cs.SC', 'cs.MS', 'math.AG']",,,[]
"Problem formulation for truth-table invariant cylindrical algebraic
  decomposition by incremental triangular decomposition",http://arxiv.org/abs/1404.6371v1,2014-04-25T09:49:49Z,2014-04-25T09:49:49Z,"  Cylindrical algebraic decompositions (CADs) are a key tool for solving
problems in real algebraic geometry and beyond. We recently presented a new CAD
algorithm combining two advances: truth-table invariance, making the CAD
invariant with respect to the truth of logical formulae rather than the signs
of polynomials; and CAD construction by regular chains technology, where first
a complex decomposition is constructed by refining a tree incrementally by
constraint. We here consider how best to formulate problems for input to this
algorithm. We focus on a choice (not relevant for other CAD algorithms) about
the order in which constraints are presented. We develop new heuristics to help
make this choice and thus allow the best use of the algorithm in practice. We
also consider other choices of problem formulation for CAD, as discussed in
CICM 2013, revisiting these in the context of the new algorithm.
","['\nMatthew England\n', '\nRussell Bradford\n', '\nChangbo Chen\n', '\nJames H. Davenport\n', '\nMarc Moreno Maza\n', '\nDavid Wilson\n']",,"Intelligent Computer Mathematics, pp. 45-60. (Lecture Notes in
  Artificial Intelligence, 8543). Springer Berlin Heidelberg, 2014",http://dx.doi.org/10.1007/978-3-319-08434-3_5,cs.SC,"['cs.SC', '68W30, O3C10', 'I.1.2']",10.1007/978-3-319-08434-3_5,,[]
Computing periods of rational integrals,http://arxiv.org/abs/1404.5069v3,2014-04-20T20:35:44Z,2015-08-31T14:33:57Z,"  A period of a rational integral is the result of integrating, with respect to
one or several variables, a rational function over a closed path. This work
focuses particularly on periods depending on a parameter: in this case the
period under consideration satisfies a linear differential equation, the
Picard-Fuchs equation. I give a reduction algorithm that extends the
Griffiths-Dwork reduction and apply it to the computation of Picard-Fuchs
equations. The resulting algorithm is elementary and has been successfully
applied to problems that were previously out of reach.
",['\nPierre Lairez\n'],"To appear in Math. comp. Supplementary material at
  http://pierre.lairez.fr/supp/periods/","Math. Comp. 85 (2016), 1719-1752",http://dx.doi.org/10.1090/mcom/3054,cs.SC,"['cs.SC', 'math.AG', 'Primary 68W30, secondary 14K20, 14F40, 33F10']",10.1090/mcom/3054,,[]
"Applying machine learning to the problem of choosing a heuristic to
  select the variable ordering for cylindrical algebraic decomposition",http://arxiv.org/abs/1404.6369v1,2014-04-25T09:43:05Z,2014-04-25T09:43:05Z,"  Cylindrical algebraic decomposition(CAD) is a key tool in computational
algebraic geometry, particularly for quantifier elimination over real-closed
fields. When using CAD, there is often a choice for the ordering placed on the
variables. This can be important, with some problems infeasible with one
variable ordering but easy with another. Machine learning is the process of
fitting a computer model to a complex function based on properties learned from
measured data. In this paper we use machine learning (specifically a support
vector machine) to select between heuristics for choosing a variable ordering,
outperforming each of the separate heuristics.
","['\nZongyan Huang\n', '\nMatthew England\n', '\nDavid Wilson\n', '\nJames H. Davenport\n', '\nLawrence C. Paulson\n', '\nJames Bridge\n']",16 pages,"Intelligent Computer Mathematics, pp. 92-107. (Lecture Notes in
  Artificial Intelligence, 8543). Springer Berlin Heidelberg, 2014",http://dx.doi.org/10.1007/978-3-319-08434-3_8,cs.SC,"['cs.SC', 'cs.LG', '68W30, 68T05, O3C10', 'I.2.6']",10.1007/978-3-319-08434-3_8,,[]
Automatic Differentiation of Algorithms for Machine Learning,http://arxiv.org/abs/1404.7456v1,2014-04-28T17:19:25Z,2014-04-28T17:19:25Z,"  Automatic differentiation---the mechanical transformation of numeric computer
programs to calculate derivatives efficiently and accurately---dates to the
origin of the computer age. Reverse mode automatic differentiation both
antedates and generalizes the method of backwards propagation of errors used in
machine learning. Despite this, practitioners in a variety of fields, including
machine learning, have been little influenced by automatic differentiation, and
make scant use of available tools. Here we review the technique of automatic
differentiation, describe its two main modes, and explain how it can benefit
machine learning practitioners. To reach the widest possible audience our
treatment assumes only elementary differential calculus, and does not assume
any knowledge of linear algebra.
","['\nAtilim Gunes Baydin\n', '\nBarak A. Pearlmutter\n']","7 pages, 1 figure",,http://arxiv.org/abs/1404.7456v1,cs.LG,"['cs.LG', 'cs.SC', 'stat.ML', '68W30, 65D25, 68T05', 'G.1.4; I.2.6']",,,[]
"The Secant-Newton Map is Optimal Among Contracting $n^{th}$ Degree Maps
  for $n^{th}$ Root Computation",http://arxiv.org/abs/1404.2371v1,2014-04-09T05:48:36Z,2014-04-09T05:48:36Z,"  Consider the problem: given a real number $x$ and an error bound $\epsilon$,
find an interval such that it contains the $\sqrt[n]{x}$ and its width is less
than $\epsilon$. One way to solve the problem is to start with an initial
interval and to repeatedly update it by applying an interval refinement map on
it until it becomes narrow enough. In this paper, we prove that the well known
Secant-Newton map is optimal among a certain family of natural generalizations.
","['\nKayla Bishop\n', '\nHoon Hong\n']",,,http://arxiv.org/abs/1404.2371v1,cs.SC,['cs.SC'],,,[]
Nearly Optimal Computations with Structured Matrices,http://arxiv.org/abs/1404.4768v1,2014-04-18T12:43:34Z,2014-04-18T12:43:34Z,"  We estimate the Boolean complexity of multiplication of structured matrices
by a vector and the solution of nonsingular linear systems of equations with
these matrices. We study four basic most popular classes, that is, Toeplitz,
Hankel, Cauchy and Van-der-monde matrices, for which the cited computational
problems are equivalent to the task of polynomial multiplication and division
and polynomial and rational multipoint evaluation and interpolation. The
Boolean cost estimates for the latter problems have been obtained by Kirrinnis
in \cite{kirrinnis-joc-1998}, except for rational interpolation, which we
supply now. All known Boolean cost estimates for these problems rely on using
Kronecker product. This implies the $d$-fold precision increase for the $d$-th
degree output, but we avoid such an increase by relying on distinct techniques
based on employing FFT. Furthermore we simplify the analysis and make it more
transparent by combining the representation of our tasks and algorithms in
terms of both structured matrices and polynomials and rational functions. This
also enables further extensions of our estimates to cover Trummer's important
problem and computations with the popular classes of structured matrices that
generalize the four cited basic matrix classes.
","['\nVictor Y. Pan\nLIP6, INRIA Paris-Rocquencourt\n', '\nElias Tsigaridas\nLIP6, INRIA Paris-Rocquencourt\n']",(2014-04-10),,http://arxiv.org/abs/1404.4768v1,cs.SC,['cs.SC'],,,"['LIP6, INRIA Paris-Rocquencourt', 'LIP6, INRIA Paris-Rocquencourt']"
"Accelerated Approximation of the Complex Roots of a Univariate
  Polynomial (Extended Abstract)",http://arxiv.org/abs/1404.4775v1,2014-04-18T12:51:33Z,2014-04-18T12:51:33Z,"  Highly efficient and even nearly optimal algorithms have been developed for
the classical problem of univariate polynomial root-finding (see, e.g.,
\cite{P95}, \cite{P02}, \cite{MNP13}, and the bibliography therein), but this
is still an area of active research. By combining some powerful techniques
developed in this area we devise new nearly optimal algorithms, whose
substantial merit is their simplicity, important for the implementation.
","['\nVictor Y. Pan\nLIP6, INRIA Paris-Rocquencourt\n', '\nElias Tsigaridas\nLIP6, INRIA Paris-Rocquencourt\n']",(10/04/2014),,http://arxiv.org/abs/1404.4775v1,cs.SC,['cs.SC'],,,"['LIP6, INRIA Paris-Rocquencourt', 'LIP6, INRIA Paris-Rocquencourt']"
Global Newton Iteration over Archimedean and non-Archimedean Fields,http://arxiv.org/abs/1404.5525v1,2014-04-17T20:00:33Z,2014-04-17T20:00:33Z,"  In this paper, we study iterative methods on the coefficients of the rational
univariate representation (RUR) of a given algebraic set, called global Newton
iteration. We compare two natural approaches to define locally quadratically
convergent iterations: the first one involves Newton iteration applied to the
approximate roots individually and then interpolation to find the RUR of these
approximate roots; the second one considers the coefficients in the exact RUR
as zeroes of a high dimensional map defined by polynomial reduction, and
applies Newton iteration on this map. We prove that over fields with a p-adic
valuation these two approaches give the same iteration function, but over
fields equipped with the usual Archimedean absolute value, they are not
equivalent. In the latter case, we give explicitly the iteration function for
both approaches. Finally, we analyze the parallel complexity of the different
versions of the global Newton iteration, compare them, and demonstrate that
they can be efficiently computed. The motivation for this study comes from the
certification of approximate roots of overdetermined and singular polynomial
systems via the recovery of an exact RUR from approximate numerical data.
","['\nJonathan D. Hauenstein\n', '\nVictor Pan\n', '\nAgnes Szanto\n']",,,http://arxiv.org/abs/1404.5525v1,cs.NA,"['cs.NA', 'cs.SC']",,,[]
"An Improvement over the GVW Algorithm for Inhomogeneous Polynomial
  Systems",http://arxiv.org/abs/1404.1428v2,2014-04-05T03:49:32Z,2014-04-15T10:31:36Z,"  The GVW algorithm is a signature-based algorithm for computing Gr\""obner
bases. If the input system is not homogeneous, some J-pairs with higher
signatures but lower degrees are rejected by GVW's Syzygy Criterion, instead,
GVW have to compute some J-pairs with lower signatures but higher degrees.
Consequently, degrees of polynomials appearing during the computations may
unnecessarily grow up higher and the computation become more expensive. In this
paper, a variant of the GVW algorithm, called M-GVW, is proposed and mutant
pairs are introduced to overcome inconveniences brought by inhomogeneous input
polynomials. Some techniques from linear algebra are used to improve the
efficiency. Both GVW and M-GVW have been implemented in C++ and tested by many
examples from boolean polynomial rings. The timings show M-GVW usually performs
much better than the original GVW algorithm when mutant pairs are found.
Besides, M-GVW is also compared with intrinsic Gr\""obner bases functions on
Maple, Singular and Magma. Due to the efficient routines from the M4RI library,
the experimental results show that M-GVW is very efficient.
","['\nYao Sun\n', '\nDongdai Lin\n', '\nDingkang Wang\n']",,,http://arxiv.org/abs/1404.1428v2,cs.SC,['cs.SC'],,,[]
Predicting zero reductions in Gröbner basis computations,http://arxiv.org/abs/1404.0161v1,2014-04-01T08:24:36Z,2014-04-01T08:24:36Z,"  Since Buchberger's initial algorithm for computing Gr\""obner bases in 1965
many attempts have been taken to detect zero reductions in advance.
Buchberger's Product and Chain criteria may be known the most, especially in
the installaton of Gebauer and M\""oller. A relatively new approach are
signature-based criteria which were first used in Faug\`ere's F5 algorithm in
2002. For regular input sequences these criteria are known to compute no zero
reduction at all. In this paper we give a detailed discussion on zero
reductions and the corresponding syzygies. We explain how the different methods
to predict them compare to each other and show advantages and drawbacks in
theory and practice. With this a new insight into algebraic structures
underlying Gr\""obner bases and their computations might be achieved.
",['\nChristian Eder\n'],"25 pages, 3 figures",,http://arxiv.org/abs/1404.0161v1,math.AC,"['math.AC', 'cs.SC']",,,[]
A survey on signature-based Gröbner basis computations,http://arxiv.org/abs/1404.1774v1,2014-04-07T13:03:01Z,2014-04-07T13:03:01Z,"  This paper is a survey on the area of signature-based Gr\""obner basis
algorithms that was initiated by Faug\`ere's F5 algorithm in 2002. We explain
the general ideas behind the usage of signatures. We show how to classify the
various known variants by 3 different orderings. For this we give translations
between different notations and show that besides notations many approaches are
just the same. Moreover, we give a general description of how the idea of
signatures is quite natural when performing the reduction process using linear
algebra. This survey shall help to outline this field of active research.
","['\nChristian Eder\n', '\nJean-Charles Faugère\n']","53 pages, 8 figures, 11 tables",,http://arxiv.org/abs/1404.1774v1,math.AC,"['math.AC', 'cs.SC']",,,[]
"Proceedings 1st International Workshop on Synthesis of Continuous
  Parameters",http://arxiv.org/abs/1403.7841v2,2014-03-31T00:31:31Z,2014-04-08T03:23:04Z,"  This volume contains the proceedings of the 1st International Workshop on
Synthesis of Continuous Parameters (SynCoP'14). The workshop was held in
Grenoble, France on April 6th, 2014, as a satellite event of the 17th European
Joint Conferences on Theory and Practice of Software (ETAPS'14).
  SynCoP aims at bringing together researchers working on parameter synthesis
for systems with continuous variables, where the parameters consist of a
(usually dense) set of constant values. Synthesis problems for such parameters
arise for real-time, hybrid or probabilistic systems in a large variety
application domains. A parameter could be, e.g., a delay in a real-time system,
or a reaction rate in a biological cell model. The objective of the synthesis
problem is to identify suitable parameters to achieve desired behavior, or to
verify the behavior for a given range of parameter values.
  This volume contains seven contributions: two invited talks and five regular
papers.
","['\nÉtienne André\nUniversité Paris 13, Sorbonne Paris Cité, LIPN, CNRS, Villetaneuse, France\n', '\nGoran Frehse\nVerimag, Grenoble, France\n']",,"EPTCS 145, 2014",http://dx.doi.org/10.4204/EPTCS.145,cs.SC,"['cs.SC', 'cs.FL', 'cs.SY', 'F.1.1; D.4.7; F.1.2; D.2.4']",10.4204/EPTCS.145,,"['Université Paris 13, Sorbonne Paris Cité, LIPN, CNRS, Villetaneuse, France', 'Verimag, Grenoble, France']"
Factoring Differential Operators in n Variables,http://arxiv.org/abs/1404.0002v1,2014-03-31T13:34:19Z,2014-03-31T13:34:19Z,"  In this paper, we present a new algorithm and an experimental implementation
for factoring elements in the polynomial n'th Weyl algebra, the polynomial n'th
shift algebra, and ZZ^n-graded polynomials in the n'th q-Weyl algebra.
  The most unexpected result is that this noncommutative problem of factoring
partial differential operators can be approached effectively by reducing it to
the problem of solving systems of polynomial equations over a commutative ring.
In the case where a given polynomial is ZZ^n-graded, we can reduce the problem
completely to factoring an element in a commutative multivariate polynomial
ring.
  The implementation in Singular is effective on a broad range of polynomials
and increases the ability of computer algebra systems to address this important
problem. We compare the performance and output of our algorithm with other
implementations in commodity computer algebra systems on nontrivial examples.
","['\nMark Giesbrecht\n', '\nAlbert Heinle\n', '\nViktor Levandovskyy\n']",,,http://arxiv.org/abs/1404.0002v1,cs.SC,"['cs.SC', 'math.AC', 'math.RA']",,,[]
"Model-based construction of Open Non-uniform Cylindrical Algebraic
  Decompositions",http://arxiv.org/abs/1403.6487v1,2014-03-24T02:10:31Z,2014-03-24T02:10:31Z,"  In this paper we introduce the notion of an Open Non-uniform Cylindrical
Algebraic Decomposition (NuCAD), and present an efficient model-based algorithm
for constructing an Open NuCAD from an input formula. A NuCAD is a
generalization of Cylindrical Algebraic Decomposition (CAD) as defined by
Collins in his seminal work from the early 1970s, and as extended in concepts
like Hong's partial CAD. A NuCAD, like a CAD, is a decomposition of
n-dimensional real space into cylindrical cells. But unlike a CAD, the cells in
a NuCAD need not be arranged cylindrically. It is in this sense that NuCADs are
not uniformly cylindrical. However, NuCADs--- like CADs --- carry a tree-like
structure that relates different cells. It is a very different tree but, as
with the CAD tree structure, it allows some operations to be performed
efficiently, for example locating the containing cell for an arbitrary input
point.
",['\nChristopher W. Brown\n'],,,http://arxiv.org/abs/1403.6487v1,cs.SC,"['cs.SC', 'I.1.2']",,,[]
Procesamiento topo-geométrico de imágenes neuronales,http://arxiv.org/abs/1403.6719v1,2014-03-26T15:45:48Z,2014-03-26T15:45:48Z,"  Fruit of the relationship of our research group with the team coordinated by
the biologist Miguel Morales (http://spineup.es), we have applied different
topo-geometric techniques for neuronal image processing. The images, captured
with a powerful confocal microscope, allow to study the evolution of synaptic
density under the influence of various substances, with the aim of studying
neurodegenerative diseases like Alzheimer.
  In the paper we make a brief review of the techniques that appear in our
bioinformatic problems, including the calculation of ordinary and persistent
homology (for which one can use the program Kenzo for symbolic computation in
algebraic topology ) and classical problems of digital topology as skeleton
location and path tracking. We focus on some particular cases of recent
application, with which we will illustrate the previous techniques.
","['\nAna Romero\n', '\nJónathan Heras\n', '\nGadea Mata\n', '\nMiguel Morales y Julio Rubio\n']",,"La Gaceta de la RSME Vol. 17 (2014), Num. 1, Pags. 109-128",http://arxiv.org/abs/1403.6719v1,cs.SC,['cs.SC'],,,[]
"Matrix-F5 algorithms over finite-precision complete discrete valuation
  fields",http://arxiv.org/abs/1403.5464v2,2014-03-20T12:37:01Z,2015-09-25T17:35:30Z,"  Let $(f\_1,\dots, f\_s) \in \mathbb{Q}\_p [X\_1,\dots, X\_n]^s$ be a sequence
of homogeneous polynomials with $p$-adic coefficients. Such system may happen,
for example, in arithmetic geometry. Yet, since $\mathbb{Q}\_p$ is not an
effective field, classical algorithm does not apply.We provide a definition for
an approximate Gr{\""o}bner basis with respect to a monomial order $w.$ We
design a strategy to compute such a basis, when precision is enough and under
the assumption that the input sequence is regular and the ideals $\langle
f\_1,\dots,f\_i \rangle$ are weakly-$w$-ideals. The conjecture of Moreno-Socias
states that for the grevlex ordering, such sequences are generic.Two variants
of that strategy are available, depending on whether one lean more on precision
or time-complexity. For the analysis of these algorithms, we study the loss of
precision of the Gauss row-echelon algorithm, and apply it to an adapted
Matrix-F5 algorithm. Numerical examples are provided.Moreover, the fact that
under such hypotheses, Gr{\""o}bner bases can be computed stably has many
applications. Firstly, the mapping sending $(f\_1,\dots,f\_s)$ to the reduced
Gr{\""o}bner basis of the ideal they span is differentiable, and its
differential can be given explicitly. Secondly, these hypotheses allows to
perform lifting on the Grobner bases, from $\mathbb{Z}/p^k \mathbb{Z}$ to
$\mathbb{Z}/p^{k+k'} \mathbb{Z}$ or $\mathbb{Z}.$ Finally, asking for the same
hypotheses on the highest-degree homogeneous components of the entry
polynomials allows to extend our strategy to the affine case.
",['\nTristan Vaccon\nIRMAR\n'],,,http://arxiv.org/abs/1403.5464v2,cs.SC,"['cs.SC', 'math.AC']",,,['IRMAR']
Defining and computing persistent Z-homology in the general case,http://arxiv.org/abs/1403.7086v2,2014-03-26T18:51:55Z,2014-03-31T10:18:54Z,"  By general case we mean methods able to process simplicial sets and chain
complexes not of finite type. A filtration of the object to be studied is the
heart of both subjects persistent homology and spectral sequences. In this
paper we present the complete relation between them, both from theoretical and
computational points of view. One of the main contributions of this paper is
the observation that a slight modification of our previous programs computing
spectral sequences is enough to compute also persistent homology. By
inheritance from our spectral sequence programs, we obtain for free persistent
homology programs applicable to spaces not of finite type (provided they are
spaces with effective homology) and with Z-coefficients (significantly
generalizing the usual presentation of persistent homology over a field). As an
illustration, we compute some persistent homology groups (and the corresponding
integer barcodes) in the case of a Postnikov tower.
","['\nAna Romero\n', '\nJónathan Heras\n', '\nJulio Rubio\n', '\nFrancis Sergeraert\n']",,,http://arxiv.org/abs/1403.7086v2,cs.CG,"['cs.CG', 'cs.SC', 'math.AT']",,,[]
"Sparse Polynomial Interpolation Codes and their decoding beyond half the
  minimal distance",http://arxiv.org/abs/1403.3594v3,2014-03-14T14:51:53Z,2014-06-18T17:16:59Z,"  We present algorithms performing sparse univariate polynomial interpolation
with errors in the evaluations of the polynomial. Based on the initial work by
Comer, Kaltofen and Pernet [Proc. ISSAC 2012], we define the sparse polynomial
interpolation codes and state that their minimal distance is precisely the
length divided by twice the sparsity. At ISSAC 2012, we have given a decoding
algorithm for as much as half the minimal distance and a list decoding
algorithm up to the minimal distance. Our new polynomial-time list decoding
algorithm uses sub-sequences of the received evaluations indexed by a linear
progression, allowing the decoding for a larger radius, that is, more errors in
the evaluations while returning a list of candidate sparse polynomials. We
quantify this improvement for all typically small values of number of terms and
number of errors, and provide a worst case asymptotic analysis of this
improvement. For instance, for sparsity T = 5 with up to 10 errors we can list
decode in polynomial-time from 74 values of the polynomial with unknown terms,
whereas our earlier algorithm required 2T (E + 1) = 110 evaluations. We then
propose two variations of these codes in characteristic zero, where appropriate
choices of values for the variable yield a much larger minimal distance: the
length minus twice the sparsity.
","['\nErich L. Kaltofen\n', '\nClément Pernet\n']","23 pages, 5 figures, In Proceedings of the International Symposium on
  Symbolic and Algebraic Computation 2014 (ISSAC'14)",,http://dx.doi.org/10.1145/2608628.2608660,cs.SC,"['cs.SC', 'cs.IT', 'math.IT', 'I.1.2; G.1.1; E.4']",10.1145/2608628.2608660,,[]
Sparse Gröbner Bases: the Unmixed Case,http://arxiv.org/abs/1402.7205v3,2014-02-28T11:24:10Z,2014-06-25T15:42:46Z,"  Toric (or sparse) elimination theory is a framework developped during the
last decades to exploit monomial structures in systems of Laurent polynomials.
Roughly speaking, this amounts to computing in a \emph{semigroup algebra},
\emph{i.e.} an algebra generated by a subset of Laurent monomials. In order to
solve symbolically sparse systems, we introduce \emph{sparse Gr\""obner bases},
an analog of classical Gr\""obner bases for semigroup algebras, and we propose
sparse variants of the $F_5$ and FGLM algorithms to compute them. Our prototype
""proof-of-concept"" implementation shows large speed-ups (more than 100 for some
examples) compared to optimized (classical) Gr\""obner bases software. Moreover,
in the case where the generating subset of monomials corresponds to the points
with integer coordinates in a normal lattice polytope $\mathcal P\subset\mathbb
R^n$ and under regularity assumptions, we prove complexity bounds which depend
on the combinatorial properties of $\mathcal P$. These bounds yield new
estimates on the complexity of solving $0$-dim systems where all polynomials
share the same Newton polytope (\emph{unmixed case}). For instance, we
generalize the bound $\min(n_1,n_2)+1$ on the maximal degree in a Gr\""obner
basis of a $0$-dim. bilinear system with blocks of variables of sizes
$(n_1,n_2)$ to the multilinear case: $\sum n_i - \max(n_i)+1$. We also propose
a variant of Fr\""oberg's conjecture which allows us to estimate the complexity
of solving overdetermined sparse systems.
","['\nJean-Charles Faugere\nINRIA Paris-Rocquencourt\n', '\nPierre-Jean Spaenlehauer\nINRIA Nancy - Grand Est / LORIA\n', '\nJules Svartz\nINRIA Paris-Rocquencourt\n']","20 pages, Corollary 6.1 has been corrected, ISSAC 2014, Kobe : Japan
  (2014)",,http://dx.doi.org/10.1145/2608628.2608663,cs.SC,['cs.SC'],10.1145/2608628.2608663,,"['INRIA Paris-Rocquencourt', 'INRIA Nancy - Grand Est / LORIA', 'INRIA Paris-Rocquencourt']"
"A Technique for Deriving Equational Conditions on the Denavit-Hartenberg
  Parameters of 6R Linkages that are Necessary for Movability",http://arxiv.org/abs/1402.5761v3,2014-02-24T09:45:18Z,2015-08-25T09:21:15Z,"  A closed 6R linkage is generically rigid. Special cases may be mobile. Many
families of mobile 6R linkages have been characterised in terms of the
invariant Denavit-Hartenberg parameters of the linkage. In other words, many
sufficient conditions for mobility are known. In this paper we give, for the
first time, equational conditions on the invariant Denavit-Hartenberg
parameters that are necessary for mobility. The method is based on the theory
of bonds. We illustrate the method by deriving the equational conditions for
various well-known linkages (Bricard's line symmetric linkage, Hooke's linkage,
Dietmaier's linkage, and recent a generalization of Bricard's orthogonal
linkage), starting from their bond diagrams; and by deriving the equations for
another bond diagram, thereby discovering a new mobile 6R linkage.
","['\nZijia Li\n', '\nJosef Schicho\n']",,Mechanism and Machine Theory 94 (2015),http://dx.doi.org/10.1016/j.mechmachtheory.2015.07.010,cs.RO,"['cs.RO', 'cs.SC']",10.1016/j.mechmachtheory.2015.07.010,,[]
Tame Decompositions and Collisions,http://arxiv.org/abs/1402.5945v1,2014-02-24T20:57:07Z,2014-02-24T20:57:07Z,"  A univariate polynomial f over a field is decomposable if f = g o h = g(h)
for nonlinear polynomials g and h. It is intuitively clear that the
decomposable polynomials form a small minority among all polynomials over a
finite field. The tame case, where the characteristic p of Fq does not divide n
= deg f, is fairly well-understood, and we have reasonable bounds on the number
of decomposables of degree n. Nevertheless, no exact formula is known if $n$
has more than two prime factors. In order to count the decomposables, one wants
to know, under a suitable normalization, the number of collisions, where
essentially different (g, h) yield the same f. In the tame case, Ritt's Second
Theorem classifies all 2-collisions.
  We introduce a normal form for multi-collisions of decompositions of
arbitrary length with exact description of the (non)uniqueness of the
parameters. We obtain an efficiently computable formula for the exact number of
such collisions at degree n over a finite field of characteristic coprime to p.
This leads to an algorithm for the exact number of decomposable polynomials at
degree n over a finite field Fq in the tame case.
",['\nKonstantin Ziegler\n'],,,http://arxiv.org/abs/1402.5945v1,math.AC,"['math.AC', 'cs.SC', '11T06, 12Y05', 'F.2.1']",,,[]
Matrix-F5 algorithms and tropical Gröbner bases computation,http://arxiv.org/abs/1402.6675v2,2014-02-26T20:36:48Z,2015-09-29T11:57:55Z,"  Let $K$ be a field equipped with a valuation. Tropical varieties over $K$ can
be defined with a theory of Gr\""obner bases taking into account the valuation
of $K$. Because of the use of the valuation, this theory is promising for
stable computations over polynomial rings over a $p$-adic fields.We design a
strategy to compute such tropical Gr\""obner bases by adapting the Matrix-F5
algorithm. Two variants of the Matrix-F5 algorithm, depending on how the
Macaulay matrices are built, are available to tropical computation with
respective modifications. The former is more numerically stable while the
latter is faster.Our study is performed both over any exact field with
valuation and some inexact fields like $\mathbb{Q}\_p$ or $\mathbb{F}\_q
\llbracket t \rrbracket.$ In the latter case, we track the loss in precision,
and show that the numerical stability can compare very favorably to the case of
classical Gr\""obner bases when the valuation is non-trivial. Numerical examples
are provided.
",['\nTristan Vaccon\nIRMAR\n'],,"International Symposium on Symbolic and Algebraic Computation,
  ISSAC 2015, Jul 2015, Bath, United Kingdom",http://dx.doi.org/10.1145/2755996.2756665,cs.SC,"['cs.SC', 'math.AC']",10.1145/2755996.2756665,,['IRMAR']
"Nonnegative Trigonometric Polynomials, Sturms Theorem, and Symbolic
  Computation",http://arxiv.org/abs/1402.6778v2,2014-02-27T03:07:02Z,2016-04-26T00:58:45Z,"  In this paper, we explain a procedure based on a classical result of Sturm
that can be used to determine rigorously whether a given trigonometric
polynomial is nonnegative in a certain interval or not. Many examples are
given. This technique has been employed by the author in several recent works.
  The procedure often involves tedious computations that are time-consuming and
error-prone. Fortunately, symbolic computation software is available to
automate the procedure. In this paper, we give the details of its
implementation in MAPLE 13. Some who are strongly attached to a more
traditional theoretical research framework may find such details boring or even
consider computer-assisted proofs suspicious. However, we emphasize again that
the procedure is completely mathematically rigorous.
",['\nMan Kam Kwong\n'],,,http://arxiv.org/abs/1402.6778v2,math.CA,"['math.CA', 'cs.SC']",,,[]
Tracking p-adic precision,http://arxiv.org/abs/1402.7142v1,2014-02-28T06:12:22Z,2014-02-28T06:12:22Z,"  We present a new method to propagate $p$-adic precision in computations,
which also applies to other ultrametric fields. We illustrate it with many
examples and give a toy application to the stable computation of the SOMOS 4
sequence.
","['\nXavier Caruso\nIRMAR\n', '\nDavid Roe\nIRMAR\n', '\nTristan Vaccon\nIRMAR\n']",,,http://dx.doi.org/10.1112/S1461157014000357,math.NT,"['math.NT', 'cs.SC']",10.1112/S1461157014000357,,"['IRMAR', 'IRMAR', 'IRMAR']"
Matrix Methods for Solving Algebraic Systems,http://arxiv.org/abs/1403.1140v1,2014-03-05T14:14:12Z,2014-03-05T14:14:12Z,"  We present our public-domain software for the following tasks in sparse (or
toric) elimination theory, given a well-constrained polynomial system. First, C
code for computing the mixed volume of the system. Second, Maple code for
defining an overconstrained system and constructing a Sylvester-type matrix of
its sparse resultant. Third, C code for a Sylvester-type matrix of the sparse
resultant and a superset of all common roots of the initial well-constrained
system by computing the eigen-decomposition of a square matrix obtained from
the resultant matrix. We conclude with experiments in computing molecular
conformations.
",['\nIoannis Z. Emiris\n'],13 pages. arXiv admin note: text overlap with arXiv:1201.5810,,http://arxiv.org/abs/1403.1140v1,cs.MS,"['cs.MS', 'cs.SC']",,,[]
"Design, Implementation and Evaluation of MTBDD based Fuzzy Sets and
  Binary Fuzzy Relations",http://arxiv.org/abs/1403.1279v1,2014-03-05T21:48:58Z,2014-03-05T21:48:58Z,"  For fast and efficient analysis of large sets of fuzzy data, elimination of
redundancies in the memory representation is needed. We used MTBDDs as the
underlying data-structure to represent fuzzy sets and binary fuzzy relations.
This leads to elimination of redundancies in the representation, less
computations, and faster analyses. We have also extended a BDD package (BuDDy)
to support MTBDDs in general and fuzzy sets and relations in particular.
Different fuzzy operations such as max, min and max-min composition were
implemented based on our representation. Effectiveness of our representation is
shown by applying it on fuzzy connectedness and image segmentation problem.
Compared to a base implementation, the running time of our MTBDD based
implementation was faster (in our test cases) by a factor ranging from 2 to 27.
Also, when the MTBDD based data-structure was employed, the memory needed to
represent the final results was improved by a factor ranging from 37.9 to
265.5.
","['\nHamid A. Toussi\n', '\nBahram Sadeghi Bigham\n']","A shorter version was published in Proceeding of International
  Conference on Computer, Information Technology and Digital Media, Tehran,
  Iran, 2013",,http://arxiv.org/abs/1403.1279v1,cs.DS,"['cs.DS', 'cs.SC']",,,[]
Computing discrete logarithms in subfields of residue class rings,http://arxiv.org/abs/1402.6658v1,2014-02-26T19:40:38Z,2014-02-26T19:40:38Z,"  Recent breakthrough methods \cite{gggz,joux,bgjt} on computing discrete
logarithms in small characteristic finite fields share an interesting feature
in common with the earlier medium prime function field sieve method \cite{jl}.
To solve discrete logarithms in a finite extension of a finite field $\F$, a
polynomial $h(x) \in \F[x]$ of a special form is constructed with an
irreducible factor $g(x) \in \F[x]$ of the desired degree. The special form of
$h(x)$ is then exploited in generating multiplicative relations that hold in
the residue class ring $\F[x]/h(x)\F[x]$ hence also in the target residue class
field $\F[x]/g(x)\F[x]$. An interesting question in this context and addressed
in this paper is: when and how does a set of relations on the residue class
ring determine the discrete logarithms in the finite fields contained in it? We
give necessary and sufficient conditions for a set of relations on the residue
class ring to determine discrete logarithms in the finite fields contained in
it. We also present efficient algorithms to derive discrete logarithms from the
relations when the conditions are met. The derived necessary conditions allow
us to clearly identify structural obstructions intrinsic to the special
polynomial $h(x)$ in each of the aforementioned methods, and propose
modifications to the selection of $h(x)$ so as to avoid obstructions.
","['\nMing-Deh Huang\n', '\nAnand Kumar Narayanan\n']",arXiv admin note: substantial text overlap with arXiv:1312.1674,,http://arxiv.org/abs/1402.6658v1,cs.CC,"['cs.CC', 'cs.CR', 'cs.SC', 'math.NT']",,,[]
Parallel computation of echelon forms,http://arxiv.org/abs/1402.3501v1,2014-02-14T15:35:37Z,2014-02-14T15:35:37Z,"  We propose efficient parallel algorithms and implementations on shared memory
architectures of LU factorization over a finite field. Compared to the
corresponding numerical routines, we have identified three main difficulties
specific to linear algebra over finite fields. First, the arithmetic complexity
could be dominated by modular reductions. Therefore, it is mandatory to delay
as much as possible these reductions while mixing fine-grain parallelizations
of tiled iterative and recursive algorithms. Second, fast linear algebra
variants, e.g., using Strassen-Winograd algorithm, never suffer from
instability and can thus be widely used in cascade with the classical
algorithms. There, trade-offs are to be made between size of blocks well suited
to those fast variants or to load and communication balancing. Third, many
applications over finite fields require the rank profile of the matrix (quite
often rank deficient) rather than the solution to a linear system. It is thus
important to design parallel algorithms that preserve and compute this rank
profile. Moreover, as the rank profile is only discovered during the algorithm,
block size has then to be dynamic. We propose and compare several block
decomposition: tile iterative with left-looking, right-looking and Crout
variants, slab and tile recursive. Experiments demonstrate that the tile
recursive variant performs better and matches the performance of reference
numerical software when no rank deficiency occur. Furthermore, even in the most
heterogeneous case, namely when all pivot blocks are rank deficient, we show
that it is possbile to maintain a high efficiency.
","['\nJean-Guillaume Dumas\nLJK\n', ""\nThierry Gautier\nINRIA Grenoble Rhône-Alpes / LIG Laboratoire d'Informatique de Grenoble\n"", ""\nClément Pernet\nINRIA Grenoble Rhône-Alpes / LIG Laboratoire d'Informatique de Grenoble\n"", ""\nZiad Sultan\nLJK, INRIA Grenoble Rhône-Alpes / LIG Laboratoire d'Informatique de Grenoble\n""]",,,http://arxiv.org/abs/1402.3501v1,cs.SC,"['cs.SC', 'cs.DC']",,,"['LJK', ""INRIA Grenoble Rhône-Alpes / LIG Laboratoire d'Informatique de Grenoble"", ""INRIA Grenoble Rhône-Alpes / LIG Laboratoire d'Informatique de Grenoble"", ""LJK, INRIA Grenoble Rhône-Alpes / LIG Laboratoire d'Informatique de Grenoble""]"
Tensor computations in computer algebra systems,http://arxiv.org/abs/1402.6635v1,2014-02-22T15:47:58Z,2014-02-22T15:47:58Z,"  This paper considers three types of tensor computations. On their basis, we
attempt to formulate criteria that must be satisfied by a computer algebra
system dealing with tensors. We briefly overview the current state of tensor
computations in different computer algebra systems. The tensor computations are
illustrated with appropriate examples implemented in specific systems: Cadabra
and Maxima.
","['\nA. V. Korolkova\n', '\nD. S. Kulyabov\n', '\nL. A. Sevastyanov\n']",in Russian; in English,"A. V. Korol'kova, D. S. Kulyabov, and L. A. Sevast'yanov. Tensor
  computations in computer algebra systems. Programming and Computer Software,
  39(3):135--142, 2013",http://dx.doi.org/10.1134/S0361768813030031,cs.SC,"['cs.SC', 'cs.MS', 'gr-qc']",10.1134/S0361768813030031,,[]
A Generalized Apagodu-Zeilberger Algorithm,http://arxiv.org/abs/1402.2409v3,2014-02-11T09:35:59Z,2014-08-02T19:32:33Z,"  The Apagodu-Zeilberger algorithm can be used for computing annihilating
operators for definite sums over hypergeometric terms, or for definite
integrals over hyperexponential functions. In this paper, we propose a
generalization of this algorithm which is applicable to arbitrary
$\partial$-finite functions. In analogy to the hypergeometric case, we
introduce the notion of proper $\partial$-finite functions. We show that the
algorithm always succeeds for these functions, and we give a tight a priori
bound for the order of the output operator.
","['\nShaoshi Chen\n', '\nManuel Kauers\n', '\nChristoph Koutschan\n']",,"Proceedings of the International Symposium on Symbolic and
  Algebraic Computation (ISSAC 2014), pages 107-114, 2014. ACM, New York, USA,
  ISBN 978-1-4503-2501-1",http://dx.doi.org/10.1145/2608628.2608641,cs.SC,"['cs.SC', 'I.1.2']",10.1145/2608628.2608641,,[]
Divide-And-Conquer Computation of Cylindrical Algebraic Decomposition,http://arxiv.org/abs/1402.0622v1,2014-02-04T05:52:47Z,2014-02-04T05:52:47Z,"  We present a divide-and-conquer version of the Cylindrical Algebraic
Decomposition (CAD) algorithm. The algorithm represents the input as a Boolean
combination of subformulas, computes cylindrical algebraic decompositions of
solution sets of the subformulas, and combines the results. We propose a
graph-based heuristic to find a suitable partitioning of the input and present
empirical comparison with direct CAD computation.
",['\nAdam Strzebonski\n'],,,http://arxiv.org/abs/1402.0622v1,cs.SC,"['cs.SC', 'cs.MS']",,,[]
"Faster Algorithms for Multivariate Interpolation with Multiplicities and
  Simultaneous Polynomial Approximations",http://arxiv.org/abs/1402.0643v2,2014-02-04T07:23:14Z,2015-02-13T16:26:30Z,"  The interpolation step in the Guruswami-Sudan algorithm is a bivariate
interpolation problem with multiplicities commonly solved in the literature
using either structured linear algebra or basis reduction of polynomial
lattices. This problem has been extended to three or more variables; for this
generalization, all fast algorithms proposed so far rely on the lattice
approach. In this paper, we reduce this multivariate interpolation problem to a
problem of simultaneous polynomial approximations, which we solve using fast
structured linear algebra. This improves the best known complexity bounds for
the interpolation step of the list-decoding of Reed-Solomon codes,
Parvaresh-Vardy codes, and folded Reed-Solomon codes. In particular, for
Reed-Solomon list-decoding with re-encoding, our approach has complexity
$\mathcal{O}\tilde{~}(\ell^{\omega-1}m^2(n-k))$, where $\ell,m,n,k$ are the
list size, the multiplicity, the number of sample points and the dimension of
the code, and $\omega$ is the exponent of linear algebra; this accelerates the
previously fastest known algorithm by a factor of $\ell / m$.
","['\nMuhammad F. I. Chowdhury\nORCCA\n', ""\nClaude-Pierre Jeannerod\nLIP, Inria Grenoble Rhône-Alpes / LIP Laboratoire de l'Informatique du Parallélisme\n"", ""\nVincent Neiger\nORCCA, LIP, Inria Grenoble Rhône-Alpes / LIP Laboratoire de l'Informatique du Parallélisme\n"", '\nEric Schost\nORCCA\n', ""\nGilles Villard\nLIP, Inria Grenoble Rhône-Alpes / LIP Laboratoire de l'Informatique du Parallélisme\n""]","Version 2: Generalized our results about Problem 1 to distinct
  multiplicities. Added Section 4 which details several applications of our
  results to the decoding of Reed-Solomon codes (list-decoding with re-encoding
  technique, Wu algorithm, and soft-decoding). Reorganized the sections, added
  references and corrected typos",,http://arxiv.org/abs/1402.0643v2,cs.IT,"['cs.IT', 'cs.SC', 'math.IT']",,,"['ORCCA', ""LIP, Inria Grenoble Rhône-Alpes / LIP Laboratoire de l'Informatique du Parallélisme"", ""ORCCA, LIP, Inria Grenoble Rhône-Alpes / LIP Laboratoire de l'Informatique du Parallélisme"", 'ORCCA', ""LIP, Inria Grenoble Rhône-Alpes / LIP Laboratoire de l'Informatique du Parallélisme""]"
"Applications of the Gauss-Jordan algorithm, done right",http://arxiv.org/abs/1401.5910v2,2014-01-23T09:40:01Z,2014-01-24T20:44:26Z,"  Computer Algebra systems are widely spread because of some of their
remarkable features such as their ease of use and performance. Nonetheless,
this focus on performance sometimes leads to unwanted consequences: algorithms
and computations are implemented and carried out in a way which is sometimes
not transparent to the users, and that can lead to unexpected failures. In this
paper we present a formalisation in a proof assistant system of a \emph{naive}
version of the Gauss-Jordan algorithm, with explicit proofs of some of its
applications, and additionally a process to obtain versions of this algorithm
in two different functional languages (SML and Haskell) by means of code
generation techniques from the verified algorithm. The obtained programs are
then applied to test cases, which, despite the simplicity of the original
algorithm, have shown remarkable features in comparison to some Computer
Algebra systems, such as Mathematica\textsuperscript{\textregistered} (where
some of these computations are even incorrect), or Sage (in comparison to which
the generated programs show a compelling performance). The aim of the paper is
to show that, with the current technology in Theorem Proving, formalising
Linear Algebra procedures is a challenging but rewarding task, which provides
programs that can be compared in some aspects to \emph{state of the art}
procedures in Computer Algebra systems, and whose correctness is formally
proved.
","['\nJesús Aransay\n', '\nJose Divasón\n']",,,http://arxiv.org/abs/1401.5910v2,cs.LO,"['cs.LO', 'cs.SC', 'I.1.2']",,,[]
"The Differential Dimension Polynomial for Characterizable Differential
  Ideals",http://arxiv.org/abs/1401.5959v1,2014-01-23T12:39:17Z,2014-01-23T12:39:17Z,"  We generalize the differential dimension polynomial from prime differential
ideals to characterizable differential ideals. Its computation is algorithmic,
its degree and leading coefficient remain differential birational invariants,
and it decides equality of characterizable differential ideals contained in
each other.
",['\nMarkus Lange-Hegermann\n'],,,http://arxiv.org/abs/1401.5959v1,math.AC,"['math.AC', 'cs.SC', '13N99']",,,[]
"Truth Table Invariant Cylindrical Algebraic Decomposition by Regular
  Chains",http://arxiv.org/abs/1401.6310v3,2014-01-24T10:52:25Z,2014-06-10T10:08:22Z,"  A new algorithm to compute cylindrical algebraic decompositions (CADs) is
presented, building on two recent advances. Firstly, the output is truth table
invariant (a TTICAD) meaning given formulae have constant truth value on each
cell of the decomposition. Secondly, the computation uses regular chains theory
to first build a cylindrical decomposition of complex space (CCD) incrementally
by polynomial. Significant modification of the regular chains technology was
used to achieve the more sophisticated invariance criteria. Experimental
results on an implementation in the RegularChains Library for Maple verify that
combining these advances gives an algorithm superior to its individual
components and competitive with the state of the art.
","['\nR. Bradford\n', '\nC. Chen\n', '\nJ. H. Davenport\n', '\nM. England\n', '\nM. Moreno Maza\n', '\nD. Wilson\n']",,"V.P. Gerdt W. Koepf, W.M. Seiler and E.V. Vorozhtsov, eds.
  Computer Algebra in Scientific Computing, pp. 44-58. (Lecture Notes in
  Computer Science, 8660). Springer International, 2014",http://dx.doi.org/10.1007/978-3-319-10515-4_4,cs.SC,"['cs.SC', 'math.AG', '68W30, 03C10', 'I.1.2']",10.1007/978-3-319-10515-4_4,,[]
"Ranks of Quotients, Remainders and $p$-Adic Digits of Matrices",http://arxiv.org/abs/1401.6667v2,2014-01-26T16:50:00Z,2014-01-31T16:46:50Z,"  For a prime $p$ and a matrix $A \in \mathbb{Z}^{n \times n}$, write $A$ as $A
= p (A \,\mathrm{quo}\, p) + (A \,\mathrm{rem}\, p)$ where the remainder and
quotient operations are applied element-wise. Write the $p$-adic expansion of
$A$ as $A = A^{[0]} + p A^{[1]} + p^2 A^{[2]} + \cdots$ where each $A^{[i]} \in
\mathbb{Z}^{n \times n}$ has entries between $[0, p-1]$. Upper bounds are
proven for the $\mathbb{Z}$-ranks of $A \,\mathrm{rem}\, p$, and $A
\,\mathrm{quo}\, p$. Also, upper bounds are proven for the
$\mathbb{Z}/p\mathbb{Z}$-rank of $A^{[i]}$ for all $i \ge 0$ when $p = 2$, and
a conjecture is presented for odd primes.
","['\nMustafa Elsheikh\n', '\nAndy Novocin\n', '\nMark Giesbrecht\n']",8 pages,,http://arxiv.org/abs/1401.6667v2,math.NT,"['math.NT', 'cs.SC', '15A03, 15B33, 15B36, 11C20']",,,[]
A Near-Optimal Algorithm for Computing Real Roots of Sparse Polynomials,http://arxiv.org/abs/1401.6011v1,2014-01-23T15:43:06Z,2014-01-23T15:43:06Z,"  Let $p\in\mathbb{Z}[x]$ be an arbitrary polynomial of degree $n$ with $k$
non-zero integer coefficients of absolute value less than $2^\tau$. In this
paper, we answer the open question whether the real roots of $p$ can be
computed with a number of arithmetic operations over the rational numbers that
is polynomial in the input size of the sparse representation of $p$. More
precisely, we give a deterministic, complete, and certified algorithm that
determines isolating intervals for all real roots of $p$ with
$O(k^3\cdot\log(n\tau)\cdot \log n)$ many exact arithmetic operations over the
rational numbers.
  When using approximate but certified arithmetic, the bit complexity of our
algorithm is bounded by $\tilde{O}(k^4\cdot n\tau)$, where $\tilde{O}(\cdot)$
means that we ignore logarithmic. Hence, for sufficiently sparse polynomials
(i.e. $k=O(\log^c (n\tau))$ for a positive constant $c$), the bit complexity is
$\tilde{O}(n\tau)$. We also prove that the latter bound is optimal up to
logarithmic factors.
",['\nMichael Sagraloff\n'],,,http://arxiv.org/abs/1401.6011v1,cs.NA,"['cs.NA', 'cs.CC', 'cs.SC', 'math.NA']",,,[]
"Multivariate sparse interpolation using randomized Kronecker
  substitutions",http://arxiv.org/abs/1401.6694v2,2014-01-26T21:37:51Z,2014-05-02T05:01:05Z,"  We present new techniques for reducing a multivariate sparse polynomial to a
univariate polynomial. The reduction works similarly to the classical and
widely-used Kronecker substitution, except that we choose the degrees randomly
based on the number of nonzero terms in the multivariate polynomial, that is,
its sparsity. The resulting univariate polynomial often has a significantly
lower degree than the Kronecker substitution polynomial, at the expense of a
small number of term collisions. As an application, we give a new algorithm for
multivariate interpolation which uses these new techniques along with any
existing univariate interpolation algorithm.
","['\nAndrew Arnold\n', '\nDaniel S. Roche\n']","21 pages, 2 tables, 1 procedure. Accepted to ISSAC 2014",,http://arxiv.org/abs/1401.6694v2,cs.SC,"['cs.SC', 'cs.DS', 'cs.MS', '68W30', 'F.2.1; G.4; I.1.2']",,,[]
The MMO problem,http://arxiv.org/abs/1401.7532v1,2014-01-29T15:07:28Z,2014-01-29T15:07:28Z,"  We consider a two polynomials analogue of the polynomial interpolation
problem. Namely, we consider the Mixing Modular Operations (MMO) problem of
recovering two polynomials $f\in \Z_p[x]$ and $g\in \Z_q[x]$ of known degree,
where $p$ and $q$ are two (un)known positive integers, from the values of
$f(t)\bmod p + g(t)\bmod q$ at polynomially many points $t \in \Z$. We show
that if $p$ and $q$ are known, the MMO problem is equivalent to computing a
close vector in a lattice with respect to the infinity norm. We also
implemented in the SAGE system a heuristic polynomial-time algorithm. If $p$
and $q$ are kept secret, we do not know how to solve this problem. This problem
is motivated by several potential cryptographic applications.
","['\nOscar Garcia-Morchon\n', '\nRonald Rietman\n', '\nLudo Tolhuizen\n', '\nDomingo Gomez\n', '\nJaime Gutierrez\n']","Submitted to Interantaional Symposium on Symbolic and Algebraic
  Computation (ISSAC) 2014",,http://arxiv.org/abs/1401.7532v1,math.RA,"['math.RA', 'cs.CR', 'cs.SC', 'math.NT']",,,[]
Powers of Tensors and Fast Matrix Multiplication,http://arxiv.org/abs/1401.7714v1,2014-01-30T01:11:22Z,2014-01-30T01:11:22Z,"  This paper presents a method to analyze the powers of a given trilinear form
(a special kind of algebraic constructions also called a tensor) and obtain
upper bounds on the asymptotic complexity of matrix multiplication. Compared
with existing approaches, this method is based on convex optimization, and thus
has polynomial-time complexity. As an application, we use this method to study
powers of the construction given by Coppersmith and Winograd [Journal of
Symbolic Computation, 1990] and obtain the upper bound $\omega<2.3728639$ on
the exponent of square matrix multiplication, which slightly improves the best
known upper bound.
",['\nFrançois Le Gall\n'],28 pages,"Proceedings of the 39th International Symposium on Symbolic and
  Algebraic Computation (ISSAC 2014), pp. 296-303, 2014",http://dx.doi.org/10.1145/2608628.2627493,cs.DS,"['cs.DS', 'cs.CC', 'cs.SC']",10.1145/2608628.2627493,,[]
On the Complexity of Computing with Planar Algebraic Curves,http://arxiv.org/abs/1401.5690v2,2014-01-22T14:54:13Z,2014-07-31T13:33:25Z,"  In this paper, we give improved bounds for the computational complexity of
computing with planar algebraic curves. More specifically, for arbitrary
coprime polynomials $f$, $g \in \mathbb{Z}[x,y]$ and an arbitrary polynomial $h
\in \mathbb{Z}[x,y]$, each of total degree less than $n$ and with integer
coefficients of absolute value less than $2^\tau$, we show that each of the
following problems can be solved in a deterministic way with a number of bit
operations bounded by $\tilde{O}(n^6+n^5\tau)$, where we ignore polylogarithmic
factors in $n$ and $\tau$:
  (1) The computation of isolating regions in $\mathbb{C}^2$ for all complex
solutions of the system $f = g = 0$,
  (2) the computation of a separating form for the solutions of $f = g = 0$,
  (3) the computation of the sign of $h$ at all real valued solutions of $f = g
= 0$, and
  (4) the computation of the topology of the planar algebraic curve
$\mathcal{C}$ defined as the real valued vanishing set of the polynomial $f$.
  Our bound improves upon the best currently known bounds for the first three
problems by a factor of $n^2$ or more and closes the gap to the
state-of-the-art randomized complexity for the last problem.
","['\nAlexander Kobel\n', '\nMichael Sagraloff\n']","41 pages, 1 figure",,http://arxiv.org/abs/1401.5690v2,cs.SC,"['cs.SC', 'cs.NA', 'math.AG', 'math.GT', 'math.NA', '13P15, 68W30, 14Q05 (Primary) 12Y05 (Secondary)', 'G.1.5; I.1.2; F.2.1; G.1.0']",,,[]
Essentially optimal interactive certificates in linear algebra,http://arxiv.org/abs/1401.4567v4,2014-01-18T17:08:05Z,2020-01-08T10:58:48Z,"  Certificates to a linear algebra computation are additional data structures
for each output, which can be used by a---possibly randomized---verification
algorithm that proves the correctness of each output. The certificates are
essentially optimal if the time (and space) complexity of verification is
essentially linear in the input size $N$, meaning $N$ times a factor
$N^{o(1)}$, i.e., a factor $N^{\eta(N)}$ with $\lim\_{N\to \infty} \eta(N)$ $=$
$0$. We give algorithms that compute essentially optimal certificates for the
positive semidefiniteness, Frobenius form, characteristic and minimal
polynomial of an $n\times n$ dense integer matrix $A$. Our certificates can be
verified in Monte-Carlo bit complexity $(n^2 \log\|A\|)^{1+o(1)}$, where
$\log\|A\|$ is the bit size of the integer entries, solving an open problem in
[Kaltofen, Nehring, Saunders, Proc.\ ISSAC 2011] subject to computational
hardness assumptions. Second, we give algorithms that compute certificates for
the rank of sparse or structured $n\times n$ matrices over an abstract field,
whose Monte Carlo verification complexity is $2$ matrix-times-vector products
$+$ $n^{1+o(1)}$ arithmetic operations in the field. For example, if the
$n\times n$ input matrix is sparse with $n^{1+o(1)}$ non-zero entries, our rank
certificate can be verified in $n^{1+o(1)}$ field operations. This extends also
to integer matrices with only an extra $\|A\|^{1+o(1)}$ factor. All our
certificates are based on interactive verification protocols with the
interaction removed by a Fiat-Shamir identification heuristic. The validity of
our verification procedure is subject to standard computational hardness
assumptions from cryptography.
","['\nJean-Guillaume Dumas\nCASYS\n', '\nErich Kaltofen\n']",,"ACM International Symposium on Symbolic and Algebraic Computation
  (ISSAC 2014), Jul 2014, Kobe, Japan. pp.146-153",http://dx.doi.org/10.1145/2608628.2608644,cs.SC,['cs.SC'],10.1145/2608628.2608644,,['CASYS']
Sparse interpolation over finite fields via low-order roots of unity,http://arxiv.org/abs/1401.4744v2,2014-01-19T21:51:32Z,2014-05-02T14:31:31Z,"  We present a new Monte Carlo algorithm for the interpolation of a
straight-line program as a sparse polynomial $f$ over an arbitrary finite field
of size $q$. We assume a priori bounds $D$ and $T$ are given on the degree and
number of terms of $f$. The approach presented in this paper is a hybrid of the
diversified and recursive interpolation algorithms, the two previous fastest
known probabilistic methods for this problem. By making effective use of the
information contained in the coefficients themselves, this new algorithm
improves on the bit complexity of previous methods by a ""soft-Oh"" factor of
$T$, $\log D$, or $\log q$.
","['\nAndrew Arnold\n', '\nMark Giesbrecht\n', '\nDaniel S. Roche\n']","18 pages, 1 table, 4 procedures, accepted to ISSAC 2014",,http://arxiv.org/abs/1401.4744v2,cs.SC,"['cs.SC', '68W30, 12Y05, 13P05', 'G.4; I.1.1']",,,[]
Constructing Fewer Open Cells by GCD Computation in CAD Projection,http://arxiv.org/abs/1401.4953v2,2014-01-20T15:55:33Z,2014-05-17T15:20:27Z,"  A new projection operator based on cylindrical algebraic decomposition (CAD)
is proposed. The new operator computes the intersection of projection factor
sets produced by different CAD projection orders. In other words, it computes
the gcd of projection polynomials in the same variables produced by different
CAD projection orders. We prove that the new operator still guarantees
obtaining at least one sample point from every connected component of the
highest dimension, and therefore, can be used for testing semi-definiteness of
polynomials. Although the complexity of the new method is still doubly
exponential, in many cases, the new operator does produce smaller projection
factor sets and fewer open cells. Some examples of testing semi-definiteness of
polynomials, which are difficult to be solved by existing tools, have been
worked out efficiently by our program based on the new method.
","['\nJingjun Han\n', '\nLiyun Dai\n', '\nBican Xia\n']","Accepted by ISSAC 2014 (July 23--25, 2014, Kobe, Japan)",,http://dx.doi.org/10.1145/2608628.2608676,cs.SC,['cs.SC'],10.1145/2608628.2608676,,[]
Melikyan algebra is a deformation of a Poisson algebra,http://arxiv.org/abs/1401.2566v3,2014-01-11T20:29:02Z,2016-08-20T18:51:58Z,"  We prove, using computer, that the restricted Melikyan algebra of dimension
125 is a deformation of a Poisson algebra.
","['\nHayk Melikyan\n', '\nPasha Zusmanovich\n']",v3: minor corrections in references; added ancillary files,"Journal of Physics: Conference Series 532 (2014), 012019",http://dx.doi.org/10.1088/1742-6596/532/1/012019,math.RA,"['math.RA', 'cs.SC', '17B50, 17B55, 17B63, 17-04']",10.1088/1742-6596/532/1/012019,,[]
Parallel Telescoping and Parameterized Picard--Vessiot Theory,http://arxiv.org/abs/1401.4666v1,2014-01-19T13:34:20Z,2014-01-19T13:34:20Z,"  Parallel telescoping is a natural generalization of differential
creative-telescoping for single integrals to line integrals. It computes a
linear ordinary differential operator $L$, called a parallel telescoper, for
several multivariate functions, such that the applications of $L$ to the
functions yield antiderivatives of a single function. We present a necessary
and sufficient condition guaranteeing the existence of parallel telescopers for
differentially finite functions, and develop an algorithm to compute minimal
ones for compatible hyperexponential functions. Besides computing annihilators
of parametric line integrals, we use the parallel telescoping for determining
Galois groups of parameterized partial differential systems of first order.
","['\nShaoshi Chen\n', '\nRuyong Feng\n', '\nZiming Li\n', '\nMichael F. Singer\n']",19 pages,,http://arxiv.org/abs/1401.4666v1,cs.SC,"['cs.SC', 'math.CA', '12H05, 33F10']",,,[]
Over-constrained Weierstrass iteration and the nearest consistent system,http://arxiv.org/abs/1401.5086v1,2014-01-20T21:07:54Z,2014-01-20T21:07:54Z,"  We propose a generalization of the Weierstrass iteration for over-constrained
systems of equations and we prove that the proposed method is the Gauss-Newton
iteration to find the nearest system which has at least $k$ common roots and
which is obtained via a perturbation of prescribed structure. In the univariate
case we show the connection of our method to the optimization problem
formulated by Karmarkar and Lakshman for the nearest GCD. In the multivariate
case we generalize the expressions of Karmarkar and Lakshman, and give
explicitly several iteration functions to compute the optimum.
  The arithmetic complexity of the iterations is detailed.
","['\nOlivier Ruatta\n', '\nMark Sciabica\n', '\nAgnes Szanto\n']",,,http://arxiv.org/abs/1401.5086v1,cs.SC,"['cs.SC', 'math.OC']",,,[]
"Computing low-degree factors of lacunary polynomials: a Newton-Puiseux
  approach",http://arxiv.org/abs/1401.4720v2,2014-01-19T19:23:24Z,2014-06-24T07:38:52Z,"  We present a new algorithm for the computation of the irreducible factors of
degree at most $d$, with multiplicity, of multivariate lacunary polynomials
over fields of characteristic zero. The algorithm reduces this computation to
the computation of irreducible factors of degree at most $d$ of univariate
lacunary polynomials and to the factorization of low-degree multivariate
polynomials. The reduction runs in time polynomial in the size of the input
polynomial and in $d$. As a result, we obtain a new polynomial-time algorithm
for the computation of low-degree factors, with multiplicity, of multivariate
lacunary polynomials over number fields, but our method also gives partial
results for other fields, such as the fields of $p$-adic numbers or for
absolute or approximate factorization for instance.
  The core of our reduction uses the Newton polygon of the input polynomial,
and its validity is based on the Newton-Puiseux expansion of roots of bivariate
polynomials. In particular, we bound the valuation of $f(X,\phi)$ where $f$ is
a lacunary polynomial and $\phi$ a Puiseux series whose vanishing polynomial
has low degree.
",['\nBruno Grenet\n'],22 pages,"Proceedings of the 39th International Symposium on Symbolic and
  Algebraic Computation (ISSAC'14), pp 224-231, ACM, 2014",http://dx.doi.org/10.1145/2608628.2608649,cs.SC,"['cs.SC', 'cs.CC', 'cs.DS', 'I.1.2; F.2.2']",10.1145/2608628.2608649,,[]
Truth Table Invariant Cylindrical Algebraic Decomposition,http://arxiv.org/abs/1401.0645v3,2014-01-03T13:23:05Z,2015-11-13T18:08:13Z,"  When using cylindrical algebraic decomposition (CAD) to solve a problem with
respect to a set of polynomials, it is likely not the signs of those
polynomials that are of paramount importance but rather the truth values of
certain quantifier free formulae involving them. This observation motivates our
article and definition of a Truth Table Invariant CAD (TTICAD).
  In ISSAC 2013 the current authors presented an algorithm that can efficiently
and directly construct a TTICAD for a list of formulae in which each has an
equational constraint. This was achieved by generalising McCallum's theory of
reduced projection operators. In this paper we present an extended version of
our theory which can be applied to an arbitrary list of formulae, achieving
savings if at least one has an equational constraint. We also explain how the
theory of reduced projection operators can allow for further improvements to
the lifting phase of CAD algorithms, even in the context of a single equational
constraint.
  The algorithm is implemented fully in Maple and we present both promising
results from experimentation and a complexity analysis showing the benefits of
our contributions.
","['\nRussell Bradford\n', '\nJames H. Davenport\n', '\nMatthew England\n', '\nScott McCallum\n', '\nDavid Wilson\n']",40 pages,"Journal of Symbolic Computation 76, pp. 1-35, 2016",http://dx.doi.org/10.1016/j.jsc.2015.11.002,cs.SC,"['cs.SC', '68W30, 03C10', 'I.1.2']",10.1016/j.jsc.2015.11.002,,[]
Cylindrical Algebraic Sub-Decompositions,http://arxiv.org/abs/1401.0647v2,2014-01-03T13:49:31Z,2014-04-24T11:27:56Z,"  Cylindrical algebraic decompositions (CADs) are a key tool in real algebraic
geometry, used primarily for eliminating quantifiers over the reals and
studying semi-algebraic sets. In this paper we introduce cylindrical algebraic
sub-decompositions (sub-CADs), which are subsets of CADs containing all the
information needed to specify a solution for a given problem.
  We define two new types of sub-CAD: variety sub-CADs which are those cells in
a CAD lying on a designated variety; and layered sub-CADs which have only those
cells of dimension higher than a specified value. We present algorithms to
produce these and describe how the two approaches may be combined with each
other and the recent theory of truth-table invariant CAD.
  We give a complexity analysis showing that these techniques can offer
substantial theoretical savings, which is supported by experimentation using an
implementation in Maple.
","['\nD. J. Wilson\n', '\nR. J. Bradford\n', '\nJ. H. Davenport\n', '\nM. England\n']",26 pages,"Mathematics in Computer Science: Volume 8, Issue 2, pages 263-288,
  Springer, 2014",http://dx.doi.org/10.1007/s11786-014-0191-z,cs.SC,"['cs.SC', 'math.AG', '68W30', 'I.1.2']",10.1007/s11786-014-0191-z,,[]
"A Fast Algorithm for the Inversion of Quasiseparable Vandermonde-like
  Matrices",http://arxiv.org/abs/1401.1874v1,2014-01-09T01:17:17Z,2014-01-09T01:17:17Z,"  The results on Vandermonde-like matrices were introduced as a generalization
of polynomial Vandermonde matrices, and the displacement structure of these
matrices was used to derive an inversion formula. In this paper we first
present a fast Gaussian elimination algorithm for the polynomial
Vandermonde-like matrices. Later we use the said algorithm to derive fast
inversion algorithms for quasiseparable, semiseparable and well-free
Vandermonde-like matrices having $\mathcal{O}(n^2)$ complexity. To do so we
identify structures of displacement operators in terms of generators and the
recurrence relations(2-term and 3-term) between the columns of the basis
transformation matrices for quasiseparable, semiseparable and well-free
polynomials. Finally we present an $\mathcal{O}(n^2)$ algorithm to compute the
inversion of quasiseparable Vandermonde-like matrices.
","['\nSirani M. Perera\n', '\nGrigory Bonik\n', '\nVadim Olshevsky\n']",,,http://arxiv.org/abs/1401.1874v1,math.NA,"['math.NA', 'cs.SC', '15A09, 15B05, 65Y04']",,,[]
Relating $p$-adic eigenvalues and the local Smith normal form,http://arxiv.org/abs/1401.1773v4,2014-01-08T18:42:52Z,2015-05-07T15:21:46Z,"  Conditions are established under which the $p$-adic valuations of the
invariant factors (diagonal entries of the Smith form) of an integer matrix are
equal to the $p$-adic valuations of the eigenvalues. It is then shown that this
correspondence is the typical case for ""most"" matrices; precise density bounds
are given for when the property holds, as well as easy transformations to this
typical case.
","['\nMustafa Elsheikh\n', '\nMark Giesbrecht\n']",To appear in Linear Algebra and Its Applications,,http://dx.doi.org/10.1016/j.laa.2015.05.001,math.RA,"['math.RA', 'cs.SC', 'math.PR', '15A36, 15A18, 15A21']",10.1016/j.laa.2015.05.001,,[]
On modular computation of Groebner bases with integer coefficients,http://arxiv.org/abs/1312.6331v1,2013-12-22T02:35:07Z,2013-12-22T02:35:07Z,"  Let $I_1\subset I_2\subset\dots$ be an increasing sequence of ideals of the
ring $\Bbb Z[X]$, $X=(x_1,\dots,x_n)$ and let $I$ be their union. We propose an
algorithm to compute the Gr\""obner base of $I$ under the assumption that the
Gr\""obner bases of the ideal $\Bbb Q I$ of the ring $\Bbb Q[X]$ and the the
ideals $I\otimes(\Bbb Z/m\Bbb Z)$ of the rings $(\Bbb Z/m\Bbb Z)[X]$ are known.
  Such an algorithmic problem arises, for example, in the construction of
Markov and semi-Markov traces on cubic Hecke algebras.
",['\nS. Yu. Orevkov\n'],3 pages,,http://arxiv.org/abs/1312.6331v1,math.AC,"['math.AC', 'cs.SC', '13P10']",,,[]
"A Quadratically Convergent Algorithm for Structured Low-Rank
  Approximation",http://arxiv.org/abs/1312.7279v2,2013-12-27T15:17:39Z,2014-10-27T09:20:30Z,"  Structured Low-Rank Approximation is a problem arising in a wide range of
applications in Numerical Analysis and Engineering Sciences. Given an input
matrix $M$, the goal is to compute a matrix $M'$ of given rank $r$ in a linear
or affine subspace $E$ of matrices (usually encoding a specific structure) such
that the Frobenius distance $\lVert M-M'\rVert$ is small. We propose a
Newton-like iteration for solving this problem, whose main feature is that it
converges locally quadratically to such a matrix under mild transversality
assumptions between the manifold of matrices of rank $r$ and the linear/affine
subspace $E$. We also show that the distance between the limit of the iteration
and the optimal solution of the problem is quadratic in the distance between
the input matrix and the manifold of rank $r$ matrices in $E$. To illustrate
the applicability of this algorithm, we propose a Maple implementation and give
experimental results for several applicative problems that can be modeled by
Structured Low-Rank Approximation: univariate approximate GCDs (Sylvester
matrices), low-rank Matrix completion (coordinate spaces) and denoising
procedures (Hankel matrices). Experimental results give evidence that this
all-purpose algorithm is competitive with state-of-the-art numerical methods
dedicated to these problems.
","['\nÉric Schost\n', '\nPierre-Jean Spaenlehauer\n']","37 pages, Maple package available at
  http://www.pjspaenlehauer.net/data/software/NewtonSLRA_notes.html",,http://arxiv.org/abs/1312.7279v2,cs.NA,"['cs.NA', 'cs.SC', 'math.NA']",,,[]
"Hrushovski's Algorithm for Computing the Galois Group of a Linear
  Differential Equation",http://arxiv.org/abs/1312.5029v3,2013-12-18T03:03:18Z,2014-02-26T01:47:04Z,"  We present a detailed and simplified version of Hrushovski's algorithm that
determines the Galois group of a linear differential equation. There are three
major ingredients in this algorithm. The first is to look for a degree bound
for proto-Galois groups, which enables one to compute one of them. The second
is to determine the identity component of the Galois group that is the pullback
of a torus to the proto-Galois group. The third is to recover the Galois group
from its identity component and a finite Galois group.
",['\nRuyong Feng\n'],27 pages,,http://dx.doi.org/10.1016/j.aam.2015.01.001,cs.SC,"['cs.SC', '12H05', 'I.1.2']",10.1016/j.aam.2015.01.001,,[]
Large Galois groups with applications to Zariski density,http://arxiv.org/abs/1312.3009v4,2013-12-11T00:35:56Z,2015-01-07T02:01:04Z,"  We introduce the first provably efficient algorithm to check if a finitely
generated subgroup of an almost simple semi-simple group over the rationals is
Zariski-dense. We reduce this question to one of computing Galois groups, and
to this end we describe efficient algorithms to check if the Galois group of a
polynomial $p$ with integer coefficients is ""generic"" (which, for arbitrary
polynomials of degree $n$ means the full symmetric group $S_n,$ while for
reciprocal polynomials of degree $2n$ it means the hyperoctahedral group $C_2
\wr S_n.$). We give efficient algorithms to verify that a polynomial has Galois
group $S_n,$ and that a reciprocal polynomial has Galois group $C_2 \wr S_n.$
We show how these algorithms give efficient algorithms to check if a set of
matrices $\mathcal{G}$ in $\mathop{SL}(n, \mathbb{Z})$ or $\mathop{Sp}(2n,
\mathbb{Z})$ generate a \emph{Zariski dense} subgroup.
  The complexity of doing this in$\mathop{SL}(n, \mathbb{Z})$ is of order
$O(n^4 \log n \log \|\mathcal{G}\|)\log \epsilon$ and in $\mathop{Sp}(2n,
\mathbb{Z})$ the complexity is of order $O(n^8 \log n\log \|\mathcal{G}\|)\log
\epsilon$ In general semisimple groups we show that Zariski density can be
confirmed or denied in time of order $O(n^14 \log \|\mathcal{G}\|\log
\epsilon),$ where $\epsilon$ is the probability of a wrong ""NO"" answer, while
$\|\mathcal{G}\|$ is the measure of complexity of the input (the maximum of the
Frobenius norms of the generating matrices). The algorithms work essentially
without change over algebraic number fields, and in other semi-simple groups.
However, we restrict to the case of the special linear and symplectic groups
and rational coefficients in the interest of clarity.
",['\nIgor Rivin\n'],25 pages,,http://arxiv.org/abs/1312.3009v4,math.NT,"['math.NT', 'cs.SC', '14L35, 15B36, 14Q99, 12F10, 11Y40']",,,[]
"Misfortunes of a mathematicians' trio using Computer Algebra Systems:
  Can we trust?",http://arxiv.org/abs/1312.3270v2,2013-12-11T18:25:46Z,2013-12-16T00:02:00Z,"  Computer algebra systems are a great help for mathematical research but
sometimes unexpected errors in the software can also badly affect it. As an
example, we show how we have detected an error of Mathematica computing
determinants of matrices of integer numbers: not only it computes the
determinants wrongly, but also it produces different results if one evaluates
the same determinant twice.
","['\nAntonio J. Durán\n', '\nMario Pérez\n', '\nJuan L. Varona\n']",4 pages,"Notices Amer. Math. Soc. 61 (2014), 1249-1252",http://arxiv.org/abs/1312.3270v2,cs.SC,"['cs.SC', 'cs.MS', '68W30']",,,[]
"Comprehensive Border Bases for Zero Dimensional Parametric Polynomial
  Ideals",http://arxiv.org/abs/1312.0453v2,2013-12-02T13:46:25Z,2013-12-28T13:13:13Z,"  In this paper, we extend the idea of comprehensive Gr\""{o}bner bases given by
Weispfenning (1992) to border bases for zero dimensional parametric polynomial
ideals. For this, we introduce a notion of comprehensive border bases and
border system, and prove their existence even in the cases where they do not
correspond to any term order. We further present algorithms to compute
comprehensive border bases and border system. Finally, we study the relation
between comprehensive Gr\""{o}bner bases and comprehensive border bases w.r.t. a
term order and give an algorithm to compute such comprehensive border bases
from comprehensive Gr\""{o}bner bases.
","['\nAbhishek Dubey\n', '\nAmbedkar Dukkipati\n']","15 pages, 8 sections and 3 algorithms",,http://arxiv.org/abs/1312.0453v2,cs.SC,['cs.SC'],,,[]
"A Generic Position Based Method for Real Root Isolation of
  Zero-Dimensional Polynomial Systems",http://arxiv.org/abs/1312.0462v1,2013-12-02T14:05:16Z,2013-12-02T14:05:16Z,"  We improve the local generic position method for isolating the real roots of
a zero-dimensional bivariate polynomial system with two polynomials and extend
the method to general zero-dimensional polynomial systems. The method mainly
involves resultant computation and real root isolation of univariate polynomial
equations. The roots of the system have a linear univariate representation. The
complexity of the method is $\tilde{O}_B(N^{10})$ for the bivariate case, where
$N=\max(d,\tau)$, $d$ resp., $\tau$ is an upper bound on the degree, resp., the
maximal coefficient bitsize of the input polynomials. The algorithm is
certified with probability 1 in the multivariate case. The implementation shows
that the method is efficient, especially for bivariate polynomial systems.
","['\nJin-San Cheng\n', '\nKai Jin\n']","24 pages, 5 figures",,http://arxiv.org/abs/1312.0462v1,cs.SC,['cs.SC'],,,[]
On the Complexity of the F5 Gröbner basis Algorithm,http://arxiv.org/abs/1312.1655v2,2013-12-05T19:47:02Z,2014-07-17T12:04:45Z,"  We study the complexity of Gr\""obner bases computation, in particular in the
generic situation where the variables are in simultaneous Noether position with
respect to the system.
  We give a bound on the number of polynomials of degree $d$ in a Gr\""obner
basis computed by Faug\`ere's $F_5$ algorithm~(Fau02) in this generic case for
the grevlex ordering (which is also a bound on the number of polynomials for a
reduced Gr\""obner basis, independently of the algorithm used). Next, we analyse
more precisely the structure of the polynomials in the Gr\""obner bases with
signatures that $F_5$ computes and use it to bound the complexity of the
algorithm.
  Our estimates show that the version of~$F_5$ we analyse, which uses only
standard Gaussian elimination techniques, outperforms row reduction of the
Macaulay matrix with the best known algorithms for moderate degrees, and even
for degrees up to the thousands if Strassen's multiplication is used. The
degree being fixed, the factor of improvement grows exponentially with the
number of variables.
","['\nMagali Bardet\n', '\nJean-Charles Faugère\n', '\nBruno Salvy\n']",24 pages,,http://arxiv.org/abs/1312.1655v2,cs.SC,['cs.SC'],,,[]
"Special Algorithm for Stability Analysis of Multistable Biological
  Regulatory Systems",http://arxiv.org/abs/1312.1780v2,2013-12-06T06:23:36Z,2013-12-26T07:58:54Z,"  We consider the problem of counting (stable) equilibriums of an important
family of algebraic differential equations modeling multistable biological
regulatory systems. The problem can be solved, in principle, using real
quantifier elimination algorithms, in particular real root classification
algorithms. However, it is well known that they can handle only very small
cases due to the enormous computing time requirements. In this paper, we
present a special algorithm which is much more efficient than the general
methods. Its efficiency comes from the exploitation of certain interesting
structures of the family of differential equations.
","['\nHoon Hong\n', '\nXiaoxian Tang\n', '\nBican Xia\n']","24 pages, 5 algorithms, 10 figures",,http://arxiv.org/abs/1312.1780v2,cs.SC,"['cs.SC', 'I.1.2']",,,[]
"Computing the multilinear factors of lacunary polynomials without
  heights",http://arxiv.org/abs/1311.5694v2,2013-11-22T10:15:20Z,2020-04-21T14:51:29Z,"  We present a deterministic algorithm which computes the multilinear factors
of multivariate lacunary polynomials over number fields. Its complexity is
polynomial in $\ell^n$ where $\ell$ is the lacunary size of the input
polynomial and $n$ its number of variables, that is in particular polynomial in
the logarithm of its degree. We also provide a randomized algorithm for the
same problem of complexity polynomial in $\ell$ and $n$.
  Over other fields of characteristic zero and finite fields of large
characteristic, our algorithms compute the multilinear factors having at least
three monomials of multivariate polynomials. Lower bounds are provided to
explain the limitations of our algorithm. As a by-product, we also design
polynomial-time deterministic polynomial identity tests for families of
polynomials which were not known to admit any.
  Our results are based on so-called Gap Theorem which reduce high-degree
factorization to repeated low-degree factorizations. While previous algorithms
used Gap Theorems expressed in terms of the heights of the coefficients, our
Gap Theorems only depend on the exponents of the polynomials. This makes our
algorithms more elementary and general, and faster in most cases.
","['\nArkadev Chattopadhyay\n', '\nBruno Grenet\n', '\nPascal Koiran\n', '\nNatacha Portier\n', '\nYann Strozecki\n']",34 pages,,http://arxiv.org/abs/1311.5694v2,cs.SC,"['cs.SC', 'cs.CC']",,,[]
On the length of integers in telescopers for proper hypergeometric terms,http://arxiv.org/abs/1311.3720v2,2013-11-15T04:10:38Z,2014-02-24T04:16:53Z,"  We show that the number of digits in the integers of a creative telescoping
relation of expected minimal order for a bivariate proper hypergeometric term
has essentially cubic growth with the problem size. For telescopers of higher
order but lower degree we obtain a quintic bound. Experiments suggest that
these bounds are tight. As applications of our results, we give an improved
bound on the maximal possible integer root of the leading coefficient of a
telescoper, and the first discussion of the bit complexity of creative
telescoping.
","['\nManuel Kauers\n', '\nLily Yen\n']","21 pages, 2 figures, to appear in the Journal of Symbolic Computation",,http://arxiv.org/abs/1311.3720v2,cs.SC,['cs.SC'],,,[]
Exact Solutions in Structured Low-Rank Approximation,http://arxiv.org/abs/1311.2376v3,2013-11-11T08:22:26Z,2017-02-22T17:31:11Z,"  Structured low-rank approximation is the problem of minimizing a weighted
Frobenius distance to a given matrix among all matrices of fixed rank in a
linear space of matrices. We study exact solutions to this problem by way of
computational algebraic geometry. A particular focus lies on Hankel matrices,
Sylvester matrices and generic linear spaces.
","['\nGiorgio Ottaviani\n', '\nPierre-Jean Spaenlehauer\n', '\nBernd Sturmfels\n']",22 pages; theorem numbering fits with the journal version,"SIAM Journal on Matrix Analysis and Applications, 35 (4) (2014),
  1521-1542",http://arxiv.org/abs/1311.2376v3,math.OC,"['math.OC', 'cs.SC', 'math.AG', 'stat.CO', '14Q15, 65K10, 68W30, 93B11']",,,[]
Chapter 10: Algebraic Algorithms,http://arxiv.org/abs/1311.3731v1,2013-11-15T05:50:38Z,2013-11-15T05:50:38Z,"  Our Chapter in the upcoming Volume I: Computer Science and Software
Engineering of Computing Handbook (Third edition), Allen Tucker, Teo Gonzales
and Jorge L. Diaz-Herrera, editors, covers Algebraic Algorithms, both symbolic
and numerical, for matrix computations and root-finding for polynomials and
systems of polynomials equations. We cover part of these large subjects and
include basic bibliography for further study. To meet space limitation we cite
books, surveys, and comprehensive articles with pointers to further references,
rather than including all the original technical papers.
","['\nIoannis Z. Emiris\n', '\nVictor Y. Pan\n', '\nElias P. Tsigaridas\n']",41.1 pages,,http://arxiv.org/abs/1311.3731v1,cs.DS,"['cs.DS', 'cs.NA', 'cs.SC']",,,[]
"Composing and Factoring Generalized Green's Operators and Ordinary
  Boundary Problems",http://arxiv.org/abs/1310.8455v1,2013-10-31T10:56:22Z,2013-10-31T10:56:22Z,"  We consider solution operators of linear ordinary boundary problems with ""too
many"" boundary conditions, which are not always solvable. These generalized
Green's operators are a certain kind of generalized inverses of differential
operators. We answer the question when the product of two generalized Green's
operators is again a generalized Green's operator for the product of the
corresponding differential operators and which boundary problem it solves.
Moreover, we show that---provided a factorization of the underlying
differential operator---a generalized boundary problem can be factored into
lower order problems corresponding to a factorization of the respective Green's
operators. We illustrate our results by examples using the Maple package
IntDiffOp, where the presented algorithms are implemented.
","['\nAnja Korporal\n', '\nGeorg Regensburger\n']",19 pages,"AADIOS 2012, LNCS 8372, pp. 116-134, 2014",http://dx.doi.org/10.1007/978-3-642-54479-8_5,cs.SC,"['cs.SC', 'cs.MS', 'cs.NA', 'math.CA', '68W30 (Primary), 34B05, 15A09 (Secondary)']",10.1007/978-3-642-54479-8_5,,[]
Automatic congruences for diagonals of rational functions,http://arxiv.org/abs/1310.8635v2,2013-10-31T18:47:07Z,2014-04-23T14:05:53Z,"  In this paper we use the framework of automatic sequences to study
combinatorial sequences modulo prime powers. Given a sequence whose generating
function is the diagonal of a rational power series, we provide a method, based
on work of Denef and Lipshitz, for computing a finite automaton for the
sequence modulo $p^\alpha$, for all but finitely many primes $p$. This method
gives completely automatic proofs of known results, establishes a number of new
theorems for well-known sequences, and allows us to resolve some conjectures
regarding the Ap\'ery numbers. We also give a second method, which applies to
an algebraic sequence modulo $p^\alpha$ for all primes $p$, but is
significantly slower. Finally, we show that a broad range of multidimensional
sequences possess Lucas products modulo $p$.
","['\nEric Rowland\n', '\nReem Yassawi\n']","42 pages, many figures; final version (minor changes)",Journal de Th\'eorie des Nombres de Bordeaux 27 (2015) 245-288,http://dx.doi.org/10.5802/jtnb.901,math.NT,"['math.NT', 'cs.SC', 'math.CO', '05A15, 11A07, 11B50, 11B85']",10.5802/jtnb.901,,[]
"A Polyhedral Method to Compute All Affine Solution Sets of Sparse
  Polynomial Systems",http://arxiv.org/abs/1310.4128v1,2013-10-15T17:44:24Z,2013-10-15T17:44:24Z,"  To compute solutions of sparse polynomial systems efficiently we have to
exploit the structure of their Newton polytopes. While the application of
polyhedral methods naturally excludes solutions with zero components, an
irreducible decomposition of a variety is typically understood in affine space,
including also those components with zero coordinates. We present a polyhedral
method to compute all affine solution sets of a polynomial system. The method
enumerates all factors contributing to a generalized permanent. Toric solution
sets are recovered as a special case of this enumeration. For sparse systems as
adjacent 2-by-2 minors our methods scale much better than the techniques from
numerical algebraic geometry.
","['\nDanko Adrovic\n', '\nJan Verschelde\n']",,,http://arxiv.org/abs/1310.4128v1,cs.SC,"['cs.SC', 'math.AG', 'math.CO']",,,[]
SymbolicData:SDEval - Benchmarking for Everyone,http://arxiv.org/abs/1310.5551v1,2013-10-18T19:58:03Z,2013-10-18T19:58:03Z,"  In this paper we will present SDeval, a software project that contains tools
for creating and running benchmarks with a focus on problems in computer
algebra. It is built on top of the Symbolic Data project, able to translate
problems in the database into executable code for various computer algebra
systems. The included tools are designed to be very flexible to use and to
extend, such that they can be utilized even in contexts of other communities.
With the presentation of SDEval, we will also address particularities of
benchmarking in the field of computer algebra. Furthermore, with SDEval, we
provide a feasible and automatizable way of reproducing benchmarks published in
current research works, which appears to be a difficult task in general due to
the customizability of the available programs. We will simultaneously present
the current developments in the Symbolic Data project.
","['\nAlbert Heinle\n', '\nViktor Levandovskyy\n', '\nAndreas Nareike\n']",,,http://arxiv.org/abs/1310.5551v1,cs.SC,"['cs.SC', 'cs.MS', 'cs.SE']",,,[]
On Jacobian group arithmetic for typical divisors on curves,http://arxiv.org/abs/1310.6324v4,2013-10-23T18:44:31Z,2017-11-14T14:50:32Z,"  In a previous joint article with F. Abu Salem, we gave efficient algorithms
for Jacobian group arithmetic of ""typical"" divisor classes on C_{3,4} curves,
improving on similar results by other authors. At that time, we could only
state that a generic divisor was typical, and hence unlikely to be encountered
if one implemented these algorithms over a very large finite field. This
article pins down an explicit characterization of these typical divisors, for
an arbitrary smooth projective curve of genus g >= 1 having at least one
rational point. We give general algorithms for Jacobian group arithmetic with
these typical divisors, and prove not only that the algorithms are correct if
various divisors are typical, but also that the success of our algorithms
provides a guarantee that the resulting output is correct and that the
resulting input and/or output divisors are also typical. These results apply in
particular to our earlier algorithms for C_{3,4} curves. As a byproduct, we
obtain a further speedup of approximately 15% on our previous algorithms for
C_{3,4} curves.
",['\nKamal Khuri-Makdisi\n'],"29 pages, further editing to take into account referee reports.
  Section 3 reworked to better separate the general results from those for
  C_{3,4} curves, which are now marked as examples. Section 4 now sets off the
  results more clearly instead of including them in narrative form in the text","Res. Number Theory 4 (2018), no. 1, Art. 3, 29 pp",http://dx.doi.org/10.1007/s40993-018-0101-6,math.NT,"['math.NT', 'cs.SC', 'math.AG', '14Q05 (primary), 11Y16, 14H40, 11G20']",10.1007/s40993-018-0101-6,,[]
"Differential elimination by differential specialization of Sylvester
  style matrices",http://arxiv.org/abs/1310.2081v2,2013-10-08T10:36:51Z,2014-09-06T10:41:26Z,"  Differential resultant formulas are defined, for a system $\mathcal{P}$ of
$n$ ordinary Laurent differential polynomials in $n-1$ differential variables.
These are determinants of coefficient matrices of an extended system of
polynomials obtained from $\mathcal{P}$ through derivations and multiplications
by Laurent monomials. To start, through derivations, a system $ps(\mathcal{P})$
of $L$ polynomials in $L-1$ algebraic variables is obtained, which is non
sparse in the order of derivation. This enables the use of existing formulas
for the computation of algebraic resultants, of the multivariate sparse
algebraic polynomials in $ps(\mathcal{P})$, to obtain polynomials in the
differential elimination ideal generated by $\mathcal{P}$. The formulas
obtained are multiples of the sparse differential resultant defined by Li, Yuan
and Gao, and provide order and degree bounds in terms of mixed volumes in the
generic case.
",['\nSonia L. Rueda\n'],,"Rueda, S.L. Differential elimination by differential
  specialization of Sylvester style matrices. Advances in Applied Mathematics
  72 (2016), 4-37",http://dx.doi.org/10.1016/j.aam.2015.07.002,math.AP,"['math.AP', 'cs.SC', '34G10, 34L99']",10.1016/j.aam.2015.07.002,,[]
Middle-Solving F4 to Compute Grobner bases for Cryptanalysis over GF(2),http://arxiv.org/abs/1310.2332v1,2013-10-09T03:14:27Z,2013-10-09T03:14:27Z,"  Algebraic cryptanalysis usually requires to recover the secret key by solving
polynomial equations. Faugere's F4 is a well-known Grobner bases algorithm to
solve this problem. However, a serious drawback exists in the Grobner bases
based algebraic attacks, namely, any information won't be got if we couldn't
work out the Grobner bases of the polynomial equations system. In this paper,
we in-depth research the F4 algorithm over GF(2). By using S-polynomials to
replace critical pairs and computing the normal form of the productions with
respect to the field equations in certain steps, many ""redundant"" reductors are
avoided during the computation process of the F4 algorithm. By slightly
modifying the logic of F4 algorithm, we solve the univariate polynomials
appeared in the algorithm and then back-substitute the values of the solved
variables at each iteration of the algorithm. We call our improvements
Middle-Solving F4. The heuristic strategy of Middle-Solving overcomes the
drawback of algebraic attacks and well suits algebraic attacks. It has never
been applied to the Grobner bases algorithm before. Experiments to some Hidden
Field Equation instances and some classical benchmarks (Cyclic 6, Gonnet83)
show that Middle-Solving F4 is faster and uses less memory than Faugere's F4.
","['\nHeliang Huang\n', '\nWansu Bao\n']",,,http://arxiv.org/abs/1310.2332v1,cs.SC,"['cs.SC', 'cs.CR', 'cs.NI', 'math.AC']",,,[]
"Efficient Algorithms for Computing Rational First Integrals and Darboux
  Polynomials of Planar Polynomial Vector Fields",http://arxiv.org/abs/1310.2778v1,2013-10-10T11:39:24Z,2013-10-10T11:39:24Z,"  We present fast algorithms for computing rational first integrals with
bounded degree of a planar polynomial vector field. Our approach is inspired by
an idea of Ferragut and Giacomini. We improve upon their work by proving that
rational first integrals can be computed via systems of linear equations
instead of systems of quadratic equations. This leads to a probabilistic
algorithm with arithmetic complexity $\bigOsoft(N^{2 \omega})$ and to a
deterministic algorithm solving the problem in $\bigOsoft(d^2N^{2 \omega+1})$
arithmetic operations, where $N$ denotes the given bound for the degree of the
rational first integral, and where $d \leq N$ is the degree of the vector
field, and $\omega$ the exponent of linear algebra. We also provide a fast
heuristic variant which computes a rational first integral, or fails, in
$\bigOsoft(N^{\omega+2})$ arithmetic operations. By comparison, the best
previous algorithm uses at least $d^{\omega+1}\, N^{4\omega +4}$ arithmetic
operations. We then show how to apply a similar method to the computation of
Darboux polynomials. The algorithms are implemented in a Maple package which is
available to interested readers with examples showing its efficiency.
","['\nAlin Bostan\nINRIA Saclay - Ile de France, MSR - INRIA\n', '\nGuillaume Chèze\nIMT\n', '\nThomas Cluzeau\nXLIM\n', '\nJacques-Arthur Weil\nXLIM\n']",,,http://arxiv.org/abs/1310.2778v1,cs.SC,"['cs.SC', 'cs.DS', 'math.CA', 'nlin.SI']",,,"['INRIA Saclay - Ile de France, MSR - INRIA', 'IMT', 'XLIM', 'XLIM']"
"The 3 x 3 x 3 hyperdeterminant as a polynomial in the fundamental
  invariants for SL(3,C) x SL(3,C) x SL(3,C)",http://arxiv.org/abs/1310.3257v2,2013-10-11T19:39:35Z,2014-02-17T19:35:36Z,"  We briefly review previous work on the invariant theory of 3 x 3 x 3 arrays.
We then recall how to generate arrays of arbitrary size m_1 x ... x m_k with
hyperdeterminant 0. Our main result is an explicit formula for the 3 x 3 x 3
hyperdeterminant as a polynomial in the fundamental invariants of degrees 6, 9
and 12 for the action of the Lie group SL(3,C) x SL(3,C) x SL(3,C). We apply
our calculations to Nurmiev's classification of normal forms for 3 x 3 x 3
arrays.
","['\nMurray Bremner\n', '\nJiaxiong Hu\n', '\nLuke Oeding\n']","10 pages, to appear in Mathematics in Computer Science (Special Issue
  on Computational Algebraic Geometry)",,http://dx.doi.org/10.1007/s11786-014-0186-9,math.AG,"['math.AG', 'cs.SC', 'math.RT', 'Primary 13A50, Secondary 15A72, 17B10']",10.1007/s11786-014-0186-9,,[]
Introduction to the Symbolic Integration System,http://arxiv.org/abs/1309.6655v1,2013-09-23T16:26:40Z,2013-09-23T16:26:40Z,"  Symbolic integration is an important module of a typical Computer Algebra
System. As for now, Mathematica, Matlab, Maple and Sage are all mainstream CAS.
They share the same framework for symbolic integration at some points. In this
book first we review the state of the art in the field of CAS. Then we focus on
typical frameworks of the current symbolic integration systems and summarize
the main mathematical theories behind these frameworks. Based on the
open-source computer algebra system maTHmU developed by our team in our
university, we propose a potential framework to improve the performance of the
current symbolic integration system.
",['\nWeiguang Mao\n'],273 pages in Chinese,,http://arxiv.org/abs/1309.6655v1,cs.SC,['cs.SC'],,,[]
"Applications of Continuous Amortization to Bisection-based Root
  Isolation",http://arxiv.org/abs/1309.5991v1,2013-09-23T21:48:06Z,2013-09-23T21:48:06Z,"  Continuous amortization is a technique for computing the complexity of
algorithms, and it was first presented by the author in Burr, Krahmer, & Yap
(2009). Continuous amortization can result in simpler and more straight-forward
complexity analyses, and it was used in Burr, Krahmer, & Yap (2009), Burr &
Krahmer (2012), and Sharma & Yap (2012) to provide complexity bounds for simple
root isolation algorithms. This paper greatly extends the reach of continuous
amortization to serve as an overarching technique which can be used to compute
complexity of many root isolation techniques in a straight-forward manner.
Additionally, the technique of continuous amortization is extended to higher
dimensions and to the computation of the bit-complexity of algorithms. In this
paper, six continuous amortization calculations are performed to compute
complexity bounds (on either the size of the subdivision tree or the bit
complexity) for several algorithms (including algorithms based on Sturm
sequences, Descartes' rule of signs, and polynomial evaluation); in each case,
continuous amortization achieves an optimal complexity bound.
",['\nMichael A. Burr\n'],,,http://arxiv.org/abs/1309.5991v1,cs.DS,"['cs.DS', 'cs.SC', '68W40, 68W30, 14Q20']",,,[]
"Finding Linear Dependencies in Integration-By-Parts Equations: A Monte
  Carlo Approach",http://arxiv.org/abs/1309.7287v2,2013-09-27T16:23:42Z,2014-02-11T14:59:01Z,"  The reduction of a large number of scalar integrals to a small set of master
integrals via Laporta's algorithm is common practice in multi-loop
calculations. It is also a major bottleneck in terms of running time and memory
consumption. It involves solving a large set of linear equations where many of
the equations are linearly dependent. We propose a simple algorithm that
eliminates all linearly dependent equations from a given system, reducing the
time and space requirements of a subsequent run of Laporta's algorithm.
",['\nPhilipp Kant\n'],"8 pages, 1 figure. Added references. Some minor additions",,http://dx.doi.org/10.1016/j.cpc.2014.01.017,hep-ph,"['hep-ph', 'cs.SC', 'physics.comp-ph', 'J.2']",10.1016/j.cpc.2014.01.017,,[]
Modernizing PHCpack through phcpy,http://arxiv.org/abs/1310.0056v2,2013-09-30T21:05:03Z,2014-04-29T15:00:39Z,"  PHCpack is a large software package for solving systems of polynomial
equations. The executable phc is menu driven and file oriented. This paper
describes the development of phcpy, a Python interface to PHCpack. Instead of
navigating through menus, users of phcpy solve systems in the Python shell or
via scripts. Persistent objects replace intermediate files.
",['\nJan Verschelde\n'],"Part of the Proceedings of the 6th European Conference on Python in
  Science (EuroSciPy 2013), Pierre de Buyl and Nelle Varoquaux editors, (2014)",,http://arxiv.org/abs/1310.0056v2,cs.MS,"['cs.MS', 'cs.SC', 'math.AG', 'math.NA']",,,[]
"Modern Summation Methods for Loop Integrals in Quantum Field Theory: The
  Packages Sigma, EvaluateMultiSums and SumProduction",http://arxiv.org/abs/1310.0160v1,2013-10-01T07:07:52Z,2013-10-01T07:07:52Z,"  A large class of Feynman integrals, like e.g., two-point parameter integrals
with at most one mass and containing local operator insertions, can be
transformed to multi-sums over hypergeometric expressions. In this survey
article we present a difference field approach for symbolic summation that
enables one to simplify such definite nested sums to indefinite nested sums. In
particular, the simplification is given -if possible- in terms of harmonic
sums, generalized harmonic sums, cyclotomic harmonic sums or binomial sums.
Special emphasis is put on the developed packages Sigma, EvaluateMultiSums and
SumProduction that assist in the task to perform these simplifications
completely automatically for huge input expressions.
",['\nCarsten Schneider\n'],Uses the style jpconf,,http://dx.doi.org/10.1088/1742-6596/523/1/012037,cs.SC,"['cs.SC', 'hep-th', 'math-ph', 'math.MP']",10.1088/1742-6596/523/1/012037,,[]
"A probabilistic and deterministic modular algorithm for computing
  Groebner basis over $\Q$",http://arxiv.org/abs/1309.4044v2,2013-09-16T17:31:38Z,2013-11-18T19:22:43Z,"  Modular algorithm are widely used in computer algebra systems (CAS), for
example to compute efficiently the gcd of multivariate polynomials. It is known
to work to compute Groebner basis over $\Q$, but it does not seem to be popular
among CAS implementers. In this paper, I will show how to check a candidate
Groebner basis (obtained by reconstruction of several Groebner basis modulo
distinct prime numbers) with a given error probability, that may be 0 if a
certified Groebner basis is desired. This algorithm is now the default
algorithm used by the Giac/Xcas computer algebra system with competitive
timings, thanks to a trick that can accelerate computing Groebner basis modulo
a prime once the computation has been done modulo another prime.
",['\nBernard Parisse\nIF\n'],,,http://arxiv.org/abs/1309.4044v2,cs.SC,['cs.SC'],,,['IF']
"Higher-order Reverse Automatic Differentiation with emphasis on the
  third-order",http://arxiv.org/abs/1309.5479v1,2013-09-21T14:00:40Z,2013-09-21T14:00:40Z,"  It is commonly assumed that calculating third order information is too
expensive for most applications. But we show that the directional derivative of
the Hessian ($D^3f(x)\cdot d$) can be calculated at a cost proportional to that
of a state-of-the-art method for calculating the Hessian matrix. We do this by
first presenting a simple procedure for designing high order reverse methods
and applying it to deduce several methods including a reverse method that
calculates $D^3f(x)\cdot d$. We have implemented this method taking into
account symmetry and sparsity, and successfully calculated this derivative for
functions with a million variables. These results indicate that the use of
third order information in a general nonlinear solver, such as Halley-Chebyshev
methods, could be a practical alternative to Newton's method.
","['\nRobert M. Gower\n', '\nArtur L. Gower\n']",,,http://dx.doi.org/10.1007/s10107-014-0827-4,cs.MS,"['cs.MS', 'cs.SC', 'math.OC']",10.1007/s10107-014-0827-4,,[]
On the Complexity of Computing Critical Points with Gröbner Bases,http://arxiv.org/abs/1309.2138v3,2013-09-09T12:49:18Z,2014-05-23T09:15:05Z,"  Computing the critical points of a polynomial function $q\in\mathbb
Q[X_1,\ldots,X_n]$ restricted to the vanishing locus $V\subset\mathbb R^n$ of
polynomials $f_1,\ldots, f_p\in\mathbb Q[X_1,\ldots, X_n]$ is of first
importance in several applications in optimization and in real algebraic
geometry. These points are solutions of a highly structured system of
multivariate polynomial equations involving maximal minors of a Jacobian
matrix. We investigate the complexity of solving this problem by using
Gr\""obner basis algorithms under genericity assumptions on the coefficients of
the input polynomials. The main results refine known complexity bounds (which
depend on the maximum $D=\max(deg(f_1),\ldots,deg(f_p),deg(q))$) to bounds
which depend on the list of degrees $(deg(f_1),\ldots,deg(f_p),deg(q))$: we
prove that the Gr\""obner basis computation can be performed in
$\delta^{O(\log(A)/\log(G))}$ arithmetic operations in $\mathbb Q$, where
$\delta$ is the algebraic degree of the ideal vanishing on the critical points,
and $A$ and $G$ are the arithmetic and geometric average of a multiset
constructed from the sequence of degrees. As a by-product, we prove that
solving such generic optimization problems with Gr\""obner bases requires at
most $D^{O(n)}$ arithmetic operations in $\mathbb Q$, which meets the best
known complexity bound for this problem. Finally, we illustrate these
complexity results with experiments, giving evidence that these bounds are
relevant for applications.
",['\nPierre-Jean Spaenlehauer\n'],25 pages,,http://arxiv.org/abs/1309.2138v3,cs.SC,['cs.SC'],,,[]
"A ""Piano Movers"" Problem Reformulated",http://arxiv.org/abs/1309.1588v2,2013-09-06T10:20:14Z,2014-04-24T11:44:20Z,"  It has long been known that cylindrical algebraic decompositions (CADs) can
in theory be used for robot motion planning. However, in practice even the
simplest examples can be too complicated to tackle. We consider in detail a
""Piano Mover's Problem"" which considers moving an infinitesimally thin piano
(or ladder) through a right-angled corridor.
  Producing a CAD for the original formulation of this problem is still
infeasible after 25 years of improvements in both CAD theory and computer
hardware. We review some alternative formulations in the literature which use
differing levels of geometric analysis before input to a CAD algorithm. Simpler
formulations allow CAD to easily address the question of the existence of a
path. We provide a new formulation for which both a CAD can be constructed and
from which an actual path could be determined if one exists, and analyse the
CADs produced using this approach for variations of the problem.
  This emphasises the importance of the precise formulation of such problems
for CAD. We analyse the formulations and their CADs considering a variety of
heuristics and general criteria, leading to conclusions about tackling other
problems of this form.
","['\nDavid Wilson\n', '\nJames H. Davenport\n', '\nMatthew England\n', '\nRussell Bradford\n']",8 pages. Copyright IEEE 2014,"Proceedings of the 15th International Symposium on Symbolic and
  Numeric Algorithms for Scientific Computing (SYNASC '13), pp. 53-60. IEEE,
  2013",http://dx.doi.org/10.1109/SYNASC.2013.14,cs.CG,"['cs.CG', 'cs.SC', '68W30', 'I.1.4; I.2.9']",10.1109/SYNASC.2013.14,,[]
"Computing Equilibria of Semi-algebraic Economies Using Triangular
  Decomposition and Real Solution Classification",http://arxiv.org/abs/1308.5029v1,2013-08-23T02:24:00Z,2013-08-23T02:24:00Z,"  In this paper, we are concerned with the problem of determining the existence
of multiple equilibria in economic models. We propose a general and complete
approach for identifying multiplicities of equilibria in semi-algebraic
economies, which may be expressed as semi-algebraic systems. The approach is
based on triangular decomposition and real solution classification, two
powerful tools of algebraic computation. Its effectiveness is illustrated by
two examples of application.
","['\nXiaoliang Li\n', '\nDongming Wang\n']","24 pages, 5 figures",,http://arxiv.org/abs/1308.5029v1,cs.SC,['cs.SC'],,,[]
Branch Cuts in Maple 17,http://arxiv.org/abs/1308.6523v1,2013-08-29T17:06:18Z,2013-08-29T17:06:18Z,"  Accurate and comprehensible knowledge about the position of branch cuts is
essential for correctly working with multi-valued functions, such as the square
root and logarithm. We discuss the new tools in Maple 17 for calculating and
visualising the branch cuts of such functions, and others built up from them.
The cuts are described in an intuitive and accurate form, offering substantial
improvement on the descriptions previously available.
","['\nM. England\n', '\nE. Cheb-Terrab\n', '\nR. Bradford\n', '\nJ. H. Davenport\n', '\nD. Wilson\n']",,"ACM Communications in Computer Algebra 48:1 pp. 24-27, ACM, 2014",http://dx.doi.org/10.1145/2644288.2644293,cs.SC,"['cs.SC', 'cs.MS', 'I.1.1, G.4', 'I.1.1; I.1.2; G.4']",10.1145/2644288.2644293,,[]
"Signature-Based Gröbner Basis Algorithms --- Extended MMM Algorithm
  for computing Gröbner bases",http://arxiv.org/abs/1308.2371v1,2013-08-11T07:35:14Z,2013-08-11T07:35:14Z,"  Signature-based algorithms is a popular kind of algorithms for computing
Gr\""obner bases, and many related papers have been published recently. In this
paper, no new signature-based algorithms and no new proofs are presented.
Instead, a view of signature-based algorithms is given, that is,
signature-based algorithms can be regarded as an extended version of the famous
MMM algorithm. By this view, this paper aims to give an easier way to
understand signature-based Gr\""obner basis algorithms.
",['\nYao Sun\n'],,,http://arxiv.org/abs/1308.2371v1,cs.SC,['cs.SC'],,,[]
xTras: a field-theory inspired xAct package for Mathematica,http://arxiv.org/abs/1308.3493v1,2013-08-15T20:00:02Z,2013-08-15T20:00:02Z,"  We present the tensor computer algebra package xTras, which provides
functions and methods frequently needed when doing (classical) field theory.
Amongst others, it can compute contractions, make Ans\""atze, and solve
tensorial equations. It is built upon the tensor computer algebra system xAct,
a collection of packages for Mathematica.
",['\nTeake Nutma\n'],"29 pages. The package can be downloaded from
  http://www.xact.es/xtras/",Comput. Phys. Commun. 185 (2014) 1719-1738,http://dx.doi.org/10.1016/j.cpc.2014.02.006,cs.SC,"['cs.SC', 'cs.MS', 'gr-qc', 'hep-th']",10.1016/j.cpc.2014.02.006,,[]
Computing Real Roots of Real Polynomials,http://arxiv.org/abs/1308.4088v2,2013-08-19T18:14:39Z,2015-03-11T08:47:35Z,"  Computing the roots of a univariate polynomial is a fundamental and
long-studied problem of computational algebra with applications in mathematics,
engineering, computer science, and the natural sciences. For isolating as well
as for approximating all complex roots, the best algorithm known is based on an
almost optimal method for approximate polynomial factorization, introduced by
Pan in 2002. Pan's factorization algorithm goes back to the splitting circle
method from Schoenhage in 1982. The main drawbacks of Pan's method are that it
is quite involved and that all roots have to be computed at the same time. For
the important special case, where only the real roots have to be computed, much
simpler methods are used in practice; however, they considerably lag behind
Pan's method with respect to complexity.
  In this paper, we resolve this discrepancy by introducing a hybrid of the
Descartes method and Newton iteration, denoted ANEWDSC, which is simpler than
Pan's method, but achieves a run-time comparable to it. Our algorithm computes
isolating intervals for the real roots of any real square-free polynomial,
given by an oracle that provides arbitrary good approximations of the
polynomial's coefficients. ANEWDSC can also be used to only isolate the roots
in a given interval and to refine the isolating intervals to an arbitrary small
size; it achieves near optimal complexity for the latter task.
","['\nMichael Sagraloff\n', '\nKurt Mehlhorn\n']",to appear in the Journal of Symbolic Computation,,http://arxiv.org/abs/1308.4088v2,cs.SC,"['cs.SC', 'cs.NA', 'math.NA', 'G.1.5; F.2.1; G.1.0; I.1.2']",,,[]
"A nearly optimal algorithm for deciding connectivity queries in smooth
  and bounded real algebraic sets",http://arxiv.org/abs/1307.7836v3,2013-07-30T06:30:48Z,2016-10-27T11:36:49Z,"  A roadmap for a semi-algebraic set $S$ is a curve which has a non-empty and
connected intersection with all connected components of $S$. Hence, this kind
of object, introduced by Canny, can be used to answer connectivity queries
(with applications, for instance, to motion planning) but has also become of
central importance in effective real algebraic geometry, since it is used in
higher-level algorithms. In this paper, we provide a probabilistic algorithm
which computes roadmaps for smooth and bounded real algebraic sets. Its output
size and running time are polynomial in $(nD)^{n\log(d)}$, where $D$ is the
maximum of the degrees of the input polynomials, $d$ is the dimension of the
set under consideration and $n$ is the number of variables. More precisely, the
running time of the algorithm is essentially subquadratic in the output size.
Even under our assumptions, it is the first roadmap algorithm with output size
and running time polynomial in $(nD)^{n\log(d)}$.
","['\nMohab Safey El Din\nLIP6, PolSys\n', '\nEric Schost\n']","Major revision, accepted for publication to Journal of the ACM",,http://arxiv.org/abs/1307.7836v3,cs.SC,['cs.SC'],,,"['LIP6, PolSys']"
"Fast Algorithms for Refined Parameterized Telescoping in Difference
  Fields",http://arxiv.org/abs/1307.7887v2,2013-07-30T09:38:54Z,2013-12-30T16:10:23Z,"  Parameterized telescoping (including telescoping and creative telescoping)
and refined versions of it play a central role in the research area of symbolic
summation. Karr introduced 1981 $\Pi\Sigma$-fields, a general class of
difference fields, that enables one to consider this problem for indefinite
nested sums and products covering as special cases, e.g., the
($q$--)hypergeometric case and their mixed versions. This survey article
presents the available algorithms in the framework of $\Pi\Sigma$-extensions
and elaborates new results concerning efficiency.
",['\nCarsten Schneider\n'],"In the updated versions typos are removed and illustrative examples
  are inluced",,http://arxiv.org/abs/1307.7887v2,cs.SC,['cs.SC'],,,[]
"Proceedings Fourth International Symposium on Symbolic Computation in
  Software Science",http://arxiv.org/abs/1307.8029v1,2013-07-30T16:01:33Z,2013-07-30T16:01:33Z,"  Symbolic computation is the science of computing with symbolic objects
(terms, formulae, programs, algebraic objects, geometrical objects, etc).
Powerful symbolic algorithms have been developed during the past decades and
have played an influential role in theorem proving, automated reasoning,
software verification, model checking, rewriting, formalisation of mathematics,
network security, Groebner bases, characteristic sets, etc.
  The international Symposium on ""Symbolic Computation in Software Science"" is
the fourth in the SCSS workshop series. SCSS 2008 and 2010 took place at the
Research Institute for Symbolic Computation (RISC), Hagenberg, Austria, and,
SCSS 2009 took place in Gammarth, Tunisia. These symposium grew out of internal
workshops that bring together researchers from: a) SCORE (Symbolic Computation
Research Group) at the University of Tsukuba, Japan, b) Theorema Group at the
Research Institute for Symbolic Computation, Johannes Kepler University Linz,
Austria, c) SSFG (Software Science Foundation Group) at Kyoto University,
Japan, and d) Sup'Com (Higher School of Communication of Tunis) at the
University of Carthage, Tunisia.
","['\nAdel Bouhoula\n', '\nTetsuo Ida\n', '\nFairouz Kamareddine\n']",,"EPTCS 122, 2013",http://dx.doi.org/10.4204/EPTCS.122,cs.SC,"['cs.SC', 'cs.LO', 'D.1, D.2, D.3, F.3, F.4,']",10.4204/EPTCS.122,,[]
"Probabilistic Algorithm for Polynomial Optimization over a Real
  Algebraic Set",http://arxiv.org/abs/1307.8281v2,2013-07-31T11:01:20Z,2014-05-07T09:33:28Z,"  Let $f, f_1, \ldots, f_\nV$ be polynomials with rational coefficients in the
indeterminates $\bfX=X_1, \ldots, X_n$ of maximum degree $D$ and $V$ be the set
of common complex solutions of $\F=(f_1,\ldots, f_\nV)$. We give an algorithm
which, up to some regularity assumptions on $\F$, computes an exact
representation of the global infimum $f^\star=\inf_{x\in V\cap\R^n} f\Par{x}$,
i.e. a univariate polynomial vanishing at $f^\star$ and an isolating interval
for $f^\star$. Furthermore, this algorithm decides whether $f^\star$ is reached
and if so, it returns $x^\star\in V\cap\R^n$ such that
$f\Par{x^\star}=f^\star$. This algorithm is probabilistic. It makes use of the
notion of polar varieties. Its complexity is essentially cubic in $\Par{\nV
D}^n$ and linear in the complexity of evaluating the input. This fits within
the best known deterministic complexity class $D^{O(n)}$. We report on some
practical experiments of a first implementation that is available as a Maple
package. It appears that it can tackle global optimization problems that were
unreachable by previous exact algorithms and can manage instances that are hard
to solve with purely numeric techniques. As far as we know, even under the
extra genericity assumptions on the input, it is the first probabilistic
algorithm that combines practical efficiency with good control of complexity
for this problem.
","['\nAurélien Greuet\nINRIA Paris-Rocquencourt, LIP6, LM-Versailles, LIFL\n', '\nMohab Safey El Din\nINRIA Paris-Rocquencourt, LIP6\n']",,,http://arxiv.org/abs/1307.8281v2,cs.SC,"['cs.SC', 'math.OC']",,,"['INRIA Paris-Rocquencourt, LIP6, LM-Versailles, LIFL', 'INRIA Paris-Rocquencourt, LIP6']"
Fast polynomial evaluation and composition,http://arxiv.org/abs/1307.5655v2,2013-07-22T11:12:44Z,2013-07-26T14:51:07Z,"  The library \emph{fast\_polynomial} for Sage compiles multivariate
polynomials for subsequent fast evaluation. Several evaluation schemes are
handled, such as H\""orner, divide and conquer and new ones can be added easily.
Notably, a new scheme is introduced that improves the classical divide and
conquer scheme when the number of terms is not a pure power of two. Natively,
the library handles polynomials over gmp big integers, boost intervals, python
numeric types. And any type that supports addition and multiplication can
extend the library thanks to the template design. Finally, the code is
parallelized for the divide and conquer schemes, and memory allocation is
localized and optimized for the different evaluation schemes. This extended
abstract presents the concepts behind the \emph{fast\_polynomial} library. The
sage package can be downloaded at
\url{http://trac.sagemath.org/sage_trac/ticket/13358}.
",['\nGuillaume Moroz\nINRIA Nancy - Grand Est / LORIA\n'],,,http://arxiv.org/abs/1307.5655v2,cs.SC,['cs.SC'],,,['INRIA Nancy - Grand Est / LORIA']
"On Computing the Elimination Ideal Using Resultants with Applications to
  Gröbner Bases",http://arxiv.org/abs/1307.5330v2,2013-07-19T20:08:11Z,2015-10-30T09:24:37Z,"  Resultants and Gr\""obner bases are crucial tools in studying polynomial
elimination theory. We investigate relations between the variety of the
resultant of two polynomials and the variety of the ideal they generate. Then
we focus on the bivariate case, in which the elimination ideal is principal. We
study - by means of elementary tools - the difference between the multiplicity
of the factors of the generator of the elimination ideal and the multiplicity
of the factors of the resultant.
","['\nMatteo Gallet\n', '\nHamid Rahkooy\n', '\nZafeirakis Zafeirakopoulos\n']",7 pages,,http://arxiv.org/abs/1307.5330v2,math.AC,"['math.AC', 'cs.SC']",,,[]
Creative Telescoping for Holonomic Functions,http://arxiv.org/abs/1307.4554v1,2013-07-17T09:48:39Z,2013-07-17T09:48:39Z,"  The aim of this article is twofold: on the one hand it is intended to serve
as a gentle introduction to the topic of creative telescoping, from a practical
point of view; for this purpose its application to several problems is
exemplified. On the other hand, this chapter has the flavour of a survey
article: the developments in this area during the last two decades are sketched
and a selection of references is compiled in order to highlight the impact of
creative telescoping in numerous contexts.
",['\nChristoph Koutschan\n'],"Tutorial and survey article, 24 pages","In Carsten Schneider and Johannes Bluemlein (eds.): Computer
  Algebra in Quantum Field Theory: Integration, Summation and Special
  Functions. Texts & Monographs in Symbolic Computation, Springer-Verlag Wien
  2013",http://dx.doi.org/10.1007/978-3-7091-1616-6_7,cs.SC,['cs.SC'],10.1007/978-3-7091-1616-6_7,,[]
An Efficient Multiplication Algorithm Using Nikhilam Method,http://arxiv.org/abs/1307.2735v1,2013-07-10T09:58:07Z,2013-07-10T09:58:07Z,"  Multiplication is one of the most important operation in computer arithmetic.
Many integer operations such as squaring, division and computing reciprocal
require same order of time as multiplication whereas some other operations such
as computing GCD and residue operation require at most a factor of $\log n$
time more than multiplication. We propose an integer multiplication algorithm
using Nikhilam method of Vedic mathematics which can be used to multiply two
binary numbers efficiently.
",['\nShri Prakash Dwivedi\n'],Extended version to appear in ITC 2013,,http://dx.doi.org/10.1049/cp.2013.2209,cs.DS,"['cs.DS', 'cs.SC']",10.1049/cp.2013.2209,,[]
Certification of Bounds of Non-linear Functions: the Templates Method,http://arxiv.org/abs/1307.3231v1,2013-07-10T09:53:14Z,2013-07-10T09:53:14Z,"  The aim of this work is to certify lower bounds for real-valued multivariate
functions, defined by semialgebraic or transcendental expressions. The
certificate must be, eventually, formally provable in a proof system such as
Coq. The application range for such a tool is widespread; for instance Hales'
proof of Kepler's conjecture yields thousands of inequalities. We introduce an
approximation algorithm, which combines ideas of the max-plus basis method (in
optimal control) and of the linear templates method developed by Manna et al.
(in static analysis). This algorithm consists in bounding some of the
constituents of the function by suprema of quadratic forms with a well chosen
curvature. This leads to semialgebraic optimization problems, solved by
sum-of-squares relaxations. Templates limit the blow up of these relaxations at
the price of coarsening the approximation. We illustrate the efficiency of our
framework with various examples from the literature and discuss the interfacing
with Coq.
","['\nXavier Allamigeon\n', '\nStéphane Gaubert\n', '\nVictor Magron\n', '\nBenjamin Werner\n']","16 pages, 3 figures, 2 tables","Proceedings of CICM 2013 (Conferences on Intelligent Computer
  Mathematics, ""Calculemus'', Track A), Bath, July 2013, volume 7961 of Lecture
  Notes in Computer Science, pages 51--65, Springer",http://dx.doi.org/10.1007/978-3-642-39320-4_4,cs.SC,"['cs.SC', 'cs.LO']",10.1007/978-3-642-39320-4_4,,[]
"Theorema 2.0: A Graphical User Interface for a Mathematical Assistant
  System",http://arxiv.org/abs/1307.1945v1,2013-07-08T04:42:02Z,2013-07-08T04:42:02Z,"  Theorema 2.0 stands for a re-design including a complete re-implementation of
the Theorema system, which was originally designed, developed, and implemented
by Bruno Buchberger and his Theorema group at RISC. In this paper, we present
the first prototype of a graphical user interface (GUI) for the new system. It
heavily relies on powerful interactive capabilities introduced in recent
releases of the underlying Mathematica system, most importantly the possibility
of having dynamic objects connected to interface elements like sliders, menus,
check-boxes, radio-buttons and the like. All these features are fully
integrated into the Mathematica programming environment and allow the
implementation of a modern user interface.
","['\nWolfgang Windsteiger\nRISC, JKU Linz, Austria\n']","In Proceedings UITP 2012, arXiv:1307.1528","EPTCS 118, 2013, pp. 72-82",http://dx.doi.org/10.4204/EPTCS.118.5,cs.MS,"['cs.MS', 'cs.HC', 'cs.SC']",10.4204/EPTCS.118.5,,"['RISC, JKU Linz, Austria']"
Software for Evaluating Relevance of Steps in Algebraic Transformations,http://arxiv.org/abs/1306.6749v1,2013-06-28T08:15:43Z,2013-06-28T08:15:43Z,"  Students of our department solve algebraic exercises in mathematical logic in
a computerized environment. They construct transformations step by step and the
program checks the syntax, equivalence of expressions and completion of the
task. With our current project, we add a program component for checking
relevance of the steps.
",['\nRein Prank\n'],"CICM 2013, Bath",,http://arxiv.org/abs/1306.6749v1,cs.SC,"['cs.SC', 'K.3.1']",,,[]
Computing Puiseux Expansions at Cusps of the Modular Curve X0(N),http://arxiv.org/abs/1307.1627v1,2013-07-05T14:57:13Z,2013-07-05T14:57:13Z,"  The goal in this preprint is to give an efficient algorithm to compute
Puiseux expansions at cusps of X0(N). It is based on a relation with a
hypergeometric function that holds for any N.
",['\nMark van Hoeij\n'],4 pages,,http://arxiv.org/abs/1307.1627v1,math.NT,"['math.NT', 'cs.SC']",,,[]
Involutive Bases Algorithm Incorporating F5 Criterion,http://arxiv.org/abs/1306.6811v1,2013-06-28T12:33:06Z,2013-06-28T12:33:06Z,"  Faugere's F5 algorithm is the fastest known algorithm to compute Groebner
bases. It has a signature-based and an incremental structure that allow to
apply the F5 criterion for deletion of unnecessary reductions. In this paper,
we present an involutive completion algorithm which outputs a minimal
involutive basis. Our completion algorithm has a nonincremental structure and
in addition to the involutive form of Buchberger's criteria it applies the F5
criterion whenever this criterion is applicable in the course of completion to
involution. In doing so, we use the G2V form of the F5 criterion developed by
Gao, Guan and Volny IV. To compare the proposed algorithm, via a set of
benchmarks, with the Gerdt-Blinkov involutive algorithm (which does not apply
the F5 criterion) we use implementations of both algorithms done on the same
platform in Maple.
","['\nVladimir P. Gerdt\n', '\nAmir Hashemi\n', '\nBenyamin M. -Alizadeh\n']","24 pages, 2 figures",,http://arxiv.org/abs/1306.6811v1,math.AC,"['math.AC', 'cs.SC', 'math.RA', '13P10', 'I.1.2']",,,[]
Deciding Nonnegativity of Polynomials by MAPLE,http://arxiv.org/abs/1306.4059v1,2013-06-18T02:23:15Z,2013-06-18T02:23:15Z,"  There have been some effective tools for solving (constant/parametric)
semi-algebraic systems in Maple's library RegularChains since Maple 13. By
using the functions of the library, e.g., RealRootClassfication, one can prove
and discover polynomial inequalities. This paper is more or less a user guide
on using RealRootClassfication to prove the nonnegativity of polynomials. We
show by examples how to use this powerful tool to prove a polynomial is
nonnegative under some polynomial inequality and/or equation constraints. Some
tricks for using the tool are also provided.
","['\nLu Yang\n', '\nBican Xia\n']","A user guide on using RealRootClassfication to prove the
  nonnegativity of polynomials with 10 examples",,http://arxiv.org/abs/1306.4059v1,cs.SC,['cs.SC'],,,[]
"An approach to first principles electronic structure calculation by
  symbolic-numeric computation",http://arxiv.org/abs/1306.3714v1,2013-06-17T00:09:39Z,2013-06-17T00:09:39Z,"  This article is an introduction to a new approach to first principles
electronic structure calculation. The starting point is the
Hartree-Fock-Roothaan equation, in which molecular integrals are approximated
by polynomials by way of Taylor expansion with respect to atomic coordinates
and other variables. It leads to a set of polynomial equations whose solutions
are eigenstate, which is designated as algebraic molecular orbital equation.
Symbolic computation, especially, Gr\""obner bases theory, enables us to rewrite
the polynomial equations into more trimmed and tractable forms with identical
roots, from which we can unravel the relationship between physical parameters
(wave function, atomic coordinates, and others) and numerically evaluate them
one by one in order. Furthermore, this method is a unified way to solve the
electronic structure calculation, the optimization of physical parameters, and
the inverse problem as a forward problem.
",['\nAkihito Kikuchi\n'],arXiv admin note: substantial text overlap with arXiv:1209.5127,"QScience Connect: Vol. 2013, 14",http://dx.doi.org/10.5339/connect.2013.14,physics.comp-ph,"['physics.comp-ph', 'cs.SC']",10.5339/connect.2013.14,,[]
Ore Polynomials in Sage,http://arxiv.org/abs/1306.4263v1,2013-06-18T16:34:18Z,2013-06-18T16:34:18Z,"  We present a Sage implementation of Ore algebras. The main features for the
most common instances include basic arithmetic and actions; gcrd and lclm;
D-finite closure properties; natural transformations between related algebras;
guessing; desingularization; solvers for polynomials, rational functions and
(generalized) power series. This paper is a tutorial on how to use the package.
","['\nManuel Kauers\n', '\nMaximilian Jaroschek\n', '\nFredrik Johansson\n']",,,http://arxiv.org/abs/1306.4263v1,cs.SC,"['cs.SC', 'math.CO']",,,[]
Detecting Similarity of Rational Plane Curves,http://arxiv.org/abs/1306.4340v2,2013-06-18T20:19:55Z,2013-12-21T21:39:50Z,"  A novel and deterministic algorithm is presented to detect whether two given
rational plane curves are related by means of a similarity, which is a central
question in Pattern Recognition. As a by-product it finds all such
similarities, and the particular case of equal curves yields all symmetries. A
complete theoretical description of the method is provided, and the method has
been implemented and tested in the Sage system for curves of moderate degrees.
","['\nJuan Gerardo Alcázar\n', '\nCarlos Hermoso\n', '\nGeorg Muntingh\n']",22 pages,"Journal of Computational and Applied Mathematics, Volume 269
  (2014), Pages 1 - 13",http://dx.doi.org/10.1016/j.cam.2014.03.013,math.AG,"['math.AG', 'cs.CG', 'cs.SC', '14Q05, 68W30']",10.1016/j.cam.2014.03.013,,[]
"An implementation of CAD in Maple utilising problem formulation,
  equational constraints and truth-table invariance",http://arxiv.org/abs/1306.3062v1,2013-06-13T09:35:05Z,2013-06-13T09:35:05Z,"  Cylindrical algebraic decomposition (CAD) is an important tool for the
investigation of semi-algebraic sets, with applications within algebraic
geometry and beyond. We recently reported on a new implementation of CAD in
Maple which implemented the original algorithm of Collins and the subsequent
improvement to projection by McCallum. Our implementation was in contrast to
Maple's in-built CAD command, based on a quite separate theory. Although
initially developed as an investigative tool to compare the algorithms, we
found and reported that our code offered functionality not currently available
in any other existing implementations. One particularly important piece of
functionality is the ability to produce order-invariant CADs. This has allowed
us to extend the implementation to produce CADs invariant with respect to
either equational constraints (ECCADs) or the truth-tables of sequences of
formulae (TTICADs). This new functionality is contained in the second release
of our code, along with commands to consider problem formulation which can be a
major factor in the tractability of a CAD. In the report we describe the new
functionality and some theoretical discoveries it prompted. We describe how the
CADs produced using equational constraints are able to take advantage of not
just improved projection but also improvements in the lifting phase. We also
present an extension to the original TTICAD algorithm which increases both the
applicability of TTICAD and its relative benefit over other algorithms. The
code and an introductory Maple worksheet / pdf demonstrating the full
functionality of the package are freely available online.
",['\nMatthew England\n'],"12 pages; University of Bath, Dept. Computer Science Technical Report
  Series, 2013-02, 2013",,http://arxiv.org/abs/1306.3062v1,cs.SC,"['cs.SC', '68W30', 'I.1.2']",,,[]
Degeneracy loci and polynomial equation solving,http://arxiv.org/abs/1306.3390v2,2013-06-14T13:23:48Z,2013-12-16T16:38:09Z,"  Let V be a smooth equidimensional quasi-affine variety of dimension r over
the complex numbers $C$ and let $F$ be a $(p\times s)$-matrix of coordinate
functions of $C[V]$, where $s\ge p+r$. The pair $(V,F)$ determines a vector
bundle $E$ of rank $s-p$ over $W:=\{x\in V:\mathrm{rk} F(x)=p\}$. We associate
with $(V,F)$ a descending chain of degeneracy loci of E (the generic polar
varieties of $V$ represent a typical example of this situation).
  The maximal degree of these degeneracy loci constitutes the essential
ingredient for the uniform, bounded error probabilistic pseudo-polynomial time
algorithm which we are going to design and which solves a series of
computational elimination problems that can be formulated in this framework. We
describe applications to polynomial equation solving over the reals and to the
computation of a generic fiber of a dominant endomorphism of an affine space.
","['\nBernd Bank\n', '\nMarc Giusti\n', '\nJoos Heintz\n', '\nGrégoire Lecerf\n', '\nGuillermo Matera\n', '\nPablo Solernó\n']","24 pages, accepted for publication in Found. Comput. Math",,http://arxiv.org/abs/1306.3390v2,math.AG,"['math.AG', 'cs.SC', '14Q20 (Primary) 14M10, 14M12, 14P05, 68W30 (Secondary)']",,,[]
"Relativistic Coulomb Integrals and Zeilberger's Holonomic Systems
  Approach II",http://arxiv.org/abs/1306.1362v2,2013-06-06T09:58:19Z,2014-02-27T20:55:28Z,"  We derive the recurrence relations for relativistic Coulomb integrals
directly from the integral representations with the help of computer algebra
methods. In order to manage the computational complexity of this problem, we
employ holonomic closure properties in a sophisticated way.
","['\nChristoph Koutschan\n', '\nPeter Paule\n', '\nSergei K. Suslov\n']",,"In: Algebraic and Algorithmic Aspects of Differential and Integral
  Operators, Lecture Notes in Computer Science 8372, pp. 135-145,
  Springer-Verlag Berlin Heidelberg, 2014. ISBN 978-3-642-54478-1",http://dx.doi.org/10.1007/978-3-642-54479-8_6,quant-ph,"['quant-ph', 'cs.SC', 'math-ph', 'math.MP']",10.1007/978-3-642-54479-8_6,,[]
"Effective Differential Nullstellensatz for Ordinary DAE Systems with
  Constant Coefficients",http://arxiv.org/abs/1305.6298v2,2013-05-27T19:14:14Z,2014-01-13T13:21:18Z,"  We give upper bounds for the differential Nullstellensatz in the case of
ordinary systems of differential algebraic equations over any field of
constants $K$ of characteristic $0$. Let $\vec{x}$ be a set of $n$ differential
variables, $\vec{f}$ a finite family of differential polynomials in the ring
$K\{\vec{x}\}$ and $f\in K\{\vec{x}\}$ another polynomial which vanishes at
every solution of the differential equation system $\vec{f}=0$ in any
differentially closed field containing $K$. Let $d:=\max\{\deg(\vec{f}),
\deg(f)\}$ and $\epsilon:=\max\{2,{\rm{ord}}(\vec{f}), {\rm{ord}}(f)\}$. We
show that $f^M$ belongs to the algebraic ideal generated by the successive
derivatives of $\vec{f}$ of order at most $L = (n\epsilon
d)^{2^{c(n\epsilon)^3}}$, for a suitable universal constant $c>0$, and
$M=d^{n(\epsilon +L+1)}$. The previously known bounds for $L$ and $M$ are not
elementary recursive.
","[""\nLisi D'Alfonso\n"", '\nGabriela Jeronimo\n', '\nPablo Solernó\n']",,,http://arxiv.org/abs/1305.6298v2,math.AC,"['math.AC', 'cs.SC']",,,[]
"Intertwining Laplace Transformations of Linear Partial Differential
  Equations",http://arxiv.org/abs/1306.1113v2,2013-05-25T10:22:37Z,2013-10-22T07:08:28Z,"  We propose a generalization of Laplace transformations to the case of linear
partial differential operators (LPDOs) of arbitrary order in R^n. Practically
all previously proposed differential transformations of LPDOs are particular
cases of this transformation (intertwining Laplace transformation, ILT). We
give a complete algorithm of construction of ILT and describe the classes of
operators in R^n suitable for this transformation.
  Keywords: Integration of linear partial differential equations, Laplace
transformation, differential transformation
",['\nElena I. Ganzha\n'],"LaTeX, 25 pages v2: minor misprints corrected",,http://arxiv.org/abs/1306.1113v2,math.AP,"['math.AP', 'cs.SC']",,,[]
"Computer-Assisted Proofs of Some Identities for Bessel Functions of
  Fractional Order",http://arxiv.org/abs/1305.4818v2,2013-05-21T13:53:11Z,2013-07-19T10:56:59Z,"  We employ computer algebra algorithms to prove a collection of identities
involving Bessel functions with half-integer orders and other special
functions. These identities appear in the famous Handbook of Mathematical
Functions, as well as in its successor, the DLMF, but their proofs were lost.
We use generating functions and symbolic summation techniques to produce new
proofs for them.
","['\nStefan Gerhold\n', '\nManuel Kauers\n', '\nChristoph Koutschan\n', '\nPeter Paule\n', '\nCarsten Schneider\n', '\nBurkhard Zimmermann\n']","Final version, some typos were corrected. 21 pages, uses svmult.cls","In Carsten Schneider and Johannes Bluemlein (eds.): Computer
  Algebra in Quantum Field Theory: Integration, Summation and Special
  Functions. Texts & Monographs in Symbolic Computation, Springer-Verlag Wien
  2013",http://dx.doi.org/10.1007/978-3-7091-1616-6_3,cs.SC,['cs.SC'],10.1007/978-3-7091-1616-6_3,,[]
A computer algebra user interface manifesto,http://arxiv.org/abs/1305.3215v1,2013-05-14T17:21:29Z,2013-05-14T17:21:29Z,"  Many computer algebra systems have more than 1000 built-in functions, making
expertise difficult. Using mock dialog boxes, this article describes a proposed
interactive general-purpose wizard for organizing optional transformations and
allowing easy fine grain control over the form of the result even by amateurs.
This wizard integrates ideas including:
  * flexible subexpression selection;
  * complete control over the ordering of variables and commutative operands,
with well-chosen defaults;
  * interleaving the choice of successively less main variables with applicable
function choices to provide detailed control without incurring a combinatorial
number of applicable alternatives at any one level;
  * quick applicability tests to reduce the listing of inapplicable
transformations;
  * using an organizing principle to order the alternatives in a helpful
manner;
  * labeling quickly-computed alternatives in dialog boxes with a preview of
their results,
  * using ellipsis elisions if necessary or helpful;
  * allowing the user to retreat from a sequence of choices to explore other
branches of the tree of alternatives or to return quickly to branches already
visited;
  * allowing the user to accumulate more than one of the alternative forms;
  * integrating direct manipulation into the wizard; and
  * supporting not only the usual input-result pair mode, but also the useful
alternative derivational and in situ replacement modes in a unified window.
",['\nDavid R. Stoutemyer\n'],"38 pages, 12 figures, to be published in Communications in Computer
  Algebra",,http://arxiv.org/abs/1305.3215v1,cs.SC,"['cs.SC', 'cs.MS']",,,[]
Zero-nonzero and real-nonreal sign determination,http://arxiv.org/abs/1305.4131v1,2013-05-17T16:46:18Z,2013-05-17T16:46:18Z,"  We consider first the zero-nonzero determination problem, which consists in
determining the list of zero-nonzero conditions realized by a finite list of
polynomials on a finite set Z included in C^k with C an algebraic closed field.
We describe an algorithm to solve the zero-nonzero determination problem and we
perform its bit complexity analysis. This algorithm, which is in many ways an
adaptation of the methods used to solve the more classical sign determination
problem, presents also new ideas which can be used to improve sign
determination. Then, we consider the real-nonreal sign determination problem,
which deals with both the sign determination and the zero-nonzero determination
problem. We describe an algorithm to solve the real-nonreal sign determination
problem, we perform its bit complexity analysis and we discuss this problem in
a parametric context.
","['\nDaniel Perrucci\n', '\nMarie-Francoise Roy\n']",,,http://arxiv.org/abs/1305.4131v1,math.AG,"['math.AG', 'cs.SC']",,,[]
"Dual bases for non commutative symmetric and quasi-symmetric functions
  via monoidal factorization",http://arxiv.org/abs/1305.4447v1,2013-05-20T06:36:08Z,2013-05-20T06:36:08Z,"  In this work, an effective construction, via Sch\""utzenberger's monoidal
factorization, of dual bases for the non commutative symmetric and
quasi-symmetric functions is proposed.
","['\nGérard Henry Edmond Duchamp\nLIPN\n', '\nLadji Kane\nLIPN\n', '\nVincel Hoang Ngoc Minh\nLIPN\n', '\nChristophe Tollu\nLIPN\n']",,,http://arxiv.org/abs/1305.4447v1,math.CO,"['math.CO', 'cs.SC']",,,"['LIPN', 'LIPN', 'LIPN', 'LIPN']"
"Schützenberger's factorization on the (completed) Hopf algebra of
  $q-$stuffle product",http://arxiv.org/abs/1305.4450v1,2013-05-20T07:25:06Z,2013-05-20T07:25:06Z,"  In order to extend the Sch\""utzenberger's factorization, the combinatorial
Hopf algebra of the $q$-stuffles product is developed systematically in a
parallel way with that of the shuffle product and and in emphasizing the Lie
elements as studied by Ree. In particular, we will give here an effective
construction of pair of bases in duality.
","['\nChen Bui\nLIPN\n', '\nGérard Henry Edmond Duchamp\nLIPN\n', '\nVincel Hoang Ngoc Minh\nLIPN\n']",arXiv admin note: text overlap with arXiv:1302.5391,,http://arxiv.org/abs/1305.4450v1,math.CO,"['math.CO', 'cs.SC']",,,"['LIPN', 'LIPN', 'LIPN']"
Numerical Reparametrization of Rational Parametric Plane Curves,http://arxiv.org/abs/1305.2461v2,2013-05-11T01:44:28Z,2013-08-15T02:04:48Z,"  In this paper, we present an algorithm for reparametrizing algebraic plane
curves from a numerical point of view. That is, we deal with mathematical
objects that are assumed to be given approximately. More precisely, given a
tolerance $\epsilon>0$ and a rational parametrization $\cal P$ with perturbed
float coefficients of a plane curve $\cal C$, we present an algorithm that
computes a parametrization $\cal Q$ of a new plane curve $\cal D$ such that
${\cal Q}$ is an {\it $\epsilon$--proper reparametrization} of $\cal D$. In
addition, the error bound is carefully discussed and we present a formula that
measures the ""closeness"" between the input curve $\cal C$ and the output curve
$\cal D$.
","['\nSonia Perez-Diaz\n', '\nLi-Yong Shen\n']","31 pages, 23 figures","Journal of Computational and Applied Mathematics Volume 277, 15
  March 2015, Pages 138-161",http://dx.doi.org/10.1016/j.cam.2014.09.012,math.AG,"['math.AG', 'cs.SC', 'I.3.5; G.1.2']",10.1016/j.cam.2014.09.012,,[]
Generalization of Risch's Algorithm to Special Functions,http://arxiv.org/abs/1305.1481v1,2013-05-07T11:53:12Z,2013-05-07T11:53:12Z,"  Symbolic integration deals with the evaluation of integrals in closed form.
We present an overview of Risch's algorithm including recent developments. The
algorithms discussed are suited for both indefinite and definite integration.
They can also be used to compute linear relations among integrals and to find
identities for special functions given by parameter integrals. The aim of this
presentation is twofold: to introduce the reader to some basic ideas of
differential algebra in the context of integration and to raise awareness in
the physics community of computer algebra algorithms for indefinite and
definite integration.
",['\nC. G. Raab\n'],"19 pages, 1 style file, in: ""Integration, Summation and Special
  Functions in Quantum Field Theory"", to appear at Springer Verlag, Vienna",,http://arxiv.org/abs/1305.1481v1,cs.SC,"['cs.SC', 'math-ph', 'math.MP']",,,[]
Characterization of Rational Ruled Surfaces,http://arxiv.org/abs/1305.2462v2,2013-05-11T01:50:15Z,2013-06-04T00:42:54Z,"  The ruled surface is a typical modeling surface in computer aided geometric
design. It is usually given in the standard parametric form. However, it can
also be in the forms than the standard one. For these forms, it is necessary to
determine and find the standard form. In this paper, we present algorithms to
determine whether a given implicit surface is a rational ruled surface. A
parametrization of the surface is computed for the affirmative case. We also
consider the parametric situation. More precisely, after a given rational
parametric surface is determined as a ruled one, we reparameterize it to the
standard form.
","['\nSonia Perez-Diaza\n', '\nLiyong Shen\n']",31 pages,"Journal of Symbolic Computation Volume 63, May 2014, Pages 21-45",http://dx.doi.org/10.1016/j.jsc.2013.11.003,cs.SC,"['cs.SC', 'cs.CG', 'math.AG', 'I.3.5; G.1.2']",10.1016/j.jsc.2013.11.003,,[]
Determination and (re)parametrization of rational developable surfaces,http://arxiv.org/abs/1305.2463v1,2013-05-11T01:58:08Z,2013-05-11T01:58:08Z,"  The developable surface is an important surface in computer aided design,
geometric modeling and industrial manufactory. It is often given in the stan-
dard parametric form, but it can also be in the implicit form which is commonly
used in algebraic geometry. Not all algebraic developable surfaces have
rational parametrizations. In this paper, we focus on the rational developable
surfaces. For a given algebraic surface, we first determine whether it is
developable by geometric inspection, and we give a rational proper
parametrization for the af- firmative case. For a rational parametric surface,
we can also determine the developability and give a proper reparametrization
for the developable surface.
","['\nSonia Perez-Diaz\n', '\nLi-Yong Shen\n']",,,http://arxiv.org/abs/1305.2463v1,cs.SC,"['cs.SC', 'cs.CG', 'math.AG', 'I.3.5; G.1.2']",,,[]
Polynomial Systems Solving by Fast Linear Algebra,http://arxiv.org/abs/1304.6039v2,2013-04-22T18:11:51Z,2013-07-12T20:16:36Z,"  Polynomial system solving is a classical problem in mathematics with a wide
range of applications. This makes its complexity a fundamental problem in
computer science. Depending on the context, solving has different meanings. In
order to stick to the most general case, we consider a representation of the
solutions from which one can easily recover the exact solutions or a certified
approximation of them. Under generic assumption, such a representation is given
by the lexicographical Gr\""obner basis of the system and consists of a set of
univariate polynomials. The best known algorithm for computing the
lexicographical Gr\""obner basis is in $\widetilde{O}(d^{3n})$ arithmetic
operations where $n$ is the number of variables and $d$ is the maximal degree
of the equations in the input system. The notation $\widetilde{O}$ means that
we neglect polynomial factors in $n$. We show that this complexity can be
decreased to $\widetilde{O}(d^{\omega n})$ where $2 \leq \omega < 2.3727$ is
the exponent in the complexity of multiplying two dense matrices. Consequently,
when the input polynomial system is either generic or reaches the B\'ezout
bound, the complexity of solving a polynomial system is decreased from
$\widetilde{O}(D^3)$ to $\widetilde{O}(D^\omega)$ where $D$ is the number of
solutions of the system. To achieve this result we propose new algorithms which
rely on fast linear algebra. When the degree of the equations are bounded
uniformly by a constant we propose a deterministic algorithm. In the unbounded
case we present a Las Vegas algorithm.
","['\nJean-Charles Faugère\nINRIA Paris-Rocquencourt, LIP6\n', '\nPierrick Gaudry\nINRIA Nancy - Grand Est / LORIA\n', '\nLouise Huot\nINRIA Paris-Rocquencourt, LIP6\n', '\nGuénaël Renault\nINRIA Paris-Rocquencourt, LIP6\n']",27 pages,,http://arxiv.org/abs/1304.6039v2,cs.SC,['cs.SC'],,,"['INRIA Paris-Rocquencourt, LIP6', 'INRIA Nancy - Grand Est / LORIA', 'INRIA Paris-Rocquencourt, LIP6', 'INRIA Paris-Rocquencourt, LIP6']"
Optimising Problem Formulation for Cylindrical Algebraic Decomposition,http://arxiv.org/abs/1304.7222v2,2013-04-26T16:45:11Z,2013-05-24T09:10:38Z,"  Cylindrical algebraic decomposition (CAD) is an important tool for the study
of real algebraic geometry with many applications both within mathematics and
elsewhere. It is known to have doubly exponential complexity in the number of
variables in the worst case, but the actual computation time can vary greatly.
It is possible to offer different formulations for a given problem leading to
great differences in tractability. In this paper we suggest a new measure for
CAD complexity which takes into account the real geometry of the problem. This
leads to new heuristics for choosing: the variable ordering for a CAD problem,
a designated equational constraint, and formulations for truth-table invariant
CADs (TTICADs). We then consider the possibility of using Groebner bases to
precondition TTICAD and when such formulations constitute the creation of a new
problem.
","['\nRussell Bradford\n', '\nJames H. Davenport\n', '\nMatthew England\n', '\nDavid Wilson\n']","To appear in: Proceedings of Conferences on Intelligent Computer
  Mathematics (CICM '13) - Calculemus strand","Intelligent Computer Mathematics. Berlin: Springer, pp. 19-34.
  (Lecture Notes in Computer Science; 7961), 2013",http://dx.doi.org/10.1007/978-3-642-39320-4_2,cs.SC,"['cs.SC', '68W30, O3C10', 'I.1.2']",10.1007/978-3-642-39320-4_2,,[]
"A Symbolic Approach to Boundary Problems for Linear Partial Differential
  Equations: Applications to the Completely Reducible Case of the Cauchy
  Problem with Constant Coefficients",http://arxiv.org/abs/1304.7380v1,2013-04-27T15:54:48Z,2013-04-27T15:54:48Z,"  We introduce a general algebraic setting for describing linear boundary
problems in a symbolic computation context, with emphasis on the case of
partial differential equations. The general setting is then applied to the
Cauchy problem for completely reducible partial differential equations with
constant coefficients. While we concentrate on the theoretical features in this
paper, the underlying operator ring is implemented and provides a sufficient
basis for all methods presented here.
","['\nMarkus Rosenkranz\n', '\nNalina Phisanbut\n']",14 pages,,http://arxiv.org/abs/1304.7380v1,cs.SC,"['cs.SC', '68W30, 35-04, 47A05, 47F05']",,,[]
Cylindrical Algebraic Decompositions for Boolean Combinations,http://arxiv.org/abs/1304.7603v1,2013-04-29T09:46:40Z,2013-04-29T09:46:40Z,"  This article makes the key observation that when using cylindrical algebraic
decomposition (CAD) to solve a problem with respect to a set of polynomials, it
is not always the signs of those polynomials that are of paramount importance
but rather the truth values of certain quantifier free formulae involving them.
This motivates our definition of a Truth Table Invariant CAD (TTICAD). We
generalise the theory of equational constraints to design an algorithm which
will efficiently construct a TTICAD for a wide class of problems, producing
stronger results than when using equational constraints alone. The algorithm is
implemented fully in Maple and we present promising results from
experimentation.
","['\nRussell Bradford\n', '\nJames H. Davenport\n', '\nMatthew England\n', '\nScott McCallum\n', '\nDavid Wilson\n']","To appear in the proceedings of the 38th International Symposium on
  Symbolic and Algebraic Computation (ISSAC '13)","In: Proceedings of the 38th International Symposium on Symbolic
  and Algebraic Computation, (ISSAC '13), pp 125-132, 2013",http://dx.doi.org/10.1145/2465506.2465516,cs.SC,"['cs.SC', '68W30, 03C10', 'I.1.2']",10.1145/2465506.2465516,,[]
"Reduced Gröbner Bases and Macaulay-Buchberger Basis Theorem over
  Noetherian Rings",http://arxiv.org/abs/1304.6889v5,2013-04-25T12:14:49Z,2016-04-04T06:49:47Z,"  In this paper, we extend the characterization of $\mathbb{Z}[x]/\ < f \ >$,
where $f \in \mathbb{Z}[x]$ to be a free $\mathbb{Z}$-module to multivariate
polynomial rings over any commutative Noetherian ring, $A$. The
characterization allows us to extend the Gr\""obner basis method of computing a
$\Bbbk$-vector space basis of residue class polynomial rings over a field
$\Bbbk$ (Macaulay-Buchberger Basis Theorem) to rings, i.e.
$A[x_1,\ldots,x_n]/\mathfrak{a}$, where $\mathfrak{a} \subseteq
A[x_1,\ldots,x_n]$ is an ideal. We give some insights into the characterization
for two special cases, when $A = \mathbb{Z}$ and $A =
\Bbbk[\theta_1,\ldots,\theta_m]$. As an application of this characterization,
we show that the concept of border bases can be extended to rings when the
corresponding residue class ring is a finitely generated, free $A$-module.
","['\nMaria Francis\n', '\nAmbedkar Dukkipati\n']",,"Journal of Symbolic Computation, 65: 1-14, 2014",http://dx.doi.org/10.1016/j.jsc.2014.01.001,cs.SC,"['cs.SC', 'math.AC']",10.1016/j.jsc.2014.01.001,,[]
Understanding Branch Cuts of Expressions,http://arxiv.org/abs/1304.7223v3,2013-04-26T16:46:48Z,2013-05-24T09:08:09Z,"  We assume some standard choices for the branch cuts of a group of functions
and consider the problem of then calculating the branch cuts of expressions
involving those functions. Typical examples include the addition formulae for
inverse trigonometric functions. Understanding these cuts is essential for
working with the single-valued counterparts, the common approach to encoding
multi-valued functions in computer algebra systems. While the defining choices
are usually simple (typically portions of either the real or imaginary axes)
the cuts induced by the expression may be surprisingly complicated. We have
made explicit and implemented techniques for calculating the cuts in the
computer algebra programme Maple. We discuss the issues raised, classifying the
different cuts produced. The techniques have been gathered in the BranchCuts
package, along with tools for visualising the cuts. The package is included in
Maple 17 as part of the FunctionAdvisor tool.
","['\nMatthew England\n', '\nRussell Bradford\n', '\nJames H. Davenport\n', '\nDavid Wilson\n']","To appear in: Proceedings of Conferences on Intelligent Computer
  Mathematics (CICM '13) - Mathematical Knowledge Management (MKM) strand","Intelligent Computer Mathematics. Berlin: Springer, pp. 136-151.
  (Lecture Notes in Computer Science; 7961), 2013",http://dx.doi.org/10.1007/978-3-642-39320-4_9,cs.MS,"['cs.MS', 'cs.SC', '68W30, 33F10', 'I.1.1; G.4']",10.1007/978-3-642-39320-4_9,,[]
Abstract Stobjs and Their Application to ISA Modeling,http://arxiv.org/abs/1304.7858v1,2013-04-30T04:14:22Z,2013-04-30T04:14:22Z,"  We introduce a new ACL2 feature, the abstract stobj, and show how to apply it
to modeling the instruction set architecture of a microprocessor. Benefits of
abstract stobjs over traditional (""concrete"") stobjs can include faster
execution, support for symbolic simulation, more efficient reasoning, and
resilience of proof developments under modeling optimization.
","['\nShilpi Goel\nDepartment of Computer Science, University of Texas at Austin\n', '\nWarren A Hunt, Jr.\nDepartment of Computer Science, University of Texas at Austin\n', '\nMatt Kaufmann\nDepartment of Computer Science, University of Texas at Austin\n']","In Proceedings ACL2 2013, arXiv:1304.7123","EPTCS 114, 2013, pp. 54-69",http://dx.doi.org/10.4204/EPTCS.114.5,cs.LO,"['cs.LO', 'cs.AR', 'cs.SC']",10.4204/EPTCS.114.5,,"['Department of Computer Science, University of Texas at Austin', 'Department of Computer Science, University of Texas at Austin', 'Department of Computer Science, University of Texas at Austin']"
Fast Approximate Polynomial Multipoint Evaluation and Applications,http://arxiv.org/abs/1304.8069v2,2013-04-30T17:01:11Z,2016-05-27T09:11:12Z,"  It is well known that, using fast algorithms for polynomial multiplication
and division, evaluation of a polynomial $F \in \mathbb{C}[x]$ of degree $n$ at
$n$ complex-valued points can be done with $\tilde{O}(n)$ exact field
operations in $\mathbb{C},$ where $\tilde{O}(\cdot)$ means that we omit
polylogarithmic factors. We complement this result by an analysis of
approximate multipoint evaluation of $F$ to a precision of $L$ bits after the
binary point and prove a bit complexity of $\tilde{O}(n(L + \tau + n\Gamma)),$
where $2^\tau$ and $2^\Gamma,$ with $\tau, \Gamma \in \mathbb{N}_{\ge 1},$ are
bounds on the magnitude of the coefficients of $F$ and the evaluation points,
respectively. In particular, in the important case where the precision demand
dominates the other input parameters, the complexity is soft-linear in $n$ and
$L$.
  Our result on approximate multipoint evaluation has some interesting
consequences on the bit complexity of further approximation algorithms which
all use polynomial evaluation as a key subroutine. Of these applications, we
discuss in detail an algorithm for polynomial interpolation and for computing a
Taylor shift of a polynomial. Furthermore, our result can be used to derive
improved complexity bounds for algorithms to refine isolating intervals for the
real roots of a polynomial. For all of the latter algorithms, we derive
near-optimal running times.
","['\nAlexander Kobel\n', '\nMichael Sagraloff\n']","minor editorial changes over the first version: revised references
  and mentioned related work",,http://arxiv.org/abs/1304.8069v2,cs.NA,"['cs.NA', 'cs.SC', 'math.NA', '65Y20', 'F.2.1; G.1.0']",,,[]
"Harmonic Sums, Polylogarithms, Special Numbers, and their
  Generalizations",http://arxiv.org/abs/1304.7071v1,2013-04-26T06:05:06Z,2013-04-26T06:05:06Z,"  In these introductory lectures we discuss classes of presently known nested
sums, associated iterated integrals, and special constants which hierarchically
appear in the evaluation of massless and massive Feynman diagrams at higher
loops. These quantities are elements of stuffle and shuffle algebras implying
algebraic relations being widely independent of the special quantities
considered. They are supplemented by structural relations. The generalizations
are given in terms of generalized harmonic sums, (generalized) cyclotomic sums,
and sums containing in addition binomial and inverse-binomial weights. To all
these quantities iterated integrals and special numbers are associated. We also
discuss the analytic continuation of nested sums of different kind to complex
values of the external summation bound N.
","['\nJakob Ablinger\n', '\nJohannes Blümlein\n']","30 pages Latex, 3 Figures, 1 style file, Cintribution to:
  ""Integration, Summation and Special Functions in Quantum Field Theory"", to
  appear at Springer Verlag, Vienna",,http://arxiv.org/abs/1304.7071v1,math-ph,"['math-ph', 'cs.SC', 'hep-ph', 'hep-th', 'math.AG', 'math.MP']",,,[]
Faster sparse interpolation of straight-line programs,http://arxiv.org/abs/1304.3483v2,2013-04-11T20:43:48Z,2014-01-22T21:51:30Z,"  We give a new probabilistic algorithm for interpolating a ""sparse"" polynomial
f given by a straight-line program. Our algorithm constructs an approximation
f* of f, such that their difference probably has at most half the number of
terms of f, then recurses on their difference. Our approach builds on previous
work by Garg and Schost (2009), and Giesbrecht and Roche (2011), and is
asymptotically more efficient in terms of the total cost of the probes required
than previous methods, in many cases.
","['\nAndrew Arnold\n', '\nMark Giesbrecht\n', '\nDaniel S. Roche\n']","15 pages, 1 table, 4 procedures, version appeared at Computer Algebra
  in Scientific Computing (CASC) 2013","Proc. CASC 2013, Lecture Notes in Computer Science, Volume 8136,
  2013, pp 61-74",http://dx.doi.org/10.1007/978-3-319-02297-0_5,cs.SC,"['cs.SC', '68W20', 'F.2.1; G.1.1; G.4; I.1.2']",10.1007/978-3-319-02297-0_5,,[]
"Efficient Calculation of Determinants of Symbolic Matrices with Many
  Variables",http://arxiv.org/abs/1304.4691v1,2013-04-17T05:07:55Z,2013-04-17T05:07:55Z,"  Efficient matrix determinant calculations have been studied since the 19th
century. Computers expand the range of determinants that are practically
calculable to include matrices with symbolic entries. However, the fastest
determinant algorithms for numerical matrices are often not the fastest for
symbolic matrices with many variables. We compare the performance of two
algorithms, fraction-free Gaussian elimination and minor expansion, on symbolic
matrices with many variables. We show that, under a simplified theoretical
model, minor expansion is faster in most situations. We then propose
optimizations for minor expansion and demonstrate their effectiveness with
empirical data.
","['\nTanya Khovanova\n', '\nZiv Scully\n']",9 pages,,http://arxiv.org/abs/1304.4691v1,cs.SC,"['cs.SC', 'I.1.2']",,,[]
Intrinsic complexity estimates in polynomial optimization,http://arxiv.org/abs/1304.5214v2,2013-04-18T18:42:46Z,2014-02-10T20:44:45Z,"  It is known that point searching in basic semialgebraic sets and the search
for globally minimal points in polynomial optimization tasks can be carried out
using $(s\,d)^{O(n)}$ arithmetic operations, where $n$ and $s$ are the numbers
of variables and constraints and $d$ is the maximal degree of the polynomials
involved.\spar \noindent We associate to each of these problems an intrinsic
system degree which becomes in worst case of order $(n\,d)^{O(n)}$ and which
measures the intrinsic complexity of the task under consideration.\spar
\noindent We design non-uniformly deterministic or uniformly probabilistic
algorithms of intrinsic, quasi-polynomial complexity which solve these
problems.
","['\nBernd Bank\nLIX\n', '\nMarc Giusti\nLIX\n', '\nJoos Heintz\nLIP6, INRIA Paris-Rocquencourt\n', '\nMohab Safey El Din\nLIP6, INRIA Paris-Rocquencourt\n']",,,http://arxiv.org/abs/1304.5214v2,cs.SC,['cs.SC'],,,"['LIX', 'LIX', 'LIP6, INRIA Paris-Rocquencourt', 'LIP6, INRIA Paris-Rocquencourt']"
Simplifying Multiple Sums in Difference Fields,http://arxiv.org/abs/1304.4134v1,2013-04-15T15:37:44Z,2013-04-15T15:37:44Z,"  In this survey article we present difference field algorithms for symbolic
summation. Special emphasize is put on new aspects in how the summation
problems are rephrased in terms of difference fields, how the problems are
solved there, and how the derived results in the given difference field can be
reinterpreted as solutions of the input problem. The algorithms are illustrated
with the Mathematica package \SigmaP\ by discovering and proving new harmonic
number identities extending those from (Paule and Schneider, 2003). In
addition, the newly developed package \texttt{EvaluateMultiSums} is introduced
that combines the presented tools. In this way, large scale summation problems
for the evaluation of Feynman diagrams in QCD (Quantum ChromoDynamics) can be
solved completely automatically.
",['\nCarsten Schneider\n'],"Uses svmult.cls, to appear as contribution in the book ""Computer
  Algebra in Quantum Field Theory: Integration, Summation and Special
  Functions"" (www.Springer.com)",,http://arxiv.org/abs/1304.4134v1,cs.SC,"['cs.SC', 'math-ph', 'math.CO', 'math.MP']",,,[]
Sparse FGLM algorithms,http://arxiv.org/abs/1304.1238v1,2013-04-04T03:50:03Z,2013-04-04T03:50:03Z,"  Given a zero-dimensional ideal I in K[x1,...,xn] of degree D, the
transformation of the ordering of its Groebner basis from DRL to LEX is a key
step in polynomial system solving and turns out to be the bottleneck of the
whole solving process. Thus it is of crucial importance to design efficient
algorithms to perform the change of ordering.
  The main contributions of this paper are several efficient methods for the
change of ordering which take advantage of the sparsity of multiplication
matrices in the classical FGLM algorithm. Combing all these methods, we propose
a deterministic top-level algorithm that automatically detects which method to
use depending on the input. As a by-product, we have a fast implementation that
is able to handle ideals of degree over 40000. Such an implementation
outperforms the Magma and Singular ones, as shown by our experiments.
  First for the shape position case, two methods are designed based on the
Wiedemann algorithm: the first is probabilistic and its complexity to complete
the change of ordering is O(D(N1+nlog(D))), where N1 is the number of nonzero
entries of a multiplication matrix; the other is deterministic and computes the
LEX Groebner basis of the radical of I via Chinese Remainder Theorem. Then for
the general case, the designed method is characterized by the
Berlekamp-Massey-Sakata algorithm from Coding Theory to handle the
multi-dimensional linearly recurring relations. Complexity analyses of all
proposed methods are also provided.
  Furthermore, for generic polynomial systems, we present an explicit formula
for the estimation of the sparsity of one main multiplication matrix, and prove
its construction is free. With the asymptotic analysis of such sparsity, we are
able to show for generic systems the complexity above becomes $O(\sqrt{6/n \pi}
D^{2+(n-1)/n}})$.
","['\nJean-Charles Faugère\n', '\nChenqi Mou\n']",40 pages,"Journal of Symbolic Computation, 2017, 80(3): 538-569",http://dx.doi.org/10.1016/j.jsc.2016.07.025,cs.SC,['cs.SC'],10.1016/j.jsc.2016.07.025,,[]
"A probabilistic algorithm to compute the real dimension of a
  semi-algebraic set",http://arxiv.org/abs/1304.1928v2,2013-04-06T19:23:00Z,2013-09-19T09:36:07Z,"  Let $\RR$ be a real closed field (e.g. the field of real numbers) and
$\mathscr{S} \subset \RR^n$ be a semi-algebraic set defined as the set of
points in $\RR^n$ satisfying a system of $s$ equalities and inequalities of
multivariate polynomials in $n$ variables, of degree at most $D$, with
coefficients in an ordered ring $\ZZ$ contained in $\RR$. We consider the
problem of computing the {\em real dimension}, $d$, of $\mathscr{S}$. The real
dimension is the first topological invariant of interest; it measures the
number of degrees of freedom available to move in the set. Thus, computing the
real dimension is one of the most important and fundamental problems in
computational real algebraic geometry. The problem is ${\rm
NP}_{\mathbb{R}}$-complete in the Blum-Shub-Smale model of computation. The
current algorithms (probabilistic or deterministic) for computing the real
dimension have complexity $(s \, D)^{O(d(n-d))}$, that becomes $(s \,
D)^{O(n^2)}$ in the worst-case. The existence of a probabilistic or
deterministic algorithm for computing the real dimension with single
exponential complexity with a factor better than ${O(n^2)}$ in the exponent in
the worst-case, is a longstanding open problem. We provide a positive answer to
this problem by introducing a probabilistic algorithm for computing the real
dimension of a semi-algebraic set with complexity $(s\, D)^{O(n)}$.
","['\nMohab Safey El Din\nLIP6, INRIA Paris-Rocquencourt\n', '\nElias Tsigaridas\nLIP6, INRIA Paris-Rocquencourt\n']","Several typos fixed in Sections 4 and 5. There is an error in Section
  5 and thus the complexity result stated does not hold",,http://arxiv.org/abs/1304.1928v2,cs.SC,['cs.SC'],,,"['LIP6, INRIA Paris-Rocquencourt', 'LIP6, INRIA Paris-Rocquencourt']"
"Synthesizing Switching Controllers for Hybrid Systems by Continuous
  Invariant Generation",http://arxiv.org/abs/1304.0825v1,2013-04-03T02:49:17Z,2013-04-03T02:49:17Z,"  We extend a template-based approach for synthesizing switching controllers
for semi-algebraic hybrid systems, in which all expressions are polynomials.
This is achieved by combining a QE (quantifier elimination)-based method for
generating continuous invariants with a qualitative approach for predefining
templates. Our synthesis method is relatively complete with regard to a given
family of predefined templates. Using qualitative analysis, we discuss
heuristics to reduce the numbers of parameters appearing in the templates. To
avoid too much human interaction in choosing templates as well as the high
computational complexity caused by QE, we further investigate applications of
the SOS (sum-of-squares) relaxation approach and the template polyhedra
approach in continuous invariant generation, which are both well supported by
efficient numerical solvers.
","['\nDeepak Kapur\n', '\nNaijun Zhan\n', '\nHengjun Zhao\n']",,,http://arxiv.org/abs/1304.0825v1,cs.SY,"['cs.SY', 'cs.NA', 'cs.SC']",,,[]
Symbolic Arithmetic and Integer Factorization,http://arxiv.org/abs/1304.1944v1,2013-04-06T23:41:29Z,2013-04-06T23:41:29Z,"  In this paper, we create a systematic and automatic procedure for
transforming the integer factorization problem into the problem of solving a
system of Boolean equations. Surprisingly, the resulting system of Boolean
equations takes on a ""life of its own"" and becomes a new type of integer, which
we call a generic integer.
  We then proceed to use the newly found algebraic structure of the ring of
generic integers to create two new integer factoring algorithms, called
respectively the Boolean factoring (BF) algorithm, and the multiplicative
Boolean factoring (MBF) algorithm. Although these two algorithms are not
competitive with current classical integer factoring algorithms, it is hoped
that they will become stepping stones to creating much faster and more
competitive algorithms, and perhaps be precursors of a new quantum algorithm
for integer factoring.
",['\nSamuel J. Lomonaco\n'],30 pages,,http://arxiv.org/abs/1304.1944v1,math.NT,"['math.NT', 'cs.SC', 'quant-ph']",,,[]
Separating linear forms for bivariate systems,http://arxiv.org/abs/1303.5041v2,2013-03-20T19:38:45Z,2014-01-20T08:47:48Z,"  We present an algorithm for computing a separating linear form of a system of
bivariate polynomials with integer coefficients, that is a linear combination
of the variables that takes different values when evaluated at distinct
(complex) solutions of the system. In other words, a separating linear form
defines a shear of the coordinate system that sends the algebraic system in
generic position, in the sense that no two distinct solutions are vertically
aligned. The computation of such linear forms is at the core of most algorithms
that solve algebraic systems by computing rational parameterizations of the
solutions and, moreover, the computation a separating linear form is the
bottleneck of these algorithms, in terms of worst-case bit complexity. Given
two bivariate polynomials of total degree at most $d$ with integer coefficients
of bitsize at most~$\tau$, our algorithm computes a separating linear form in
$\sOB(d^{8}+d^7\tau)$ bit operations in the worst case, where the previously
known best bit complexity for this problem was $\sOB(d^{10}+d^9\tau)$ (where
$\sO$ refers to the complexity where polylogarithmic factors are omitted and
$O_B$ refers to the bit complexity).
","['\nYacine Bouzidi\nINRIA Nancy - Grand Est / LORIA\n', '\nSylvain Lazard\nINRIA Nancy - Grand Est / LORIA\n', '\nMarc Pouget\nINRIA Nancy - Grand Est / LORIA\n', '\nFabrice Rouillier\nINRIA Paris-Rocquencourt\n']",,N&deg; RR-8261 (2013),http://arxiv.org/abs/1303.5041v2,cs.SC,"['cs.SC', 'cs.CG']",,,"['INRIA Nancy - Grand Est / LORIA', 'INRIA Nancy - Grand Est / LORIA', 'INRIA Nancy - Grand Est / LORIA', 'INRIA Paris-Rocquencourt']"
"Rational Univariate Representations of Bivariate Systems and
  Applications",http://arxiv.org/abs/1303.5042v2,2013-03-20T19:39:37Z,2013-11-25T15:21:41Z,"  We address the problem of solving systems of two bivariate polynomials of
total degree at most $d$ with integer coefficients of maximum bitsize $\tau$.
It is known that a linear separating form, that is a linear combination of the
variables that takes different values at distinct solutions of the system, can
be computed in $\sOB(d^{8}+d^7\tau)$ bit operations (where $O_B$ refers to bit
complexities and $\sO$ to complexities where polylogarithmic factors are
omitted) and we focus here on the computation of a Rational Univariate
Representation (RUR) given a linear separating form. We present an algorithm
for computing a RUR with worst-case bit complexity in $\sOB(d^7+d^6\tau)$ and
bound the bitsize of its coefficients by $\sO(d^2+d\tau)$. We show in addition
that isolating boxes of the solutions of the system can be computed from the
RUR with $\sOB(d^{8}+d^7\tau)$ bit operations. Finally, we show how a RUR can
be used to evaluate the sign of a bivariate polynomial (of degree at most $d$
and bitsize at most $\tau$) at one real solution of the system in
$\sOB(d^{8}+d^7\tau)$ bit operations and at all the $\Theta(d^2)$ {real}
solutions in only $O(d)$ times that for one solution.
","['\nYacine Bouzidi\nINRIA Nancy - Grand Est / LORIA\n', '\nSylvain Lazard\nINRIA Nancy - Grand Est / LORIA\n', '\nMarc Pouget\nINRIA Nancy - Grand Est / LORIA\n', '\nFabrice Rouillier\nINRIA Paris-Rocquencourt\n']","Changed the title of RR_paper_rur_bitsize to match the one of
  ISSAC'13",N&deg; RR-8262 (2013),http://arxiv.org/abs/1303.5042v2,cs.SC,"['cs.SC', 'cs.CG']",,,"['INRIA Nancy - Grand Est / LORIA', 'INRIA Nancy - Grand Est / LORIA', 'INRIA Nancy - Grand Est / LORIA', 'INRIA Paris-Rocquencourt']"
"Numerical method for real root isolation of semi-algebraic system and
  its applications",http://arxiv.org/abs/1303.5503v1,2013-03-22T02:27:43Z,2013-03-22T02:27:43Z,"  In this paper, based on the homotopy continuation method and the interval
Newton method, an efficient algorithm is introduced to isolate the real roots
of semi-algebraic system.
  Tests on some random examples and a variety of problems including
transcendental functions arising in many applications show that the new
algorithm reduces the cost substantially compared with the traditional symbolic
approaches.
","['\nZhenyi Ji\n', '\nWenyuan Wu\n', '\nYi Li\n', '\nYong Feng\n']",,,http://arxiv.org/abs/1303.5503v1,math.NA,"['math.NA', 'cs.SC']",,,[]
A Variant of the Gröbner Basis Algorithm for Computing Hilbert Bases,http://arxiv.org/abs/1303.6296v1,2013-03-25T20:03:06Z,2013-03-25T20:03:06Z,"  Gr\""obner bases can be used for computing the Hilbert basis of a numerical
submonoid. By using these techniques, we provide an algorithm that calculates a
basis of a subspace of a finite-dimensional vector space over a finite prime
field given as a matrix kernel.
","['\nNatalia Dück\n', '\nKarl-Heinz Zimmermann\n']",11 pages,"IJPAM, Vol. 81, 2012, 145-155",http://arxiv.org/abs/1303.6296v1,math.AG,"['math.AG', 'cs.SC', 'math.AC', '13P10 (Primary), 94B05 (Secondary)']",,,[]
"Highly Scalable Multiplication for Distributed Sparse Multivariate
  Polynomials on Many-core Systems",http://arxiv.org/abs/1303.7425v1,2013-03-29T15:47:45Z,2013-03-29T15:47:45Z,"  We present a highly scalable algorithm for multiplying sparse multivariate
polynomials represented in a distributed format. This algo- rithm targets not
only the shared memory multicore computers, but also computers clusters or
specialized hardware attached to a host computer, such as graphics processing
units or many-core coprocessors. The scal- ability on the large number of cores
is ensured by the lacks of synchro- nizations, locks and false-sharing during
the main parallel step.
","['\nMicka\x7fel Gastineau\n', '\nJacques Laskar\n']","15 pages, 5 figures",,http://arxiv.org/abs/1303.7425v1,cs.SC,"['cs.SC', 'astro-ph.IM', 'cs.DC', 'cs.MS']",,,[]
"New modular multiplication and division algorithms based on continued
  fraction expansion",http://arxiv.org/abs/1303.3445v1,2013-03-14T13:44:44Z,2013-03-14T13:44:44Z,"  In this paper, we apply results on number systems based on continued fraction
expansions to modular arithmetic. We provide two new algorithms in order to
compute modular multiplication and modular division. The presented algorithms
are based on the Euclidean algorithm and are of quadratic complexity.
",['\nMourad Gouicem\n'],,,http://arxiv.org/abs/1303.3445v1,cs.DS,"['cs.DS', 'cs.SC', 'G.1.0']",,,[]
An implementation of CAD in Maple utilising McCallum projection,http://arxiv.org/abs/1302.6401v1,2013-02-26T11:33:33Z,2013-02-26T11:33:33Z,"  Cylindrical algebraic decomposition (CAD) is an important tool for the
investigation of semi-algebraic sets. Originally introduced by Collins in the
1970s for use in quantifier elimination it has since found numerous
applications within algebraic geometry and beyond. Following from his original
work in 1988, McCallum presented an improved algorithm, CADW, which offered a
huge increase in the practical utility of CAD. In 2009 a team based at the
University of Western Ontario presented a new and quite separate algorithm for
CAD, which was implemented and included in the computer algebra system Maple.
As part of a wider project at Bath investigating CAD and its applications,
Collins and McCallum's CAD algorithms have been implemented in Maple. This
report details these implementations and compares them to Qepcad and the
Ontario algorithm.
  The implementations were originally undertaken to facilitate research into
the connections between the algorithms. However, the ability of the code to
guarantee order-invariant output has led to its use in new research on CADs
which are minimal for certain problems. In addition, the implementation
described here is of interest as the only full implementation of CADW, (since
Qepcad does not currently make use of McCallum's delineating polynomials), and
hence can solve problems not admissible to other CAD implementations.
",['\nMatthew England\n'],"9 pages; University of Bath, Dept. Computer Science Technical Report
  Series, 2013-02, 2013",,http://arxiv.org/abs/1302.6401v1,cs.SC,['cs.SC'],,,[]
Module Border Bases,http://arxiv.org/abs/1302.6383v1,2013-02-26T10:22:12Z,2013-02-26T10:22:12Z,"  In this paper, we generalize the notion of border bases of zero-dimensional
polynomial ideals to the module setting. To this end, we introduce order
modules as a generalization of order ideals and module border bases of
submodules with finite codimension in a free module as a generalization of
border bases of zero-dimensional ideals in the first part of this paper. In
particular, we extend the division algorithm for border bases to the module
setting, show the existence and uniqueness of module border bases, and
characterize module border bases analogously like border bases via the special
generation property, border form modules, rewrite rules, commuting matrices,
and liftings of border syzygies. Furthermore, we deduce Buchberger's Criterion
for Module Border Bases and give an algorithm for the computation of module
border bases that uses linear algebra techniques. In the second part, we
further generalize the notion of module border bases to quotient modules. We
then show the connection between quotient module border bases and special
module border bases and deduce characterizations similar to the ones for module
border bases. Moreover, we give an algorithm for the computation of quotient
module border bases using linear algebra techniques, again. At last, we prove
that subideal border bases are isomorphic to special quotient module border
bases. This isomorphy immediately yields characterizations and an algorithm for
the computation of subideal border bases.
",['\nMarkus Kriegl\n'],65 pages,,http://arxiv.org/abs/1302.6383v1,math.AC,"['math.AC', 'cs.SC', '13P10']",,,[]
"Normalization of Polynomials in Algebraic Invariants of
  Three-Dimensional Orthogonal Geometry",http://arxiv.org/abs/1302.7194v1,2013-02-27T05:59:29Z,2013-02-27T05:59:29Z,"  In classical invariant theory, the Gr\""obner base of the ideal of syzygies
and the normal forms of polynomials of invariants are two core contents. To
improve the performance of invariant theory in symbolic computing of classical
geometry, advanced invariants are introduced via Clifford product. This paper
addresses and solves the two key problems in advanced invariant theory: the
Gr\""obner base of the ideal of syzygies among advanced invariants, and the
normal forms of polynomials of advanced invariants. These results beautifully
extend the straightening of Young tableaux to advanced invariants.
",['\nHongbo Li\n'],,,http://arxiv.org/abs/1302.7194v1,cs.SC,"['cs.SC', 'math.RA']",,,[]
Domain-of-Attraction Estimation for Uncertain Non-polynomial Systems,http://arxiv.org/abs/1303.0452v1,2013-03-03T03:01:29Z,2013-03-03T03:01:29Z,"  In this paper, we consider the problem of computing estimates of the
domain-of-attraction for non-polynomial systems. A polynomial approximation
technique, based on multivariate polynomial interpolation and error analysis
for remaining functions, is applied to compute an uncertain polynomial system,
whose set of trajectories contains that of the original non-polynomial system.
Experiments on the benchmark non-polynomial systems show that our approach
gives better estimates of the domain-of-attraction.
","['\nMin Wu\n', '\nZhengfeng Yang\n', '\nWang Lin\n']",,,http://dx.doi.org/10.1016/j.cnsns.2013.12.001,cs.SC,"['cs.SC', 'math.OC']",10.1016/j.cnsns.2013.12.001,,[]
"Explicit Noether Normalization for Simultaneous Conjugation via
  Polynomial Identity Testing",http://arxiv.org/abs/1303.0084v2,2013-03-01T04:44:00Z,2013-03-08T05:34:47Z,"  Mulmuley recently gave an explicit version of Noether's Normalization lemma
for ring of invariants of matrices under simultaneous conjugation, under the
conjecture that there are deterministic black-box algorithms for polynomial
identity testing (PIT). He argued that this gives evidence that constructing
such algorithms for PIT is beyond current techniques. In this work, we show
this is not the case. That is, we improve Mulmuley's reduction and
correspondingly weaken the conjecture regarding PIT needed to give explicit
Noether Normalization. We then observe that the weaker conjecture has recently
been nearly settled by the authors, who gave quasipolynomial size hitting sets
for the class of read-once oblivious algebraic branching programs (ROABPs).
This gives the desired explicit Noether Normalization unconditionally, up to
quasipolynomial factors.
  As a consequence of our proof we give a deterministic parallel
polynomial-time algorithm for deciding if two matrix tuples have intersecting
orbit closures, under simultaneous conjugation.
  We also study the strength of conjectures that Mulmuley requires to obtain
similar results as ours. We prove that his conjectures are stronger, in the
sense that the computational model he needs PIT algorithms for is equivalent to
the well-known algebraic branching program (ABP) model, which is provably
stronger than the ROABP model.
  Finally, we consider the depth-3 diagonal circuit model as defined by Saxena,
as PIT algorithms for this model also have implications in Mulmuley's work.
Previous work have given quasipolynomial size hitting sets for this model. In
this work, we give a much simpler construction of such hitting sets, using
techniques of Shpilka and Volkovich.
","['\nMichael A. Forbes\n', '\nAmir Shpilka\n']","30 pages; updated to reflect that Theorem 4.1 (of the first version)
  is already known, as pointed out to us by Josh Grochow",,http://arxiv.org/abs/1303.0084v2,cs.CC,"['cs.CC', 'cs.SC', 'math.AG', 'math.RA']",,,[]
Elimination for generic sparse polynomial systems,http://arxiv.org/abs/1303.0266v3,2013-03-01T20:12:54Z,2014-01-23T17:22:58Z,"  We present a new probabilistic symbolic algorithm that, given a variety
defined in an n-dimensional affine space by a generic sparse system with fixed
supports, computes the Zariski closure of its projection to an l-dimensional
coordinate affine space with l < n. The complexity of the algorithm depends
polynomially on combinatorial invariants associated to the supports.
","['\nMaría Isabel Herrero\n', '\nGabriela Jeronimo\n', '\nJuan Sabia\n']",22 pages,,http://arxiv.org/abs/1303.0266v3,math.AG,"['math.AG', 'cs.CC', 'cs.SC', 'math.AC', '14Q20']",,,[]
"New Symbolic Algorithms For Solving A General Bordered Tridiagonal
  Linear System",http://arxiv.org/abs/1303.0738v1,2013-03-04T15:52:39Z,2013-03-04T15:52:39Z,"  In this paper, the author present reliable symbolic algorithms for solving a
general bordered tridiagonal linear system. The first algorithm is based on the
LU decomposition of the coefficient matrix and the computational cost of it is
O(n). The second is based on The Sherman-Morrison-Woodbury formula. The
algorithms are implementable to the Computer Algebra System (CAS) such as
MAPLE, MATLAB and MATHEMATICA. Three examples are presented for the sake of
illustration.
",['\nA. A. Karawia\n'],,,http://arxiv.org/abs/1303.0738v1,cs.SC,"['cs.SC', 'cs.NA', 'math.NA', '15A15, 15A23, 68W30, 11Y05, 33F10, F.2.1, G.1.0']",,,[]
Tropicalization of classical moduli spaces,http://arxiv.org/abs/1303.1132v2,2013-03-05T18:36:36Z,2013-11-16T05:55:10Z,"  The image of the complement of a hyperplane arrangement under a monomial map
can be tropicalized combinatorially using matroid theory. We apply this to
classical moduli spaces that are associated with complex reflection
arrangements. Starting from modular curves, we visit the Segre cubic, the Igusa
quartic, and moduli of marked del Pezzo surfaces of degrees 2 and 3. Our
primary example is the Burkhardt quartic, whose tropicalization is a
3-dimensional fan in 39-dimensional space. This effectuates a synthesis of
concrete and abstract approaches to tropical moduli of genus 2 curves.
","['\nQingchun Ren\n', '\nSteven V Sam\n', '\nBernd Sturmfels\n']",33 pages,"Math. Comput. Sci. 8 (2014), no. 2, 119-145",http://dx.doi.org/10.1007/s11786-014-0185-x,math.AG,"['math.AG', 'cs.SC', 'math.CO']",10.1007/s11786-014-0185-x,,[]
"An Algorithm for Computing the Limit Points of the Quasi-component of a
  Regular Chain",http://arxiv.org/abs/1302.4688v1,2013-02-19T17:25:14Z,2013-02-19T17:25:14Z,"  For a regular chain $R$, we propose an algorithm which computes the
(non-trivial) limit points of the quasi-component of $R$, that is, the set
$\bar{W(R)} \setminus W(R)$. Our procedure relies on Puiseux series expansions
and does not require to compute a system of generators of the saturated ideal
of $R$. We focus on the case where this saturated ideal has dimension one and
we discuss extensions of this work in higher dimensions. We provide
experimental results illustrating the benefits of our algorithms.
","['\nParisa Alvandi\n', '\nChangbo Chen\n', '\nMarc Moreno Maza\n']",,,http://arxiv.org/abs/1302.4688v1,cs.SC,['cs.SC'],,,[]
Combinatorics of $φ$-deformed stuffle Hopf algebras,http://arxiv.org/abs/1302.5391v7,2013-02-21T19:53:26Z,2014-03-01T07:04:25Z,"  In order to extend the Sch\""utzenberger's factorization to general
perturbations, the combinatorial aspects of the Hopf algebra of the
$\phi$-deformed stuffle product is developed systematically in a parallel way
with those of the shuffle product.
","['\nGérard Henry Edmond Duchamp\nLIPN\n', '\nVincel Hoang Ngoc Minh\nLIPN\n', '\nChristophe Tollu\nLIPN\n', '\nBùi Chiên\nLIPN\n', '\nNguyen Hoang Nghia\nLIPN\n']",,,http://arxiv.org/abs/1302.5391v7,math.CO,"['math.CO', 'cs.SC']",,,"['LIPN', 'LIPN', 'LIPN', 'LIPN', 'LIPN']"
"Representation, simplification and display of fractional powers of
  rational numbers in computer algebra",http://arxiv.org/abs/1302.2169v1,2013-02-08T22:41:08Z,2013-02-08T22:41:08Z,"  Simplification of fractional powers of positive rational numbers and of sums,
products and powers of such numbers is taught in beginning algebra. Such
numbers can often be expressed in many ways, as this article discusses in some
detail. Since they are such a restricted subset of algebraic numbers, it might
seem that good simplification of them must already be implemented in all widely
used computer algebra systems. However, the algorithm taught in beginning
algebra uses integer factorization, which can consume unacceptable time for the
large numbers that often arise within computer algebra. Therefore some systems
apparently use various ad hoc techniques that can return an incorrect result
because of not simplifying to 0 the difference between two equivalent such
expressions. Even systems that avoid this flaw often do not return the same
result for all equivalent such input forms, or return an unnecessarily bulky
result that does not have any other compensating useful property. This article
identifies some of these deficiencies, then describes the advantages and
disadvantages of various alternative forms and how to overcome the deficiencies
without costly integer factorization.
","['\nAlbert D. Rich\n', '\nDavid R. Stoutemyer\n']","23 pages, 1 figure, 4 tables",,http://arxiv.org/abs/1302.2169v1,cs.SC,"['cs.SC', '11 Number Theory, Algebraic Number Theory Computations', 'I.1.1']",,,[]
Computer-Aided Derivation of Multi-scale Models: A Rewriting Framework,http://arxiv.org/abs/1302.2224v1,2013-02-09T11:56:12Z,2013-02-09T11:56:12Z,"  We introduce a framework for computer-aided derivation of multi-scale models.
It relies on a combination of an asymptotic method used in the field of partial
differential equations with term rewriting techniques coming from computer
science.
  In our approach, a multi-scale model derivation is characterized by the
features taken into account in the asymptotic analysis. Its formulation
consists in a derivation of a reference model associated to an elementary
nominal model, and in a set of transformations to apply to this proof until it
takes into account the wanted features. In addition to the reference model
proof, the framework includes first order rewriting principles designed for
asymptotic model derivations, and second order rewriting principles dedicated
to transformations of model derivations. We apply the method to generate a
family of homogenized models for second order elliptic equations with periodic
coefficients that could be posed in multi-dimensional domains, with possibly
multi-domains and/or thin domains.
","['\nBin Yang\n', '\nWalid Belkhir\n', '\nMichel Lenczner\n']",26 pages,,http://arxiv.org/abs/1302.2224v1,cs.SC,['cs.SC'],,,[]
"Introduction to Redberry: a computer algebra system designed for tensor
  manipulation",http://arxiv.org/abs/1302.1219v2,2013-02-05T22:15:43Z,2015-01-06T19:51:01Z,"  In this paper we introduce Redberry --- an open source computer algebra
system with native support of tensorial expressions. It provides basic computer
algebra tools (algebraic manipulations, substitutions, basic simplifications
etc.) which are aware of specific features of indexed expressions: contractions
of indices, permutational symmetries, multiple index types etc. Redberry
supports conventional \LaTeX-style input notation for tensorial expressions.
The high energy physics package includes tools for Feynman diagrams
calculation: Dirac and SU(N) algebra, Levi-Civita simplifications and tools for
one-loop calculations in quantum field theory. In the paper we give detailed
overview of Redberry features: from basic manipulations with tensors to real
Feynman diagrams calculation, accompanied by many examples. Redberry is written
in Java 7 and provides convenient Groovy-based user interface inside the
high-level general purpose programming language environment. Redberry is
available from http://redberry.cc
","['\nD. A. Bolotin\n', '\nS. V. Poslavsky\n']","27 pages, 2 figures",,http://arxiv.org/abs/1302.1219v2,cs.SC,"['cs.SC', 'hep-ph', 'hep-th']",,,[]
A simple and fast algorithm for computing exponentials of power series,http://arxiv.org/abs/1301.5804v1,2013-01-24T14:49:29Z,2013-01-24T14:49:29Z,"  As was initially shown by Brent, exponentials of truncated power series can
be computed using a constant number of polynomial multiplications. This note
gives a relatively simple algorithm with a low constant factor.
","['\nAlin Bostan\nINRIA Saclay - Ile de France\n', '\nEric Schost\n']",,"Information Processing Letters 109, 13 (2009) 754-756",http://dx.doi.org/10.1016/j.ipl.2009.03.012,cs.SC,['cs.SC'],10.1016/j.ipl.2009.03.012,,['INRIA Saclay - Ile de France']
Superfast solution of Toeplitz systems based on syzygy reduction,http://arxiv.org/abs/1301.5798v1,2013-01-24T14:45:50Z,2013-01-24T14:45:50Z,"  We present a new superfast algorithm for solving Toeplitz systems. This
algorithm is based on a relation between the solution of such problems and
syzygies of polynomials or moving lines. We show an explicit connection between
the generators of a Toeplitz matrix and the generators of the corresponding
module of syzygies. We show that this module is generated by two elements and
the solution of a Toeplitz system T u=g can be reinterpreted as the remainder
of a vector depending on g, by these two generators. We obtain these generators
and this remainder with computational complexity O(n log^2 n) for a Toeplitz
matrix of size nxn.
","['\nHoussam Khalil\nINRIA Sophia Antipolis, ICJ\n', '\nBernard Mourrain\nINRIA Sophia Antipolis\n', '\nMichelle Schatzman\nICJ\n']","(07/11/2011). arXiv admin note: substantial text overlap with
  arXiv:0903.1244",,http://arxiv.org/abs/1301.5798v1,cs.SC,"['cs.SC', 'math.NA']",,,"['INRIA Sophia Antipolis, ICJ', 'INRIA Sophia Antipolis', 'ICJ']"
Fast algorithms for ell-adic towers over finite fields,http://arxiv.org/abs/1301.6021v1,2013-01-25T11:54:48Z,2013-01-25T11:54:48Z,"  Inspired by previous work of Shoup, Lenstra-De Smit and Couveignes-Lercier,
we give fast algorithms to compute in (the first levels of) the ell-adic
closure of a finite field. In many cases, our algorithms have quasi-linear
complexity.
","['\nLuca De Feo\n', '\nJavad Doliskani\n', '\nÉric Schost\n']",,,http://dx.doi.org/10.1145/2465506.2465956,cs.SC,"['cs.SC', 'math.NT', 'F.2.1; G.4']",10.1145/2465506.2465956,,[]
"Analytic and Algorithmic Aspects of Generalized Harmonic Sums and
  Polylogarithms",http://arxiv.org/abs/1302.0378v1,2013-02-02T13:27:33Z,2013-02-02T13:27:33Z,"  In recent three--loop calculations of massive Feynman integrals within
Quantum Chromodynamics (QCD) and, e.g., in recent combinatorial problems the
so-called generalized harmonic sums (in short $S$-sums) arise. They are
characterized by rational (or real) numerator weights also different from $\pm
1$. In this article we explore the algorithmic and analytic properties of these
sums systematically. We work out the Mellin and inverse Mellin transform which
connects the sums under consideration with the associated Poincar\'{e} iterated
integrals, also called generalized harmonic polylogarithms. In this regard, we
obtain explicit analytic continuations by means of asymptotic expansions of the
$S$-sums which started to occur frequently in current QCD calculations. In
addition, we derive algebraic and structural relations, like differentiation
w.r.t. the external summation index and different multi-argument relations, for
the compactification of $S$-sum expressions. Finally, we calculate algebraic
relations for infinite $S$-sums, or equivalently for generalized harmonic
polylogarithms evaluated at special values. The corresponding algorithms and
relations are encoded in the computer algebra package {\tt HarmonicSums}.
","['\nJakob Ablinger\n', '\nJohannes Blümlein\n', '\nCarsten Schneider\n']","75 pages, 1 figure",,http://dx.doi.org/10.1063/1.4811117,math-ph,"['math-ph', 'cs.SC', 'hep-ph', 'hep-th', 'math.MP']",10.1063/1.4811117,,[]
"Creative telescoping for rational functions using the Griffiths-Dwork
  method",http://arxiv.org/abs/1301.4313v2,2013-01-18T07:40:01Z,2013-04-21T06:22:26Z,"  Creative telescoping algorithms compute linear differential equations
satisfied by multiple integrals with parameters. We describe a precise and
elementary algorithmic version of the Griffiths-Dwork method for the creative
telescoping of rational functions. This leads to bounds on the order and degree
of the coefficients of the differential equation, and to the first complexity
result which is simply exponential in the number of variables. One of the
important features of the algorithm is that it does not need to compute
certificates. The approach is vindicated by a prototype implementation.
","['\nAlin Bostan\nINRIA Saclay - Ile de France\n', '\nPierre Lairez\nINRIA Saclay - Ile de France\n', ""\nBruno Salvy\nInria Grenoble Rhône-Alpes / LIP Laboratoire de l'Informatique du Parallélisme\n""]",,"Proceedings of ISSAC 2013, ACM, pp 93-100",http://dx.doi.org/10.1145/2465506.2465935,cs.SC,['cs.SC'],10.1145/2465506.2465935,,"['INRIA Saclay - Ile de France', 'INRIA Saclay - Ile de France', ""Inria Grenoble Rhône-Alpes / LIP Laboratoire de l'Informatique du Parallélisme""]"
"From Approximate Factorization to Root Isolation with Application to
  Cylindrical Algebraic Decomposition",http://arxiv.org/abs/1301.4870v2,2013-01-21T14:14:07Z,2014-01-23T12:57:15Z,"  We present an algorithm for isolating the roots of an arbitrary complex
polynomial $p$ that also works for polynomials with multiple roots provided
that the number $k$ of distinct roots is given as part of the input. It outputs
$k$ pairwise disjoint disks each containing one of the distinct roots of $p$,
and its multiplicity. The algorithm uses approximate factorization as a
subroutine.
  In addition, we apply the new root isolation algorithm to a recent algorithm
for computing the topology of a real planar algebraic curve specified as the
zero set of a bivariate integer polynomial and for isolating the real solutions
of a bivariate polynomial system. For input polynomials of degree $n$ and
bitsize $\tau$, we improve the currently best running time from
$\tO(n^{9}\tau+n^{8}\tau^{2})$ (deterministic) to $\tO(n^{6}+n^{5}\tau)$
(randomized) for topology computation and from $\tO(n^{8}+n^{7}\tau)$
(deterministic) to $\tO(n^{6}+n^{5}\tau)$ (randomized) for solving bivariate
systems.
","['\nKurt Mehlhorn\n', '\nMichael Sagraloff\n', '\nPengming Wang\n']",,,http://arxiv.org/abs/1301.4870v2,cs.SC,['cs.SC'],,,[]
"Closed form solutions of linear difference equations in terms of
  symmetric products",http://arxiv.org/abs/1301.4983v1,2013-01-21T20:54:32Z,2013-01-21T20:54:32Z,"  In this paper we show how to find a closed form solution for third order
difference operators in terms of solutions of second order operators. This work
is an extension of previous results on finding closed form solutions of
recurrence equations and a counterpart to existing results on differential
equations. As motivation and application for this work, we discuss the problem
of proving positivity of sequences given merely in terms of their defining
recurrence relation. The main advantage of the present approach to earlier
methods attacking the same problem is that our algorithm provides
human-readable and verifiable, i.e., certified proofs.
",['\nYongjae Cha\n'],,,http://arxiv.org/abs/1301.4983v1,cs.SC,['cs.SC'],,,[]
Complexity of Creative Telescoping for Bivariate Rational Functions,http://arxiv.org/abs/1301.5045v1,2013-01-22T00:26:00Z,2013-01-22T00:26:00Z,"  The long-term goal initiated in this work is to obtain fast algorithms and
implementations for definite integration in Almkvist and Zeilberger's framework
of (differential) creative telescoping. Our complexity-driven approach is to
obtain tight degree bounds on the various expressions involved in the method.
To make the problem more tractable, we restrict to bivariate rational
functions. By considering this constrained class of inputs, we are able to
blend the general method of creative telescoping with the well-known Hermite
reduction. We then use our new method to compute diagonals of rational power
series arising from combinatorics.
","['\nAlin Bostan\n', '\nShaoshi Chen\n', '\nFrédéric Chyzak\n', '\nZiming Li\n']",8 pages,"Proceedings of the 2010 International Symposium on Symbolic and
  Algebraic Computation, pages 203--210, 2010, ACM",http://arxiv.org/abs/1301.5045v1,cs.SC,"['cs.SC', '33F10']",,,[]
"Hermite Reduction and Creative Telescoping for Hyperexponential
  Functions",http://arxiv.org/abs/1301.5038v1,2013-01-21T23:21:44Z,2013-01-21T23:21:44Z,"  We present a reduction algorithm that simultaneously extends Hermite's
reduction for rational functions and the Hermite-like reduction for
hyperexponential functions. It yields a unique additive decomposition and
allows to decide hyperexponential integrability. Based on this reduction
algorithm, we design a new method to compute minimal telescopers for bivariate
hyperexponential functions. One of its main features is that it can avoid the
costly computation of certificates. Its implementation outperforms Maple's
function DEtools[Zeilberger]. Moreover, we derive an order bound on minimal
telescopers, which is more general and tighter than the known one.
","['\nAlin Bostan\n', '\nShaoshi Chen\n', '\nFrédéric Chyzak\n', '\nZiming Li\n', '\nGuoce Xin\n']",8 pages,,http://arxiv.org/abs/1301.5038v1,cs.SC,"['cs.SC', 'math.CO', '33F10']",,,[]
On the Structure of Compatible Rational Functions,http://arxiv.org/abs/1301.5046v2,2013-01-22T00:37:06Z,2013-01-23T01:24:01Z,"  A finite number of rational functions are compatible if they satisfy the
compatibility conditions of a first-order linear functional system involving
differential, shift and q-shift operators. We present a theorem that describes
the structure of compatible rational functions. The theorem enables us to
decompose a solution of such a system as a product of a rational function,
several symbolic powers, a hyperexponential function, a hypergeometric term,
and a q-hypergeometric term. We outline an algorithm for computing this
product, and present an application.
","['\nShaoshi Chen\n', '\nRuyong Feng\n', '\nGuofeng Fu\n', '\nZiming Li\n']",,"Proceedings of the 2011 International Symposium on Symbolic and
  Algebraic Computation, pages 91--98, 2011, ACM",http://arxiv.org/abs/1301.5046v2,cs.SC,"['cs.SC', 'math.CO', '33F10']",,,[]
Factorization of Z-homogeneous polynomials in the First (q)-Weyl Algebra,http://arxiv.org/abs/1302.5674v3,2013-01-20T18:37:47Z,2016-02-18T16:04:15Z,"  We present algorithms to factorize weighted homogeneous elements in the first
polynomial Weyl algebra and $q$-Weyl algebra, which are both viewed as a
$\mathbb{Z}$-graded rings. We show, that factorization of homogeneous
polynomials can be almost completely reduced to commutative univariate
factorization over the same base field with some additional uncomplicated
combinatorial steps. This allows to deduce the complexity of our algorithms in
detail. Furthermore, we will show for homogeneous polynomials that
irreducibility in the polynomial first Weyl algebra also implies irreducibility
in the rational one, which is of interest for practical reasons. We report on
our implementation in the computer algebra system \textsc{Singular}. It
outperforms for homogeneous polynomials currently available implementations
dealing with factorization in the first Weyl algebra both in speed and elegancy
of the results.
","['\nAlbert Heinle\n', '\nViktor Levandovskyy\n']","26 pages, Singular implementation, 2 algorithms, 1 figure, 2 tables",,http://arxiv.org/abs/1302.5674v3,cs.SC,"['cs.SC', 'cs.MS']",,,[]
Generic Regular Decompositions for Parametric Polynomial Systems,http://arxiv.org/abs/1301.3991v1,2013-01-17T06:16:02Z,2013-01-17T06:16:02Z,"  This paper presents a generalization of our earlier work in [19]. In this
paper, the two concepts, generic regular decomposition (GRD) and
regular-decomposition-unstable (RDU) variety introduced in [19] for generic
zero-dimensional systems, are extended to the case where the parametric systems
are not necessarily zero-dimensional. An algorithm is provided to compute GRDs
and the associated RDU varieties of parametric systems simultaneously on the
basis of the algorithm for generic zero-dimensional systems proposed in [19].
Then the solutions of any parametric system can be represented by the solutions
of finitely many regular systems and the decomposition is stable at any
parameter value in the complement of the associated RDU variety of the
parameter space. The related definitions and the results presented in [19] are
also generalized and a further discussion on RDU varieties is given from an
experimental point of view. The new algorithm has been implemented on the basis
of DISCOVERER with Maple 16 and experimented with a number of benchmarks from
the literature.
","['\nZhenghong Chen\n', '\nXiaoxian Tang\n', '\nBican Xia\n']","It is the latest version. arXiv admin note: text overlap with
  arXiv:1208.6112",,http://arxiv.org/abs/1301.3991v1,cs.SC,"['cs.SC', 'cs.IT', 'math.IT']",,,[]
Classification of Angle-Symmetric 6R Linkage,http://arxiv.org/abs/1301.4072v1,2013-01-17T12:33:37Z,2013-01-17T12:33:37Z,"  In this paper, we consider a special kind of overconstrained 6R closed
linkages which we call angle-symmetric 6R linkages. These are linkages with the
property that the rotation angles are equal for each of the three pairs of
opposite joints. We give a classification of these linkages. It turns that
there are three types. First, we have the linkages with line symmetry. The
second type is new. The third type is related to cubic motion polynomials.
","['\nZijia Li\n', '\nJosef Schicho\n']",,,http://dx.doi.org/10.1016/j.mechmachtheory.2013.08.002,math.AG,"['math.AG', 'cs.RO', 'cs.SC']",10.1016/j.mechmachtheory.2013.08.002,,[]
Desingularization Explains Order-Degree Curves for Ore Operators,http://arxiv.org/abs/1301.0917v1,2013-01-05T16:45:53Z,2013-01-05T16:45:53Z,"  Desingularization is the problem of finding a left multiple of a given Ore
operator in which some factor of the leading coefficient of the original
operator is removed. An order-degree curve for a given Ore operator is a curve
in the $(r,d)$-plane such that for all points $(r,d)$ above this curve, there
exists a left multiple of order $r$ and degree $d$ of the given operator. We
give a new proof of a desingularization result by Abramov and van Hoeij for the
shift case, and show how desingularization implies order-degree curves which
are extremely accurate in examples.
","['\nShaoshi Chen\n', '\nMaximilian Jaroschek\n', '\nManuel Kauers\n', '\nMichael F. Singer\n']",,,http://arxiv.org/abs/1301.0917v1,cs.SC,"['cs.SC', 'I.1.2']",,,[]
"Finding Hyperexponential Solutions of Linear ODEs by Numerical
  Evaluation",http://arxiv.org/abs/1301.2486v1,2013-01-11T13:00:24Z,2013-01-11T13:00:24Z,"  We present a new algorithm for computing hyperexponential solutions of
ordinary linear differential equations with polynomial coefficients. The
algorithm relies on interpreting formal series solutions at the singular points
as analytic functions and evaluating them numerically at some common ordinary
point. The numerical data is used to determine a small number of combinations
of the formal series that may give rise to hyperexponential solutions.
","['\nFredrik Johansson\n', '\nManuel Kauers\n', '\nMarc Mezzarobba\n']",,,http://arxiv.org/abs/1301.2486v1,cs.SC,"['cs.SC', 'I.1.2']",,,[]
"Pfaffian Systems of A-Hypergeometric Equations I: Bases of Twisted
  Cohomology Groups",http://arxiv.org/abs/1212.6103v3,2012-12-25T23:49:03Z,2014-06-17T23:43:28Z,"  This is the third revision. We study bases of Pfaffian systems for
$A$-hypergeometric system. Gr\""obner deformations give bases. These bases also
give those for twisted cohomology groups. For hypergeometric system associated
to a class of order polytopes, these bases have a combinatorial description.
The size of the bases associated to a subclass of the order polytopes have the
growth rate of the polynomial order. Bases associated to two chain posets and
bouquets are studied.
","['\nTakayuki Hibi\n', '\nKenta Nishiyama\n', '\nNobuki Takayama\n']",,,http://arxiv.org/abs/1212.6103v3,math.CA,"['math.CA', 'cs.SC', 'math.CO']",,,[]
"Multiple precision evaluation of the Airy Ai function with reduced
  cancellation",http://arxiv.org/abs/1212.4731v2,2012-12-19T16:45:44Z,2013-04-29T05:34:41Z,"  The series expansion at the origin of the Airy function Ai(x) is alternating
and hence problematic to evaluate for x > 0 due to cancellation. Based on a
method recently proposed by Gawronski, M\""uller, and Reinhard, we exhibit two
functions F and G, both with nonnegative Taylor expansions at the origin, such
that Ai(x) = G(x)/F(x). The sums are now well-conditioned, but the Taylor
coefficients of G turn out to obey an ill-conditioned three-term recurrence. We
use the classical Miller algorithm to overcome this issue. We bound all errors
and our implementation allows an arbitrary and certified accuracy, that can be
used, e.g., for providing correct rounding in arbitrary precision.
","['\nSylvain Chevillard\nINRIA Sophia Antipolis\n', ""\nMarc Mezzarobba\nInria Grenoble Rhône-Alpes / LIP Laboratoire de l'Informatique du Parallélisme\n""]",,21st IEEE Symposium on Computer Arithmetic (2013),http://arxiv.org/abs/1212.4731v2,cs.SC,['cs.SC'],,,"['INRIA Sophia Antipolis', ""Inria Grenoble Rhône-Alpes / LIP Laboratoire de l'Informatique du Parallélisme""]"
Sparse Difference Resultant,http://arxiv.org/abs/1212.3090v2,2012-12-13T09:02:32Z,2013-09-24T09:51:04Z,"  In this paper, the concept of sparse difference resultant for a Laurent
transformally essential system of difference polynomials is introduced and a
simple criterion for the existence of sparse difference resultant is given. The
concept of transformally homogenous polynomial is introduced and the sparse
difference resultant is shown to be transformally homogenous. It is shown that
the vanishing of the sparse difference resultant gives a necessary condition
for the corresponding difference polynomial system to have non-zero solutions.
The order and degree bounds for sparse difference resultant are given. Based on
these bounds, an algorithm to compute the sparse difference resultant is
proposed, which is single exponential in terms of the number of variables, the
Jacobi number, and the size of the Laurent transformally essential system.
Furthermore, the precise order and degree, a determinant representation, and a
Poisson-type product formula for the difference resultant are given.
","['\nWei Li\n', '\nChun-Ming Yuan\n', '\nXiao-Shan Gao\n']",43 pages. arXiv admin note: text overlap with arXiv:1111.1084,,http://arxiv.org/abs/1212.3090v2,cs.SC,"['cs.SC', 'math.AG', '12H10, 68W30', 'I.1']",,,[]
Relative parametrization of linear multidimensional systems,http://arxiv.org/abs/1212.4590v1,2012-12-19T07:47:53Z,2012-12-19T07:47:53Z,"  In the last chapter of his book ""The Algebraic Theory of Modular Systems ""
published in 1916, F. S. Macaulay developped specific techniques for dealing
with "" unmixed polynomial ideals "" by introducing what he called "" inverse
systems "". The purpose of this paper is to extend such a point of view to
differential modules defined by linear multidimensional systems, that is by
linear systems of ordinary differential (OD) or partial differential (PD)
equations of any order, with any number of independent variables, any number of
unknowns and even with variable coefficients in a differential field. The first
and main idea is to replace unmixed polynomial ideals by "" pure differential
modules "". The second idea is to notice that a module is 0-pure if and only if
it is torsion-free and thus if and only if it admits an "" absolute
parametrization "" by means of arbitrary potential like functions, or,
equivalently, if it can be embedded into a free module by means of an ""
absolute localization "". The third idea is to refer to a difficult theorem of
algebraic analysis saying that an r-pure module can be embedded into a module
of projective dimension equal to r, that is a module admitting a projective
resolution with exactly r operators. The fourth and final idea is to establish
a link between the use of extension modules for such a purpose and specific
formal properties of the underlying multidimensional system through the use of
involution and a ""relative localization "" leading to a ""relative
parametrization "", that is to the use of potential-like functions satisfying a
kind of ""minimum differential constraint "" limiting, in some sense, the number
of independent variables appearing in these functions, in a way similar to the
situation met in the Cartan-K\""ahler theorem of analysis. The paper is written
in a rather effective self-contained way and we provide many explicit examples
that should become test examples for a future use of computer algebra.
",['\nJean-François Pommaret\nCERMICS\n'],"Presented for publication in the Springer journal
  MSSP:Multidimensional Systems and Signal Processing",,http://arxiv.org/abs/1212.4590v1,math.AP,"['math.AP', 'cs.SC', 'math.AC', 'math.DG', 'math.RA']",,,['CERMICS']
Theano: new features and speed improvements,http://arxiv.org/abs/1211.5590v1,2012-11-23T20:42:41Z,2012-11-23T20:42:41Z,"  Theano is a linear algebra compiler that optimizes a user's
symbolically-specified mathematical computations to produce efficient low-level
implementations. In this paper, we present new features and efficiency
improvements to Theano, and benchmarks demonstrating Theano's performance
relative to Torch7, a recently introduced machine learning library, and to
RNNLM, a C++ library targeted at recurrent neural networks.
","['\nFrédéric Bastien\n', '\nPascal Lamblin\n', '\nRazvan Pascanu\n', '\nJames Bergstra\n', '\nIan Goodfellow\n', '\nArnaud Bergeron\n', '\nNicolas Bouchard\n', '\nDavid Warde-Farley\n', '\nYoshua Bengio\n']","Presented at the Deep Learning Workshop, NIPS 2012",,http://arxiv.org/abs/1211.5590v1,cs.SC,"['cs.SC', 'cs.LG']",,,[]
"Confusion of Tagged Perturbations in Forward Automatic Differentiation
  of Higher-Order Functions",http://arxiv.org/abs/1211.4892v4,2012-11-20T22:08:31Z,2019-06-29T20:44:41Z,"  Forward Automatic Differentiation (AD) is a technique for augmenting programs
to compute derivatives. The essence of Forward AD is to attach perturbations to
each number, and propagate these through the computation. When derivatives are
nested, the distinct derivative calculations, and their associated
perturbations, must be distinguished. This is typically accomplished by
creating a unique tag for each derivative calculation, tagging the
perturbations, and overloading the arithmetic operators. We exhibit a subtle
bug, present in fielded implementations, in which perturbations are confused
despite the tagging machinery. The essence of the bug is this: each invocation
of a derivative creates a unique tag but a unique tag is needed for each
derivative calculation. When taking derivatives of higher-order functions,
these need not correspond! The derivative of a higher-order function $f$ that
returns a function $g$ will be a function $f'$ that returns a function
$\bar{g}$ that performs a derivative calculation. A single invocation of $f'$
will create a single fresh tag but that same tag will be used for each
derivative calculation resulting from an invocation of $\bar{g}$. This
situation arises when taking derivatives of curried functions. Two potential
solutions are presented, and their serious deficiencies discussed. One requires
eta expansion to delay the creation of fresh tags from the invocation of $f'$
to the invocation of $\bar{g}$, which can be difficult or even impossible in
some circumstances. The other requires $f'$ to wrap $\bar{g}$ with tag
renaming, which is difficult to implement without violating the desirable
complexity properties of forward AD.
","['\nOleksandr Manzyuk\n', '\nBarak A. Pearlmutter\n', '\nAlexey Andreyevich Radul\n', '\nDavid R. Rush\n', '\nJeffrey Mark Siskind\n']",,,http://dx.doi.org/10.1017/S095679681900008X,cs.SC,"['cs.SC', 'cs.MS', 'math.DG']",10.1017/S095679681900008X,,[]
Irreducibility of q-difference operators and the knot 7_4,http://arxiv.org/abs/1211.6020v3,2012-11-26T16:49:57Z,2013-10-21T15:04:59Z,"  Our goal is to compute the minimal-order recurrence of the colored Jones
polynomial of the 7_4 knot, as well as for the first four double twist knots.
As a corollary, we verify the AJ Conjecture for the simplest knot 7_4 with
reducible non-abelian SL(2,C) character variety. To achieve our goal, we use
symbolic summation techniques of Zeilberger's holonomic systems approach and an
irreducibility criterion for q-difference operators. For the latter we use an
improved version of the qHyper algorithm of Abramov-Paule-Petkovsek to show
that a given q-difference operator has no linear right factors. En route, we
introduce exterior power Adams operations on the ring of bivariate polynomials
and on the corresponding affine curves.
","['\nStavros Garoufalidis\n', '\nChristoph Koutschan\n']","20 pages, 3 figures, 2 tables",Algebr. Geom. Topol. 13 (2013) 3261-3286,http://dx.doi.org/10.2140/agt.2013.13.3261,math.GT,"['math.GT', 'cs.SC', 'math.CO', '57N10 (Primary) 57M25, 33F10, 39A13 (Secondary)']",10.2140/agt.2013.13.3261,,[]
On the Existence of Telescopers for Mixed Hypergeometric Terms,http://arxiv.org/abs/1211.2430v2,2012-11-11T15:09:59Z,2012-11-13T06:59:26Z,"  We present a criterion for the existence of telescopers for mixed
hypergeometric terms, which is based on multiplicative and additive
decompositions. The criterion enables us to determine the termination of
Zeilberger's algorithms for mixed hypergeometric inputs.
","['\nShaoshi Chen\n', '\nFrédéric Chyzak\n', '\nRuyong Feng\n', '\nGuofeng Fu\n', '\nZiming Li\n']",,,http://arxiv.org/abs/1211.2430v2,cs.SC,"['cs.SC', 'math.CO', '33F10']",,,[]
"How to compute the constant term of a power of a Laurent polynomial
  efficiently",http://arxiv.org/abs/1211.3959v1,2012-11-16T17:25:36Z,2012-11-16T17:25:36Z,"  We present an algorithm for efficient computation of the constant term of a
power of a multivariate Laurent polynomial. The algorithm is based on
univariate interpolation, does not require the storage of intermediate data and
can be easily parallelized. As an application we compute the power series
expansion of the principal period of some toric Calabi-Yau varieties and find
previously unknown differential operators of Calabi-Yau type.
",['\nPavel Metelitsyn\n'],12 pages,,http://arxiv.org/abs/1211.3959v1,cs.SC,"['cs.SC', 'math.AG']",,,[]
"Unified Form Language: A domain-specific language for weak formulations
  of partial differential equations",http://arxiv.org/abs/1211.4047v2,2012-11-16T21:56:02Z,2013-04-25T20:18:09Z,"  We present the Unified Form Language (UFL), which is a domain-specific
language for representing weak formulations of partial differential equations
with a view to numerical approximation. Features of UFL include support for
variational forms and functionals, automatic differentiation of forms and
expressions, arbitrary function space hierarchies for multi-field problems,
general differential operators and flexible tensor algebra. With these
features, UFL has been used to effortlessly express finite element methods for
complex systems of partial differential equations in near-mathematical
notation, resulting in compact, intuitive and readable programs. We present in
this work the language and its construction. An implementation of UFL is freely
available as an open-source software library. The library generates abstract
syntax tree representations of variational problems, which are used by other
software libraries to generate concrete low-level implementations. Some
application examples are presented and libraries that support UFL are
highlighted.
","['\nMartin S. Alnaes\n', '\nAnders Logg\n', '\nKristian B. Oelgaard\n', '\nMarie E. Rognes\n', '\nGarth N. Wells\n']",To appear in ACM Transactions on Mathematical Software,,http://arxiv.org/abs/1211.4047v2,cs.MS,"['cs.MS', 'cs.NA', 'cs.SC', '97N80', 'G.4; G.1.8; G.1.4']",,,[]
A new Truncated Fourier Transform algorithm,http://arxiv.org/abs/1210.4960v3,2012-10-17T21:16:30Z,2013-01-29T18:57:34Z,"  Truncated Fourier Transforms (TFTs), first introduced by Van der Hoeven,
refer to a family of algorithms that attempt to smooth ""jumps"" in complexity
exhibited by FFT algorithms. We present an in-place TFT whose time complexity,
measured in terms of ring operations, is comparable to existing not-in-place
TFT methods. We also describe a transformation that maps between two families
of TFT algorithms that use different sets of evaluation points.
",['\nAndrew Arnold\n'],"8 pages, submitted to the 38th International Symposium on Symbolic
  and Algebraic Computation (ISSAC 2013)",,http://arxiv.org/abs/1210.4960v3,cs.SC,['cs.SC'],,,[]
"An Incremental Algorithm for Computing Cylindrical Algebraic
  Decompositions",http://arxiv.org/abs/1210.5543v1,2012-10-19T21:31:13Z,2012-10-19T21:31:13Z,"  In this paper, we propose an incremental algorithm for computing cylindrical
algebraic decompositions. The algorithm consists of two parts: computing a
complex cylindrical tree and refining this complex tree into a cylindrical tree
in real space. The incrementality comes from the first part of the algorithm,
where a complex cylindrical tree is constructed by refining a previous complex
cylindrical tree with a polynomial constraint. We have implemented our
algorithm in Maple. The experimentation shows that the proposed algorithm
outperforms existing ones for many examples taken from the literature.
","['\nChangbo Chen\n', '\nMarc Moreno Maza\n']",,,http://arxiv.org/abs/1210.5543v1,cs.SC,['cs.SC'],,,[]
A New Recursive Algorithm For Inverting A General Comrade Matrix,http://arxiv.org/abs/1210.4662v1,2012-10-17T08:09:08Z,2012-10-17T08:09:08Z,"  In this paper, the author present a reliable symbolic computational algorithm
for inverting a general comrade matrix by using parallel computing along with
recursion. The computational cost of our algorithm is O(n^2). The algorithm is
implementable to the Computer Algebra System (CAS) such as MAPLE, MATLAB and
MATHEMATICA. Three examples are presented for the sake of illustration.
",['\nA. A. Karawia\n'],,,http://arxiv.org/abs/1210.4662v1,cs.SC,"['cs.SC', 'cs.MS', '15A15, 15A23, 68W30, 11Y05, 33F10, F.2.1, G.1.0']",,,[]
On the Summability of Bivariate Rational Functions,http://arxiv.org/abs/1210.6366v1,2012-10-23T20:12:42Z,2012-10-23T20:12:42Z,"  We present criteria for deciding whether a bivariate rational function in two
variables can be written as a sum of two (q-)differences of bivariate rational
functions. Using these criteria, we show how certain double sums can be
evaluated, first, in terms of single sums and, finally, in terms of values of
special functions.
","['\nShaoshi Chen\n', '\nMichael F. Singer\n']",24 pages,,http://arxiv.org/abs/1210.6366v1,math.CO,"['math.CO', 'cs.SC']",,,[]
Regular and Singular Boundary Problems in Maple,http://arxiv.org/abs/1210.2951v1,2012-10-10T15:13:55Z,2012-10-10T15:13:55Z,"  We describe a new Maple package for treating boundary problems for linear
ordinary differential equations, allowing two-/multipoint as well as Stieltjes
boundary conditions. For expressing differential operators, boundary
conditions, and Green's operators, we employ the algebra of
integro-differential operators. The operations implemented for regular boundary
problems include computing Green's operators as well as composing and factoring
boundary problems. Our symbolic approach to singular boundary problems is new;
it provides algorithms for computing compatibility conditions and generalized
Green's operators.
","['\nAnja Korporal\n', '\nGeorg Regensburger\n', '\nMarkus Rosenkranz\n']","14 pages; Berlin/Heidelberg, Springer","Computer Algebra in Scientific Computing (CASC 2011), LNCS 6885,
  pp. 280-293, 2011",http://dx.doi.org/10.1007/978-3-642-23568-9_22,cs.SC,"['cs.SC', 'cs.MS', '68W30']",10.1007/978-3-642-23568-9_22,,[]
"Symbolic Analysis for Boundary Problems: From Rewriting to Parametrized
  Gröbner Bases",http://arxiv.org/abs/1210.2950v1,2012-10-10T15:11:49Z,2012-10-10T15:11:49Z,"  We review our algebraic framework for linear boundary problems (concentrating
on ordinary differential equations). Its starting point is an appropriate
algebraization of the domain of functions, which we have named
integro-differential algebras. The algebraic treatment of boundary problems
brings up two new algebraic structures whose symbolic representation and
computational realization is based on canonical forms in certain commutative
and noncommutative polynomial domains. The first of these, the ring of
integro-differential operators, is used for both stating and solving linear
boundary problems. The other structure, called integro-differential
polynomials, is the key tool for describing extensions of integro-differential
algebras. We use the canonical simplifier for integro-differential polynomials
for generating an automated proof establishing a canonical simplifier for
integro-differential operators. Our approach is fully implemented in the
Theorema system; some code fragments and sample computations are included.
","['\nMarkus Rosenkranz\n', '\nGeorg Regensburger\n', '\nLoredana Tec\n', '\nBruno Buchberger\n']",54 pages,"Numerical and Symbolic Scientific Computing, Vol. 1, pp. 273-331,
  2012",http://dx.doi.org/10.1007/978-3-7091-0794-2_13,cs.SC,"['cs.SC', 'cs.MS', 'math.CA', '65L10, 34B05, 13P10, 54J05, 45P05, 68W30']",10.1007/978-3-7091-0794-2_13,,[]
"On Newton-Raphson iteration for multiplicative inverses modulo prime
  powers",http://arxiv.org/abs/1209.6626v5,2012-09-28T19:52:06Z,2018-05-15T10:10:49Z,"  We study algorithms for the fast computation of modular inverses.
Newton-Raphson iteration over $p$-adic numbers gives a recurrence relation
computing modular inverse modulo $p^m$, that is logarithmic in $m$. We solve
the recurrence to obtain an explicit formula for the inverse. Then we study
different implementation variants of this iteration and show that our explicit
formula is interesting for small exponent values but slower or large exponent,
say of more than $700$ bits. Overall we thus propose a hybrid combination of
our explicit formula and the best asymptotic variants. This hybrid combination
yields then a constant factor improvement, also for large exponents.
",['\nJean-Guillaume Dumas\nCASYS\n'],,"IEEE Transactions on Computers, Institute of Electrical and
  Electronics Engineers, 2014, 63 (8), pp.2106-2109",http://dx.doi.org/10.1109/TC.2013.94,cs.SC,"['cs.SC', 'cs.MS']",10.1109/TC.2013.94,,['CASYS']
On the Complexity of the Multivariate Resultant,http://arxiv.org/abs/1210.1451v1,2012-10-04T14:11:43Z,2012-10-04T14:11:43Z,"  The multivariate resultant is a fundamental tool of computational algebraic
geometry. It can in particular be used to decide whether a system of n
homogeneous equations in n variables is satisfiable (the resultant is a
polynomial in the system's coefficients which vanishes if and only if the
system is satisfiable). In this paper, we investigate the complexity of
computing the multivariate resultant.
  First, we study the complexity of testing the multivariate resultant for
zero. Our main result is that this problem is NP-hard under deterministic
reductions in any characteristic, for systems of low-degree polynomials with
coefficients in the ground field (rather than in an extension). In
characteristic zero, we observe that this problem is in the Arthur-Merlin class
AM if the generalized Riemann hypothesis holds true, while the best known upper
bound in positive characteristic remains PSPACE.
  Second, we study the classical algorithms to compute the resultant. They
usually rely on the computation of the determinant of an exponential-size
matrix, known as Macaulay matrix. We show that this matrix belongs to a class
of succinctly representable matrices, for which testing the determinant for
zero is proved PSPACE-complete. This means that improving Canny's PSPACE upper
bound requires either to look at the fine structure of the Macaulay matrix to
find an ad hoc algorithm for computing its determinant, or to use altogether
different techniques.
","['\nBruno Grenet\n', '\nPascal Koiran\n', '\nNatacha Portier\n']","25 pages. arXiv admin note: substantial text overlap with
  arXiv:0912.2607","Journal of Complexity, 29(2), pp 142-157, 2013",http://dx.doi.org/10.1016/j.jco.2012.10.001,cs.CC,"['cs.CC', 'cs.SC']",10.1016/j.jco.2012.10.001,,[]
A Note on the Space Complexity of Fast D-Finite Function Evaluation,http://arxiv.org/abs/1209.5097v1,2012-09-23T19:02:26Z,2012-09-23T19:02:26Z,"  We state and analyze a generalization of the ""truncation trick"" suggested by
Gourdon and Sebah to improve the performance of power series evaluation by
binary splitting. It follows from our analysis that the values of D-finite
functions (i.e., functions described as solutions of linear differential
equations with polynomial coefficients) may be computed with error bounded by
2^(-p) in time O(p*(lg p)^(3+o(1))) and space O(p). The standard fast algorithm
for this task, due to Chudnovsky and Chudnovsky, achieves the same time
complexity bound but requires \Theta(p*lg p) bits of memory.
","[""\nMarc Mezzarobba\nInria Grenoble Rhône-Alpes / LIP Laboratoire de l'Informatique du Parallélisme\n""]",,"CASC - Computer Algebra in Scientific Computing 7442 (2012)
  212-223",http://dx.doi.org/10.1007/978-3-642-32973-9_18,cs.SC,['cs.SC'],10.1007/978-3-642-32973-9_18,,"[""Inria Grenoble Rhône-Alpes / LIP Laboratoire de l'Informatique du Parallélisme""]"
The expansion of real forms on the simplex and applications,http://arxiv.org/abs/1209.3080v2,2012-09-14T02:49:19Z,2012-09-18T01:59:02Z,"  If n points B_1,---,B_n$ in the standard simplex \Delta_n are affinely
independent, then they can span an (n-1)-simplex denoted by
\Lambda=Con(B_1,---,B_n). Here \Lambda corresponds to an n*n matrix [\Lambda]
whose columns are B_1,---,B_n. In this paper, we firstly proved that if \Lambda
of diameter sufficiently small contains a point $P$, and f(P)>0 (<0) for a form
f in R[X], then the coefficients of f([\Lambda] X) are all positive (negative).
Next, as an application of this result, a necessary and sufficient condition
for determining the real zeros on \Delta_n of a system of homogeneous algebraic
equations with integral coefficients is established.
","['\nYong Yao\n', '\nJia Xu\n', '\nJingzhong Zhang\n']","10 pages, 1 figures",,http://arxiv.org/abs/1209.3080v2,math.AG,"['math.AG', 'cs.SC', '13J25, 13J30, 16Y60, 68T15, 26D05']",,,[]
logcf: An Efficient Tool for Real Root Isolation,http://arxiv.org/abs/1209.3555v1,2012-09-17T05:27:33Z,2012-09-17T05:27:33Z,"  This paper revisits an algorithm for isolating real roots of univariate
polynomials based on continued fractions. It follows the work of Vincent,
Uspen- sky, Collins and Akritas, Johnson and Krandick. We use some tricks,
especially a new algorithm for computing an upper bound of positive roots. In
this way, the algorithm of isolating real roots is improved. The complexity of
our method for computing an upper bound of positive roots is O(n log(u+1))
where u is the optimal upper bound satisfying Theorem 3 and n is the degree of
the polynomial. Our method has been implemented as a software package logcf
using C++ language. For many benchmarks logcf is two or three times faster than
the function RootIntervals of Mathematica. And it is much faster than another
continued fractions based software CF, which seems to be one of the fastest
available open software for exact real root isolation. For those benchmarks
which have only real roots, logcf is much faster than Sleeve and eigensolve
which are based on numerical computation.
","['\nLiyun Dai\n', '\nBican Xia\n']","18 pages, 2 figures",,http://arxiv.org/abs/1209.3555v1,cs.SC,"['cs.SC', 'math.AG']",,,[]
"A proposal to first principles electronic structure calculation:
  Symbolic-Numeric method",http://arxiv.org/abs/1209.5127v4,2012-09-23T23:50:56Z,2013-02-25T02:08:12Z,"  This study proposes an approach toward the first principles electronic
structure calculation with the aid of symbolic-numeric solving. The symbolic
computation enables us to express the Hartree-Fock-Roothaan equation and the
molecular integrals in analytic forms and approximate them as a set of
polynomial equations. By use of the Grobner bases technique, the polynomial
equations are transformed into other ones which have identical roots. The
converted equations take more convenient forms which will simplify numerical
procedures, from which we can derive necessary physical properties in order, in
an a la carte way. This method enables us to solve the electronic structure
calculation, the optimization of any kind, or the inverse problem as a forward
problem in a unified way, in which there is no need for iterative
self-consistent procedures with trials and errors.
",['\nAkihito Kikuchi\n'],"49 pages. This paper is originally written in Japanese and published
  in a Japanese journal ""bussei-kenkyu"". The author submits here the original
  Japanese text, accompanied with a brief (almost full) English translation. In
  this revised version, the author has corrected some typos. If the reader
  would like to get the complete full translation, contact the author [
  kikuchi.akihito@canon.co.jp ]","Bussei Kenkyu, Vol 1. No.3, 013101, 2012, (
  http://bussei-kenkyu.jp/pdf/01/3/0030-013101.pdf )",http://arxiv.org/abs/1209.5127v4,cs.SC,"['cs.SC', 'cond-mat.mtrl-sci', 'physics.comp-ph']",,,[]
