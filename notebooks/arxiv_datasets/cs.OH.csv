Title,ID,Published,Updated,Summary,Author,Comments,Journal_Ref,Link,Primary_Category,Categories,DOI,License,Affiliation
Is 3-(F)WL Enough to Distinguish All 3D Graphs?,http://arxiv.org/abs/2402.08429v1,2024-01-24T06:22:08Z,2024-01-24T06:22:08Z,"  The problem of graph isomorphism is an important but challenging problem in
the field of graph analysis, for example: analyzing the similarity of two
chemical molecules, or studying the expressive ability of graph neural
networks. WL test is a method to judge whether two graphs are isomorphic, but
it cannot distinguish all non-isomorphic graphs. As an improvement of WL, k-WL
has stronger isomorphism discrimination ability, and as k increases, its
discrimination ability is strictly increasing. However, whether the isomorphic
discrimination power of k-WL is strictly increasing for more complex 3D graphs,
or whether there exists k that can discriminate all 3D graphs, remains
unexplored. This paper attempts to explore this problem from the perspective of
graph generation.
",['\nWanghan Xu\n'],,,http://arxiv.org/abs/2402.08429v1,cs.OH,"['cs.OH', 'cs.AI']",,,[]
Foundations of Work-Systems Modeling,http://arxiv.org/abs/2401.16221v1,2024-01-13T22:02:53Z,2024-01-13T22:02:53Z,"  In 2006, the course ""Modeling of Organizations"" is taught for the third time.
This third time will be the second time we will use the new lecture notes ""Work
Systems Modelling"" from the DA VINCI series. These lecture notes, however, will
be evolved further hand-in-hand with the actual process of lecturing. In the
academic year 2005/2006, a second incarnation of these lecture notes will be
created, where the aim is to deliver these lecture notes in three increments.
An important step that will be taken in this academic year is the integration
of the ICIS Work Systems Modelling lecture notes with the NICI course on
Organisational Dynamics. The first results of this integration will start to
appear in the second and third trimester.
",['\nHenderik Alex Proper\n'],,,http://arxiv.org/abs/2401.16221v1,cs.OH,"['cs.OH', 'cs.SE']",,,[]
Challenge design roadmap,http://arxiv.org/abs/2401.13693v1,2024-01-15T10:58:30Z,2024-01-15T10:58:30Z,"  Challenges can be seen as a type of game that motivates participants to solve
serious tasks. As a result, competition organizers must develop effective game
rules. However, these rules have multiple objectives beyond making the game
enjoyable for participants. These objectives may include solving real-world
problems, advancing scientific or technical areas, making scientific
discoveries, and educating the public. In many ways, creating a challenge is
similar to launching a product. It requires the same level of excitement and
rigorous testing, and the goal is to attract ''customers'' in the form of
participants. The process begins with a solid plan, such as a competition
proposal that will eventually be submitted to an international conference and
subjected to peer review. Although peer review does not guarantee quality, it
does force organizers to consider the impact of their challenge, identify
potential oversights, and generally improve its quality. This chapter provides
guidelines for creating a strong plan for a challenge. The material draws on
the preparation guidelines from organizations such as Kaggle 1 , ChaLearn 2 and
Tailor 3 , as well as the NeurIPS proposal template, which some of the authors
contributed to.
","['\nHugo Jair Escalante Balderas\nLISN, TAU\n', '\nIsabelle Guyon\nLISN, TAU\n', '\nAddison Howard\nTAU\n', '\nWalter Reade\nTAU\n', '\nSebastien Treguer\nTAU\n']",,"AI Competitions and Benchmarks: The Science Behind the Contests,
  In press",http://arxiv.org/abs/2401.13693v1,cs.OH,"['cs.OH', 'cs.AI', 'cs.HC']",,,"['LISN, TAU', 'LISN, TAU', 'TAU', 'TAU', 'TAU']"
"Using Terrestrial Laser Scanning, Unmanned Aerial Vehicles and Mixed
  Reality Methodologies for Digital Survey, 3D Modelling and Historical
  Recreation of Religious Heritage Monuments",http://arxiv.org/abs/2401.01380v1,2023-12-31T16:24:22Z,2023-12-31T16:24:22Z,"  Preserving and safeguarding the Cultural Heritage (CH) of our world from
unforeseen hazards should be viewed as a collective responsibility for
humanity. Consequently, there is a growing imperative for targeted measures
aimed at conserving, rejuvenating, and safeguarding historical assets that
carry cultural significance. In recent times, Terrestrial Laser Scanning (TLS),
Unmanned Aerial Vehicle (UAV) Photogrammetry, and applications in Mixed Reality
(MR) have assumed a pivotal role in the mapping, recording, preservation, and
promotion of Cultural Heritage. This ar-ticle endeavors to present a
comprehensive approach spanning from 3D surveying to the 3D representation and
promotion of Religious Cultural Heritage, offering an overview of the applied
methodologies. Through the integration of TLS and UAV photogrammetry
techniques, a comprehensive digital record of Panagia Ekatontapyliani, the
adjoining Church of Agios Nikolaos, and the Baptistery, along with their wall
paintings (hagiographies) and natural surroundings, has been obtained. This
record serves as the foundation for historical documentation and recreation
using the HBIM concept, paving the way for the development of diverse Mixed
Reality applications. These applications aim to enhance the visibility,
accessibility, and visitability of the Monument.
","['\nAristeidis Zachos\n', '\nChristos-Nikolaos Anagnostopoulos\n']",,,http://arxiv.org/abs/2401.01380v1,cs.OH,['cs.OH'],,,[]
"Portable medical devices creation technology by using the Bluetooth
  module",http://arxiv.org/abs/2402.03323v1,2024-01-07T15:37:56Z,2024-01-07T15:37:56Z,"  The article is devoted Bluetooth wireless personal area networks
specification, which provides standard for exchanging data over short
distances. It is shown how the technology has evolved and its application in
the design of devices. Health Device Profile considered in details, which the
main feature is the work of a medical orientation devices.
","['\nA. O. Dadukin\n', '\nN. I. Pchelintseva\n']","7 pages, Russian language, Published in ""Digital journal: science,
  machinery, and education"" Reference: https://nto-journal.ru/authors/354/",,http://arxiv.org/abs/2402.03323v1,cs.OH,['cs.OH'],,,[]
"On the relevance of the Godot Engine in the indie game development
  industry",http://arxiv.org/abs/2401.01909v2,2023-12-27T12:14:21Z,2024-01-10T17:00:28Z,"  This paper examines the relevance of the Godot Engine in the indie game
industry. The Godot Engine is a relatively new game engine from 2014 and
competes with leading market players. To get to the bottom of its relevance,
two major online sales platforms and the game engines that are commonly used,
Steam and itch[dot]io, are examined. Mainly, these findings are compared with
reference data from 2018. It turns out that the Godot engine has gained massive
relevance in 2020 and now seems to be one of the leading players in the indie
game industry. The exact causes are difficult to determine. However, this paper
provides many clues for further research in this area.
",['\nJulian Holfeld\n'],"9 pages, 2 figures, 3 tables v2.0.1 changes: * replaced UE AAA source
  * better visibility for graphs",,http://arxiv.org/abs/2401.01909v2,cs.OH,['cs.OH'],,,[]
"DanceMeld: Unraveling Dance Phrases with Hierarchical Latent Codes for
  Music-to-Dance Synthesis",http://arxiv.org/abs/2401.10242v1,2023-11-30T12:36:21Z,2023-11-30T12:36:21Z,"  In the realm of 3D digital human applications, music-to-dance presents a
challenging task. Given the one-to-many relationship between music and dance,
previous methods have been limited in their approach, relying solely on
matching and generating corresponding dance movements based on music rhythm. In
the professional field of choreography, a dance phrase consists of several
dance poses and dance movements. Dance poses composed of a series of basic
meaningful body postures, while dance movements can reflect dynamic changes
such as the rhythm, melody, and style of dance. Taking inspiration from these
concepts, we introduce an innovative dance generation pipeline called
DanceMeld, which comprising two stages, i.e., the dance decouple stage and the
dance generation stage. In the decouple stage, a hierarchical VQ-VAE is used to
disentangle dance poses and dance movements in different feature space levels,
where the bottom code represents dance poses, and the top code represents dance
movements. In the generation stage, we utilize a diffusion model as a prior to
model the distribution and generate latent codes conditioned on music features.
We have experimentally demonstrated the representational capabilities of top
code and bottom code, enabling the explicit decoupling expression of dance
poses and dance movements. This disentanglement not only provides control over
motion details, styles, and rhythm but also facilitates applications such as
dance style transfer and dance unit editing. Our approach has undergone
qualitative and quantitative experiments on the AIST++ dataset, demonstrating
its superiority over other methods.
","['\nXin Gao\n', '\nLi Hu\n', '\nPeng Zhang\n', '\nBang Zhang\n', '\nLiefeng Bo\n']","10 pages, 8 figures",,http://arxiv.org/abs/2401.10242v1,cs.OH,"['cs.OH', 'cs.GR', 'cs.HC', 'cs.SD', 'eess.AS']",,,[]
Rock Climbing Route Generation and Grading as Computational Creativity,http://arxiv.org/abs/2311.02211v1,2023-11-03T19:50:31Z,2023-11-03T19:50:31Z,"  In this paper, we bridge work in rock climbing route generation and grading
into the computational creativity community. We provide the necessary
background to situate that literature and demonstrate the domain's intellectual
merit in the computational creativity community. We provide a guiding set of
desiderata for future work in this area. We propose an approach to
computational route grading. Finally, we identify important gaps in the
literature and consider how they may be filled. This paper thus also serves as
a pilot study, planting a flag for our ongoing research in this domain.
",['\nJesse Roberts\n'],,,http://arxiv.org/abs/2311.02211v1,cs.OH,"['cs.OH', 'cs.AI']",,,[]
"""Just a little bit on the outside for the whole time"": Social belonging
  confidence and the persistence of Machine Learning and Artificial
  Intelligence students",http://arxiv.org/abs/2311.10745v1,2023-10-30T19:59:38Z,2023-10-30T19:59:38Z,"  The growing field of machine learning (ML) and artificial intelligence (AI)
presents a unique and unexplored case within persistence research, meaning it
is unclear how past findings from engineering will apply to this developing
field. We conduct an exploratory study to gain an initial understanding of
persistence in this field and identify fruitful directions for future work. One
factor that has been shown to predict persistence in engineering is belonging;
we study belonging through the lens of confidence, and discuss how attention to
social belonging confidence may help to increase diversity in the profession.
In this research paper, we conduct a small set of interviews with students in
ML/AI courses. Thematic analysis of these interviews revealed initial
differences in how students see a career in ML/AI, which diverge based on
interest and programming confidence. We identified how exposure and initiation,
the interpretation of ML and AI field boundaries, and beliefs of the skills
required to succeed might influence students' intentions to persist. We discuss
differences in how students describe being motivated by social belonging and
the importance of close mentorship. We motivate further persistence research in
ML/AI with particular focus on social belonging and close mentorship, the role
of intersectional identity, and introductory ML/AI courses.
","['\nKatherine Mao\n', '\nSharon Ferguson\n', '\nJames Magarian\n', '\nAlison Olechowski\n']","Published in the 2023 Annual Conference of the American Society for
  Engineering Education","2023 ASEE Annual Conference & Exposition, Baltimore , Maryland",http://arxiv.org/abs/2311.10745v1,cs.CY,"['cs.CY', 'cs.OH']",,,[]
"CTMaaS: An innovative platform for C-ITS-enabled dynamic Traffic and
  Fleet Management as a Service",http://arxiv.org/abs/2310.14052v1,2023-10-21T16:14:09Z,2023-10-21T16:14:09Z,"  Fleet management systems have been one of the most important research fields
in transportation science. Nowadays the enhancement of fleet management systems
with technologies such as the Cooperative Intelligent Transport System (CITS)
that allows fleets to communicate with their environment, with other vehicles
or with the road infrastructure, resulting in safer and more efficient road
travel. This paper aims to present the CTMaaS platform, a tool which integrates
CITS services and traffic management processes to manage vehicle fleets.
Starting with a literature review, the paper presents various fleet management
systems, that have been developed in the last years, and the most typical CITS
services. The next chapters present the CTMaaS platform, use cases, and
methodology.
","['\nAreti Kotsi\n', '\nVasileia Klimi\n', '\nEvangelos Mitsakis\n']",11 pages,,http://arxiv.org/abs/2310.14052v1,cs.OH,['cs.OH'],,,[]
"All you need is data: the added value of National Access Points as
  backbone European ITS data exchange infrastructures",http://arxiv.org/abs/2310.14054v1,2023-10-21T16:18:00Z,2023-10-21T16:18:00Z,"  Intelligent Transport Systems are crucial in the digital transformation of
transportation. The EC mandates the establishment of National Access Points
(NAPs) in each Member State, serving as common national interfaces for ITS data
exchange. While progress has been made in standardizing NAP data, integration
with operational ITS practices remain limited. This paper presents five NAP use
cases from the NAPCORE (National Access Point Coordination Organization for
Europe) CEF funded project. The first one outlines a National Virtual Traffic
Management Center offering real time visualized KPIs supporting motorway
traffic operations. The second focuses on NAP enabled Cooperative ITS and
dynamic traffic management services. Next use case involves a Pan European
interface, providing visualizations of data availability. The fourth use case
enhances the digitization of traffic management plans, among different TMC.
Finally, the fifth use case demonstrates a technical interface combining NAP
traffic data with meteorological information for KPIs on extreme weather
impacts on traffic.
","['\nChrysostomos Mylonas\n', '\nMaria Stavara\n', '\nEvangelos Mitsakis\n']",6 pages,,http://arxiv.org/abs/2310.14054v1,cs.OH,['cs.OH'],,,[]
"Large scale deployment of C-ITS: Impact assessment results of the
  C-Roads Greece pilots",http://arxiv.org/abs/2311.10734v1,2023-10-21T10:41:28Z,2023-10-21T10:41:28Z,"  This paper aims to provide insights related to the impact assessment and
evaluation results from the use of CITS services in the Greek pilot of the
CRoads Greece project, i.e., Attica Tollway and Egnatia Odos Tollway. The
impact assessment and evaluation of the CITS services includes aspects related
to user acceptance, real world pilot logs collected from the two pilots, and
simulation experiments that were conducted for the impact assessment of the
CITS services. The paper concludes with a roadmap and guidelines for the
extended deployment of CITS services in the Greek highway and urban road
networks.
","['\nAreti Kotsi\n', '\nEvangelos Mitsakis\n']",18 pages,,http://arxiv.org/abs/2311.10734v1,cs.OH,['cs.OH'],,,[]
"Solving the multiplication problem of a large language model system
  using a graph-based method",http://arxiv.org/abs/2310.13016v1,2023-10-18T08:02:00Z,2023-10-18T08:02:00Z,"  The generative pre-trained transformer (GPT)-based chatbot software ChatGPT
possesses excellent natural language processing capabilities but is inadequate
for solving arithmetic problems, especially multiplication. Its GPT structure
uses a computational graph for multiplication, which has limited accuracy
beyond simple multiplication operations. We developed a graph-based
multiplication algorithm that emulated human-like numerical operations by
incorporating a 10k operator, where k represents the maximum power to base 10
of the larger of two input numbers. Our proposed algorithm attained 100%
accuracy for 1,000,000 large number multiplication tasks, effectively solving
the multiplication challenge of GPT-based and other large language models. Our
work highlights the importance of blending simple human insights into the
design of artificial intelligence algorithms. Keywords: Graph-based
multiplication; ChatGPT; Multiplication problem
","['\nTurker Tuncer\n', '\nSengul Dogan\n', '\nMehmet Baygin\n', '\nPrabal Datta Barua\n', '\nAbdul Hafeez-Baig\n', '\nRu-San Tan\n', '\nSubrata Chakraborty\n', '\nU. Rajendra Acharya\n']","9 pages, 3 figures",,http://arxiv.org/abs/2310.13016v1,cs.OH,"['cs.OH', 'cs.AI']",,,[]
How do users design scientific workflows? The Case of Snakemake,http://arxiv.org/abs/2309.14097v1,2023-09-25T12:48:49Z,2023-09-25T12:48:49Z,"  Scientific workflows automate the analysis of large-scale scientific data,
fostering the reuse of data processing operators as well as the reproducibility
and traceability of analysis results. In exploratory research, however,
workflows are continuously adapted, utilizing a wide range of tools and
software libraries, to test scientific hypotheses. Script-based workflow
engines cater to the required flexibility through direct integration of
programming primitives but lack abstractions for interactive exploration of the
workflow design by a user during workflow execution. To derive requirements for
such interactive workflows, we conduct an empirical study on the use of
Snakemake, a popular Python-based workflow engine. Based on workflows collected
from 1602 GitHub repositories, we present insights on common structures of
Snakemake workflows, as well as the language features typically adopted in
their specification.
","['\nSebastian Pohl\n', '\nNourhan Elfaramawy\n', '\nKedi Cao\n', '\nBirte Kehr\n', '\nMatthias Weidlich\n']",,,http://arxiv.org/abs/2309.14097v1,cs.OH,['cs.OH'],,,[]
"Morphological Computing as Logic Underlying Cognition in Human, Animal,
  and Intelligent Machine",http://arxiv.org/abs/2309.13979v1,2023-09-25T09:31:25Z,2023-09-25T09:31:25Z,"  This work examines the interconnections between logic, epistemology, and
sciences within the Naturalist tradition. It presents a scheme that connects
logic, mathematics, physics, chemistry, biology, and cognition, emphasizing
scale-invariant, self-organizing dynamics across organizational tiers of
nature. The inherent logic of agency exists in natural processes at various
levels, under information exchanges. It applies to humans, animals, and
artifactual agents. The common human-centric, natural language-based logic is
an example of complex logic evolved by living organisms that already appears in
the simplest form at the level of basal cognition of unicellular organisms.
Thus, cognitive logic stems from the evolution of physical, chemical, and
biological logic. In a computing nature framework with a self-organizing
agency, innovative computational frameworks grounded in
morphological/physical/natural computation can be used to explain the genesis
of human-centered logic through the steps of naturalized logical processes at
lower levels of organization. The Extended Evolutionary Synthesis of living
agents is essential for understanding the emergence of human-level logic and
the relationship between logic and information processing/computational
epistemology. We conclude that more research is needed to elucidate the details
of the mechanisms linking natural phenomena with the logic of agency in nature.
",['\nGordana Dodig-Crnkovic\n'],"20 pages, no figures",,http://arxiv.org/abs/2309.13979v1,cs.OH,"['cs.OH', 'cs.AI', '03A10']",,,[]
Towards an Interoperability Roadmap for the Energy Transition,http://arxiv.org/abs/2309.08284v1,2023-09-15T09:55:31Z,2023-09-15T09:55:31Z,"  Smart grid interoperability is the means to achieve the twin green and
digital transition but re-mains heterogeneous and fragmented to date. This work
presents the first ideas and corner-stones of an Interoperability Roadmap for
the Energy Transition that is being developed by the Horizon Europe int:net
project. This roadmap builds on four cornerstones that address open
interoperability issues. These are a knowledge base to address the lack of
convergence among existing initiatives, a maturity model and a network of
testing and certification facilities to ad-dress the lack of practical tools
for the industry, and a governance process to address the gap between
standards-related approaches of Standards Development Organisations and
Research and Innovation projects. A community of practice will be set up to
ensure the continuity of the ongoing activities related to smart grid
interoperability. To outlive the duration of the int:net project, the aim is to
formalise the community of practice as a legal entity.
","['\nValerie Reif\n', '\nThomas I. Strasser\n', '\nJoseba Jimeno\n', '\nMarjolaine Farre\n', '\nOliver Genest\n', '\nAmélie Gyrard\n', '\nMark McGranaghan\n', '\nGianluca Lipari\n', '\nJohann Schütz\n', '\nMathias Uslar\n', '\nSebastian Vogel\n', '\nArsim Bytyqi\n', '\nRita Dornmair\n', '\nAndreas Corusa\n', '\nGaurav Roy\n', '\nFerdinanda Ponci\n', '\nAlberto Dognini\n', '\nAntonello Monti\n']","12. (Hybrid) Symposium Communications for Energy Systems (ComForEn
  2023)",,http://arxiv.org/abs/2309.08284v1,cs.OH,['cs.OH'],,,[]
"Proceedings of the XII International Workshop on Locational Analysis and
  Related Problems",http://arxiv.org/abs/2309.08337v2,2023-09-15T11:45:54Z,2023-10-05T05:24:58Z,"  The International Workshop on Locational Analysis and Related Problems will
take place during September 7-8, 2023 in Edinburgh (United Kingdom). It is
organized by the Spanish Location Network and the Location Group GELOCA from
the Spanish Society of Statistics and Operations Research (SEIO). The Spanish
Location Network is a group of more than 140 researchers from several Spanish
universities organized into 7 thematic groups. The Network has been funded by
the Spanish Government since 2003. The current project is RED2022-134149-T. One
of the main activities of the Network is a yearly meeting aimed at promoting
the communication among its members and between them and other researchers, and
to contribute to the development of the location field and related problems. As
a proof of the internationalization of this research group, this will be the
first time that the meeting is held out of Spain. The topics of interest are
location analysis and related problems. This includes location models,
networks, transportation, logistics, exact and heuristic solution methods, and
computational geometry, among others.
","['\nMarta Baldomero-Naranjo\n', '\nVíctor Blanco\n', '\nSergio García\n', '\nRicardo Gázquez\n', '\nJörg Kalcsics\n', '\nLuisa I. Martínez-Merino\n', '\nJuan M. Muñoz-Ocaña\n', '\nFrancisco Temprano\n', '\nAlberto Torrejón\n']","The proceedings book of the previous editions can be found at
  arXiv:2002.08287 arXiv:2002.08293 arXiv:2002.08300 arXiv:2002.01702
  arXiv:2202.13878",,http://arxiv.org/abs/2309.08337v2,cs.OH,['cs.OH'],,,[]
"Inferring Power Grid Information with Power Line Communications: Review
  and Insights",http://arxiv.org/abs/2308.10598v1,2023-08-21T09:56:37Z,2023-08-21T09:56:37Z,"  High-frequency signals were widely studied in the last decade to identify
grid and channel conditions in PLNs. PLMs operating on the grid's physical
layer are capable of transmitting such signals to infer information about the
grid. Hence, PLC is a suitable communication technology for SG applications,
especially suited for grid monitoring and surveillance. In this paper, we
provide several contributions: 1) a classification of PLC-based applications;
2) a taxonomy of the related methodologies; 3) a review of the literature in
the area of PLC Grid Information Inference (GII); and, insights that can be
leveraged to further advance the field. We found research contributions
addressing PLMs for three main PLC-GII applications: topology inference,
anomaly detection, and physical layer key generation. In addition, various
PLC-GII measurement, processing, and analysis approaches were found to provide
distinctive features in measurement resolution, computation complexity, and
analysis accuracy. We utilize the outcome of our review to shed light on the
current limitations of the research contributions and suggest future research
directions in this field.
","['\nAbdulah Jarouf\n', '\nJavier Hernandez Fernandez\n', '\nAymen Omri\n', '\nRoberto Di Pietro\n']",IEEE Communication Surveys and Tutorials Journal,,http://arxiv.org/abs/2308.10598v1,eess.SP,"['eess.SP', 'cs.OH']",,,[]
Wordification: A New Way of Teaching English Spelling Patterns,http://arxiv.org/abs/2309.12981v2,2023-08-29T14:14:01Z,2023-11-10T10:49:12Z,"  Literacy, or the ability to read and write, is a crucial indicator of success
in life and greater society. It is estimated that 85% of people in juvenile
delinquent systems cannot adequately read or write, that more than half of
those with substance abuse issues have complications in reading or writing and
that two-thirds of those who do not complete high school lack proper literacy
skills. Furthermore, young children who do not possess reading skills matching
grade level by the fourth grade are approximately 80% likely to not catch up at
all. Many may believe that in a developed country such as the United States,
literacy fails to be an issue; however, this is a dangerous misunderstanding.
Globally an estimated 1.19 trillion dollars are lost every year due to issues
in literacy; in the USA, the loss is an estimated 300 billion. To put it in
more shocking terms, one in five American adults still fail to comprehend basic
sentences. Making matters worse, the only tools available now to correct a lack
of reading and writing ability are found in expensive tutoring or other
programs that oftentimes fail to be able to reach the required audience. In
this paper, our team puts forward a new way of teaching English spelling and
word recognitions to grade school students in the United States: Wordification.
Wordification is a web application designed to teach English literacy using
principles of linguistics applied to the orthographic and phonological
properties of words in a manner not fully utilized previously in any
computer-based teaching application.
","['\nLexington Whalen\n', '\nNathan Bickel\n', '\nShash Comandur\n', '\nDalton Craven\n', '\nStanley Dubinsky\n', '\nHomayoun Valafar\n']","8 pages, 7 figures, IEEE International Conference on Frontiers in
  Education",,http://arxiv.org/abs/2309.12981v2,cs.OH,"['cs.OH', 'cs.CL', 'K.3']",,,[]
"An Efficient Early-breaking Estimation and Tree-splitting Missing RFID
  Tag Identification Protocol",http://arxiv.org/abs/2308.09484v1,2023-08-16T13:08:16Z,2023-08-16T13:08:16Z,"  Recent statistics have demonstrated that missing items have become the main
cause of loss for retailers in inventory management. To quickly identify
missing tags, traditional protocols adopt Aloha-based strategies which take a
long time, especially when the number of tags is large. Among them, few works
considered the effect of unexpected unknown tags on the missing tag
identification process. With the presence of unknown tags, some missing tags
may be falsely identified as present. Thus, the system's reliability is hardly
guaranteed. In this work, we propose an efficient early-breaking estimation and
tree-splitting-based missing tag identification (ETMTI) protocol for
large-scale RFID systems. In ETMTI, a new early-breaking estimation and
deactivation method is developed to effectively estimate the number of unknown
tags and deactivate them within a short time. Next, a new tree-splitting-based
missing tag identification method is proposed to quickly identify missing tags
with a B-ary splitting tree. Besides, a bit-tracking response strategy is
designed to further reduce the time cost. The optimal parameters, time cost,
and false negative rate of ETMTI are analyzed theoretically. Simulation results
are presented to demonstrate that the proposed ETMTI protocol takes a smaller
time and has a lower false negative rate than the best-performing benchmarks.
","['\nLijuan Zhang\n', '\nMingqiu Fan\n', '\nChunni Yu\n', '\nLei Lei\n']",,,http://arxiv.org/abs/2308.09484v1,cs.OH,['cs.OH'],,,[]
"IoT Based Smart Attendance System Using Rfid: A Systematic Literature
  Review",http://arxiv.org/abs/2308.02591v1,2023-08-03T19:03:09Z,2023-08-03T19:03:09Z,"  The use of Radio Frequency Identification (RFID) technology is ubiquitous in
a number of businesses and sectors, including retail sales, smart cities,
agriculture, and transportation. Additionally, educational institutions have
started using RFID to track student attendance, combining this technology with
Google Sheets and the Internet of Things (IoT) to build a real-time attendance
tracking system. For a thorough examination of the creation of a student
attendance system, this paper includes a systematic literature evaluation of 21
major research published on IoT based attendance systems employing RFID. This
RFID-based attendance system enables automation, eliminating several problems
connected with the manual process, such as time wasting, proxies, and the
possibility of losing the attendance sheet, in contrast to the traditional
attendance system, which depends on manual signatures. By creating a system
that automatically registers students' attendance by merely flashing their
student cards at the RFID reader, all the aforementioned difficulties may be
successfully addressed. This automated method guarantees attendance monitoring
accuracy and dependability while also saving time. This paper's conclusion
highlights the significant advantages of implementing an IoT-based attendance
system based on RFID technology. The suggested solution provides a trustworthy,
efficient, and secure alternative to manual attendance techniques, successfully
addressing their shortcomings. This paper offers helpful insights for
institutions looking to create a cutting-edge attendance system that increases
student involvement and academic achievement by looking at guiding principles,
best practices, and the successful resolution of difficulties.
","['\nKashif Ishaq\n', '\nSamra Bibi\n']",26 Pages,,http://arxiv.org/abs/2308.02591v1,cs.OH,['cs.OH'],,,[]
"Impacts of Business Architecture in the Context of Digital
  Transformation: An Empirical Study Using PLS-SEM Approach",http://arxiv.org/abs/2307.11895v1,2023-07-21T20:42:53Z,2023-07-21T20:42:53Z,"  Despite the critical importance of Digital Transformation, up to 95% of
initiatives fail to deliver expected business benefits. This paper explores the
role of Business Architecture practices in enhancing digital transformation
success. Using an adapted Balanced Scorecard approach and a Structural Equation
Model (SEM), we analysed survey responses from 129 industry practitioners using
a Partial Least Squares (PLS) approach. Our findings indicate that effective
business architecture practices significantly improve business alignment,
efficiency, service delivery, and strategic outcomes, leading to successful
digital transformation. The study also validates factors proposed by AL-Malaise
AL-Ghamdi (2017) in the context of digital transformation. The paper presents
an adapted conceptual model addressing discriminant validity issues in previous
models and benefiting from the robustness of the Balanced Scorecard approach.
The study concludes by highlighting the essential role of business architecture
in driving digital transformation success.
","[""\nDennis O'Higgins\n""]",13 pages,,http://dx.doi.org/10.32996/jbms.2023.5.4.7,cs.OH,['cs.OH'],10.32996/jbms.2023.5.4.7,,[]
"Analytical Solution of Poisson's Equation with Application to VLSI
  Global Placement",http://arxiv.org/abs/2307.12041v1,2023-07-22T10:20:53Z,2023-07-22T10:20:53Z,"  Poisson's equation has been used in VLSI global placement for describing the
potential field caused by a given charge density distribution. Unlike previous
global placement methods that solve Poisson's equation numerically, in this
paper, we provide an analytical solution of the equation to calculate the
potential energy of an electrostatic system. The analytical solution is derived
based on the separation of variables method and an exact density function to
model the block distribution in the placement region, which is an infinite
series and converges absolutely. Using the analytical solution, we give a fast
computation scheme of Poisson's equation and develop an effective and efficient
global placement algorithm called Pplace. Experimental results show that our
Pplace achieves smaller placement wirelength than ePlace and NTUplace3. With
the pervasive applications of Poisson's equation in scientific fields, in
particular, our effective, efficient, and robust computation scheme for its
analytical solution can provide substantial impacts on these fields.
","['\nWenxing Zhu\n', '\nZhipeng Huang\n', '\nJianli Chen\n', '\nYao-Wen Chang\n']",,"2018 IEEE/ACM International Conference on Computer-Aided Design
  (ICCAD)",http://arxiv.org/abs/2307.12041v1,cs.OH,['cs.OH'],,,[]
"Development of an Autonomous Reverse Engineering Capability for
  Controller Area Network Messages to Support Autonomous Control Retrofits",http://arxiv.org/abs/2307.11781v1,2023-07-20T08:00:24Z,2023-07-20T08:00:24Z,"  As the autonomous vehicle industry continues to grow, various companies are
exploring the use of aftermarket kits to retrofit existing vehicles with
semi-autonomous capabilities. However, differences in implementation of the
controller area network (CAN) used by each vehicle manufacturer poses a
significant challenge to achieving large-scale implementation of retrofits. To
address this challenge, this research proposes a method for reverse engineering
the CAN channels associated with a vehicle's accelerator and brake pedals,
without any prior knowledge of the vehicle. By simultaneously recording
inertial measurement unit (IMU) and CAN data during vehicle operation, the
proposed algorithms can identify the CAN channels that correspond to each
control. During testing of six vehicles from three manufacturers, the proposed
method was shown to successfully identify the CAN channels for the accelerator
pedal and brake pedal for each vehicle tested. These promising results
demonstrate the potential for using this approach for developing aftermarket
autonomous vehicle kits - potentially with additional research to facilitate
real-time use. Notably, the proposed system has the potential to maintain its
effectiveness despite changes in vehicle CAN standards, and it could
potentially be adapted to function with any vehicle communications medium.
","['\nKevin Setterstrom\n', '\nJeremy Straub\n']",,,http://arxiv.org/abs/2307.11781v1,cs.OH,"['cs.OH', 'cs.RO', 'cs.SY', 'eess.SY']",,,[]
"The Impact of Process Complexity on Process Performance: A Study using
  Event Log Data",http://arxiv.org/abs/2307.06106v1,2023-07-11T15:11:58Z,2023-07-11T15:11:58Z,"  Complexity is an important characteristic of any business process. The key
assumption of much research in Business Process Management is that process
complexity has a negative impact on process performance. So far, behavioral
studies have measured complexity based on the perception of process
stakeholders. The aim of this study is to investigate if such a connection can
be supported based on the analysis of event log data. To do so, we employ a set
of 38 metrics that capture different dimensions of process complexity. We use
these metrics to build various regression models that explain process
performance in terms of throughput time. We find that process complexity as
captured in event logs explains the throughput time of process executions to a
considerable extent, with the respective R-squared reaching up to 0.96. Our
study offers implications for empirical research on process performance and can
serve as a toolbox for practitioners.
","['\nMaxim Vidgof\n', '\nBastian Wurm\n', '\nJan Mendling\n']",,,http://arxiv.org/abs/2307.06106v1,cs.OH,['cs.OH'],,,[]
"Towards The Ultimate Brain: Exploring Scientific Discovery with ChatGPT
  AI",http://arxiv.org/abs/2308.12400v1,2023-07-08T09:59:22Z,2023-07-08T09:59:22Z,"  This paper presents a novel approach to scientific discovery using an
artificial intelligence (AI) environment known as ChatGPT, developed by OpenAI.
This is the first paper entirely generated with outputs from ChatGPT. We
demonstrate how ChatGPT can be instructed through a gamification environment to
define and benchmark hypothetical physical theories. Through this environment,
ChatGPT successfully simulates the creation of a new improved model, called
GPT$^4$, which combines the concepts of GPT in AI (generative pretrained
transformer) and GPT in physics (generalized probabilistic theory). We show
that GPT$^4$ can use its built-in mathematical and statistical capabilities to
simulate and analyze physical laws and phenomena. As a demonstration of its
language capabilities, GPT$^4$ also generates a limerick about itself. Overall,
our results demonstrate the promising potential for human-AI collaboration in
scientific discovery, as well as the importance of designing systems that
effectively integrate AI's capabilities with human intelligence.
",['\nGerardo Adesso\n'],26 pages. To appear in AI Magazine,"AI Magazine 44, 328 (2023)",http://dx.doi.org/10.1002/aaai.12113,cs.OH,['cs.OH'],10.1002/aaai.12113,,[]
"Cyber Framework for Steering and Measurements Collection Over
  Instrument-Computing Ecosystems",http://arxiv.org/abs/2307.06883v1,2023-07-12T15:38:47Z,2023-07-12T15:38:47Z,"  We propose a framework to develop cyber solutions to support the remote
steering of science instruments and measurements collection over
instrument-computing ecosystems. It is based on provisioning separate data and
control connections at the network level, and developing software modules
consisting of Python wrappers for instrument commands and Pyro server-client
codes that make them available across the ecosystem network. We demonstrate
automated measurement transfers and remote steering operations in a microscopy
use case for materials research over an ecosystem of Nion microscopes and
computing platforms connected over site networks. The proposed framework is
currently under further refinement and being adopted to science workflows with
automated remote experiments steering for autonomous chemistry laboratories and
smart energy grid simulations.
","['\nAnees Al-Najjar\n', '\nNageswara S. V. Rao\n', '\nRamanan Sankaran\n', '\nHelia Zandi\n', '\nDebangshu Mukherjee\n', '\nMaxim Ziatdinov\n', '\nCraig Bridges\n']",Paper accepted for presentation at IEEE SMARTCOMP 2023,,http://arxiv.org/abs/2307.06883v1,cs.OH,"['cs.OH', 'physics.ins-det']",,,[]
"Towards a unified language in experimental designs propagated by a
  software framework",http://arxiv.org/abs/2307.11593v2,2023-07-11T13:47:58Z,2023-07-24T19:57:47Z,"  Experiments require human decisions in the design process, which in turn are
reformulated and summarized as inputs into a system (computational or
otherwise) to generate the experimental design. I leverage this system to
promote a language of experimental designs by proposing a novel computational
framework, called ""the grammar of experimental designs"", to specify
experimental designs based on an object-oriented programming system that
declaratively encapsulates the experimental structure. The framework aims to
engage human cognition by building experimental designs with modular functions
that modify a targeted singular element of the experimental design object. The
syntax and semantics of the framework are built upon consideration from
multiple perspectives. While the core framework is language-agnostic, the
framework is implemented in the `edibble` R-package. A range of examples is
shown to demonstrate the utility of the framework.
",['\nEmi Tanaka\n'],,,http://arxiv.org/abs/2307.11593v2,cs.OH,"['cs.OH', 'q-bio.QM', 'stat.ME']",,,[]
Compression Performance Analysis of Different File Formats,http://arxiv.org/abs/2308.12275v1,2023-07-08T02:36:55Z,2023-07-08T02:36:55Z,"  In data storage and transmission, file compression is a common technique for
reducing the volume of data, reducing data storage space and transmission time
and bandwidth. However, there are significant differences in the compression
performance of different types of file formats, and the benefits vary. In this
paper, 22 file formats with approximately 178GB of data were collected and the
Zlib algorithm was used for compression experiments to compare performance in
order to investigate the compression gains of different file types. The
experimental results show that some file types are poorly compressed, with
almost constant file size and long compression time, resulting in lower gains;
some other file types are significantly reduced in file size and compression
time after compression, which can effectively reduce the data volume. Based on
the above experimental results, this paper will then selectively reduce the
data volume by compression in data storage and transmission for the file types
in order to obtain the maximum compression yield.
","['\nHan Yang\n', '\nGuangjun Qin\n', '\nYongqing Hu\n']",,,http://arxiv.org/abs/2308.12275v1,cs.OH,"['cs.OH', 'cs.IT', 'math.IT']",,,[]
"Enjeux de communication dans la multirepr{é}sentation cartographique
  reproductible",http://arxiv.org/abs/2306.10862v1,2023-06-19T11:26:08Z,2023-06-19T11:26:08Z,"  This chapter deepens cartographic communication through a cartographic
multirepresentation exercise. Using a single dataset on World population data,
the chapter presents a series of 13 different maps to illustrate how mapping is
primarily a matter of choices and methods.
","['\nNicolas Lambert\nRIATE, CNRS\n', '\nTimothée Giraud\nRIATE, CNRS\n', '\nRonan Ysebaert\nRIATE, UPCité\n']",in French language,"Communication cartographique, ISTE Group, pp.73-102, 2022",http://dx.doi.org/10.51926/ISTE.9091.ch3,cs.OH,['cs.OH'],10.51926/ISTE.9091.ch3,,"['RIATE, CNRS', 'RIATE, CNRS', 'RIATE, UPCité']"
Towards Mobility Data Science (Vision Paper),http://arxiv.org/abs/2307.05717v4,2023-06-21T17:17:18Z,2024-03-07T16:20:45Z,"  Mobility data captures the locations of moving objects such as humans,
animals, and cars. With the availability of GPS-equipped mobile devices and
other inexpensive location-tracking technologies, mobility data is collected
ubiquitously. In recent years, the use of mobility data has demonstrated
significant impact in various domains including traffic management, urban
planning, and health sciences. In this paper, we present the emerging domain of
mobility data science. Towards a unified approach to mobility data science, we
envision a pipeline having the following components: mobility data collection,
cleaning, analysis, management, and privacy. For each of these components, we
explain how mobility data science differs from general data science, we survey
the current state of the art and describe open challenges for the research
community in the coming years.
","['\nMohamed Mokbel\nUniversity of Minnesota, Minneapolis, USA\n', '\nMahmoud Sakr\nUniversité Libre, Brussels, Belgium\n', '\nLi Xiong\nEmory University, Atlanta, USA\n', '\nAndreas Züfle\nEmory University, Atlanta, USA\n', '\nJussara Almeida\nFederal University of Minas Gerais, Belo Horizonte, Brazil\n', '\nTaylor Anderson\nGeorge Mason University, Fairfax, USA\n', '\nWalid Aref\nPurdue University, West Lafayette, USA\n', '\nGennady Andrienko\nFraunhofer IAIS, St. Augustin, Germany\n', '\nNatalia Andrienko\nFraunhofer IAIS, St. Augustin, Germany\n', '\nYang Cao\nKyoto University, Kyoto, Japan\n', '\nSanjay Chawla\nQatar Computing Research Institute, Doha, Qatar\n', '\nReynold Cheng\nUniversity of Hong Kong, Hong Kong, China\n', '\nPanos Chrysanthis\nUniversity of Pittsburgh, Pennsylvania, USA\n', '\nXiqi Fei\nGeorge Mason University, Fairfax, USA\n', '\nGabriel Ghinita\nUniversity of Massachusetts at Boston, Boston, USA\n', '\nAnita Graser\nAustrian Institute of Technology, Vienna, Austria\n', '\nDimitrios Gunopulos\nUniversity of Athens, Greece\n', '\nChristian Jensen\nAalborg University, Denmark\n', '\nJoon-Seok Kim\nOak Ridge National Laboratory, USA\n', '\nKyoung-Sook Kim\nAIST, Tokyo Waterfront, Japan\n', '\nPeer Kröger\nUniversity of Kiel, Germany\n', '\nJohn Krumm\nUniversity of Southern California, Log Angeles, USA\n', '\nJohannes Lauer\nHERE Technologies, Germany\n', '\nAmr Magdy\nUniversity of California, Riverside, USA\n', '\nMario Nascimento\nNortheastern University, Boston, USA\n', '\nSiva Ravada\nOracle Corp., Nashua, USA\n', '\nMatthias Renz\nUniversity of Kiel, Germany\n', '\nDimitris Sacharidis\nUniversité Libre, Brussels, Belgium\n', '\nCyrus Shahabi\nUniversity of Southern California, Log Angeles, USA\n', '\nFlora Salim\nUniversity of New South Wales, Sydney, Australia\n', '\nMohamed Sarwat\nArizona State University, Tempe\n', '\nMaxime Schoemans\nUniversité Libre, Brussels, Belgium\n', '\nBettina Speckmann\nTU Eindhoven, Netherlands\n', '\nEgemen Tanin\nUniversity of Melbourne, Australia\n', '\nXu Teng\nEsri, California, USA\n', '\nYannis Theodoridis\nUniversity of Piraeus, Greece\n', '\nKristian Torp\nAalborg University, Denmark\n', '\nGoce Trajcevski\nIowa State University, Ames, USA\n', '\nMarc van Kreveld\nUtrecht University, Netherlands\n', '\nCarola Wenk\nTulane University, New Orleans, USA\n', '\nMartin Werner\nTechnical University of Munich, Munich, Germany\n', '\nRaymond Wong\nHong Kong University of Science and Technology, Hong Kong, China\n', '\nSong Wu\nUniversité Libre, Brussels, Belgium\n', '\nJianqiu Xu\nNanjing University of Aeronautics and Astronautics, Nanjing, China\n', '\nMoustafa Youssef\nAUC and Alexandria University, Egypt\n', '\nDemetris Zeinalipour\nUniversity of Cyprus, Nicosia, Cyprus\n', '\nMengxuan Zhang\nIowa State University, Ames, USA\n', '\nEsteban Zimányi\nUniversité Libre, Brussels, Belgium\n']","Updated to reflect the major revision for ACM Transactions on Spatial
  Algorithms and Systems (TSAS). This version reflects the final version
  accepted by ACM TSAS",,http://arxiv.org/abs/2307.05717v4,cs.OH,['cs.OH'],,,"['University of Minnesota, Minneapolis, USA', 'Université Libre, Brussels, Belgium', 'Emory University, Atlanta, USA', 'Emory University, Atlanta, USA', 'Federal University of Minas Gerais, Belo Horizonte, Brazil', 'George Mason University, Fairfax, USA', 'Purdue University, West Lafayette, USA', 'Fraunhofer IAIS, St. Augustin, Germany', 'Fraunhofer IAIS, St. Augustin, Germany', 'Kyoto University, Kyoto, Japan', 'Qatar Computing Research Institute, Doha, Qatar', 'University of Hong Kong, Hong Kong, China', 'University of Pittsburgh, Pennsylvania, USA', 'George Mason University, Fairfax, USA', 'University of Massachusetts at Boston, Boston, USA', 'Austrian Institute of Technology, Vienna, Austria', 'University of Athens, Greece', 'Aalborg University, Denmark', 'Oak Ridge National Laboratory, USA', 'AIST, Tokyo Waterfront, Japan', 'University of Kiel, Germany', 'University of Southern California, Log Angeles, USA', 'HERE Technologies, Germany', 'University of California, Riverside, USA', 'Northeastern University, Boston, USA', 'Oracle Corp., Nashua, USA', 'University of Kiel, Germany', 'Université Libre, Brussels, Belgium', 'University of Southern California, Log Angeles, USA', 'University of New South Wales, Sydney, Australia', 'Arizona State University, Tempe', 'Université Libre, Brussels, Belgium', 'TU Eindhoven, Netherlands', 'University of Melbourne, Australia', 'Esri, California, USA', 'University of Piraeus, Greece', 'Aalborg University, Denmark', 'Iowa State University, Ames, USA', 'Utrecht University, Netherlands', 'Tulane University, New Orleans, USA', 'Technical University of Munich, Munich, Germany', 'Hong Kong University of Science and Technology, Hong Kong, China', 'Université Libre, Brussels, Belgium', 'Nanjing University of Aeronautics and Astronautics, Nanjing, China', 'AUC and Alexandria University, Egypt', 'University of Cyprus, Nicosia, Cyprus', 'Iowa State University, Ames, USA', 'Université Libre, Brussels, Belgium']"
Robo Sapiens,http://arxiv.org/abs/2310.08323v1,2023-06-05T08:39:42Z,2023-06-05T08:39:42Z,"  This paper proposes a new method of natural language acquisition for robots
that does not require the conversion of speech to text. Folks'Talks employs
voice2voice technology that enables a robot to understand the meaning of what
it is told and to have the ability to learn and understand new languages -
inclusive of accent, dialect, and physiological differences. To do this, sound
processing and computer vision are incorporated to give the robot a sense of
spatiotemporal causality. The ""language model"" we are proposing equips a robot
to imitate a natural speaker's conversational behavior by thinking contextually
and articulating its surroundings.
","['\nChaim Ash\n', '\nAmelia Hans\n']",,,http://arxiv.org/abs/2310.08323v1,cs.OH,['cs.OH'],,,[]
"An Interdisciplinary Survey on Origin-destination Flows Modeling: Theory
  and Techniques",http://arxiv.org/abs/2306.10048v3,2023-06-12T13:47:46Z,2023-07-18T13:43:08Z,"  Origin-destination~(OD) flow modeling is an extensively researched subject
across multiple disciplines, such as the investigation of travel demand in
transportation and spatial interaction modeling in geography. However,
researchers from different fields tend to employ their own unique research
paradigms and lack interdisciplinary communication, preventing the
cross-fertilization of knowledge and the development of novel solutions to
challenges. This article presents a systematic interdisciplinary survey that
comprehensively and holistically scrutinizes OD flows from utilizing
fundamental theory to studying the mechanism of population mobility and solving
practical problems with engineering techniques, such as computational models.
Specifically, regional economics, urban geography, and sociophysics are adept
at employing theoretical research methods to explore the underlying mechanisms
of OD flows. They have developed three influential theoretical models: the
gravity model, the intervening opportunities model, and the radiation model.
These models specifically focus on examining the fundamental influences of
distance, opportunities, and population on OD flows, respectively. In the
meantime, fields such as transportation, urban planning, and computer science
primarily focus on addressing four practical problems: OD prediction, OD
construction, OD estimation, and OD forecasting. Advanced computational models,
such as deep learning models, have gradually been introduced to address these
problems more effectively. Finally, based on the existing research, this survey
summarizes current challenges and outlines future directions for this topic.
Through this survey, we aim to break down the barriers between disciplines in
OD flow-related research, fostering interdisciplinary perspectives and modes of
thinking.
","['\nCan Rong\n', '\nJingtao Ding\n', '\nYong Li\n']","49 pages, 6 figures",,http://arxiv.org/abs/2306.10048v3,cs.OH,"['cs.OH', 'cs.CY']",,,[]
"Software Development Vehicles to enable extended and early co-design: a
  RISC-V and HPC case of study",http://arxiv.org/abs/2306.01797v1,2023-06-01T13:58:58Z,2023-06-01T13:58:58Z,"  Prototyping HPC systems with low-to-mid technology readiness level (TRL)
systems is critical for providing feedback to hardware designers, the system
software team (e.g., compiler developers), and early adopters from the
scientific community. The typical approach to hardware design and HPC system
prototyping often limits feedback or only allows it at a late stage. In this
paper, we present a set of tools for co-designing HPC systems, called software
development vehicles (SDV). We use an innovative RISC-V design as a
demonstrator, which includes a scalar CPU and a vector processing unit capable
of operating large vectors up to 16 kbits. We provide an incremental
methodology and early tangible evidence of the co-design process that provide
feedback to improve both architecture and system software at a very early stage
of system development.
","['\nFilippo Mantovani\n', '\nPablo Vizcaino\n', '\nFabio Banchelli\n', '\nMarta Garcia-Gasulla\n', '\nRoger Ferrer\n', '\nGiorgos Ieronymakis\n', '\nNikos Dimou\n', '\nVassilis Papaefstathiou\n', '\nJesus Labarta\n']","Presented at the ""First International workshop on RISC-V for HPC""
  co-located with ISC23 in Hamburg",,http://arxiv.org/abs/2306.01797v1,cs.OH,['cs.OH'],,,[]
"From Full-fledged ERP Systems Towards Process-centric Business Process
  Platforms",http://arxiv.org/abs/2306.02995v1,2023-06-02T16:43:27Z,2023-06-02T16:43:27Z,"  Enterprise Resource Planning (ERP) systems are critical to the success of
enterprises, facilitating business operations through standardized digital
processes. However, existing ERP systems are unsuitable for startups and small
and medium-sized enterprises that grow quickly and require adaptable solutions
with low barriers to entry. Drawing upon 15 explorative interviews with
industry experts, we examine the challenges of current ERP systems using the
task technology fit theory across companies of varying sizes. We describe high
entry barriers, high costs of implementing implicit processes, and insufficient
interoperability of already employed tools. We present a vision of a future
business process platform based on three enablers: Business processes as
first-class entities, semantic data and processes, and cloud-native elasticity
and high availability. We discuss how these enablers address current ERP
systems' challenges and how they may be used for research on the next
generation of business software for tomorrow's enterprises.
","['\nLukas Böhme\n', '\nTobias Wuttke\n', '\nRalf Teusner\n', '\nMichael Perscheid\n', '\nSebastian Baltes\n', '\nChristoph Matthies\n', '\nBenedict Bender\n']","10 pages, 2 figures, 29th Americas Conference on Information Systems
  (AMCIS 2023)",,http://arxiv.org/abs/2306.02995v1,cs.OH,['cs.OH'],,,[]
"Assessing Exoplanet Habitability through Data-driven Approaches: A
  Comprehensive Literature Review",http://arxiv.org/abs/2305.11204v1,2023-05-18T17:18:15Z,2023-05-18T17:18:15Z,"  The exploration and study of exoplanets remain at the frontier of
astronomical research, challenging scientists to continuously innovate and
refine methodologies to navigate the vast, complex data these celestial bodies
produce. This literature the review aims to illuminate the emerging trends and
advancements within this sphere, specifically focusing on the interplay between
exoplanet detection, classification, and visualization, and the the
increasingly pivotal role of machine learning and computational models. Our
journey through this realm of exploration commences with a comprehensive
analysis of fifteen meticulously selected, seminal papers in the field. These
papers, each representing a distinct facet of exoplanet research, collectively
offer a multi-dimensional perspective on the current state of the field. They
provide valuable insights into the innovative application of machine learning
techniques to overcome the challenges posed by the analysis and interpretation
of astronomical data. From the application of Support Vector Machines (SVM) to
Deep Learning models, the review encapsulates the broad spectrum of machine
learning approaches employed in exoplanet research. The review also seeks to
unravel the story woven by the data within these papers, detailing the triumphs
and tribulations of the field. It highlights the increasing reliance on diverse
datasets, such as Kepler and TESS, and the push for improved accuracy in
exoplanet detection and classification models. The narrative concludes with key
takeaways and insights, drawing together the threads of research to present a
cohesive picture of the direction in which the field is moving. This literature
review, therefore, serves not just as an academic exploration, but also as a
narrative of scientific discovery and innovation in the quest to understand our
cosmic neighborhood.
",['\nMithil Sai Jakka\n'],,,http://arxiv.org/abs/2305.11204v1,cs.OH,['cs.OH'],,,[]
"Exploring the use of Eye Tracking Technology to improve Website
  Usability",http://arxiv.org/abs/2305.11345v1,2023-05-18T23:39:08Z,2023-05-18T23:39:08Z,"  This study investigates the capability blessings of the use of eye-monitoring
technology to beautify the usability of web sites. With the upward thrust of
on-line interactions, website usability has turn out to be increasingly
important for making sure person pleasure and engagement. Eye-tracking
technology offers a non-invasive way to measure how users interact with web
sites with the aid of monitoring their eye actions and gaze styles. By studying
those statistics, internet site designers and builders can advantage insights
into how customers navigate, examine, and method data on their web sites. This
paper affords an overview of applicable literature on eye-monitoring and
website usability, as well as a precis of research which have explored the
usage of eye-tracking era to improve website design and overall performance.
The outcomes propose that eye-tracking era can offer valuable records for
enhancing internet site usability, inclusive of insights into consumer
attention, visible hierarchy, and consumer engagement. Further studies are
wanted to explore the full potential of eye-tracking generation and to develop
great practices for incorporating this technology into website design and
development techniques.
",['\nNiharika Veeravalli\n'],,,http://arxiv.org/abs/2305.11345v1,cs.OH,['cs.OH'],,,[]
"From Electronic Design Automation to Building Design Automation:
  Challenges and Opportunities",http://arxiv.org/abs/2305.06380v1,2023-05-10T18:00:04Z,2023-05-10T18:00:04Z,"  Design automation, which involves the use of software tools and technologies
to streamline the design process, has been widely adopted in the electronics
industry, resulting in significant advancements in product development and
manufacturing. However, building design, which involves the creation of complex
structures and systems, has traditionally lagged behind in leveraging design
automation technologies. Despite extensive research on design automation in the
building industry, its application in the current design of buildings is
limited. This paper aims to (1) compare the design processes between
electronics and building design, (2) highlight similarities and differences in
their approaches, and (3) examine challenges and opportunities associated with
bringing the concept of design automation from electronics to building design.
","['\nYu-Wen Lin\n', '\nTsz Ling Elaine Tang\n', '\nAlberto L. Sangiovanni-Vincentelli\n', '\nStefano Schiavon\n', '\nCostas J. Spanos\n']",,,http://arxiv.org/abs/2305.06380v1,cs.OH,['cs.OH'],,,[]
Toward Platform-based Building Design,http://arxiv.org/abs/2305.10949v1,2023-05-11T03:51:14Z,2023-05-11T03:51:14Z,"  The electronic design industry has undergone a significant transformation,
transitioning from traditional hand-drawn designs to modern automated design
processes. While Computer-Aided Design (CAD) tools emerged alongside the
electronic industry, the current building design process has little to no
automation. There is a need for a unified platform to address the complexity of
building design and provide a more systematic approach. Platform-based design
(PBD), originally developed in the electronic industry, enables efficient
design processes by promoting the reuse of hardware and software systems. It
also facilitates design space exploration while optimizing performance. This
paper proposes a modular approach that divides the building into various
disciplines and introduces a design flow using the PBD framework to streamline
the design process. We also present a case study that demonstrates the use of
the PBD framework in the Heating, Ventilation, and Air Conditioning (HVAC)
systems design.
","['\nYu-Wen Lin\n', '\nTsz Ling Elaine Tang\n', '\nStefano Schiavon\n', '\nCostas J. Spanos\n']","17 pages, 3 figures",,http://arxiv.org/abs/2305.10949v1,cs.OH,"['cs.OH', 'cs.AR']",,,[]
"Manuscript of a method for improving wear in intermittently computing
  file systems",http://arxiv.org/abs/2304.05399v1,2023-04-11T05:56:57Z,2023-04-11T05:56:57Z,"  For the first time, the repeated wear phenomenon of high-frequency power
failure on the data block area in intermittent computing file system is found.
A method to improve NVM wear in ICFS under high-frequency power failure
scenarios is proposed.
","['\nYeteng Liao\n', '\nHan wang\n']",,,http://arxiv.org/abs/2304.05399v1,cs.OH,['cs.OH'],,,[]
The Standard Problem,http://arxiv.org/abs/2304.09114v2,2023-04-17T10:32:15Z,2023-08-16T00:11:51Z,"  Objective: This paper proposes a framework to support the scientific research
of standards so that they can be better measured, evaluated, and designed.
Methods: Beginning with the notion of common models, the framework describes
the general standard problem - the seeming impossibility of creating a
singular, persistent and definitive standard which is not subject to change
over time in an open system. Results: The standard problem arises from
uncertainty driven by variations in operating context, standard quality,
differences in implementation, and drift over time. As a result, fitting work
using conformance services is needed to repair these gaps between a standard
and what is required for real-world use. To guide standards design and repair,
a framework for measuring performance in context is suggested, based on signal
detection theory and technomarkers. Based on the type of common model in
operation, different conformance strategies are identified: (a) Universal
conformance (all agents access the same standard); (b) Mediated conformance (an
interoperability layer supports heterogeneous agents) and (c) Localized
conformance (autonomous adaptive agents manage their own needs). Conformance
methods include incremental design, modular design, adaptors, and creating
interactive and adaptive agents. Discussion: Machine learning should have a
major role in adaptive fitting. Research to guide the choice and design of
conformance services may focus on the stability and homogeneity of shared
tasks, and whether common models are shared ahead of time or adjusted at task
time. Conclusion: This analysis conceptually decouples interoperability and
standardization. While standards facilitate interoperability, interoperability
is achievable without standardization.
",['\nEnrico Coiera\n'],"Keywords: information standard, interoperability, machine learning,
  technology evaluation 25 Pages Main text word Count: 5108 Abstract word
  count: 206 Tables: 1 Figures: 7 Boxes: 1. JAMIA Post-print version",,http://arxiv.org/abs/2304.09114v2,cs.OH,"['cs.OH', 'K.1']",,,[]
Choose Your Weapon: Survival Strategies for Depressed AI Academics,http://arxiv.org/abs/2304.06035v2,2023-03-31T17:33:10Z,2024-02-08T00:03:07Z,"  Are you an AI researcher at an academic institution? Are you anxious you are
not coping with the current pace of AI advancements? Do you feel you have no
(or very limited) access to the computational and human resources required for
an AI research breakthrough? You are not alone; we feel the same way. A growing
number of AI academics can no longer find the means and resources to compete at
a global scale. This is a somewhat recent phenomenon, but an accelerating one,
with private actors investing enormous compute resources into cutting edge AI
research. Here, we discuss what you can do to stay competitive while remaining
an academic. We also briefly discuss what universities and the private sector
could do improve the situation, if they are so inclined. This is not an
exhaustive list of strategies, and you may not agree with all of them, but it
serves to start a discussion.
","['\nJulian Togelius\n', '\nGeorgios N. Yannakakis\n']",,"Proceedings of the IEEE, 2024",http://arxiv.org/abs/2304.06035v2,cs.OH,"['cs.OH', 'cs.CY', 'cs.NE']",,,[]
Modelling Maritime SAR Effective Sweep Widths for Helicopters in VDM,http://arxiv.org/abs/2304.00983v1,2023-03-29T12:44:18Z,2023-03-29T12:44:18Z,"  Search and Rescue (SAR) is searching for and providing help to people in
danger. In the UK, SAR teams are typically charities with limited resources,
and SAR missions are time critical. Search managers need to objectively decide
which search assets (e.g. helicopter vs drone) would be better. A key metric in
the SAR community is effective sweep width (W), which provides a single measure
for a search asset's ability to detect a specific object in specific
environmental conditions. Tables of W for different search assets are provided
in various manuals, such as the International Aeronautical and Maritime SAR
(IAMSAR) Manual. However, these tables take years of expensive testing and
experience to produce, and no such tables exist for drones. This paper uses the
Vienna Development Method (VDM) to build an initial model of W for a known case
(helicopters at sea) with a view to predicting W tables for drones. The model
computes W for various search object sizes, helicopter altitude and visibility.
The results for the model are quite different from the published tables, which
shows that the abstraction level is not yet correct, however it produced useful
insights and directions for the next steps.
","['\nAlexander Sulaiman\n', '\nKen Pierce\n']",https://www.overturetool.org/workshops/21st-overture-workshop.html,,http://arxiv.org/abs/2304.00983v1,cs.OH,"['cs.OH', 'cs.RO']",,,[]
"CoCoMo: Computational Consciousness Modeling for Generative and Ethical
  AI",http://arxiv.org/abs/2304.02438v2,2023-03-17T17:13:46Z,2023-04-08T08:07:26Z,"  The CoCoMo model proposes a computational solution to the challenge of
incorporating ethical and emotional intelligence considerations into AI
systems, with the aim of creating AI agents that combine knowledge with
compassion. To achieve this goal, CoCoMo prioritizes fairness, beneficence,
non-maleficence, empathy, adaptability, transparency, and critical and
exploratory thinking abilities. The model employs consciousness modeling,
reinforcement learning, and prompt template formulation to support these
desired traits. By incorporating ethical and emotional intelligence
considerations, a generative AI model can potentially lead to improved
fairness, reduced toxicity, and increased reliability.
",['\nEdward Y. Chang\n'],"10 pages, 3 figures, 5 tables",,http://arxiv.org/abs/2304.02438v2,cs.OH,"['cs.OH', 'I.2.7']",,,[]
"Real-Time Adaptive Abstraction and Approximation Using Validity Frames
  -- an Experience Report",http://arxiv.org/abs/2303.07144v2,2023-03-09T14:42:52Z,2023-03-14T15:46:36Z,"  Designing a Cyber-Physical System (CPS), including modeling the control
components and services, is a challenging task. Using models and simulations
during run-time is crucial for successfully implementing advanced control and
prediction components. The complexity of designing an effective CPS system
increases due to real-time constraints. Generating accurate predictions and
making decisions using detailed models in various contexts is %can be
computationally demanding and complex to manage within the available
computational resources. Employing approximated models and switching to the
most suited model adaptively at run-time is an effective technique. But an
approximated model is most probable not valid in all the different contexts the
system will be in. This experience report uses the Validity Frame concept to
enable %this adaptation at run-time. In each environment, some influencing
factors are outside the model's control, but these properties influence the
model's behavior. By defining Validity Frames, based on specific contexts and
related models, we present a possible perspective to address the issue of
selecting the more appropriate model in various contexts. Furthermore, we
discuss the insights and lessons obtained and determine future challenges.
","['\nRaheleh Biglari\n', '\nJoachim Denil\n']",,,http://arxiv.org/abs/2303.07144v2,cs.OH,"['cs.OH', 'cs.SY', 'eess.SY']",,,[]
"Digital Twins for Trust Building in Autonomous Drones through Dynamic
  Safety Evaluation",http://arxiv.org/abs/2303.12805v1,2023-03-15T23:06:17Z,2023-03-15T23:06:17Z,"  The adoption process of innovative software-intensive technologies leverages
complex trust concerns in different forms and shapes. Perceived safety plays a
fundamental role in technology adoption, being especially crucial in the case
of those innovative software-driven technologies characterized by a high degree
of dynamism and unpredictability, like collaborating autonomous systems. These
systems need to synchronize their maneuvers in order to collaboratively engage
in reactions to unpredictable incoming hazardous situations. That is however
only possible in the presence of mutual trust.
  In this paper, we propose an approach for machine-to-machine dynamic trust
assessment for collaborating autonomous systems that supports trust-building
based on the concept of dynamic safety assurance within the collaborative
process among the software-intensive autonomous systems. In our approach, we
leverage the concept of digital twins which are abstract models fed with
real-time data used in the run-time dynamic exchange of information. The
information exchange is performed through the execution of specialized models
that embed the necessary safety properties. More particularly, we examine the
possible role of the Digital Twins in machine-to-machine trust building and
present their design in supporting dynamic trust assessment of autonomous
drones. Ultimately, we present a proof of concept of direct and indirect trust
assessment by employing the Digital Twin in a use case involving two autonomous
collaborating drones.
","['\nDanish Iqbal\n', '\nBarbora Buhnova\n', '\nEmilia Cioroaica\n']",Proceedings of ENASE 2023,,http://arxiv.org/abs/2303.12805v1,cs.OH,"['cs.OH', 'cs.RO', 'cs.SY', 'eess.SY']",,,[]
"EuroCrops: All you need to know about the Largest Harmonised Open Crop
  Dataset Across the European Union",http://arxiv.org/abs/2302.10202v1,2023-02-20T10:35:32Z,2023-02-20T10:35:32Z,"  EuroCrops contains geo-referenced polygons of agricultural croplands from 16
countries of the European Union (EU) as well as information on the respective
crop species grown there. These semantic annotations are derived from
self-declarations by farmers receiving subsidies under the Common Agriculture
Policy (CAP) of the European Commission (EC). Over the last 1.5 years, the
individual national crop datasets have been manually collected, the crop
classes have been translated into the English language and transferred into the
newly developed Hierarchical Crop and Agriculture Taxonomy (HCAT). EuroCrops is
publicly available under continuous improvement through an active user
community.
","['\nMaja Schneider\n', '\nTobias Schelte\n', '\nFelix Schmitz\n', '\nMarco Körner\n']","11 pages, 3 figures, for associated dataset, see
  https://github.com/maja601/EuroCrops and
  https://www.doi.org/10.5281/zenodo.6866846 , submitted to Scientific Data",,http://arxiv.org/abs/2302.10202v1,cs.OH,['cs.OH'],,,[]
"A Systematic Review on Human Modeling: Digging into Human Digital Twin
  Implementations",http://arxiv.org/abs/2302.03593v1,2023-02-04T13:29:33Z,2023-02-04T13:29:33Z,"  Human Digital Twins (HDTs) are digital replicas of humans that either mirror
a complete human body, some parts of it as can be organs, flows, cells, or even
human behaviors. An HDT is a human specific replica application inferred from
the digital twin (DT) manufacturing concept, defined as a technique that
creates digital replicas of physical systems or processes aimed at optimizing
their performance and supporting more accurate decision-making processes. The
main goal of this paper is to provide readers with a comprehensive overview of
current efforts in the HDT field, by browsing its basic concepts, differences
with DTs, existing developments, and the distinct areas of application. The
review methodology includes an exhaustive review of scientific literature,
patents, and industrial initiatives, as well as a discussion about ongoing and
foreseen HDT research activity, emphasizing its potential benefits and
limitations.
","['\nHeribert Pascual\n', '\nXavi Masip Bruin\n', '\nAlbert Alonso\n', '\nJudit Cerdà\n']",,,http://arxiv.org/abs/2302.03593v1,cs.OH,['cs.OH'],,,[]
"HERMES: qualification of High pErformance pRogrammable Microprocessor
  and dEvelopment of Software ecosystem",http://arxiv.org/abs/2302.06427v1,2023-02-09T09:20:45Z,2023-02-09T09:20:45Z,"  European efforts to boost competitiveness in the sector of space services
promote the research and development of advanced software and hardware
solutions. The EU-funded HERMES project contributes to the effort by qualifying
radiation-hardened, high-performance programmable microprocessors, and by
developing a software ecosystem that facilitates the deployment of complex
applications on such platforms. The main objectives of the project include
reaching a technology readiness level of 6 (i.e., validated and demonstrated in
relevant environment) for the rad-hard NG-ULTRA FPGA with its ceramic hermetic
package CGA 1752, developed within projects of the European Space Agency,
French National Centre for Space Studies and the European Union. An equally
important share of the project is dedicated to the development and validation
of tools that support multicore software programming and FPGA acceleration,
including Bambu for High-Level Synthesis and the XtratuM hypervisor with a
level one boot loader for virtualization.
","['\nNadia Ibellaatti\n', '\nEdouard Lepape\n', '\nAlp Kilic\n', '\nKaya Akyel\n', '\nKassem Chouayakh\n', '\nFabrizio Ferrandi\n', '\nClaudio Barone\n', '\nSerena Curzel\n', '\nMichele Fiorito\n', '\nGiovanni Gozzi\n', '\nMiguel Masmano\n', '\nAna Risquez Navarro\n', '\nManuel Muñoz\n', '\nVicente Nicolau Gallego\n', '\nPatricia Lopez Cueva\n', '\nJean-noel Letrillard\n', '\nFranck Wartel\n']",Accepted for publication at DATE 2023,,http://arxiv.org/abs/2302.06427v1,cs.OH,"['cs.OH', 'cs.AR', 'cs.SE']",,,[]
"A Survey of Process-Oriented Data Science and Analytics for supporting
  Business Process Management",http://arxiv.org/abs/2301.10398v1,2023-01-25T03:37:27Z,2023-01-25T03:37:27Z,"  Process analytics approaches allow organizations to support the practice of
Business Process Management and continuous improvement by leveraging all
process-related data to extract knowledge, improve process performance and
support decision-making across the organization. Process execution data once
collected will contain hidden insights and actionable knowledge that are of
considerable business value enabling firms to take a data-driven approach for
identifying performance bottlenecks, reducing costs, extracting insights and
optimizing the utilization of available resources. Understanding the properties
of 'current deployed process' (whose execution trace is often available in
these logs), is critical to understanding the variation across the process
instances, root-causes of inefficiencies and determining the areas for
investing improvement efforts. In this survey, we discuss various methods that
allow organizations to understand the behaviour of their processes, monitor
currently running process instances, predict the future behavior of those
instances and provide better support for operational decision-making across the
organization.
","['\nAsjad Khan\n', '\nAditya Ghose\n', '\nHoa Dam\n', '\nArsal Syed\n']",,,http://arxiv.org/abs/2301.10398v1,cs.OH,['cs.OH'],,,[]
A Novel IoT-Based System for Ten Pin Bowling,http://arxiv.org/abs/2301.10523v1,2023-01-25T11:04:31Z,2023-01-25T11:04:31Z,"  Bowling is a target sport that is popular among all age groups with
professionals and amateur players. Delivering an accurate and consistent
bowling throw into the lane requires the incorporation of motion techniques.
Consequently, this research presents a novel IoT-Cloud based system for
providing real-time monitoring and coaching services to bowling athletes. The
system includes two inertial measurement units (IMUs) sensors for capturing
motion data, a mobile application and a cloud server for processing the data.
First, the quality of each phase of a throw is assessed using a Dynamic Time
Wrapping (DTW) based algorithm. Second, an on device-level technique is
proposed to identify common bowling errors. Finally, an SVM classification
model is employed for assessing the skill level of bowler athletes. We
recruited nine right-handed bowlers to perform 50 throws wearing the two
sensors and using the proposed system. The results of our experiments suggest
that the proposed system can effectively and efficiently assess the quality of
the throw, detect common bowling errors and classify the skill level of the
bowler.
","['\nIlias Zosimadis\n', '\nIoannis Stamelos\n']","20 pages, 6 figures",,http://arxiv.org/abs/2301.10523v1,cs.OH,['cs.OH'],,,[]
Foundations and Scoping of Data Science,http://arxiv.org/abs/2301.13761v3,2023-01-31T16:54:33Z,2024-01-04T14:33:06Z,"  There has been an increasing recognition of the value of data and of
data-based decision making. As a consequence, the development of data science
as a field of study has intensified in recent years. However, there is no
systematic and comprehensive treatment and understanding of data science. This
article describes a systematic and end-to-end framing of the field based on an
inclusive definition. It identifies the core components making up the data
science ecosystem, presents its lifecycle modeling the development process, and
argues its interdisciplinarity.
",['\nM. Tamer Özsu\n'],"This is an extended version of the original submission. The original
  has now been published by Communications of ACM, Volume 66, Number 7, pages
  106-116, 2023. The original version was only 10 pages and the new version is
  100 pages; the original had a restricted number of references (42) while the
  longer one is much more complete with over 150 references",,http://arxiv.org/abs/2301.13761v3,cs.OH,['cs.OH'],,,[]
Automating Crochet Patterns for Surfaces of Revolution,http://arxiv.org/abs/2302.02205v1,2023-02-01T01:05:52Z,2023-02-01T01:05:52Z,"  A surface of revolution is created by taking a curve in the $xy$-plane and
rotating it about some axis. We develop a program which automatically generates
crochet patterns for surfaces by revolution when they are obtained by rotating
about the $x$-axis. In order to accomplish this, we invoke the arclength
integral to determine where to take measurements for each row. In addition, a
distance measure is created to optimally space increases and decreases. The
result is a program that will take a function, $x$-bounds, crochet gauge, and a
scale in order to produce a polished crochet pattern.
","['\nMegan Martinez\n', '\nAmanda Taylor Lipnicki\n']",,,http://arxiv.org/abs/2302.02205v1,cs.OH,"['cs.OH', '00A66']",,,[]
"Invoice discounting using kelly criterion by automated market
  makers-like implementations",http://arxiv.org/abs/2302.09009v1,2023-01-30T09:00:06Z,2023-01-30T09:00:06Z,"  There is a persistent lack of funding, especially for SMEs, that cyclically
worsens. The factoring and invoice discounting market appears to address delays
in paying commercial invoices: sellers bring still-to-be-paid invoices to
financial organizations, intermediaries, typically banks that provide an
advance payment. This article contains research on novel decentralized
approaches to said lending services without intermediaries by using liquidity
pools and its associated heuristics, creating an Automated Market Maker. In our
approach, the contributed collateral and the invoice trades with risk is
measured with a formula: The Kelly criterion is used to calculate the optimal
premium to be contributed to a liquidity pool in the funding of the said
invoices. The behavior of the algorithm is studied in several scenarios of
streams of invoices with representative amounts, collaterals, payment delays,
and nonpayments rates or mora. We completed the study with hack scenarios with
bogus, nonpayable invoices. As a result, we have created a resilient solution
that performs the best with partially collateralized invoices. The outcome is
decentralized market developed with the Kelly criterion that is reasonably
resilient to a wide variety of the invoicing cases that provides sound profit
to liquidity providers, and several premium distribution policies were checked
that contributed with extra resilience to the performance of the algorithm.
","['\nPeplluis R. Esteva\n', '\nAlberto Ballesteros Rodríguez\n']","43 pages, UCL-CBT Report",,http://arxiv.org/abs/2302.09009v1,q-fin.GN,"['q-fin.GN', 'cs.OH']",,,[]
"A Framework for Operational Volume Generation for Urban Air Mobility
  Strategic Deconfliction",http://arxiv.org/abs/2301.12961v8,2023-01-26T22:20:12Z,2023-06-27T08:59:03Z,"  Strategic pre-flight functions focus on the planning and deconfliction of
routes for aircraft systems. The urban air mobility concept calls for higher
levels of autonomy with onboard and en route functions but also strategic and
pre-flight systems. Existing endeavours into strategic pre-flight functions
focus on improving the route generation and strategic deconfliction of these
routes. Introduced with the urban air mobility concept is the premise of
operational volumes. These 4D regions of airspace, describe the intended
operational region for an aircraft for finite time. Chaining these together
forms a contract of finite operational volumes over a given route. It is no
longer enough to only deconflict routes within the airspace, but to now
consider these 4D operational volumes. To provide an effective all-in-one
approach, we propose a novel framework for generating routes and accompanying
contracts of operational volumes, along with deconfliction focused around 4D
operational volumes. Experimental results show efficiency of operational volume
generation utilising reachability analysis and demonstrate sufficient success
in deconfliction of operational volumes.
","['\nEllis Lee Thompson\n', '\nYan Xu\n', '\nPeng Wei\n']","8 pages, 3 Figures","2023 International Conference on Unmanned Aircraft Systems
  (ICUAS), Warsaw, Poland, 2023, pp. 71-78",http://dx.doi.org/10.1109/ICUAS57906.2023.10156164,cs.OH,"['cs.OH', 'cs.SY', 'eess.SY']",10.1109/ICUAS57906.2023.10156164,,[]
A Digital Delay Model Supporting Large Adversarial Delay Variations,http://arxiv.org/abs/2301.09588v2,2023-01-10T20:09:02Z,2023-04-06T18:10:07Z,"  Dynamic digital timing analysis is a promising alternative to analog
simulations for verifying particularly timing-critical parts of a circuit. A
necessary prerequisite is a digital delay model, which allows to accurately
predict the input-to-output delay of a given transition in the input signal(s)
of a gate. Since all existing digital delay models for dynamic digital timing
analysis are deterministic, however, they cannot cover delay fluctuations
caused by PVT variations, aging and analog signal noise. The only exception
known to us is the $\eta$-IDM introduced by F\""ugger et al. at DATE'18, which
allows to add (very) small adversarially chosen delay variations to the
deterministic involution delay model, without endangering its faithfulness. In
this paper, we show that it is possible to extend the range of allowed delay
variations so significantly that realistic PVT variations and aging are covered
by the resulting extended $\eta$-IDM.
","['\nDaniel Öhlinger\n', '\nUlrich Schmid\n']",,,http://arxiv.org/abs/2301.09588v2,cs.OH,['cs.OH'],,,[]
"HQAlign: Aligning nanopore reads for SV detection using current-level
  modeling",http://arxiv.org/abs/2301.03834v1,2023-01-10T08:04:48Z,2023-01-10T08:04:48Z,"  Motivation: Detection of structural variants (SV) from the alignment of
sample DNA reads to the reference genome is an important problem in
understanding human diseases. Long reads that can span repeat regions, along
with an accurate alignment of these long reads play an important role in
identifying novel SVs. Long read sequencers such as nanopore sequencing can
address this problem by providing very long reads but with high error rates,
making accurate alignment challenging. Many errors induced by nanopore
sequencing have a bias because of the physics of the sequencing process and
proper utilization of these error characteristics can play an important role in
designing a robust aligner for SV detection problems. In this paper, we design
and evaluate HQAlign, an aligner for SV detection using nanopore sequenced
reads. The key ideas of HQAlign include (i) using basecalled nanopore reads
along with the nanopore physics to improve alignments for SVs (ii)
incorporating SV specific changes to the alignment pipeline (iii) adapting
these into existing state-of-the-art long read aligner pipeline, minimap2
(v2.24), for efficient alignments.
  Results: We show that HQAlign captures about 4%-6% complementary SVs across
different datasets which are missed by minimap2 alignments while having a
standalone performance at par with minimap2 for real nanopore reads data. For
the common SV calls between HQAlign and minimap2, HQAlign improves the start
and the end breakpoint accuracy for about 10%-50% of SVs across different
datasets. Moreover, HQAlign improves the alignment rate to 89.35% from minimap2
85.64% for nanopore reads alignment to recent telomere-to-telomere CHM13
assembly, and it improves to 86.65% from 83.48% for nanopore reads alignment to
GRCh37 human genome.
","['\nDhaivat Joshi\n', '\nSuhas Diggavi\n', '\nMark J. P. Chaisson\n', '\nSreeram Kannan\n']",,,http://arxiv.org/abs/2301.03834v1,q-bio.GN,"['q-bio.GN', 'cs.OH']",,,[]
"The Evolution of Real-time Remote Intraoperative Neurophysiological
  Monitoring (IONM)",http://arxiv.org/abs/2301.10225v1,2023-01-10T14:57:37Z,2023-01-10T14:57:37Z,"  Real-time monitoring of nervous system function with immediate communication
of relevant information to the surgeon enables prevention and/or mitigation of
iatrogenic injury in many surgical procedures. The hardware and software
infrastructure and demonstrated usefulness of telemedicine in support of IONM
originated in a busy university health center environment and then spread
widely as comparable functional capabilities were added by commercial equipment
manufacturers. The earliest implementations included primitive data archival
and case documentation capabilities and relied primarily on deidentification
for security. They emphasized full-featured control of the real-time data
display by remote observers. Today, remote IONM is routinely utilized in more
than 200,000 high-risk surgical procedures/year in the United States. For many
cases, remote observers rely on screen capture to view the data as it is
displayed in the remote operating room while providing sophisticated security
capabilities and data archival and standardized metadata and case
documentation.
","['\nJeffrey Balzer\n', '\nJulia Caviness\n', '\nDon Krieger\n']","12 pages, 3 figures",,http://arxiv.org/abs/2301.10225v1,cs.OH,"['cs.OH', 'cs.CY']",,,[]
Lost in Algorithms,http://arxiv.org/abs/2301.10333v1,2023-01-02T16:09:05Z,2023-01-02T16:09:05Z,"  Algorithms are becoming more capable, and with that comes hic sunt dracones
(here be dragons). The term symbolizes areas beyond our known maps. We use this
term since we are stepping into an exciting, potentially dangerous, and unknown
area with algorithms. Our curiosity to understand the natural world drives our
search for new methods. For this reason, it is crucial to explore this subject.
  The project's objective is to overlay the information obtained, in
conjunction with the state of hardware today, to see if we can determine the
likely directions for future algorithms'. Even though we slightly cover
non-classical computing in this paper, our primary focus is on classical
computing (i.e., digital computers). It is worth noting that non-classical
quantum computing requires classical computers to operate; they are not
mutually exclusive.
",['\nAndrew N. Sloss\n'],Experimental exploration of Algorithms,,http://arxiv.org/abs/2301.10333v1,cs.OH,"['cs.OH', 'cs.AI', 'quant-ph']",,,[]
"Predicting the Students Involvements and its Impacts on Learning
  Outcomes Through Online Education During Covid-19",http://arxiv.org/abs/2301.00031v1,2022-12-28T18:11:07Z,2022-12-28T18:11:07Z,"  Everybody knows very well about the COVID-19 pandemic, lockdown, and its
impacts and effects on every field of life, from childhood to senior citizens,
from local to global. The underlying research study focuses on students'
involvement in online classes. This paper assesses the effect of the COVID-19
pandemic on the students' participation and involvement during online classes
compared to the physical classes, cheating behavior, health effects, and study
styles of the students of diverse degrees and age groups. This research study
contributes to the real problems and challenges that students faced during
online classes during the COVID-19 pandemic. The percentages of the students'
responses with different color schemes shown in Fig. 1, Fig. 2, Fig.3(a),
Fig.3(b) and Fig.4 are conveying powerful and meaningful insight. These figures
and the results given in Table I and Table II indicate that most students are
not fully involved during online classes due to technical issues, remote
distance, etc. We applied the Test here because we do not have exact population
means. We used ttest_1samp with default value 0 to compute the variables'
statistics and p-value. These values are minimal in favor of rejecting the null
or H0 (hypothesis) and accepting the alternate or H1 (hypothesis). It further
means that students' involvement during online classes is severely affected.
","['\nMuhammad Nadeem\n', '\nFaisal Bukhari\n', '\nAli Hussain\n']","10 pages, 4",,http://arxiv.org/abs/2301.00031v1,cs.OH,['cs.OH'],,,[]
"A Portal for High-Precision Atomic Data and Computation: Design and Best
  Practices",http://arxiv.org/abs/2212.10665v1,2022-12-20T21:54:14Z,2022-12-20T21:54:14Z,"  The Atom portal, udel.edu/atom, provides the scientific community with easily
accessible high-quality data about properties of atoms and ions, such as
energies, transition matrix elements, transition rates, radiative lifetimes,
branching ratios, polarizabilities, and hyperfine constants. The data are
calculated using a high-precision state-of-the-art linearized coupled-cluster
method, high-precision experimental values are used where available. All values
include estimated uncertainties. Where available, experimental results are
provided with references. This paper provides an overview of the portal and
describes the design as well as applied software engineering practices.
","['\nParinaz Barakhshan\n', '\nAkshay Bhosale\n', '\nAmani Kiruga\n', '\nRudolf Eigenmann\n', '\nMarianna S. Safronova\n', '\nBindiya Arora\n']","9 pages, 3 figures",,http://arxiv.org/abs/2212.10665v1,physics.atom-ph,"['physics.atom-ph', 'cs.OH']",,,[]
"Sharing Linkable Learning Objects with the use of Metadata and a
  Taxonomy Assistant for Categorization",http://arxiv.org/abs/2212.05947v1,2022-12-09T18:32:18Z,2022-12-09T18:32:18Z,"  In this work, a re-design of the Moodledata module functionalities is
presented to share learning objects between e-learning content platforms, e.g.,
Moodle and G-Lorep, in a linkable object format. The e-learning courses content
of the Drupal-based Content Management System G-Lorep for academic learning is
exchanged designing an object incorporating metadata to support the reuse and
the classification in its context. In such an Artificial Intelligence
environment, the exchange of Linkable Learning Objects can be used for dialogue
between Learning Systems to obtain information, especially with the use of
semantic or structural similarity measures to enhance the existent Taxonomy
Assistant for advanced automated classification.
","['\nValentina Franzoni\n', '\nSergio Tasso\n', '\nSimonetta Pallottelli\n', '\nDamiano Perri\n']",,,http://dx.doi.org/10.1007/978-3-030-24296-1_28,cs.OH,"['cs.OH', 'cs.AI']",10.1007/978-3-030-24296-1_28,,[]
An Accurate Hybrid Delay Model for Multi-Input Gates,http://arxiv.org/abs/2211.10628v2,2022-11-19T09:24:31Z,2023-05-15T12:45:12Z,"  In order to facilitate the analysis of timing relations between individual
transitions in a signal trace, dynamic digital timing analysis offers a less
accurate but much faster alternative to analog simulations of digital circuits.
This primarily requires gate delay models that also account for the fact that
the input-to-output delay of a particular input transition also depends on the
temporal distance to the previous output transitions. In the case of
multi-input gates, the delay also experiences variations caused by multi-input
switching (MIS) effects, i.e., transitions at different inputs that occur in
close temporal proximity. In this paper, we advocate the development of hybrid
delay models for CMOS gates obtained by replacing transistors with time-variant
resistors. We exemplify our approach by applying it to a NOR gate (and, hence,
to the dual NAND gate) and a Muller C gate. We analytically solve the resulting
first-order differential equations with non-constant-coefficients, and derive
analytic expressions for the resulting MIS gate delays. The resulting formulas
not only pave the way to a sound model parametrization procedure, but are also
instrumental for implementing fast and efficient digital timing simulation. By
comparison with analog simulation data, we show that our models faithfully
represent all relevant MIS effects. Using an implementation in the Involution
Tool, we demonstrate that our model surpasses the alternative digital delay
models for NOR gates known to us in terms of accuracy, with comparably short
running times.
","['\nArman Ferdowsi\n', '\nUlrich Schmid\n', '\nJosef Salzmann\n']",,,http://arxiv.org/abs/2211.10628v2,cs.OH,['cs.OH'],,,[]
"$\texttt{Davos}$: a Python ""smuggler"" for constructing lightweight
  reproducible notebooks",http://arxiv.org/abs/2211.15445v2,2022-11-23T14:12:20Z,2023-10-01T21:36:03Z,"  Reproducibility is a core requirement of modern scientific research. For
computational research, reproducibility means that code should produce the same
results, even when run on different systems. A standard approach to ensuring
reproducibility entails packaging a project's dependencies along with its
primary code base. Existing solutions vary in how deeply these dependencies are
specified, ranging from virtual environments, to containers, to virtual
machines. Each of these existing solutions requires installing or setting up a
system for running the desired code, increasing the complexity and time cost of
sharing or engaging with reproducible science. Here, we propose a
lighter-weight solution: the $\texttt{Davos}$ package. When used in combination
with a notebook-based Python project, $\texttt{Davos}$ provides a mechanism for
specifying the correct versions of the project's dependencies directly within
the code that requires them, and automatically installing them in an isolated
environment when the code is run. The $\texttt{Davos}$ package further ensures
that those packages and specific versions are used every time the notebook's
code is executed. This enables researchers to share a complete reproducible
copy of their code within a single Jupyter notebook file.
","['\nPaxton C. Fitzpatrick\n', '\nJeremy R. Manning\n']",,,http://arxiv.org/abs/2211.15445v2,cs.OH,['cs.OH'],,,[]
"Extracting task trees using knowledge retrieval search algorithms in
  functional object-oriented network",http://arxiv.org/abs/2211.08314v1,2022-11-15T17:20:08Z,2022-11-15T17:20:08Z,"  The functional object-oriented network (FOON) has been developed as a
knowledge representation method that can be used by robots in order to perform
task planning. A FOON can be observed as a graph that can provide an ordered
plan for robots to retrieve a task tree, through the knowledge retrieval
process. We compare two search algorithms to evaluate their performance in
extracting task trees: iterative deepening search (IDS) and greedy best-first
search (GBFS) with two different heuristic functions. Then, we determine which
algorithm is capable of obtaining a task tree for various cooking recipes using
the least number of functional units. Preliminary results show that each
algorithm can perform better than the other, depending on the recipe provided
to the search algorithm.
",['\nTyree Lewis\n'],"5 pages, 8 figures, 8 tables",,http://arxiv.org/abs/2211.08314v1,cs.OH,['cs.OH'],,,[]
"Brief on tool path generation/optimization methods for multi-axis CNC
  machining",http://arxiv.org/abs/2212.07941v1,2022-11-12T12:25:11Z,2022-11-12T12:25:11Z,"  The quality of tool paths is a dominant factor in CNC machining, determining
its efficiency and accuracy. This paper provides a brief review (in Chinese) on
tool path planning methods reported in the literature, focusing on their
categorization, motivating problems/issues, and historical development. Some
promising research trends have also been stressed, especially for the paradigm
shift from tool path generation towards tool path optimization.
",['\nQiang Zou\n'],in Chinese,,http://arxiv.org/abs/2212.07941v1,cs.OH,"['cs.OH', 'I.3.5']",,,[]
QMKPy: A Python Testbed for the Quadratic Multiple Knapsack Problem,http://arxiv.org/abs/2211.17222v1,2022-11-14T10:17:30Z,2022-11-14T10:17:30Z,"  QMKPy provides a Python framework for modeling and solving the quadratic
multiple knapsack problem (QMKP). It is primarily aimed at researchers who
develop new solution algorithms for the QMKP. QMKPy therefore mostly functions
as a testbed to quickly implement novel algorithms and compare their results
with existing ones. However, the package also already includes implementations
of established algorithms for those who only need to solve a QMKP as part of
their research.
","['\nKarl-Ludwig Besser\n', '\nEduard A. Jorswieck\n']",3 pages,"Journal of Open Source Software, vol. 7, no. 79: 4718, Nov 2022",http://dx.doi.org/10.21105/joss.04718,cs.OH,"['cs.OH', 'math.OC']",10.21105/joss.04718,,[]
"O Problema do Roteamento de Interligações Elétricas em Circuitos
  Integrados",http://arxiv.org/abs/2210.10483v1,2022-10-18T12:41:35Z,2022-10-18T12:41:35Z,"  Integrated circuit design automation tools are essential for the feasibility
of complex designs with millions of transistors. One of the steps performed
within the process is the routing of interconnections between components of a
circuit. This problem, which also aims to optimize the utilization of
connection resources, has been shown to be NP-Complete and requires heuristic
algorithms to look for the best achievable solutions. In this work, we present
a definition of this problem in context with a brief review of existing
solutions in the literature. Then, we propose a methodology for the development
of an original algorithm, which aims to differentiate itself, in certain
domains, from the solutions already proposed.
",['\nTiago Matos Santos\n'],in Portuguese language,,http://arxiv.org/abs/2210.10483v1,cs.OH,['cs.OH'],,,[]
Knowledge Retrieval With Functional Object-Oriented Networks,http://arxiv.org/abs/2211.02608v1,2022-10-22T01:46:14Z,2022-10-22T01:46:14Z,"  In this experiment, three different search algorithms are implemented for the
purpose of extracting a task tree from a large knowledge graph, known as the
Functional Object-Oriented Network (FOON). Using a universal FOON, which
contains knowledge extracted by annotating online cooking videos, and a desired
goal, a task tree can be retrieved. The process of searching the universal FOON
for task tree retrieval is tested using iterative deepening search and greedy
best-first search with two different heuristic functions. The performance of
these three algorithms is analyzed and compared. The results of the experiment
show that iterative deepening performs strongly overall. However, different
heuristics in an informed search proved to be beneficial for certain
situations.
",['\nShawn Diaz\n'],"3 pages, 1 figure",,http://arxiv.org/abs/2211.02608v1,cs.OH,['cs.OH'],,,[]
The Executable Digital Twin: merging the digital and the physics worlds,http://arxiv.org/abs/2210.17402v2,2022-10-25T08:35:08Z,2023-04-14T11:22:50Z,"  While the digital twin has become an intrinsic part of the product creation
process, its true power lies in the connectivity of the digital representation
with its physical counterpart. Data acquired on the physical asset can
validate, update and enrich the digital twin. The knowledge contained in the
digital representation brings value to the physical asset itself. When a
dedicated encapsulation is extracted from the digital twin to model a specific
set of behaviors in a specific context, delivering a stand-alone executable
representation, such instantiated and self-contained model is referred to as an
Executable Digital Twin. In this contribution, key building blocks such as
model order reduction, real-time models, state estimation and co-simulation are
reviewed, and a number of characteristic use cases are presented. These include
virtual sensing, hybrid testing and hardware-in-the loop, model-based control
and model-based diagnostics.
","['\nHerman Van der Auweraer\n', '\nDirk Hartmann\n']",,,http://arxiv.org/abs/2210.17402v2,cs.OH,"['cs.OH', 'cs.CY', 'J.2, J.6, I.6.0, G.0']",,,[]
"Dragoman: Efficiently Evaluating Declarative Mapping Languages over
  Frameworks for Knowledge Graph Creation",http://arxiv.org/abs/2210.15645v1,2022-10-26T14:01:36Z,2022-10-26T14:01:36Z,"  In recent years, there have been valuable efforts and contributions to make
the process of RDF knowledge graph creation traceable and transparent;
extending and applying declarative mapping languages is an example. One
challenging step is the traceability of procedures that aim to overcome
interoperability issues, a.k.a. data-level integration. In most pipelines, data
integration is performed by ad-hoc programs, preventing traceability and
reusability. However, formal frameworks provided by function-based declarative
mapping languages such as FunUL and RML+FnO empower expressiveness. Data-level
integration can be defined as functions and integrated as part of the mappings
performing schema-level integration. However, combining functions with the
mappings introduces a new source of complexity that can considerably impact the
required number of resources and execution time. We tackle the problem of
efficiently executing mappings with functions and formalize the transformation
of them into function-free mappings. These transformations are the basis of an
optimization process that aims to perform an eager evaluation of function-based
mapping rules. These techniques are implemented in a framework named Dragoman.
We demonstrate the correctness of the transformations while ensuring that the
function-free data integration processes are equivalent to the original one.
The effectiveness of Dragoman is empirically evaluated in 230 testbeds composed
of various types of functions integrated with mapping rules of different
complexity. The outcomes suggest that evaluating function-free mapping rules
reduces execution time in complex knowledge graph creation pipelines composed
of large data sources and multiple types of mapping rules. The savings can be
up to 75%, suggesting that eagerly executing functions in mapping rules enable
making these pipelines applicable and scalable in real-world settings.
","['\nSamaneh Jozashoori\n', '\nEnrique Iglesias\n', '\nMaria-Esther Vidal\n']",,,http://arxiv.org/abs/2210.15645v1,cs.DB,"['cs.DB', 'cs.OH', 'cs.PL']",,,[]
"PoolLines: Modeling Carpooling as Ephemeral Lines in GTFS for effective
  integration with Public Transit",http://arxiv.org/abs/2210.07578v1,2022-10-14T07:13:29Z,2022-10-14T07:13:29Z,"  In carpooling systems, a set of drivers owning a private car can accept a
small detour to pick-up and drop-off other riders. However, carpooling is
widely used for long-distance trips, where rider-driver matching can be done
days ahead. Making carpooling a viable option for daily commute is more
challenging, as trips are shorter and, proportionally, the detours tolerated by
drivers are more tight. As a consequence, finding riders and drivers sharing
close-enough origins, destinations and departure time is less likely, which
limits potential matching. In this paper we propose an Integrated System, where
carpooling matching is synchronized with Public Transit (PT) schedules, so as
to serve as a feeder service to PT in the first mile. Driver detours are
proposed towards PT selected stations, which are used as consolidation points,
thus increasing matching probability. We present a computationally efficient
method to represent PT schedules and drivers trajectory in a single General
Transit Feed Specification database, which allows to compute multimodal rider
journeys using any off the shelf planners. We showcase our approach in the
metropolitan area of Portland, Oregon, considering 8k randomly generated trips.
We show the benefits of our Integrated System. We find that 10% more riders
find a feasible matching with respect to the status quo, where carpooling and
PT are operated separately. We release our code as open source.
","['\nYoussef Chaabouni\n', '\nAndrea Araldo\n', '\nAndré de Palma\n', '\nSouhila Arib\n']","15th ACM SIGSPATIAL International Workshop on Computational
  Transportation Science (IWCTS 2022)",,http://arxiv.org/abs/2210.07578v1,cs.OH,['cs.OH'],,,[]
Agile Systems Engineering for sub-CubeSat scale spacecraft,http://arxiv.org/abs/2210.10653v1,2022-10-14T11:57:24Z,2022-10-14T11:57:24Z,"  Space systems miniaturization has been increasingly popular for the past
decades, with over 1600 CubeSats and 300 sub-CubeSat sized spacecraft estimated
to have been launched since 1998. This trend towards decreasing size enables
the execution of unprecedented missions in terms of quantity, cost and
development time, allowing for massively distributed satellite networks, and
rapid prototyping of space equipment. Pocket-sized spacecraft can be designed
in-house in less than a year and can reach weights of less than 10g, reducing
the considerable effort typically associated with orbital flight. However,
while Systems Engineering methodologies have been proposed for missions down to
CubeSat size, there is still a gap regarding design approaches for
picosatellites and smaller spacecraft, which can exploit their potential for
iterative and accelerated development. In this paper, we propose a Systems
Engineering methodology that abstains from the classic waterfall-like approach
in favor of agile practices, focusing on available capabilities, delivery of
features and design ""sprints"". Our method, originating from the software
engineering disciplines, allows quick adaptation to imposed constraints,
changes to requirements and unexpected events (e.g. chip shortages or delays),
by making the design flexible to well-defined modifications. Two femtosatellite
missions, currently under development and due to be launched in 2023, are used
as case studies for our approach, showing how miniature spacecraft can be
designed, developed and qualified from scratch in 6 months or less. We claim
that the proposed method can simultaneously increase confidence in the design
and decrease turnaround time for extremely small satellites, allowing
unprecedented missions to take shape without the overhead traditionally
associated with sending cutting-edge hardware to space.
","['\nKonstantinos Kanavouras\n', '\nAndreas Makoto Hein\n', '\nMaanasa Sachidanand\n']","15 pages, 6 figures, 3 tables, presented in the 73rd International
  Astronautical Congress",,http://arxiv.org/abs/2210.10653v1,astro-ph.IM,"['astro-ph.IM', 'cs.OH', 'physics.space-ph']",,,[]
"Road Network Variation Based on HD Map Analysis for the Simulative
  Safety Assurance of Automated Vehicles",http://arxiv.org/abs/2210.00853v1,2022-09-29T14:45:24Z,2022-09-29T14:45:24Z,"  The validation and verification of automated driving functions (ADFs) is a
challenging task on the journey of making those functions available to the
public beyond the current research context. Simulation is a valuable building
block for scenario-based testing that can help to model traffic situations that
are relevant for ADFs. In addition to the surrounding traffic and environment
of the ADF under test, the logical description and automated generation of
concrete road networks have an important role. We aim to reduce efforts for
manual map generation and to improve the automated testing process during
development.
  Hence, this paper proposes a method to analyze real road networks and extract
relevant parameters for the variation of synthetic simulation maps that
correspond to real-world properties. Consequently, characteristics for
inner-city junctions are selected from Here HD map. Then, parameter
distributions are determined, analyzed and used to generate variations of road
networks in the OpenDRIVE standard. The presented methodology enables efficient
road network modeling which can be used for large scale simulations. The
developed road network generation tool is publicly available on GitHub.
","['\nDaniel Becker\n', '\nChristian Geller\n', '\nLutz Eckstein\n']","Proc. of the International Conference on Electrical, Computer,
  Communications and Mechatronics Engineering (ICECCME), 16-18 November 2022,
  Maldives",,http://arxiv.org/abs/2210.00853v1,cs.OH,"['cs.OH', 'cs.SY', 'eess.SY']",,,[]
"Generalization of Higher Order Methods for Fast Iterative Matrix
  Inversion Suitable for GPU Acceleration",http://arxiv.org/abs/2212.02406v3,2022-10-03T14:42:05Z,2023-05-23T06:29:46Z,"  Recent technological developments have led to big data processing, which
resulted in significant computational difficulties when solving large-scale
linear systems or inverting matrices. As a result, fast approximate iterative
matrix inversion methodologies via Graphical Processing Unit (GPU) acceleration
has been a subject of extensive research, to find solutions where classic and
direct inversion are too expensive to conduct. Some currently used methods are
Neumann Series (NS), Newton iteration (NI), Chebyshev Iteration (CI), and
Successive Over-Relaxation, to cite a few. In this work, we develop a new
iterative algorithm based off the NS, which we named 'Nested Neumann' (NN).
This new methodology generalizes higher orders of the NI (or CI), by taking
advantage of a computationally free iterative update of the preconditioning
matrix as a function of a given 'inception depth'. It has been mathematically
demonstrated that the NN: (i) convergences given the preconditioning satisfies
the spectral norm condition of the NS, (ii) has an order of rate of convergence
has been shown to be equivalent to the order (inception depth plus one), and
(iii) has an optimal inception depth is an inception depth of one or preferably
two, depending on RAM constraints. Furthermore, we derive an explicit formula
for the NN, which is applicable to massive sparse matrices, given an increase
in computational cost. Importantly, the NN finds an analytic equivalancy
statement between the NS and the the NN (NI, CI, and higher orders), which is
of importance for mMIMO systems. Finally, the NN method is applicable positive
semi-definite matrices for matrix inversion, and applicable to any linear
system (sparse, non-sparse, complex, etc.).
","['\nMarcus Engsig\n', '\nQingjie Yang\n']",,,http://arxiv.org/abs/2212.02406v3,cs.OH,"['cs.OH', 'cs.NA', 'math.NA']",,,[]
"A Digital Twin Description Framework and its Mapping to Asset
  Administration Shell",http://arxiv.org/abs/2209.12661v2,2022-09-23T17:34:52Z,2023-08-10T19:31:21Z,"  The pace of reporting on Digital Twin (DT) projects continues to accelerate
both in industry and academia. However, these experience reports often leave
out essential characteristics of the DT, such as the scope of the
system-under-study, the insights and actions enabled, and the time-scale of
processing. A lack of these details could therefore hamper both understanding
of these DTs and development of DT tools and techniques. Our previous work
developed a DT description framework with fourteen characteristics as a
checklist for experience report authors to better describe the capabilities of
their DT projects. This report provides an extended example of reporting to
highlight the utility of this description framework, focusing on the DT of an
industrial drilling machine. Furthermore, we provide a mapping from our
description framework to the Asset Administration Shell (AAS) which is an
emerging standard for Industry 4.0 system integration. This mapping aids
practitioners in understanding how our description framework relates to AAS,
potentially aiding in description or implementation activities.
","['\nBentley James Oakes\n', '\nAli Parsai\n', '\nBart Meyers\n', '\nIstvan David\n', '\nSimon Van Mierlo\n', '\nSerge Demeyer\n', '\nJoachim Denil\n', '\nPaul De Meulenaere\n', '\nHans Vangheluwe\n']",,"Communications in Computer and Information Science (2023), vol
  1708",http://dx.doi.org/10.1007/978-3-031-38821-7_1,cs.OH,['cs.OH'],10.1007/978-3-031-38821-7_1,,[]
Star Anagram Detection and Classification,http://arxiv.org/abs/2210.06397v1,2022-09-18T16:54:35Z,2022-09-18T16:54:35Z,"  A star anagram is a rearrangement of the letters of one word to produce
another word where no letter retains its original neighbors. These maximally
shuffled anagrams are rare, comprising only about 5.7% of anagrams in English.
They can also be depicted as unicursal polygons with varying forms, including
the eponymous stars. We develop automated methods for detecting stars among
other anagrams and for classifying them based on their polygon's degree of both
rotational and reflective symmetry. Next, we explore several properties of star
anagrams including proofs for two results about the edge lengths of perfect,
i.e., maximally symmetric, stars leveraging perhaps surprising connections to
modular arithmetic and the celebrated Chinese Remainder Theorem. Finally, we
conduct an exhaustive search of English for star anagrams and provide numerical
results about their clustering into common shapes along with examples of
geometrically noteworthy stars.
","['\nJason Parker\n', '\nDan Barker\n']","14 pages, 14 figures in main article. Appendix contains several
  thousand figures over 250+ pages. In preparation for submission to
  Computational Geometry",,http://arxiv.org/abs/2210.06397v1,cs.OH,"['cs.OH', 'I.3.5']",,,[]
Towards Standardized Mobility Reports with User-Level Privacy,http://arxiv.org/abs/2209.08921v1,2022-09-19T11:03:29Z,2022-09-19T11:03:29Z,"  The importance of human mobility analyses is growing in both research and
practice, especially as applications for urban planning and mobility rely on
them. Aggregate statistics and visualizations play an essential role as
building blocks of data explorations and summary reports, the latter being
increasingly released to third parties such as municipal administrations or in
the context of citizen participation. However, such explorations already pose a
threat to privacy as they reveal potentially sensitive location information,
and thus should not be shared without further privacy measures.
  There is a substantial gap between state-of-the-art research on privacy
methods and their utilization in practice. We thus conceptualize a standardized
mobility report with differential privacy guarantees and implement it as
open-source software to enable a privacy-preserving exploration of key aspects
of mobility data in an easily accessible way. Moreover, we evaluate the
benefits of limiting user contributions using three data sets relevant to
research and practice. Our results show that even a strong limit on user
contribution alters the original geospatial distribution only within a
comparatively small range, while significantly reducing the error introduced by
adding noise to achieve privacy guarantees.
","['\nAlexandra Kapp\n', '\nSaskia Nuñez von Voigt\n', '\nHelena Mihaljević\n', '\nFlorian Tschorsch\n']",,"Journal of Location Based Services, 2022",http://dx.doi.org/10.1080/17489725.2022.2148008,cs.CR,"['cs.CR', 'cs.OH']",10.1080/17489725.2022.2148008,,[]
Trading Strategies: Earning More in Investment,http://arxiv.org/abs/2209.03294v1,2022-09-07T16:52:05Z,2022-09-07T16:52:05Z,"  Gold and bitcoin are not new to us, but with limited cash and time, given
only the past stream of the daily price of gold and bitcoin, it is a kind of
new problem for us to develop a certain model and determine the best strategy
to get the most return. Here, our team members analyzed the data provided and
finally made a unified system of models to predict the price and evaluate the
risk and return in our act of investment, and we name this series of models and
measurements as CTP Model. This is a model which can determine and describe
what transaction should the trader make each day and what is the certain
maximum return he will get under different risk levels.
","['\nYueying Ma\n', '\nYan Mi\n', '\nYujing Bian\n']",,,http://arxiv.org/abs/2209.03294v1,cs.OH,['cs.OH'],,,[]
Alternate Timelines for TidalCycles,http://arxiv.org/abs/2209.04289v1,2022-09-08T11:33:53Z,2022-09-08T11:33:53Z,"  The TidalCycles (or Tidal for short) live coding environment has been
developed since around 2009, via several rewrites of its core representation.
Rather than having fixed goals, this development has been guided by use,
motivated by the open aim to make music. This development process can be seen
as a long-form improvisation, with insights into the nature of Tidal gained
through the process of writing it, feeding back to guide the next steps of
development.
  This brings the worrying thought that key insights will have been missed
along this development journey, that would otherwise have lead to very
different software. Indeed participants at beginners' workshops that I have
lead or co-lead have often asked questions without good answers, because they
made deficiencies or missing features in the software clear. It is well known
that a beginner's mind is able to see much that an expert has become blind to.
Running workshops are an excellent way to find new development ideas, but the
present paper explores a different technique - the rewrite.
",['\nAlex McLean\n'],Accepted for the International Conference on Live Coding (ICLC) 2021,,http://dx.doi.org/10.5281/zenodo.5786102,cs.OH,"['cs.OH', 'cs.PL']",10.5281/zenodo.5786102,,[]
Cloud Process Execution Engine: Architecture and Interfaces,http://arxiv.org/abs/2208.12214v2,2022-08-25T17:04:10Z,2022-09-18T23:10:40Z,"  Process Execution Engines are a vital part of Business Process Management
(BPM) and Manufacturing Orchestration Management (MOM), as they allow the
business or manufacturing logic (expressed in a graphical notation such as
BPMN) to be executed. This execution drives and supervises all interactions
between humans, machines, software, and the environment. If done right, this
will lead to a highly flexible, low-code, and easy to maintain solution, that
allows for ad-hoc changes and functional evolution, as well as delivering a
wealth of data for data-science applications. The Cloud Process Execution
Engine CPEE.org implements a radically distributed scale-out architecture,
together with a minimal set of interfaces, to allow for the simplest possible
integration with existing services, machines, and existing data-analysis tools.
Its open-source components can serve as a blueprint for future development of
commercial solutions, and serves as a proven testbed for academic research,
teaching, and industrial application since 2008. In this paper we present the
architecture, interfaces that make CPEE.org possible, as well as discuss
different lifecycle models utilized during execution to provide overarching
support for a wide range of data-analysis tasks.
","['\nJuergen Mangler\n', '\nStefanie Rinderle-Ma\n']","30 pages, 12 figures, 2 illustrations",,http://arxiv.org/abs/2208.12214v2,cs.OH,"['cs.OH', 'D.2.11; D.2.12; D.2.13; H.4']",,,[]
Repairing Activity Start Times to Improve Business Process Simulation,http://arxiv.org/abs/2208.12224v1,2022-08-24T12:23:32Z,2022-08-24T12:23:32Z,"  Business Process Simulation (BPS) is a common technique to estimate the
impact of business process changes, e.g. what would be the cycle time of a
process if the number of traces increases? The starting point of BPS is a
business process model annotated with simulation parameters (a BPS model).
Several studies have proposed methods to automatically discover BPS models from
event logs -- extracted from enterprise information systems -- via process
mining techniques. These approaches model the processing time of each activity
based on the start and end timestamps recorded in the event log. In practice,
however, it is common that the recorded start times do not precisely reflect
the actual start of the activities. For example, a resource starts working on
an activity, but its start time is not recorded until she/he interacts with the
system. If not corrected, these situations induce waiting times in which the
resource is considered to be free, while she/he is actually working. To address
this limitation, this article proposes a technique to identify the waiting time
previous to each activity instance in which the resource is actually working on
them, and repair their start time so that they reflect the actual processing
time. The idea of the proposed technique is that, as far as simulation is
concerned, an activity instance may start once it is enabled and the
corresponding resource is available. Accordingly, for each activity instance,
the proposed technique estimates the activity enablement and the resource
availability time based on the information available in the event log, and
repairs the start time to include the non-recorded processing time. An
empirical evaluation involving eight real-life event logs shows that the
proposed approach leads to BPS models that closely reflect the temporal
dynamics of the process.
","['\nDavid Chapela-Campa\n', '\nMarlon Dumas\n']",arXiv admin note: text overlap with arXiv:2206.14051,,http://arxiv.org/abs/2208.12224v1,cs.OH,['cs.OH'],,,[]
"Development of an IoT-Based Sleep Apnea Monitoring System for Healthcare
  Applications",http://arxiv.org/abs/2209.05449v1,2022-08-26T17:57:52Z,2022-08-26T17:57:52Z,"  Sleep is an essential and vital element of a person life and health that
helps to refresh and recharge the mind and body of a person. The quality of
sleep is very important in every person lifestyle, removing various diseases.
Bad sleep is a big problem for a lot of people for a very long time. People
suffering from various diseases are dealing with various sleeping disorders,
commonly known as sleep apnea. Real-time monitoring of sleep is the key to
detecting sleep apnea. To solve this problem, an IoT based real-time sleep
apnea monitoring system has been developed. It will allow the user to measure
different indexes of sleep and will notify them through a mobile application
when anything odd occurs. The system contains various sensors to measure the
ECG, Heart Rate, Pulse rate, Skin response, and SpO2 of any person during the
entire sleeping period. To analyze and detect sleep apnea in real time, the
system monitors several people during the sleeping period. The results are
displayed on the monitor of the Arduino boards and in the mobile application.
The analysis of the achieved data can detect sleep apnea in some of the people
that the system monitored, and it can also display the reason why sleep apnea
happens. This paper will help everyone learn about sleep apnea and will help
people detect it and take the necessary steps to prevent it.
","['\nAbdur Rab Dhruba\n', '\nKazi Nabiul Alam\n', '\nMd Shakib Khan\n', '\nSami Bourouis\n', '\nMohammad Monirujjaman Khan\n']",,,http://dx.doi.org/10.1155/2021/7152576,cs.OH,['cs.OH'],10.1155/2021/7152576,,[]
"Language and Intelligence, Artificial vs. Natural or What Can and What
  Cannot AI Do with NL?",http://arxiv.org/abs/2209.12829v1,2022-08-31T10:11:50Z,2022-08-31T10:11:50Z,"  In this talk, I argue that there are certain pragmatic features of natural
language (that I will call 'productivity' and 'malleability', on top of
syntactical generativity and semantical compositionality), which are not only
hard, but even impossible to capture in an artificial language used by an AI
system, and the reason for this is to be found in certain deep, metaphysical
differences between artificial and natural intelligence, accounting for the
differences in their respective processes of concept-formation.
",['\nGyula Klima\nFordham University\n'],"In Proceedings NCMA 2022, arXiv:2208.13015","EPTCS 367, 2022, pp. 1-10",http://dx.doi.org/10.4204/EPTCS.367.1,cs.OH,['cs.OH'],10.4204/EPTCS.367.1,,['Fordham University']
SAP Signavio Academic Models: A Large Process Model Dataset,http://arxiv.org/abs/2208.12223v1,2022-08-24T12:50:04Z,2022-08-24T12:50:04Z,"  In this paper, we introduce the SAP Signavio Academic Models (SAP-SAM)
dataset, a collection of hundreds of thousands of business models, mainly
process models in BPMN notation. The model collection is a subset of the models
that were created over the course of roughly a decade on academic.signavio.com,
a free-of-charge software-as-a-service platform that researchers, teachers, and
students can use to create business (process) models. We provide a preliminary
analysis of the model collection, as well as recommendations on how to work
with it. In addition, we discuss potential use cases and limitations of the
model collection from academic and industry perspectives.
","['\nDiana Sola\n', '\nChristian Warmuth\n', '\nBernhard Schäfer\n', '\nPeyman Badakhshan\n', '\nJana-Rebecca Rehse\n', '\nTimotheus Kampik\n']",,,http://arxiv.org/abs/2208.12223v1,cs.OH,"['cs.OH', 'cs.SE']",,,[]
"Business Process Simulation with Differentiated Resources: Does it Make
  a Difference?",http://arxiv.org/abs/2208.07928v1,2022-08-16T20:03:53Z,2022-08-16T20:03:53Z,"  Business process simulation is a versatile technique to predict the impact of
one or more changes on the performance of a process. Mainstream approaches in
this space suffer from various limitations, some stemming from the fact that
they treat resources as undifferentiated entities grouped into resource pools.
These approaches assume that all resources in a pool have the same performance
and share the same availability calendars. Previous studies have acknowledged
these assumptions, without quantifying their impact on simulation model
accuracy. This paper addresses this gap in the context of simulation models
automatically discovered from event logs. The paper proposes a simulation
approach and a method for discovering simulation models, wherein each resource
is treated as an individual entity, with its own performance and availability
calendar. An evaluation shows that simulation models with differentiated
resources more closely replicate the distributions of cycle times and the work
rhythm in a process than models with undifferentiated resources.
","['\nOrlenys Lopez-Pintado\n', '\nMarlon Dumas\n']","Published in the Proceedings of the International Conference on
  Business Process Management (BPM'2022)",,http://arxiv.org/abs/2208.07928v1,cs.OH,['cs.OH'],,,[]
The Bounded Beam Search algorithm for the Block Relocation Problem,http://arxiv.org/abs/2206.12303v1,2022-06-22T08:46:31Z,2022-06-22T08:46:31Z,"  In this paper we deal with the restricted Block Relocation Problem. We
present a new lower bound and a heuristic approach for the problem. The
proposed lower bound can be computed in polynomial time and it is provably
better than some previously known lower bounds. We use it within a bounded beam
search algorithm to solve the Block Relocation Problem and show that the
considered heuristic approach outperforms the other existing algorithms on most
of the instances in the literature. In order to test the approaches on
real-size dimensions, new large instances of the Block Relocation Problem are
also introduced.
","['\nTiziano Bacci\n', '\nSara Mattia\n', '\nPaolo Ventura\n']",,,http://dx.doi.org/10.1016/j.cor.2018.11.008,cs.OH,"['cs.OH', 'cs.DM', '05-08']",10.1016/j.cor.2018.11.008,,[]
"Functional Component Descriptions for Electrical Circuits based on
  Semantic Technology Reasoning",http://arxiv.org/abs/2209.05533v1,2022-06-23T13:22:52Z,2022-06-23T13:22:52Z,"  Circuit diagrams have been used in electrical engineering for decades to
describe the wiring of devices and facilities. They depict electrical
components in a symbolic and graph-based manner. While the circuit design is
usually performed electronically, there are still legacy paper-based diagrams
that require digitization in order to be used in CAE systems. Generally,
knowledge on specific circuits may be lost between engineering projects, making
it hard for domain novices to understand a given circuit design. The
graph-based nature of these documents can be exploited by semantic
technology-based reasoning in order to generate human-understandable
descriptions of their functional principles. More precisely, each electrical
component (e.g. a diode) of a circuit may be assigned a high-level function
label which describes its purpose within the device (e.g. flyback diode for
reverse voltage protection). In this paper, forward chaining rules are used for
such a generation. The described approach is applicable for both CAE-based
circuits as well as raw circuits yielded by an image understanding pipeline.
The viability of the approach is demonstrated by application to an existing set
of circuits.
","['\nJohannes Bayer\n', '\nMina Karami Zadeh\n', '\nMarkus Schröder\n', '\nAndreas Dengel\n']","5 pages, 8 figures",,http://arxiv.org/abs/2209.05533v1,cs.OH,"['cs.OH', 'cs.AR']",,,[]
"Measuring Discrimination Abilities of Monk Parakeets Between Discreet
  and Continuous Quantities Through a Digital Life Enrichment Application",http://arxiv.org/abs/2206.00734v1,2022-06-01T19:49:31Z,2022-06-01T19:49:31Z,"  Ain et al. measured three African Grey (Psittacus erithacus) parrot's
discrimination abilities between discreet and continuous quantities. Some
features of their experimental protocol make it difficult to apply to other
subjects and/or species without introducing a risk for some bias, as subjects
could read cues from the experimenter (even though the study's subjects
probably did not). Can digital life enrichment techniques permit us to
replicate their results with other species with less risk for experimental
bias, with a better precision, and at lower cost? Inspired by previous informal
digital life enrichment experiments with parrots, we designed and tested a web
application to digitally replicate and extend Ain et al.'s experimental setup.
We were able to obtain similar results to theirs for two individuals from a
distinct species, Monk Parakeets (Myiopsitta Monachus), with increased
guarantees against potential experimental biases, in a way which should allow
to replicate such experiments at larger scale and at a much lower cost.
","['\nJérémy Barbay\n', '\nFabián Jaña\n', '\nCristóbal Sepulveda Álvarez\n']",Long preliminary version,,http://arxiv.org/abs/2206.00734v1,cs.OH,['cs.OH'],,,[]
"A Non-Habituating Configurable Audio Visual Animal Deterrent System to
  Mitigate Roadkill",http://arxiv.org/abs/2205.11514v1,2022-05-23T03:53:36Z,2022-05-23T03:53:36Z,"  Growth economies continue to be more reliant on roadways than ever before.
Over 30,000 miles of road are added yearly to the already enormous road system
that exists in the United States. As roads segment habitats, animals have no
option but to walk across them for food, water and companionship. In this
process they end up becoming roadkill. Wherever wildlife habitat and roadways
overlap, death and destruction seem impossible to control. These animal deaths
have a direct impact on the biodiversity and dynamics of an ecosystem and
roadkill poses a threat to many species that are fighting extinction. Vehicles
colliding with animals results in human fatalities, life changing injuries and
extensive property damage. Current methods of handling roadkill are primarily
passive and do not utilize the animal's natural instincts. This paper presents
an alternative approach by actively involving the animal, warning them of an
oncoming vehicle and triggering their survival instincts. Making the animal an
integral part of the solution, augmenting their sensory perception with science
and technology and utilizing their heightened reflexes and survival instincts
provides a better chance at mitigating the problem of roadkill. The results
show that this solution is able to provide animals a warning of an oncoming
vehicle, consistently, reliably and in a wide range of testing conditions.
",['\nVedant Malolan Srinivas\n'],"5 pages, 2 figures, 1 table",,http://arxiv.org/abs/2205.11514v1,cs.OH,['cs.OH'],,,[]
"The openESEA Modelling Language for Ethical, Social and Environmental
  Accounting: Technical Report",http://arxiv.org/abs/2205.15279v2,2022-05-22T15:44:04Z,2023-09-10T13:57:31Z,"  Over the years ethical, social and environmental accounting (ESEA) has become
a common practice among responsible organisations. ESEA entails assessing and
reporting organisations"" performance on environmental, social and governance
topics. In this report, we present a textual grammar for specifying ESEA
methods. With the grammar ESEA models can be created. Such models can be
interpreted by our open-source, model-driven tool, called openESEA. The report
presents the metamodel of the grammar, the grammar itself, and explanations of
each grammar primitive.
","['\nVijanti Ramautar\n', '\nSergio España\n']",,,http://arxiv.org/abs/2205.15279v2,cs.OH,['cs.OH'],,,[]
Device for measuring the plant physiology and electrophysiology,http://arxiv.org/abs/2206.10459v1,2022-05-24T08:48:17Z,2022-05-24T08:48:17Z,"  This paper briefly describes the device - the phytosensor - for measuring
physiological and electrophysiological parameters of plants. This system is
developed as a bio-physiological sensor in precise agriculture, as a tool in
plant research and environmental biology, and for plant enthusiasts in smart
home or entertainment applications. The phytosentor measures main physiological
parameters such as the leaf transpiration rate, sap flow, tissue conductivity
and frequency response, biopotentials (action potentials and variation
potentials), and can conduct electrochemical impedance spectroscopy with
organic tissues. Soil moisture and temperature, air quality (CO2, NO2, O3 and
other sensors on I2C bus), and general environmental parameters (light,
temperature, humidity, air pressure, electromagnetic and magnetic fields) are
also recorded in real time. In addition to phytosensing, the device can also
perform phytoactuation, i.e. execute electrical or light stimulation of plants,
control irrigation and lighting modes, conduct fully autonomous experiments
with complex feedback-based and adaptive scenarios in robotic or biohybrid
systems. This article represents the revised and extended version of original
paper and includes some descriptions and images from the FloraRobotica and
BioHybrids projects.
",['\nSerge Kernbach\n'],,,http://arxiv.org/abs/2206.10459v1,cs.OH,"['cs.OH', 'cs.RO']",,,[]
Step Size is a Consequential Parameter in Continuous Cellular Automata,http://arxiv.org/abs/2205.12728v1,2022-05-20T23:41:06Z,2022-05-20T23:41:06Z,"  Step size in continuous cellular automata (CA) plays an important role in the
stability and behavior of self-organizing patterns. Continous CA dynamics are
defined by formula very similar to numerical estimation of physics-based
ordinary differential equations, specifically Euler's method, for which a large
step size is often inaccurate and unstable. Rather than asymptotically
approaching more accurate estimates of CA dynamics with decreasing step size,
continuous CA may support different self-organizing patterns at different
ranges of step size. We discuss several examples of mobile patterns that become
unstable at step sizes that are too small as well as too large. Additionally,
an individual mobile pattern may exhibit qualitatively different behavior
across a range of step sizes. We demonstrate examples of the effects of step
size in pattern stability and qualitative behavior in continuous CA implemented
in the Lenia framework and its variant, Glaberish.
","['\nQ. Tyrell Davis\n', '\nJosh Bongard\n']",Accepted to ALife 2022 as an extended abstract,,http://arxiv.org/abs/2205.12728v1,nlin.CG,"['nlin.CG', 'cs.OH', 'nlin.AO']",,,[]
"Utilizing Low-Cost Linux Micro-Computer & Android Phone Solutions on
  Cube-Satellites",http://arxiv.org/abs/2205.08255v1,2022-05-14T23:31:59Z,2022-05-14T23:31:59Z,"  Realizing functional space systems using flight-tested components is
problematic in developing economies, as such components are costly for most
institutions to sponsor. The B.Sc. project, Subsystems for 2nd Iteration Cairo
University Cube-Satellite, addresses technology demonstration using
commercially available electronics and low cost computing platforms, such as
Android phones and Raspberry Pi Linux micro-computer as computing hardware. As
for software, the project makes use of open-source modules and locally
developed code to implement needed functionalities, in addition to a mechanism
to operate a virtual desktop Linux OS in parallel to an Android application.
The paper aims to demonstrate the significance, operation design, and problem
solving of such approaches. The paper concludes with future prospects for
improving upon the proposed computing systems
","['\nAhmed Farid\n', '\nAhmed Samy\n', '\nAhmed Shalaby\n', '\nAhmed Tarek\n', '\nMahmoud Ayyad\n', '\nMuhammad Assem\n', '\nSamy Amin\n']",,,http://arxiv.org/abs/2205.08255v1,cs.OH,['cs.OH'],,,[]
Quality Assessment of the 20m SPOT DEM in Nigeria,http://arxiv.org/abs/2205.02946v1,2022-05-05T21:57:48Z,2022-05-05T21:57:48Z,"  The 20m SPOT DEM (Digital Elevation Model) was acquired by the Office of the
Surveyor-General of the Federation (OSGoF) in Nigeria for use in topographic
mapping. A localized assessment of the DEM is needed to validate its stated
accuracies as it is well known that the vertical accuracy of global DEMs varies
in different landscape contexts. Hence, this study assessed the quality of the
DEM in variable land cover types by comparing its heights against 780 Ground
Control Points (GCPs) in Lagos State and the Federal Capital Territory (FCT) of
Nigeria. The pattern of distribution of the height differences was analysed
using spatial autocorrelation analysis, including other accuracy metrics
employed. In the general accuracy assessment, the DEM yielded root mean square
errors of 2.33m in Lagos and 3.69m in the FCT. It was shown that heights over
the bare lands were the most accurately represented on the DEM while heights
over built-up areas are the least accurate at both locations. Despite this, the
SPOT DEM accuracies in the varying land cover types surpassed its stated global
accuracy. While the spatial distribution of low and high height differences in
Lagos State showed clustering, the converse was the case in the FCT. Following
this assessment, it is recommended that the country should extend the
application scope of the DEM in order to exploit its utility to the maximum.
","['\nPeter Nwilo\n', '\nJohanson Onyegbula\n', '\nChukwuma Okolie\n', '\nOlagoke Daramola\n', '\nOluwatimileyin Abolaji\n', '\nIkenna Arungwa\n']",,"Applied Geomatics volume 14, (2022)",http://dx.doi.org/10.1007/s12518-021-00404-0,stat.AP,"['stat.AP', 'cs.OH']",10.1007/s12518-021-00404-0,,[]
Relatively acceptable notation,http://arxiv.org/abs/2205.00791v1,2022-05-02T10:23:34Z,2022-05-02T10:23:34Z,"  Shapiro's notations for natural numbers, and the associated desideratum of
acceptability - the property of a notation that all recursive functions are
computable in it - is well-known in philosophy of computing. Computable
structure theory, however, although capable of fully reconstructing Shapiro's
approach, seems to be off philosophers' radar. Based on the case study of
natural numbers with standard order, we make initial steps to reconcile these
two perspectives. First, we lay the elementary conceptual groundwork for the
reconstruction of Shapiro's approach in terms of computable structures and
show, on a few examples, how results pertinent to the former can inform our
understanding of the latter. Secondly, we prove a new result, inspired by
Shapiro's notion of acceptability, but also relevant for computable structure
theory. The result explores the relationship between the classical notion of
degree spectrum of a computable function on the structure in question -
specifically, having all c.e. degrees as a spectrum - and our ability to
compute the (image of the) successor from the (image of the) function in any
computable copy of the structure. The latter property may be otherwise seen as
relativized acceptability of every notation for the structure.
","['\nNikolay Bazhenov\n', '\nDariusz Kalociński\n']","15 pages, 0 figures",,http://arxiv.org/abs/2205.00791v1,math.LO,"['math.LO', 'cs.OH', '03D45, 03A99']",,,[]
Optimal Layered Defense For Site Protection,http://arxiv.org/abs/2204.08961v1,2022-04-18T17:00:01Z,2022-04-18T17:00:01Z,"  We present a model for layered security with applications to the protection
of sites such as stadiums or large gathering places. We formulate the problem
as one of maximizing the capture of illegal contraband. The objective function
is indefinite and only limited information can be gained when the problem is
solved by standard convex optimization methods. In order to solve the model, we
develop a dynamic programming approach, and study its convergence properties.
Additionally, we formulate a version of the problem aimed at addressing
intelligent adversaries who can adjust their direction of attack as they
observe changes in the site security. Furthermore, we also develop a method for
the solution of the latter model. Finally, we perform computational experiments
to demonstrate the use of our methods.
","['\nTsvetan Asamov\n', '\nEmre Yamangil\n', '\nEndre Boros\n', '\nPaul Kantor\n', '\nFred Roberts\n']",,,http://arxiv.org/abs/2204.08961v1,cs.OH,"['cs.OH', 'math.OC']",,,[]
Exhaustive Survey of Rickrolling in Academic Literature,http://arxiv.org/abs/2204.06826v1,2022-04-14T08:53:53Z,2022-04-14T08:53:53Z,"  Rickrolling is an Internet cultural phenomenon born in the mid 2000s.
Originally confined to Internet fora, it has spread to other channels and
media. In this paper, we hypothesize that rickrolling has reached the formal
academic world. We design and conduct a systematic experiment to survey
rickrolling in the academic literature. As of March 2022, there are 23 academic
documents intentionally rickrolling the reader. Rickrolling happens in
footnotes, code listings, references. We believe that rickrolling in academia
proves inspiration and facetiousness, which is healthy for good science. This
original research suggests areas of improvement for academic search engines and
calls for more investigations about academic pranks and humor.
","['\nBenoit Baudry\n', '\nMartin Monperrus\n']","https://youtu.be/dQw4w9WgXcQ , Proceedings of SIGBOVIK, 2022",,http://dx.doi.org/10.48550/arXiv.2204.06826,cs.OH,"['cs.OH', 'cs.CY', 'cs.DL', 'cs.SE']",10.48550/arXiv.2204.06826,,[]
"Application of Stochastic Optimization Techniques to the Unit Commitment
  Problem -- A Review",http://arxiv.org/abs/2204.00922v1,2022-04-02T18:33:19Z,2022-04-02T18:33:19Z,"  Due to the established energy production methods contribution to the climate
crisis, renewable energy is to replace a substantial part of coal or nuclear
plants to prevent greenhouse gases or toxic waste entering the atmosphere. This
relatively quick shift in energy production, primarily pushed by increasing
political and economical pressure, requires enormous effort on the part of the
energy providers to balance out production fluctuations. Consequently, a lot of
research is conducted in the key area of stochastic unit commitment (UC) on
electrical grids and microgrids. The term unit commitment includes a large
variety of optimization techniques, and in this paper we will review recent
developments in this area. We start by giving an overview over different
problem definitions and stochastic optimization procedures, to then assess
recent contributions to this topic. We therefore compare the proposals and case
studies of several papers.
",['\nVincent Meilinger\n'],"Seminar thesis in the course ""Intelligent Software Systems"" at
  Technische Universit\""at Berlin",,http://arxiv.org/abs/2204.00922v1,cs.OH,['cs.OH'],,,[]
"A Validation Procedure for the Estimation of Reachable Regions in
  Football",http://arxiv.org/abs/2204.13992v1,2022-04-07T08:34:19Z,2022-04-07T08:34:19Z,"  Modelling the trajectorial motion of humans along the ground is a
foundational task in the quantitative analysis of sports like association
football. Most existing models of football player motion have not been
validated yet with respect to actual data. One of the reasons for this lack is
that performing such a validation is not straightforward, because models of
player motion are usually phrased in a way that emphasises possibly reachable
positions rather than expected positions. Since positional data of football
players typically contains outliers, this data may misrepresent the range of
actually reachable positions.
  This paper proposes a validation routine for trajectorial motion models that
measures and optimises the ability of a motion model to accurately predict all
possibly reachable positions by favoring the smallest predicted area of
reachable positions that encompasses all observed reached positions up to a
manually defined threshold. We demonstrate validation and optimisation on four
different motion models, assuming (a) motion with constant speed, (b) motion
with constant acceleration, (c) motion with constant acceleration with a speed
limit, and (d) motion along two segments with constant speed. Our results show
that assuming motion with constant speed or constant acceleration without a
limit on the achievable speed is particularly inappropriate for an accurate
distinction between reachable and unreachable locations. Motion along two
segments of constant speed provides by far the highest accuracy among the
tested models and serves as an efficient and accurate approximation of
real-world player motion.
","['\nM. Renkin\n', '\nJ. Bischofberger\n', '\nE. Schikuta\n', '\nA. Baca\n']","14 pages, 3 figures",,http://arxiv.org/abs/2204.13992v1,cs.OH,['cs.OH'],,,[]
"The Extended and Asymmetric Extended Krylov Subspace in
  Moment-Matching-Based Order Reduction of Large Circuit Models",http://arxiv.org/abs/2204.02467v1,2022-04-05T19:56:37Z,2022-04-05T19:56:37Z,"  The rapid growth of circuit complexity has rendered Model Order Reduction
(MOR) a key enabler for the efficient simulation of large circuit models. MOR
techniques based on moment-matching are well established due to their
simplicity and computational performance in the reduction process. However,
moment-matching methods based on the ordinary Krylov subspace are usually
inadequate to accurately approximate the original circuit behavior, and at the
same time do not produce reduced-order models as compact as needed. In this
paper, we present a moment-matching method which utilizes the extended and the
asymmetric extended Krylov subspace (EKS and AEKS), while it allows the
parallel computation of the transfer function in order to deal with circuits
that have many terminals. The proposed method can handle large-scale regular
and singular circuits and generate accurate and efficient reduced-order models
for circuit simulation. Experimental results on industrial IBM power grids
demonstrate that the EKS method can achieve an error reduction up to 85.28%
over a standard Krylov subspace method, while the AEKS method greatly reduces
the runtime of EKS, introducing a negligible overhead in the reduction error.
","['\nPavlos Stoikos\n', '\nDimitrios Garyfallou\n', '\nGeorge Floros\n', '\nNestor Evmorfopoulos\n', '\nGeorge Stamoulis\n']",arXiv admin note: substantial text overlap with arXiv:2007.01948,,http://arxiv.org/abs/2204.02467v1,cs.OH,"['cs.OH', 'cs.NA', 'math.NA']",,,[]
Quantum Fault Trees,http://arxiv.org/abs/2204.10877v1,2022-04-10T17:45:38Z,2022-04-10T17:45:38Z,"  Fault tree analysis is a technique widely used in risk and reliability
analysis of complex engineering systems given its deductive nature and
relatively simple interpretation. In a fault tree, events are usually
represented by a binary variable that indicates whether an event occurs or not,
traditionally associated with the values 1 and 0, respectively. Different
events are linked together using logical gates, modelling the dependencies that
a subsystem or system may have over its basic components. In this study,
quantum computing is leveraged to propose a novel approach to encode a
traditional fault tree into a quantum algorithm. This quantum fault tree method
uses quantum bits to represent basic events, effectively encoding the original
fault tree into a quantum circuit. The execution of the resulting quantum
circuit represents a full simulation of the fault tree, and multiple executions
can be utilized to compute the failure probability of the whole system. The
proposed approach is tested on a case study portraying a dynamic positioning
system. Results verify that the quantum-based proposed approach is able to
effectively obtain the dynamic positioning failure probability through
simulation, opening promising opportunities for future investigations in the
area.
","['\nGabriel San Martin Silva\n', '\nTarannom Parhizkar\n', '\nEnrique Lopez Droguett\n']","12 pages, 7 figures, 2 tables",,http://arxiv.org/abs/2204.10877v1,cs.OH,"['cs.OH', 'cs.SY', 'eess.SY']",,,[]
"Computational Language $ β$ based on Orthomodular Lattices with the
  Non-distributivity of Quantum Logic",http://arxiv.org/abs/2203.12385v3,2022-03-23T13:02:11Z,2023-04-03T03:11:34Z,"  It is argued that transformation processes (generation rules) showing
evidence of a long evolutionary history in universal computing systems can be
generalized. The explicit function class $ \Omega $ is defined as follows:
""Operators whose eigenvectors (or eigenvalues) have an irrational number in
their components constitute a class of functions with quasi-periodic structure,
$ \Omega $, and the class $ \Omega $ shows evidence of a long evolutionary
history."" In order to empirically prove this theorem by examining physical
systems carrying out life activities or intellectual outputs of developed
intelligence, the basic framework of the universal machine model C and the
computational language $ \beta $ is presented as a model for general
computational methods, which allow transformation processes (generation rules)
with deep algorithmic complexity to be derived from generation results. C and $
\beta $ perform massively parallel computations on event-state systems
consisting of exponential combinations of propositional elements expressed in
terms of correlations between subsystems. The logical structure of the
computational language relies on a non-distributivity in Hilbert spaces or
orthogonal modular lattices, allowing for the manipulation and deduction of
simultaneous propositions. In this logical local structure, the propositions
implying certain consequences are not uniquely determined.
",['\nKazuki Otsuka\n'],,,http://arxiv.org/abs/2203.12385v3,cs.OH,['cs.OH'],,,[]
Building AI Innovation Labs together with Companies,http://arxiv.org/abs/2203.08465v1,2022-03-16T08:45:52Z,2022-03-16T08:45:52Z,"  In the future, most companies will be confronted with the topic of Artificial
Intelligence (AI) and will have to decide on their strategy in this regards.
Currently, a lot of companies are thinking about whether and how AI and the
usage of data will impact their business model and what potential use cases
could look like. One of the biggest challenges lies in coming up with
innovative solution ideas with a clear business value. This requires business
competencies on the one hand and technical competencies in AI and data
analytics on the other hand. In this article, we present the concept of AI
innovation labs and demonstrate a comprehensive framework, from coming up with
the right ideas to incrementally implementing and evaluating them regarding
their business value and their feasibility based on a company's capabilities.
The concept is the result of nine years of working on data-driven innovations
with companies from various domains. Furthermore, we share some lessons learned
from its practical applications. Even though a lot of technical publications
can be found in the literature regarding the development of AI models and many
consultancy companies provide corresponding services for building AI
innovations, we found very few publications sharing details about what an
end-to-end framework could look like.
","['\nJens Heidrich\n', '\nAndreas Jedlitschka\n', '\nAdam Trendowicz\n', '\nAnna Maria Vollmer\n']",,,http://arxiv.org/abs/2203.08465v1,cs.OH,"['cs.OH', 'cs.AI', '68N30', 'I.2.0; D.2.1']",,,[]
"Generic Lithography Modeling with Dual-band Optics-Inspired Neural
  Networks",http://arxiv.org/abs/2203.08616v1,2022-03-12T08:08:50Z,2022-03-12T08:08:50Z,"  Lithography simulation is a critical step in VLSI design and optimization for
manufacturability. Existing solutions for highly accurate lithography
simulation with rigorous models are computationally expensive and slow, even
when equipped with various approximation techniques. Recently, machine learning
has provided alternative solutions for lithography simulation tasks such as
coarse-grained edge placement error regression and complete contour prediction.
However, the impact of these learning-based methods has been limited due to
restrictive usage scenarios or low simulation accuracy. To tackle these
concerns, we introduce an dual-band optics-inspired neural network design that
considers the optical physics underlying lithography. To the best of our
knowledge, our approach yields the first published via/metal layer contour
simulation at 1nm^2/pixel resolution with any tile size. Compared to previous
machine learning based solutions, we demonstrate that our framework can be
trained much faster and offers a significant improvement on efficiency and
image quality with 20X smaller model size. We also achieve 85X simulation
speedup over traditional lithography simulator with 1% accuracy loss.
","['\nHaoyu Yang\n', '\nZongyi Li\n', '\nKumara Sastry\n', '\nSaumyadip Mukhopadhyay\n', '\nMark Kilgard\n', '\nAnima Anandkumar\n', '\nBrucek Khailany\n', '\nVivek Singh\n', '\nHaoxing Ren\n']","9 pages, 9 figures; accepted at 59th Design Automation Conference",,http://arxiv.org/abs/2203.08616v1,cs.OH,"['cs.OH', 'cs.LG']",,,[]
"Accuracy of Garmin GPS Running Watches over Repetitive Trials on the
  Same Route",http://arxiv.org/abs/2203.00491v1,2022-03-01T14:39:21Z,2022-03-01T14:39:21Z,"  Many runners use watches incorporating Global Positioning System technology
to track their workouts. These devices can be valuable training aids, but they
have limitations. For several reasons including variations in satellite
position, environmental factors, and design decisions made by the manufacturer,
GPS-enabled watches can produce position measurement errors. These can result
in incorrect estimations of total distance covered as well as running pace.
This study examined the accuracy of three Garmin running watches of different
technological generations using repetitive trials, over several years, by the
same runner over the same route. The older watches, a Forerunner 205 and a
Forerunner 220, showed similar accuracy when traversing the route. The newer
generation watch, a Forerunner 45S, was found to be significantly less accurate
in terms of both the trueness and precision of its distance measurements. This
may indicate that Garmin, in competition with other manufacturers of similar
devices, has chosen in recent years to prioritize device miniaturization and
battery life over accuracy.
",['\nJoe Dumas\n'],,"International Journal of Computer Science and Information
  Technology, Volume 14, Number 1, February 2022",http://arxiv.org/abs/2203.00491v1,cs.OH,['cs.OH'],,,[]
"Innovating at Speed and at Scale: A Next Generation Infrastructure for
  Accelerating Semiconductor Technologies",http://arxiv.org/abs/2204.02216v1,2022-03-07T12:16:44Z,2022-03-07T12:16:44Z,"  Semiconductor innovation drives improvements to technologies that are
critical to modern society. The country that successfully accelerates
semiconductor innovation is positioned to lead future semiconductor-driven
industries and benefit from the resulting economic growth. It is our view that
a next generation infrastructure is necessary to accelerate and enhance
semiconductor innovation in the U.S. In this paper, we propose such an advanced
infrastructure composed of a national network of facilities with enhancements
in technology and business models. These enhancements enable application-driven
and challenge-based research and development, and ensure that facilities are
accessible and sustainable. The main tenets are: a challenge-driven operational
model, a next-generation infrastructure to serve that operational model,
technology innovations needed for advanced facilities to speed up learning
cycles, and innovative cost-effective business models for sustainability.
Ultimately, the expected outcomes of such a participatory, scalable, and
sustainable nation-level advanced infrastructure will have tremendous impact on
government, industry, and academia alike.
","['\nRichard A. Gottscho\n', '\nEdlyn V. Levine\n', '\nTsu-Jae King Liu\n', '\nPaul C. McIntyre\n', '\nSubhasish Mitra\n', '\nBoris Murmann\n', '\nJan M. Rabaey\n', '\nSayeef Salahuddin\n', '\nWilly C. Shih\n', '\nH. -S. Philip Wong\n']",,,http://arxiv.org/abs/2204.02216v1,cs.OH,['cs.OH'],,,[]
"Defining a synthetic data generator for realistic electric vehicle
  charging sessions",http://arxiv.org/abs/2203.01129v1,2022-02-28T11:18:40Z,2022-02-28T11:18:40Z,"  Electric vehicle (EV) charging stations have become prominent in electricity
grids in the past years. Analysis of EV charging sessions is useful for
flexibility analysis, load balancing, offering incentives to customers, etc.
Yet, the limited availability of such EV sessions data hinders further
development in these fields. Addressing this need for publicly available and
realistic data, we develop a synthetic data generator (SDG) for EV charging
sessions. Our SDG assumes the EV inter-arrival time to follow an exponential
distribution. Departure times are modeled by defining a conditional probability
density function (pdf) for connection times. This pdf for connection time and
required energy is fitted by Gaussian mixture models. Since we train our SDG
using a large real-world dataset, its output is realistic.
","['\nManu Lahariya\n', '\nDries Benoit\n', '\nChris Develder\n']",,"e-Energy 20 Proceedings of the Eleventh ACM International
  Conference on Future Energy Systems June 2020 Pages 406 407",http://dx.doi.org/10.1145/3396851.3403509,cs.OH,"['cs.OH', 'cs.LG']",10.1145/3396851.3403509,,[]
"Heterogeneous Federated CubeSat System: problems, constraints and
  capabilities",http://arxiv.org/abs/2203.14721v1,2022-03-08T13:14:31Z,2022-03-08T13:14:31Z,"  Different arguments were being presented in the last decade about CubeSats
and their applications. Some of them address wireless communication (5G and 6G
technologies) trying to achieve better characteristics as coverage and
connectivity. Some arrived with terms as IoST (Internet of Space Things),
Internet of Satellites (IoSat), DSS (Distributed Space Systems), and FSS
(Federated Satellite Systems). All of them aim to use Small/NanoSatellites as
constellations/swarms is to provide specific services, share unused resources,
and evolve the concept of satellites-as-a-service (SaS). This paper aims to
emophasize performance attributes of such cyber-physical systems, model their
inherent operational constraints and at the very end, evaluate the quality of
service in terms of figures of merit for the entering/leaving of new
heterogeneous constituent systems, a.k.a satellites, to the constellation. This
""whitepaper""-styled work focuses on presenting the definitions of this
heterogeneous constellation problem, aims at its main capabilities and
constraints, and proposes modeling approaches for this system representation
and evaluation.
","['\nCarlos L G Batista\n', '\nFatima Mattiello-Francisco\n', '\nAndras Pataricza\n']","4 pages, 4 figures",,http://arxiv.org/abs/2203.14721v1,cs.DC,"['cs.DC', 'cs.OH']",,,[]
"The Theory, Practice, and Ethical Challenges of Designing a
  Diversity-Aware Platform for Social Relations",http://arxiv.org/abs/2202.12700v1,2022-02-23T22:31:56Z,2022-02-23T22:31:56Z,"  Diversity-aware platform design is a paradigm that responds to the ethical
challenges of existing social media platforms. Available platforms have been
criticized for minimizing users' autonomy, marginalizing minorities, and
exploiting users' data for profit maximization. This paper presents a design
solution that centers the well-being of users. It presents the theory and
practice of designing a diversity-aware platform for social relations. In this
approach, the diversity of users is leveraged in a way that allows like-minded
individuals to pursue similar interests or diverse individuals to complement
each other in a complex activity. The end users of the envisioned platform are
students, who participate in the design process. Diversity-aware platform
design involves numerous steps, of which two are highlighted in this paper: 1)
defining a framework and operationalizing the ""diversity"" of students, 2)
collecting ""diversity"" data to build diversity-aware algorithms. The paper
further reflects on the ethical challenges encountered during the design of a
diversity-aware platform.
","['\nLaura Schelenz\n', '\nIvano Bison\n', '\nMatteo Busso\n', '\nAmalia de Götzen\n', '\nDaniel Gatica-Perez\n', '\nFausto Giunchiglia\n', '\nLakmal Meegahapola\n', '\nSalvador Ruiz-Correa\n']","AAAI/ACM Conference on AI, Ethics, and Society (AIES) 2021",,http://dx.doi.org/10.1145/3461702.3462595,cs.HC,"['cs.HC', 'cs.OH']",10.1145/3461702.3462595,,[]
Arithmetic logical Irreversibility and the Turing's Halt Problem,http://arxiv.org/abs/2204.00563v4,2022-02-14T07:36:17Z,2023-03-25T12:49:23Z,"  The Turing machine halting problem can be explained by several factors,
including arithmetic logic irreversibility and memory erasure, which contribute
to computational uncertainty due to information loss during computation.
Essentially, this means that an algorithm can only preserve information about
an input, rather than generate new information. This uncertainty arises from
characteristics such as arithmetic logical irreversibility, Landauer's
principle, and memory erasure, which ultimately lead to a loss of information
and an increase in entropy. To measure this uncertainty and loss of
information, the concept of arithmetic logical entropy can be used. The Turing
machine and its equivalent, general recursive functions can be understood
through the {\lambda} calculus and the Turing/Church thesis. However, there are
certain recursive functions that cannot be fully understood or predicted by
other algorithms due to the loss of information during logical-arithmetic
operations. In other words, the behaviour of these functions cannot be
completely determined at every stage of the computation due to a lack of
information in their definition. While there are some cases where the behaviour
of recursive functions is highly predictable, the lack of information in most
cases makes it impossible for algorithms to determine if a program will halt or
not. This inability to predict the outcome of the computation is the essence of
the halting problem of the Turing machine. Even in cases where more information
is available about the program, it is still difficult to determine with
certainty if the program will halt or not. This also highlights the importance
of the Turing oracle machine, which introduces information from outside the
computation to compensate for the lack of information and ultimately decide the
result of the computation.
",['\nYair Lapin\n'],"New version , thesis fixed due to some problems in the original work.
  This is different version of the original thesis",,http://arxiv.org/abs/2204.00563v4,cs.OH,['cs.OH'],,,[]
Uncertainty in fMRI Functional Networks of Autism Brain Imaging Data,http://arxiv.org/abs/2202.02228v1,2022-02-04T16:53:25Z,2022-02-04T16:53:25Z,"  In this paper we review the preprocessing pipeline through which fMRI data is
transformed into a network. We discuss three parameters that mostly affect our
understanding of the existence of functional correlations between the brain
regions. In the end, we conclude that the existence of functional correlations
between pairs of the brain's regions can be modeled with probabilistic edges,
not to lose the uncertainty that is inherent in the network generation process.
","['\nAmin Kaveh\n', '\nMatteo Magnani\n', '\nChristian Rohner\n']",,,http://arxiv.org/abs/2202.02228v1,q-bio.NC,"['q-bio.NC', 'cs.OH']",,,[]
"Interactive Process Improvement using Simulation of Enriched Process
  Trees",http://arxiv.org/abs/2201.07755v1,2022-01-18T12:54:15Z,2022-01-18T12:54:15Z,"  Event data provide the main source of information for analyzing and improving
processes in organizations. Process mining techniques capture the state of
running processes w.r.t. various aspects, such as activity-flow and performance
metrics. The next step for process owners is to take the provided insights and
turn them into actions in order to improve their processes. These actions may
be taken in different aspects of a process. However, simply being aware of the
process aspects that need to be improved as well as potential actions is
insufficient. The key step in between is to assess the outcomes of the
decisions and improvements. In this paper, we propose a framework to
systematically compare event data and the simulated event data of
organizations, as well as comparing the results of modified processes in
different settings. The proposed framework could be provided as an analytic
service to enable organizations in easily accessing event data analytics. The
framework is supported with a simulation tool that enables applying changes to
the processes and re-running the process in various scenarios. The simulation
step includes different perspectives of a process that can be captured
automatically and modified by the user. Then, we apply a state-of-the-art
comparison approach for processes using their event data which visually
reflects the effects of these changes in the process, i.e., evaluating the
process improvement. Our framework also includes the implementation of the
change measurement module as a tool.
","['\nMahsa Pourbafrani\n', '\nWil M. P. van der Aalst\n']",,,http://arxiv.org/abs/2201.07755v1,cs.OH,['cs.OH'],,,[]
A Brief Analysis of the Apollo Guidance Computer,http://arxiv.org/abs/2201.08230v1,2022-01-06T18:18:41Z,2022-01-06T18:18:41Z,"  The AGC was designed with the sole purpose of providing navigational guidance
and spacecraft control during the Apollo program throughout the 1960s and early
1970s. The AGC sported 72kb of ROM, 4kb of RAM, and a whopping 14,245 FLOPS,
roughly 30 million times fewer than the computer this report is being written
on. These limitations are what make the AGC so interesting, as its programmers
had to ration each individual word of memory due to the bulk of memory
technology of the time. Despite these limitations (or perhaps due to them), the
AGC was highly optimized, and arguably the most advanced computer of its time,
as its computational power was only matched in the late 1970s by computers like
the Apple II. It is safe to say that the AGC had no intended market, and was
explicitly designed to enhance control of the Apollo Command Module and Apollo
Lunar Module. The AGC was not entirely internal to NASA, however, and was
designed in MIT's Instrumentation Laboratory, and manufactured by Raytheon, a
weapons and defense contractor.
",['\nCharles Averill\n'],,,http://arxiv.org/abs/2201.08230v1,cs.OH,"['cs.OH', 'astro-ph.IM']",,,[]
Dimensional Complexity and Algorithmic Efficiency,http://arxiv.org/abs/2201.05050v3,2021-12-24T19:44:09Z,2022-03-11T15:57:27Z,"  This paper uses the concept of algorithmic efficiency to present a unified
theory of intelligence. Intelligence is defined informally, formally, and
computationally. We introduce the concept of Dimensional complexity in
algorithmic efficiency and deduce that an optimally efficient algorithm has
zero Time complexity, zero Space complexity, and an infinite Dimensional
complexity. This algorithm is used to generate the number line.
",['\nAlexander Ngu\n'],"10 pages, 7 figures, 1 table","International Journal of Modern Nonlinear Theory and Application,
  11, 1-10 (2022)",http://dx.doi.org/10.4236/ijmnta.2022.111001,cs.OH,"['cs.OH', 'cs.LO']",10.4236/ijmnta.2022.111001,,[]
Smart Support for Mission Success,http://arxiv.org/abs/2112.04957v1,2021-12-08T08:07:48Z,2021-12-08T08:07:48Z,"  Today's battlefield environment is complex, dynamic and uncertain, and
requires efficient support to ensure mission success. This relies on a proper
support strategy to provide supported equipment able to fulfill the mission. In
the context of defense where both systems and organization are complex, having
a holistic approach is challenging by nature, forces and support agencies need
to rely on an efficient decision support system. Logistics, readiness and
sustainability are critical factors for asset management, which can benefit
from AI to reach ""Smart In Service"" level relying especially on predictive and
prescriptive approaches and on effective management of operational re-sources.
Smart Support capacities can be then monitored by appropriate metrics and
improved by multi-criteria decision support and knowledge management system.
Depending on the operational context in terms of information and the objective,
different AI paradigms (data-driven AI, knowledge-based AI) are suitable even a
combination through hybrid AI.
","['\nJuliette Mattioli\n', '\nPierre-Olivier Robic\n']","8 pages, 2 figures",,http://arxiv.org/abs/2112.04957v1,cs.OH,"['cs.OH', 'cs.AI']",,,[]
"A Two-Level Approximate Logic Synthesis Combining Cube Insertion and
  Removal",http://arxiv.org/abs/2112.00621v1,2021-11-22T00:37:50Z,2021-11-22T00:37:50Z,"  Approximate computing is an attractive paradigm for reducing the design
complexity of error-resilient systems, therefore improving performance and
saving power consumption. In this work, we propose a new two-level approximate
logic synthesis method based on cube insertion and removal procedures.
Experimental results have shown significant literal count and runtime reduction
compared to the state-of-the-art approach. The method scalability is
illustrated for a high error threshold over large benchmark circuits. The
obtained solutions have presented a literal number reduction up to 38%, 56% and
93% with respect to an error rate of 1%, 3% and 5%, respectively.
","['\nGabriel Ammes\n', '\nWalter Lau Neto\n', '\nPaulo Butzen\n', '\nPierre-Emmanuel Gaillardon\n', '\nRenato P. Ribas\n']","5 Pages, submitted to IEEE Transactions on Computer-Aided Design of
  Integrated Circuits and Systems",,http://dx.doi.org/10.1109/TCAD.2022.3143489,cs.OH,"['cs.OH', 'cs.AR', 'cs.LO']",10.1109/TCAD.2022.3143489,,[]
Piano Fingering with Reinforcement Learning,http://arxiv.org/abs/2111.08009v1,2021-11-15T09:51:29Z,2021-11-15T09:51:29Z,"  Hand and finger movements are a mainstay of piano technique. Automatic
Fingering from symbolic music data allows us to simulate finger and hand
movements. Previous proposals achieve automatic piano fingering based on
knowledge-driven or data-driven techniques. We combine both approaches with
deep reinforcement learning techniques to derive piano fingering. Finally, we
explore how to incorporate past experience into reinforcement learning-based
piano fingering in further work.
","['\nPedro Ramoneda\n', '\nMarius Miron\n', '\nXavier Serra\n']",,,http://arxiv.org/abs/2111.08009v1,cs.OH,['cs.OH'],,,[]
A Simple Hybrid Model for Accurate Delay Modeling of a Multi-Input Gate,http://arxiv.org/abs/2111.11182v1,2021-11-16T10:19:58Z,2021-11-16T10:19:58Z,"  Faithfully representing small gate delay variations caused by input
switchings on different inputs in close temporal proximity is a very
challenging task for digital delay models. In this paper, we use the example of
a 2-input NOR gate to show that a simple hybrid model leads to a surprisingly
accurate digital delay model. Our model utilizes simple first-order ordinary
differential equations (ODEs) in all modes, resulting from considering
transistors as ideal switches in a simple RC model of the gate. By analytically
solving the resulting ODEs, we derive expressions for the gate delays, as well
as formulas that facilitate model parametrization. It turns out that our model
almost faithfully captures the Charlie effect, except in just one specific
situation. In addition, we experimentally compare our model's predictions both
to SPICE simulations, using some 15 nm technology, and to some existing delay
models. Our results show a significant improvement of the achievable modeling
accuracy.
","['\nArman Ferdowsi\n', '\nJürgen Maier\n', '\nDaniel Öhlinger\n', '\nUlrich Schmid\n']","9 pages, 14 figures, accepted for DATE'22",,http://arxiv.org/abs/2111.11182v1,cs.OH,"['cs.OH', 'cs.AR', 'B.5.2; B.6.3; B.8.2']",,,[]
"Proxy System with JPEG Bitstream-Based File-Size Preserving Encryption
  for Cloud Photo Streams",http://arxiv.org/abs/2201.03469v1,2021-11-04T09:24:24Z,2021-11-04T09:24:24Z,"  In this paper, we propose a proxy system with JPEG bitstream-based file-size
preserving encryption to securely store compressed images in cloud
environments. The proposed system, which is settled between client's device and
the Internet, allows us not only to have exact the same file size as that of
original JPEG streams but also to maintain a predetermined image format. In an
experiment, the proposed system is verified to be effective in two cloud photo
steams: Google Photo and iCloud Photo.
","['\nHiroyuki Kobayashi\n', '\nHitoshi Kiya\n']","to appear in 2022 International Workshop on Advanced Image Technology
  (IWAIT)",,http://arxiv.org/abs/2201.03469v1,cs.OH,['cs.OH'],,,[]
"Easy and structured approach for software and firmware co-simulation for
  bus centric designs",http://arxiv.org/abs/2110.10447v1,2021-10-20T09:25:47Z,2021-10-20T09:25:47Z,"  Although software and firmware co-simulation is gaining popularity, it is
still not widely used in the FPGA designs. This work presents easy and
structured approach for software and firmware co-simulation for bus centric
designs. The proposed approach is very modular and software language agnostic.
The only requirement is that the firmware design is accessible via some kind of
system bus. The concept has been used for testing DAQ system being developed
for high energy physics experiment.
",['\nMichał Kruszewski\n'],,,http://arxiv.org/abs/2110.10447v1,cs.OH,['cs.OH'],,,[]
Towards Grassroots Peering at the Edge,http://arxiv.org/abs/2201.03462v1,2021-10-20T13:34:14Z,2021-10-20T13:34:14Z,"  Fog Computing allows applications to address their latency and privacy
requirements while coping with bandwidth limitations of Internet service
providers (ISPs). Existing research on fog systems has so far mostly taken a
very high-level view on the actual fog infrastructure. In this position paper,
we identify and discuss the problem of having multiple ISPs in edge-to-edge
communication. As a possible solution we propose that edge operators create
direct edge-to-edge links in a grassroots fashion and discuss different
implementation options. Based on this, we highlight some important open
research challenges that result from this.
","['\nDavid Bermbach\n', '\nSergio Lucia\n', '\nVlado Handziski\n', '\nAdam Wolisz\n']",accepted for publication in ACM M4IoT 2021,,http://arxiv.org/abs/2201.03462v1,cs.OH,['cs.OH'],,,[]
"Brilliant Challenges Optimization Problem Submission Contest Final
  Report",http://arxiv.org/abs/2110.04916v1,2021-10-10T22:25:42Z,2021-10-10T22:25:42Z,"  This paper concludes the Brilliant Challenges contest. Participants had to
design interesting optimization problems and publish them using the Optil.io
platform. It was the first widely-advertised contest in the area of operational
research where the objective was to submit the problem definition instead of
the algorithmic solutions. Thus, it is a crucial contribution to Open Science
and the application of crowdsourcing methodology to solve discrete optimization
problems. The paper briefly describes submitted problems, presents the winners,
and discusses the contest's achievements and shortcomings. Finally, we define
guidelines supporting the organization of contests of similar type in the
future.
","['\nJan Badura\n', '\nArtur Laskowski\n', '\nMaciej Antczak\n', '\nJacek Blazewicz\n', '\nGrzegorz Pawlak\n', '\nErwin Pesch\n', '\nThomas Villmann\n', '\nSzymon Wasik\n']",,,http://arxiv.org/abs/2110.04916v1,cs.OH,['cs.OH'],,,[]
NetSD: Remote Access to Integrated SD Cards of Embedded Devices,http://arxiv.org/abs/2109.15322v1,2021-09-29T12:50:46Z,2021-09-29T12:50:46Z,"  Digitalization continuously pervades all areas and the Internet of Things
(IoT) is still on the rise. This leads to an increased need for efficiency in
the development of embedded devices and systems composed thereof. Hybrid
testbeds are common environments to representatively assess, e.g.,
hardware-software interaction, interoperability, and scalability. Although
automation is inevitable to achieve efficiency, not all devices offer
interfaces to be fully software-controlled. Most notably, block devices tend to
be inaccessible for software outside a Device under Test (DuT), especially when
the latter is in a dysfunctional state.
  This paper introduces the Networked SD card (NetSD) which enables remote
access to removable block devices. The proposed system consists of a hardware
part, which enables multiplexed access to a block device (e.g., an SD card) and
a software part which enables remote access to the block device (e.g., via HTTP
or network block device). NetSD thus adds testing and automation possibilities
to DuTs without the need to modify their hard- or software. During the hardware
design, we fund that different SD transfer modes and access profiles (read or
write focus) benefit from different pull-up resistor configurations for the
data lines.
","['\nValentin Schröter\n', '\nArne Boockmeyer\n', '\nLukas Pirl\n']","to be presented at the 1st International Workshop on Testing
  Distributed Internet of Things Systems; associated implementation code can be
  found at https://gitlab.com/hpi-potsdam/osm/netsd",,http://arxiv.org/abs/2109.15322v1,cs.OH,['cs.OH'],,,[]
"The Turing machine of a harmonic oscillator: from the code to the
  dynamic system",http://arxiv.org/abs/2110.06119v1,2021-10-01T08:58:28Z,2021-10-01T08:58:28Z,"  In this work we consider a dynamic system consisting of a damped harmonic
oscillator and we formalize a Turing Machine whose definition in terms of
states, alphabet and transition rules, can be considered equivalent to that of
the oscillator. We prove that the Turing Machine of a FOR loop corresponds to
that of the oscillator and we ask ourselves if it is possible to obtain the
dynamic system of the harmonic oscillator as a physical realization of the FOR
loop. We discuss the relationship between the results found and the science of
Can and Can't. We discuss the possibility of an evolution of computer science
also towards non-computerized specialized machines whose operating principle is
designed as an automatic process starting from a source code instead of as a
work of human ingenuity. The approach to the implementation of algorithms in
dynamic systems instead of universal computers can be particularly interesting
for the field of both diagnostic and implantable medical devices.
","['\nFrancesco Sisini\n', '\nValentina Sisini\n']","10 pages, 2 figures",,http://arxiv.org/abs/2110.06119v1,cs.OH,"['cs.OH', 'physics.pop-ph', 'F.m']",,,[]
"Findings of the 2nd Photonics and Electronics Technology for Extreme
  Scale Computing Workgroup (REPETE): Design Challenges for Socket Level
  Photonic I/O",http://arxiv.org/abs/2110.11759v1,2021-10-04T16:57:29Z,2021-10-04T16:57:29Z,"  To inform research activities in HPC interconnect of strategic importance to
the USG beyond 2018, in January of 2018, the DoD sponsored the 2nd Photonics
and Electronics Technology for Extreme-scale Computing (REPETE) workgroup.
REPETE investigated new challenges in the area of HPC interconnect inspired by
technology trends and challenges of vital importance to USG stakeholders.
  The REPETE Working group investigated two focus areas of interest to the USG:
Socket Level Photonic IO and Cryogenic Photonic IO. The working group spanned
industry, academia, and government, in research, development, product, and
technology investment areas.
  The workgroup team focusing on current and future design challenges for
socket level photonic IO began discussing technical challenges and current
state of applying photonics to off-chip IO in April of 2019 through biweekly
meetings that concluded in early September of 2019. The focus of these meetings
was to discuss what technology exists for moving the conversion of electrical
signaling to photonic signaling from the node (as is currently done) down to
within the socket for off-chip IO.
  Areas discussed include: I/O Requirements and Trends at the Compute Socket
for 2025 and Beyond; Current and Near-Term Copper Solutions for Off-Chip,
Socket-Level Interconnect; Current Photonic Solutions for Off-Chip or
Socket-Level Interconnect; Light Generation for Off-Chip, Socket-Level IO;
Fabrication and Packaging of Photonic Integrated Circuits; All-Photonic
Switching Technology; and Simulation of Photonic Interconnects.
","['\nKaren Grutter\n', '\nTom Salter\n', '\nTim Horton\n']",,,http://arxiv.org/abs/2110.11759v1,cs.OH,"['cs.OH', 'cs.ET']",,,[]
Smart Grids Co-Simulations: Survey & Research Directions,http://arxiv.org/abs/2109.02349v1,2021-09-06T10:38:54Z,2021-09-06T10:38:54Z,"  The integration of renewable sources, communication and power networks with
information and communication technologies is one of the main challenges in
Smart Grids (SG) large-scale testing. For this reason, the coupling of
simulators is commonly used to dynamically simulate several aspects of the SG
infrastructure, in the so-called co-simulations. In this paper, we provide a
scoping review of research of co-simulations in the context of Smart Grids: i)
research areas and research problems addressed by co-simulations, ii) specific
co-simulation aspects focus of research, iii) typical coupling of simulators in
co-simulation studies. Based on the results, we discuss research directions of
future SG co-simulation research in each of the identified areas.
","['\nPeter Mihal\n', '\nMartin Schvarcbacher\n', '\nBruno Rossi\n', '\nTomáš Pitner\n']",,,http://arxiv.org/abs/2109.02349v1,cs.OH,['cs.OH'],,,[]
Quantifying Intrinsic Value of Information of Trajectories,http://arxiv.org/abs/2108.12450v2,2021-08-27T18:24:05Z,2021-09-08T00:30:15Z,"  A trajectory, defined as a sequence of location measurements, contains
valuable information about movements of an individual. Its value of information
(VOI) may change depending on the specific application. However, in a variety
of applications, knowing the intrinsic VOI of a trajectory is important to
guide other subsequent tasks or decisions. This work aims to find a principled
framework to quantify the intrinsic VOI of trajectories from the owner's
perspective. This is a challenging problem because an appropriate framework
needs to take into account various characteristics of the trajectory, prior
knowledge, and different types of trajectory degradation. We propose a
framework based on information gain (IG) as a principled approach to solve this
problem. Our IG framework transforms a trajectory with discrete-time
measurements to a canonical representation, i.e., continuous in time with
continuous mean and variance estimates, and then quantifies the reduction of
uncertainty about the locations of the owner over a period of time as the VOI
of the trajectory. Qualitative and extensive quantitative evaluation show that
the IG framework is capable of effectively capturing important characteristics
contributing to the VOI of trajectories.
","['\nKien Nguyen\n', '\nJohn Krumm\n', '\nCyrus Shahabi\n']","10 pages, SIGSPATIAL'21",,http://arxiv.org/abs/2108.12450v2,cs.OH,['cs.OH'],,,[]
A New Rational Approach to the Square Root of 5,http://arxiv.org/abs/2108.13110v3,2021-08-30T10:35:36Z,2021-09-07T14:25:01Z,"  In this paper, authors construct a new type of sequence which is named an
extra-super increasing sequence, and give the definitions of the minimal super
increasing sequence {a[0], a[1], ..., a[n]} and minimal extra-super increasing
sequence {z[0], z[1], ..., z[n]}. Find that there always exists a fit n which
makes (z[n] / z[n-1] - a[n] / a[n-1])= PHI, where PHI is the golden ratio
conjugate with a finite precision in the range of computer expression. Further,
derive the formula radic(5) = 2(z[n] / z[n-1] - a[n] / a[n-1]) + 1, where n
corresponds to the demanded precision. Experiments demonstrate that the
approach to radic(5) through a term ratio difference is more smooth and
expeditious than through a Taylor power series, and convince the authors that
lim(n to infinity) (z[n] / z[n-1] - a[n] / a[n-1]) = PHI holds.
","['\nShenghui Su\n', '\nJianhua Zheng\n', '\nShuwang Lv\n']",6 pages,,http://arxiv.org/abs/2108.13110v3,cs.OH,['cs.OH'],,,[]
An Indoor Crowd Movement Trajectory Benchmark Dataset,http://arxiv.org/abs/2109.01091v1,2021-08-31T01:45:43Z,2021-08-31T01:45:43Z,"  In recent years, technologies of indoor crowd positioning and movement data
analysis have received widespread attention in the fields of reliability
management, indoor navigation, and crowd behavior monitoring. However, only a
few indoor crowd movement trajectory datasets are available to the public, thus
restricting the development of related research and application. This paper
contributes a new benchmark dataset of indoor crowd movement trajectories. This
dataset records the movements of over 5000 participants at a three day large
academic conference in a two story indoor venue. The conference comprises
varied activities, such as academic seminars, business exhibitions, a hacking
contest, interviews, tea breaks, and a banquet. The participants are divided
into seven types according to participation permission to the activities. Some
of them are involved in anomalous events, such as loss of items, unauthorized
accesses, and equipment failures, forming a variety of spatial temporal
movement patterns. In this paper, we first introduce the scenario design,
entity and behavior modeling, and data generator of the dataset. Then, a
detailed ground truth of the dataset is presented. Finally, we describe the
process and experience of applying the dataset to the contest of ChinaVis Data
Challenge 2019. Evaluation results of the 75 contest entries and the feedback
from 359 contestants demonstrate that the dataset has satisfactory
completeness, and usability, and can effectively identify the performance of
methods, technologies, and systems for indoor trajectory analysis.
","['\nYing Zhao\n', '\nXin Zhao\n', '\nSiming Chen\n', '\nZhuo Zhang\n', '\nXin Huang\n']",,,http://dx.doi.org/10.1109/TR.2021.3109122,cs.OH,['cs.OH'],10.1109/TR.2021.3109122,,[]
"Research on Brick Schema Representation for Building Operation with
  Variable Refrigerant Flow Systems",http://arxiv.org/abs/2108.07037v2,2021-08-16T11:59:41Z,2021-10-15T09:04:01Z,"  Building metadata is regarded as the signpost in organizing massive building
data. The application of building metadata simplifies the creation of digital
representations and provides portable data analytics. Typical metadata
standards such as Brick and Haystack are used to describe the data of the
building system. Brick uses standard ontologies to create building metadata.
However, neither Haystack nor Brick has provided definitions about the Variable
Refrigerant Flow (VRF) system so far. For years, both Brick and Haystack
working groups have been discussing how to describe VRF in their schema, mainly
about the classification of VRF and the definitions of VRF units. There were no
settled solutions for these problems. Meanwhile, the global VRF market is
growing increasingly fast because of the energy efficiency and installation
simplicity of the VRF system. It is needed to have the metadata to describe VRF
units in buildings for data analysis and management. Addressing this challenge,
this paper extended Brick Schema with the VRF module and verified the Brick VRF
module. Then, the model and the service framework were developed and applied
for a building in China. The framework can serve portable energy analysis for
different areas. The VRF module of this paper provides a possible solution for
the expression of the VRF system in the building semantic web. The works in
this paper will support semantic web in automation strategies for building
management and scalable building operation.
","['\nJingming Li\n', '\nNianping Li\n', '\nRui Yan\n', '\nKushnazarov Farruh\n', '\nAnbang Li\n', '\nKehua Li\n']",,,http://dx.doi.org/10.1016/j.jobe.2022.104792,cs.OH,['cs.OH'],10.1016/j.jobe.2022.104792,,[]
A Two-step Heuristic for the Periodic Demand Estimation Problem,http://arxiv.org/abs/2108.08331v1,2021-08-18T18:08:22Z,2021-08-18T18:08:22Z,"  Freight carriers rely on tactical plans to satisfy demand in a cost-effective
way. For computational tractability in real large-scale settings, such plans
are typically computed by solving deterministic and cyclic formulations. An
important input is the periodic demand, i.e., the demand that is expected to
repeat in each period of the planning horizon. Motivated by the discrepancy
between time series forecasts of demand in each period and the periodic demand,
Laage et al. (2021) recently introduced the Periodic Demand Estimation (PDE)
problem and showed that it has a high value. However, they made strong
assumptions on the solution space so that the problem could be solved by
enumeration. In this paper we significantly extend their work. We propose a new
PDE formulation that relaxes the strong assumptions on the solution space. We
solve large instances of this formulation with a two-step heuristic. The first
step reduces the dimension of the feasible space by performing clustering of
commodities based on instance-specific information about demand and supply
interactions. The formulation along with the first step allow to solve the
problem in a second step by either metaheuristics or the state-of-the-art
black-box optimization solver NOMAD. In an extensive empirical study using real
data from the Canadian National Railway Company, we show that our methodology
produces high quality solutions and outperforms existing ones.
","['\nGreta Laage\n', '\nEmma Frejinger\n', '\nGilles Savard\n']",,,http://arxiv.org/abs/2108.08331v1,math.OC,"['math.OC', 'cs.OH']",,,[]
"JOET: Sustainable Vehicle-assisted Edge Computing for Internet of
  Vehicles",http://arxiv.org/abs/2108.02443v1,2021-08-05T08:23:29Z,2021-08-05T08:23:29Z,"  Task offloading in Internet of Vehicles (IoV) involves numerous steps and
optimization variables such as: where to offload tasks, how to allocate
computation resources, how to adjust offloading ratio and transmit power for
offloading, and such optimization variables and hybrid combination features are
highly coupled with each other. Thus, this is a fully challenge issue to
optimize these variables for task offloading to sustainably reduce energy
consumption with load balancing while ensuring that a task is completed before
its deadline. In this paper, we first provide a Mixed Integer Nonlinear
Programming Problem (MINLP) formulation for such task offloading under energy
and deadline constraints in IoV. Furthermore, in order to efficiently solve the
formulated MINLP, we decompose it into two subproblems, and design a
low-complexity Joint Optimization for Energy Consumption and Task Processing
Delay (JOET) algorithm to optimize selection decisions, resource allocation,
offloading ratio and transmit power adjustment. We carry out extensive
simulation experiments to validate JOET. Simulation results demonstrate that
JOET outperforms many representative existing approaches in quickly converge
and effectively reduce energy consumption and delay. Specifically, average
energy consumption and task processing delay have been reduced by 15.93% and
15.78%, respectively, and load balancing efficiency has increased by 10.20%.
","['\nWei Huang\n', '\nNeal N. Xiong\n', '\nShahid Mumtaz\n']",,,http://arxiv.org/abs/2108.02443v1,cs.OH,['cs.OH'],,,[]
Impressions of the GDMC AI Settlement Generation Challenge in Minecraft,http://arxiv.org/abs/2108.02955v1,2021-08-06T06:28:40Z,2021-08-06T06:28:40Z,"  The GDMC AI settlement generation challenge is a PCG competition about
producing an algorithm that can create an ""interesting"" Minecraft settlement
for a given map. This paper contains a collection of written experiences with
this competition, by participants, judges, organizers and advisors. We asked
people to reflect both on the artifacts themselves, and on the competition in
general. The aim of this paper is to offer a shareable and edited collection of
experiences and qualitative feedback - which seem to contain a lot of insights
on PCG and computational creativity, but would otherwise be lost once the
output of the competition is reduced to scalar performance values. We reflect
upon some organizational issues for AI competitions, and discuss the future of
the GDMC competition.
","['\nChristoph Salge\n', '\nClaus Aranha\n', '\nAdrian Brightmoore\n', '\nSean Butler\n', '\nRodrigo Canaan\n', '\nMichael Cook\n', '\nMichael Cerny Green\n', '\nHagen Fischer\n', '\nChristian Guckelsberger\n', '\nJupiter Hadley\n', '\nJean-Baptiste Hervé\n', '\nMark R Johnson\n', '\nQuinn Kybartas\n', '\nDavid Mason\n', '\nMike Preuss\n', '\nTristan Smith\n', '\nRuck Thawonmas\n', '\nJulian Togelius\n']","28 pages, 5 figures",,http://arxiv.org/abs/2108.02955v1,cs.OH,['cs.OH'],,,[]
"A hydraulic model outperforms work-balance models for predicting
  recovery kinetics from intermittent exercise",http://arxiv.org/abs/2108.04510v2,2021-08-10T08:26:03Z,2022-06-13T09:39:49Z,"  Data Science advances in sports commonly involve ""big data"", i.e., large
sport-related data sets. However, such big data sets are not always available,
necessitating specialized models that apply to relatively few observations. One
important area of sport-science research that features small data sets is the
study of recovery from exercise. In this area, models are typically fitted to
data collected from exhaustive exercise test protocols, which athletes can
perform only a few times. Recent findings highlight that established recovery
such as the so-called work-balance models are too simple to adequately fit
observed trends in the data. Therefore, we investigated a hydraulic model that
requires the same few data points as work-balance models to be applied, but
promises to predict recovery dynamics more accurately.
  To compare the hydraulic model to established work-balance models, we
retrospectively applied them to data compiled from published studies. In total,
one hydraulic model and three work-balance models were compared on data
extracted from five studies. The hydraulic model outperformed established
work-balance models on all defined metrics, even those that penalize models
featuring higher numbers of parameters. These results incentivize further
investigation of the hydraulic model as a new alternative to established
performance models of energy recovery.
","['\nFabian C. Weigend\n', '\nDavid C. Clarke\n', '\nOliver Obst\n', '\nJason Siegler\n']","26 pages, 9 figures, 6 tables, this manuscript has been submitted and
  is currently under review",,http://arxiv.org/abs/2108.04510v2,cs.OH,"['cs.OH', 'I.6.4; J.3']",,,[]
"Hierarchical Structural Analysis Method for Complex Equation-oriented
  Models",http://arxiv.org/abs/2108.04525v2,2021-08-10T09:11:21Z,2021-10-27T01:33:00Z,"  Structural analysis is a method for verifying equation-oriented models in the
design of industrial systems. Existing structural analysis methods need
flattening of the hierarchical models into an equation system for analysis.
However, the large-scale equations in complex models make structural analysis
difficult. Aimed to address the issue, this study proposes a hierarchical
structural analysis method by exploring the relationship between the
singularities of the hierarchical equation-oriented model and its components.
This method obtains the singularity of a hierarchical equation-oriented model
by analyzing a dummy model constructed with the parts from the decomposing
results of its components. Based on this, the structural singularity of a
complex model can be obtained by layer-by-layer analysis according to their
natural hierarchy. The hierarchical structural analysis method can reduce the
equation scale in each analysis and achieve efficient structural analysis of
very complex models. This method can be adaptively applied to
nonlinear-algebraic and differential-algebraic equation models. The main
algorithms, application cases and comparison with the existing methods are
present in this paper. The complexity analysis results show the enhanced
efficiency of the proposed method in the structural analysis of complex
equation-oriented models. Compared with the existing methods, the time
complexity of the proposed method is improved significantly.
","['\nChao Wang\n', '\nLi Wan\n', '\nTifan Xiong\n', '\nYuanlong Xie\n', '\nShuting Wang\n', '\nJianwan Ding\n', '\nLiping Chen\n']","23 pages, 10 figures","Mathematics 2021, 9, 2660",http://dx.doi.org/10.3390/math9212660,cs.OH,['cs.OH'],10.3390/math9212660,,[]
"On the optimal layout of a dining room in the era of COVID-19 using
  mathematical optimization",http://arxiv.org/abs/2108.04233v2,2021-08-07T15:49:17Z,2022-01-18T11:48:07Z,"  We consider the problem of maximizing the number of people that a dining room
can accommodate provided that the chairs belonging to different tables are
socially distant. We introduce an optimization model that incorporates several
characteristics of the problem, namely: the type and size of surface of the
dining room, the shapes and sizes of the tables, the positions of the chairs,
the sitting sense of the customers, and the possibility of adding space
separators to increase the capacity. We propose a simple, yet general,
set-packing formulation for the problem. We investigate the efficiency of space
separators and the impact of considering the sitting sense of customers in the
room capacity. We also perform an algorithmic analysis of the model, and assess
its scalability to the problem size, the presence of (or lack thereof) room
separators, and the consideration of the sitting sense of customers. We also
propose two constructive heuristics capable of coping with large problem
instances otherwise intractable for the optimization model.
","['\nClaudio Contardo\n', '\nLuciano Costa\n']",,,http://arxiv.org/abs/2108.04233v2,cs.OH,"['cs.OH', 'math.OC']",,,[]
"Analysing Design Approaches for the Power Consumption in Cyber-Physical
  Systems",http://arxiv.org/abs/2107.07745v1,2021-07-16T07:46:06Z,2021-07-16T07:46:06Z,"  The importance of Cyber Physical Systems (CPS) and Internet of Things (IoT)
applications is constantly increasing, especially in the context of Industry
4.0. Architectural decisions are crucial not just for performance, security and
resilience reasons but also regarding costs and resource usage. In this paper
we analyse two of the fundamental approaches to design control loops (i.e.
time-driven and event-driven), show how they can be realised and evaluate their
power requirements. Through this the design criteria can be extended also
considering the optimization of energy related aspects.
","['\nPatrizia Sailer\n', '\nIgor Ivkic\n', '\nMarkus Tauber\n', '\nAndreas Mauthe\n', '\nAntonios Gouglidis\n']",IEEE Link: https://ieeexplore.ieee.org/document/9464065,,http://arxiv.org/abs/2107.07745v1,cs.OH,['cs.OH'],,,[]
"Video or Image Transmission Security for ESP-EYE IoT device used in
  Business Processes",http://arxiv.org/abs/2107.08321v1,2021-07-17T23:17:14Z,2021-07-17T23:17:14Z,"  Internet of Things is the name of a communication network that is formed by
physical objects such as RFID tags, sensors and some lightweight development
platforms that have the ability to connect to the internet. While the devices
can communicate among themselves in this network, they can also be part of a
large network. The data produced by those physical objects which are the member
of IoT network are processed by different methods and the outputs obtained are
used in processes such as decision making and learning. With this aspect of the
Internet of Things, it affects all areas of human life and its number is
increasing day by day. These devices appear to have security gaps due to their
limited resources, their wide range of usage area and incomplete security
standards. These devices, which are located in people's living areas,
manufacturing and business processes also cause difficulties in protecting
privacy. In this study, a solution has been developed for the communication
security of the internet of things called ESP-Eye which includes a camera,
wireless communication module and face recognition software. The proposed
solution was implemented on the ESP-Eye.
","['\nOmer Aydin\n', '\nIbrahim Ismail Erhan\n']",,"Ayd{\i}n, \""O , Erhan, \.I . (2021). Video or Image Transmission
  Security for ESP-EYE IoT device used in Business Processes . Y\""onetim
  Bili\c{s}im Sistemleri Dergisi , 7 (1) , 1-9 . Retrieved from
  https://dergipark.org.tr/tr/pub/ybs/issue/63606/857203",http://arxiv.org/abs/2107.08321v1,cs.OH,['cs.OH'],,,[]
EnergySaver Software Manual,http://arxiv.org/abs/2107.06664v1,2021-07-13T13:25:20Z,2021-07-13T13:25:20Z,"  Energy efficiency is a topic that has attracted the attention of researchers
in recent years, in order to seek sustainability solutions for energy
production and reduction of its costs, aiming to provide a balance between
development and protection of natural resources. Thus, we proposed the
EnergySaver software that has as its objective the monitoring of electric
energy consumption, from data capture to consumption forecast for the following
month. To create Energy Saver, we used Open Source technologies applied to the
Internet of Things (IoT), embedded systems, and Long Short-Term Memory Neural
Networks (LSTM). However, in order to have harmony between the current
researchers and those who may manipulate this software in the future, it is
essential to create a Software Manual, where all the details of its
implementation are described in detail. Therefore, this article describes all
the steps for the implementation of the system, from the methodological scheme
of the system, its modeling with UML, to the modules that compose it, becoming
a Manual for its use.
","['\nDavi Guimarães da Silva\n', '\nMarla Teresinha Barbosa Geller\n', '\nDalton Felipe Silva Varão\n', '\nJoão Bentes\n', '\nMauro Sérgio dos Santos Moura\n', '\nYasmin Braga Teixeira\n', '\nClayton André Maia dos Santos\n', '\nAnderson Alvarenga de Moura Meneses\n']","8 pages, in Portuguese, 21 figures",,http://arxiv.org/abs/2107.06664v1,cs.OH,"['cs.OH', 'cs.SE']",,,[]
Learning MR-Sort Models from Non-Monotone Data,http://arxiv.org/abs/2107.09668v1,2021-07-20T13:51:16Z,2021-07-20T13:51:16Z,"  The Majority Rule Sorting (MR-Sort) method assigns alternatives evaluated on
multiple criteria to one of the predefined ordered categories. The Inverse
MR-Sort problem (Inv-MR-Sort) computes MR-Sort parameters that match a dataset.
Existing learning algorithms for Inv-MR-Sort consider monotone preferences on
criteria. We extend this problem to the case where the preferences on criteria
are not necessarily monotone, but possibly single-peaked (or single-valley). We
propose a mixed-integer programming based algorithm that learns the preferences
on criteria together with the other MR-Sort parameters from the training data.
We investigate the performance of the algorithm using numerical experiments and
we illustrate its use on a real-world case study.
","['\nPegdwende Minoungou\n', '\nVincent Mousseau\n', '\nWassila Ouerdane\n', '\nPaolo Scotton\n']",,,http://arxiv.org/abs/2107.09668v1,cs.AI,"['cs.AI', 'cs.OH']",,,[]
Network and Sequence-Based Prediction of Protein-Protein Interactions,http://arxiv.org/abs/2107.03694v4,2021-07-08T09:16:46Z,2022-02-06T17:21:18Z,"  Background:Typically, proteins perform key biological functions by
interacting with each other. As a consequence, predicting which protein pairs
interact is a fundamental problem. Experimental methods are slow, expensive,
and may be error prone.Many computational methods have been proposed to
identify candidate interacting pairs. When accurate, they can serve as an
inexpensive, preliminary filtering stage, to be followed by downstream
experimental validation. Among such methods, sequence-based ones are very
promising.Results:We present, a new algorithm that leverages both topological
and biological information to predict protein-protein interactions. We
comprehensively compare our Framework with state-of-the-art approaches on
reliable PPIs datasets, showing that they have competitive or higher accuracy
on biologically validated test sets.Conclusion:We shown that topological plus
sequence-based computational methods can effectively predict the entire human
interactome compared with methods that leverage only one source of biological
information.
","['\nLeonardo Martini\n', '\nAdriano Fazzone\n', '\nLuca Becchetti\n']",,,http://arxiv.org/abs/2107.03694v4,q-bio.BM,"['q-bio.BM', 'cs.OH']",,,[]
Digital Twin As A Cost Reduction Method,http://arxiv.org/abs/2107.14109v1,2021-07-10T22:01:34Z,2021-07-10T22:01:34Z,"  Many fields have been affected by the introduction of concepts such as
sensors, industry 4.0, internet of things, machine learning and artificial
intelligence in recent years. As a result of the interaction of cyber physical
systems with these concepts, digital twin model has emerged. The concept of
digital twin has been used in many areas with its emergence. The use of this
model has made significant gains, especially in decision making processes. The
gains in decision making processes contribute to every field and cause changes
in terms of cost. In this study, the historical development of the concept of
digital twin has been mentioned and general information about the usage areas
of digital twin has been given. In the light of this information, the cost
effect of the digital twin model, therefore its appearance from the cost
accounting window and its use as a cost reduction method were evaluated. This
study was carried out in order to shed light on the studies with the
insufficient resources in the Turkish literature and the cost accounting
perspective.
","['\nSuleyman Yukcu\n', '\nOmer Aydin\n']","Digital Twin, Cost Accounting, Cyber Physical Systems, Artificial
  Intelligence, Internet of Things, Sensor","Muhasebe Bilim Dunyas{\i} Dergisi , 22 (3) , 563-579",http://dx.doi.org/10.31460/mbdd.694571,cs.OH,"['cs.OH', 'econ.GN', 'q-fin.EC', 'q-fin.GN']",10.31460/mbdd.694571,,[]
"To Block or Not to Block: Accelerating Mobile Web Pages On-The-Fly
  Through JavaScript Classification",http://arxiv.org/abs/2106.13764v1,2021-06-20T10:32:10Z,2021-06-20T10:32:10Z,"  The increasing complexity of JavaScript in modern mobile web pages has become
a critical performance bottleneck for low-end mobile phone users, especially in
developing regions. In this paper, we propose SlimWeb, a novel approach that
automatically derives lightweight versions of mobile web pages on-the-fly by
eliminating the use of unnecessary JavaScript. SlimWeb consists of a JavaScript
classification service powered by a supervised Machine Learning (ML) model that
provides insights into each JavaScript element embedded in a web page. SlimWeb
aims to improve the web browsing experience by predicting the class of each
element, such that essential elements are preserved and non-essential elements
are blocked by the browsers using the service. We motivate the core design of
SlimWeb using a user preference survey of 306 users and perform a detailed
evaluation of SlimWeb across 500 popular web pages in a developing region on
real 3G and 4G cellular networks, along with a user experience study with 20
real-world users and a usage willingness survey of 588 users. Evaluation
results show that SlimWeb achieves a 50% reduction in the page load time
compared to the original pages, and more than 30% reduction compared to
competing solutions, while achieving high similarity scores to the original
pages measured via a qualitative evaluation study of 62 users. SlimWeb improves
the overall user experience by more than 60% compared to the original pages,
while maintaining 90%-100% of the visual and functional components of most
pages. Finally, the SlimWeb classifier achieves a median accuracy of 90% in
predicting the JavaScript category.
","['\nMoumena Chaqfeh\n', '\nMuhammad Haseeb\n', '\nWaleed Hashmi\n', '\nPatrick Inshuti\n', '\nManesha Ramesh\n', '\nMatteo Varvello\n', '\nFareed Zaffar\n', '\nLakshmi Subramanian\n', '\nYasir Zaki\n']","11 pages, 11 figures",,http://arxiv.org/abs/2106.13764v1,cs.OH,['cs.OH'],,,[]
Data-Enhanced Process Models in Process Mining,http://arxiv.org/abs/2107.00565v1,2021-06-29T08:47:06Z,2021-06-29T08:47:06Z,"  Understanding and improving business processes have become important success
factors for organizations. Process mining has proven very successful with a
variety of methods and techniques, including discovering process models based
on event logs. Process mining has traditionally focussed on control flow and
timing aspects. However, getting insights about a process is not only based on
activities and their orderings, but also on the data generated and manipulated
during process executions. Today, almost every process activity generates data;
these data do not play the role in process mining that it deserves. This paper
introduces a visualization technique for enhancing discovered process models
with domain data, thereby allowing data-based exploration of processes.
Data-enhanced process models aim at supporting domain experts to explore the
process, where they can select attributes of interest and observe their
influence on the process. The visualization technique is illustrated by the
MIMIC-IV real-world data set on hospitalizations in the US.
","['\nJonas Cremerius\n', '\nMathias Weske\n']",,,http://arxiv.org/abs/2107.00565v1,cs.OH,['cs.OH'],,,[]
Design an IT Policy Implementation Plan,http://arxiv.org/abs/2107.03327v1,2021-06-29T07:34:08Z,2021-06-29T07:34:08Z,"  Information technology (IT) companies implement multi-dimensional policy
plans that include procedures, sub-plans, and instructions to outline their
business scopes, targets, and communications. This work outlined the IT policy
implementation plan designed by an imaginary company with a random name called
Northcentral Cloud Consulting Firm (NCCF), containing proposed IT policies,
milestones and roadmaps, control framework, stakeholder responsibilities,
knowledge transfer plan, and leadership roles. As NCCF's major customers seek
data-driven solutions in cloud computing, the NCCF IT policy plan provides
various data policies, including security and proper usage of machine learning
services. The plan offers a detailed roadmap of its financial, geographical,
and reputational expansion within three years. The IT policy plan also
compromises an IT risk management, contingency, and emergency communication
plan, mainly for protecting data and business continuity. Stakeholder
responsibilities are incorporated into the IT policy plan, as NCCF considers
any engagement with its customers as a collaborative effort in which both
parties have and share several responsibilities.
","['\nSaman Sarraf\n', '\nMilton Kabia\n']",,,http://arxiv.org/abs/2107.03327v1,cs.OH,['cs.OH'],,,[]
Gain and Pain of a Reliable Delay Model,http://arxiv.org/abs/2107.06814v2,2021-06-25T11:32:44Z,2021-07-19T06:50:22Z,"  State-of-the-art digital circuit design tools almost exclusively rely on pure
and inertial delay for timing simulations. While these provide reasonable
estimations at very low execution time in the average case, their ability to
cover complex signal traces is limited. Research has provided the dynamic
Involution Delay Model (IDM) as a promising alternative, which was shown (i) to
depict reality more closely and recently (ii) to be compatible with modern
simulation suites. In this paper we complement these encouraging results by
experimentally exploring the behavioral coverage for more advanced circuits. In
detail we apply the IDM to three simple circuits (a combinatorial loop, an SR
latch and an adder), interpret the delivered results and evaluate the overhead
in realistic settings. Comparisons to digital (inertial delay) and analog
(SPICE) simulations reveal, that the IDM delivers very fine-grained results,
which match analog simulations very closely. Moreover, severe shortcomings of
inertial delay become apparent in our simulations, as it fails to depict a
range of malicious behaviors. Overall the Involution Delay Model hence
represents a viable upgrade to the available delay models in modern digital
timing simulation tools.
",['\nJürgen Maier\n'],"9 pages, 11 figures, 2 tables, extension of conference submission",,http://arxiv.org/abs/2107.06814v2,cs.OH,"['cs.OH', 'cs.AR', 'B.8.1; B.8.2']",,,[]
"Interaction of Multiple Tensor Product Operators of the Same Type: an
  Introduction",http://arxiv.org/abs/2106.15576v1,2021-06-29T17:11:23Z,2021-06-29T17:11:23Z,"  Tensor product operators on finite dimensional Hilbert spaces are studied.
The focus is on bilinear tensor product operators. A tensor product operator on
a pair of Hilbert spaces is a maximally general bilinear operator into a target
Hilbert space. By 'maximally general' is meant every bilinear operator from the
same pair of spaces to any Hilbert space factors into the composition of the
tensor product operator with a uniquely determined linear mapping on the target
space. There are multiple distinct tensor product operators of the same type;
there is no ""the"" tensor product. Distinctly different tensor product operators
can be associated with different parts of a multipartite system without
difficulty. Separability of states, and locality of operators and observables
is tensor product operator dependent. The same state in the target state space
can be inseparable with respect to one tensor product operator but separable
with respect to another, and no tensor product operator is distinguished
relative to the others; the unitary operator used to construct a Bell state
from a pair of |0>'s being highly tensor product operator-dependent is a prime
example. The relationship between two tensor product operators of the same type
is given by composition with a unitary operator. There is an equivalence
between change of tensor product operator and change of basis in the target
space. Among the gains from change of tensor product operator is the
localization of some nonlocal operators as well as separability of inseparable
states. Examples are given.
","['\nHoward A. Blair\n', '\nH Shelton Jacinto\n', '\nPaul M. Alsing\n']",15 pages,,http://arxiv.org/abs/2106.15576v1,quant-ph,"['quant-ph', 'cs.OH', 'math-ph', 'math.MP']",,,[]
"Generalized ""Square roots of Not"" matrices, their application to the
  unveiling of hidden logical operators and to the definition of fully matrix
  circular Euler functions",http://arxiv.org/abs/2107.06067v2,2021-06-22T03:43:53Z,2021-07-14T11:03:28Z,"  The square root of Not is a logical operator of importance in quantum
computing theory and of interest as a mathematical object in its own right. In
physics, it is a square complex matrix of dimension 2. In the present work it
is a complex square matrix of arbitrary dimension. The introduction of linear
algebra into logical theory has been enhanced in recent decades by the
researches in the field of neural networks and quantum computing. Here we will
make a brief description of the representation of logical operations through
matrices and we show how general expressions for the two square roots of the
Not operator are obtained. Then, we explore two topics. First, we study an
extension to a non-quantum domain of a short form of Deutsch's algorithm. Then,
we assume that a root of Not is a matrix extension of the imaginary unit i, and
under this idea we obtain fully matrix versions for the Euler expansions and
for the representations of circular functions by complex exponentials.
",['\nEduardo Mizraji\n'],25 pages,,http://arxiv.org/abs/2107.06067v2,cs.OH,"['cs.OH', 'cs.ET', 'quant-ph', '15A24, 03G05, 15A16']",,,[]
IoT Solution for Winter Survival of Indoor Plants,http://arxiv.org/abs/2106.05130v1,2021-06-09T15:14:18Z,2021-06-09T15:14:18Z,"  Not only does cold climate pose a problem for outdoor plants during winter in
the northern hemisphere, but for indoor plants as well: low sunlight, low
humidity, and simultaneous cold breezes from windows and heat from radiators
all cause problems for indoor plants. People often treat their indoor plants
like mere decoration, which can often lead to health issues for the plant or
even death of the plant, especially during winter. A plant monitoring system
was developed to solve this problem, collecting information on plants' indoor
environmental conditions (light, humidity, and temperature) and providing this
information in an accessible format for the user. Preliminary functional tests
were conducted in similar settings where the system would be used. In addition,
the concept was evaluated by interviewing an expert in the field of
horticulture.
  The evaluation results indicate that this kind of system could prove useful;
however, the tests indicated that the system requires further development to
achieve more practical value and wider usage.
","['\nMd Saroar Jahan\n', '\nJhuma kabir Mim\n', '\nSampo Niittyviita\n', '\nSanteri Moberg\n', '\nMurad Ahmad\n', '\nNijar Hossain\n']","10 pages, 2 tables, 4 figures",,http://arxiv.org/abs/2106.05130v1,cs.OH,['cs.OH'],,,[]
"Identifying intracity freight trip ends from heavy truck GPS
  trajectories",http://arxiv.org/abs/2106.09881v2,2021-06-18T02:47:28Z,2021-06-21T00:57:40Z,"  Intracity heavy truck freight trips are basic data in city freight system
planning and management. In the big data era, massive heavy truck GPS
trajectories can be acquired cost effectively in real-time. Identifying freight
trip ends (origins and destinations) from heavy truck GPS trajectories is an
outstanding problem. Although previous studies proposed a variety of trip end
identification methods from different perspectives, these studies subjectively
defined key threshold parameters and ignored the complex intracity heavy truck
travel characteristics. Here, we propose a data-driven trip end identification
method in which the speed threshold for identifying truck stops and the
multilevel time thresholds for distinguishing temporary stops and freight trip
ends are objectively defined. Moreover, an appropriate time threshold level is
dynamically selected by considering the intracity activity patterns of heavy
trucks. Furthermore, we use urban road networks and point-of-interest (POI)
data to eliminate misidentified trip ends to improve method accuracy. The
validation results show that the accuracy of the method we propose is 87.45%.
Our method incorporates the impact of the city freight context on truck
trajectory characteristics, and its results can reflect the spatial
distribution and chain patterns of intracity heavy truck freight trips, which
have a wide range of practical applications.
","['\nYitao Yang\n', '\nBin Jia\n', '\nXiao-Yong Yan\n', '\nRui Jiang\n', '\nHao Ji\n', '\nZiyou Gao\n']",,,http://arxiv.org/abs/2106.09881v2,cs.OH,['cs.OH'],,,[]
"A Fuzzy Post-project Evaluation Approach for Security Video Surveillance
  System",http://arxiv.org/abs/2106.15316v1,2021-06-13T08:48:50Z,2021-06-13T08:48:50Z,"  Video surveillance is an essential component of the public security system.
The security video surveillance system is a powerful means to prevent violence
and crimes, and it is closely coupled with the construction of smart cities. A
post-project evaluation is an evaluation of a project's actions and outcomes
after its completion. Post-project evaluation can scientifically and
objectively evaluate the construction effectiveness of video surveillance
system at a certain stage. Utilizing post-project evaluation can find out the
causes of success or failure to make recommendations for the construction of a
security video surveillance system in the next stage. Therefore, we propose a
fuzzy post-project evaluation approach for the security video surveillance
system in a real-world community. The fuzzy theory and fuzzy multi-level
evaluation method are applied. The evaluation result demonstrates that the
proposed approach is practically applicable to real-world security video
surveillance systems.
","['\nMing Liu\n', '\nZhi Xue\n']",,,http://arxiv.org/abs/2106.15316v1,cs.OH,['cs.OH'],,,[]
"SQUADfps: Integrated Model-Based Machine Safety and Product Quality for
  Flexible Production Systems",http://arxiv.org/abs/2105.14817v2,2021-05-31T09:28:38Z,2021-06-04T05:42:48Z,"  Growing individualization of products up to lot-size-1 and high volatility of
product mixes lead to new challenges in the manufacturing domain, including the
need for frequent reconfiguration of the system and reacting to changing
orders. Thus, apart from functional aspects, safety aspects of the production
system as well as product quality assurance aspects must be addressed for
flexible and reconfigurable manufacturing systems at runtime. To cope with the
mentioned challenges, we present an integrated model-based approach SQUADfps
(machine Safety and product QUAlity for flexible proDuction systems) to support
the automatic conduct of the risk assessment of flexible production scenarios
in terms of safety as well as the process-FMEA to ensure that the requirements
w.r.t. the quality of the production process and the resulting product are met.
Our approach is based on a meta-model which captures all information needed to
conduct both risk assessment and process-FMEA dynamically during the runtime,
and thus enables flexible manufacturing scenarios with frequent changes of the
production system and orders up to a lot-size of one while guaranteeing safety
and product quality requirements. The automatically generated results will
assist human in making further decisions. To demonstrate the feasibility of our
approach, we apply it to a case study.
","['\nChee Hung Koo\n', '\nStefan Rothbauer\n', '\nMarian Vorderer\n', '\nKai Hoefig\n', '\nMarc Zeller\n']",,"Papadopoulos Y., Aslansefat K., Katsaros P., Bozzano M. (eds)
  Model-Based Safety and Assessment. IMBSA 2019. Lecture Notes in Computer
  Science, vol 11842. Springer, Cham",http://dx.doi.org/10.1007/978-3-030-32872-6_15,cs.OH,['cs.OH'],10.1007/978-3-030-32872-6_15,,[]
WAP: Digital Dependability Identities,http://arxiv.org/abs/2105.14984v1,2021-05-31T14:13:25Z,2021-05-31T14:13:25Z,"  Cyber-Physical Systems (CPS) provide enormous potential for innovation but a
precondition for this is that the issue of dependability has been addressed.
This paper presents the concept of a Digital Dependability Identity (DDI) of a
component or system as foundation for assuring the dependability of CPS. A DDI
is an analyzable and potentially executable model of information about the
dependability of a component or system. We argue that DDIs must fulfill a
number of properties including being universally useful across supply chains,
enabling off-line certification of systems where possible, and providing
capabilities for in-field certification of safety of CPS. In this paper, we
focus on system safety as one integral part of dependability and as a practical
demonstration of the concept, we present an initial implementation of DDIs in
the form of Conditional Safety Certificates (also known as ConSerts). We
explain ConSerts and their practical operationalization based on an
illustrative example.
","['\nDaniel Schneider\n', '\nMario Trapp\n', '\nYiannis Papadopoulos\n', '\nEric Armengaud\n', '\nMarc Zeller\n', '\nKai Hoefig\n']",,"2015 IEEE 26th International Symposium on Software Reliability
  Engineering (ISSRE)",http://dx.doi.org/10.1109/ISSRE.2015.7381825,cs.OH,['cs.OH'],10.1109/ISSRE.2015.7381825,,[]
"INSiDER: Incorporation of system and safety analysis models using a
  dedicated reference model",http://arxiv.org/abs/2105.14992v1,2021-05-31T14:23:53Z,2021-05-31T14:23:53Z,"  In order to enable model-based, iterative design of safety-relevant systems,
an efficient incorporation of safety and system engineering is a pressing need.
Our approach interconnects system design and safety analysis models efficiently
using a dedicated reference model. Since all information are available in a
structured way, traceability between the model elements and consistency checks
enable automated synchronization to guarantee that information within both kind
of models are consistent during the development life-cycle.
","['\nMarc Zeller\n', '\nKai Hoefig\n']",,2016 Annual Reliability and Maintainability Symposium (RAMS),http://dx.doi.org/10.1109/RAMS.2016.7448074,cs.OH,['cs.OH'],10.1109/RAMS.2016.7448074,,[]
Low cost cloud based remote microscopy for biological sciences,http://arxiv.org/abs/2106.07419v1,2021-06-07T22:50:04Z,2021-06-07T22:50:04Z,"  A low cost remote imaging platform for biological applications was developed.
The ""Picroscope"" is a device that allows the user to perform longitudinal
imaging studies on multi-well cell culture plates. Here we present the network
architecture and software used to facilitate communication between modules
within the device as well as external cloud services. A web based console was
created to control the device and view experiment results. Post processing
tools were developed to analyze captured data in the cloud. The result is a
platform for controlling biological experiments from outside the lab.
","['\nPierre V Baudin\n', '\nVictoria T Ly\n', '\nPattawong Pansodtee\n', '\nErik A Jung\n', '\nRobert Currie\n', '\nRyan Hoffman\n', '\nHelen Rankin Willsey\n', '\nAlex A Pollen\n', '\nTomasz J Nowakowski\n', '\nDavid Haussler\n', '\nMohammed Andres Mostajo-Radji\n', '\nSofie Salama\n', '\nMircea Teodorescu\n']","The authors Pierre V Baudin and Victoria T Ly contributed equally to
  this work. 21 pages, 12 figures",,http://dx.doi.org/10.1016/j.iot.2021.100454,cs.OH,"['cs.OH', 'eess.IV']",10.1016/j.iot.2021.100454,,[]
"Multi-objective Digital Design Optimisation via Improved Drive
  Granularity Standard Cells",http://arxiv.org/abs/2105.11248v2,2021-05-21T15:37:22Z,2021-09-06T10:47:12Z,"  To tackle the complexity of state-of-the-art electronic systems, silicon
foundries continuously shrink the technology nodes and electronic design
automation (EDA) vendors offer hierarchical design flows to decompose systems
into smaller blocks. However, such a staged design methodology consists of
various levels of abstraction, where margins will be accumulated and result in
degradation of the overall design quality. This limits the full use of
capabilities of both the process technology and EDA tools. In this work, a
study of drive granularity of standard cells is performed and an interpolation
method is proposed for drive option expansion within original cell libraries.
These aim to investigate how industrial synthesis tools deal with the drive
strength selection using different granularity sets. In addition, a
fully-automated, multi-objective (MO) EDA digital flow is introduced for power,
performance, area (PPA) optimisation based on drive strength refinement. This
population-based search method better handles the increased difficulty of cell
selection when using larger logic libraries, producing better optimised
solutions than standard tool flow in this case. The achieved experimental
results demonstrate how the improved drive granularity cells overall enhance
the quality of designs and how a significant improvement in trading off PPA is
achieved by the MOEDA flow.
","['\nLinan Cao\n', '\nSimon J. Bale\n', '\nMartin A. Trefzer\n']",This work has been accepted for publication in IEEE TCAS-1,,http://dx.doi.org/10.1109/TCSI.2021.3109239,cs.OH,['cs.OH'],10.1109/TCSI.2021.3109239,,[]
Business Suitability Principles for Workflow Modelling,http://arxiv.org/abs/2105.12654v1,2021-05-26T16:12:10Z,2021-05-26T16:12:10Z,"  By incorporating aspects of coordination and collaboration, workflow
implementations of information systems require a sound conceptualisation of
\EM{business processing} semantics. Traditionally, the success of conceptual
modelling techniques has depended largely on the adequacy of conceptualisation,
expressive power, comprehensibility and formal foundation. An equally important
requirement, particularly with the increased conceptualisation of business
aspects, is \EM{business suitability}. In this paper, the focus is on the
business suitability of workflow modelling for a commonly encountered class of
(operational) business processing, e.g. those of insurance claims, bank loans
and land conveyancing. A general assessment is first conducted on some
\EM{integrated} techniques characterising well-known paradigms - structured
process modelling, object-oriented modelling, behavioural process modelling and
business-oriented modelling. Through this, an insight into business suitability
within the broader perspective of technique adequacy, is gained. A specific
business suitability diagnosis then follows using a particular characterisation
of business processing, i.e.\ one where the intuitive semantics and
inter-relationship of business services and business processes are nuanced. As
a result, five business suitability principles are elicited. These are proposed
for a more detailed understanding and (synthetic) development of workflow
modelling techniques. Accordingly, further insight into workflow specification
languages and workflow globalisation in open distributed architectures may also
be gained.
","['\nAlistair P. Barros\n', '\nArthur H. M. ter Hofstede\n', '\nHenderik A. Proper\n', '\nPeter N. Creasy\n']",,,http://arxiv.org/abs/2105.12654v1,cs.OH,"['cs.OH', 'cs.SE']",,,[]
Three-body problem -- from Newton to supercomputer plus machine learning,http://arxiv.org/abs/2106.11010v2,2021-05-20T12:02:58Z,2022-05-05T13:40:23Z,"  The famous three-body problem can be traced back to Newton in 1687, but quite
few families of periodic orbits were found in 300 years thereafter. In this
paper, we propose an effective approach and roadmap to numerically gain planar
periodic orbits of three-body systems with arbitrary masses by means of machine
learning based on an artificial neural network (ANN) model. Given any a known
periodic orbit as a starting point, this approach can provide more and more
periodic orbits (of the same family name) with variable masses, while the mass
domain having periodic orbits becomes larger and larger, and the ANN model
becomes wiser and wiser. Finally we have an ANN model trained by means of all
obtained periodic orbits of the same family, which provides a convenient way to
give accurate enough predictions of periodic orbits with arbitrary masses for
physicists and astronomers. It suggests that the high-performance computer and
artificial intelligence (including machine learning) should be the key to gain
periodic orbits of the famous three-body problem.
","['\nShijun Liao\n', '\nXiaoming Li\n', '\nYu Yang\n']","17 pages, 9 figures, 6 tables, accepted by New Astronomy in May 2022.
  See also https://www.researchsquare.com/article/rs-395522/v1",New Astronomy 96 (2022) 101850,http://dx.doi.org/10.1016/j.newast.2022.101850,cs.OH,"['cs.OH', 'nlin.CD', 'physics.comp-ph']",10.1016/j.newast.2022.101850,,[]
Optimal Seat Allocation Under Social Distancing Constraints,http://arxiv.org/abs/2105.05017v1,2021-05-11T13:25:15Z,2021-05-11T13:25:15Z,"  The Covid-19 pandemic introduces new challenges and constraints for return to
work business planning. We describe a space allocation problem that
incorporates social distancing constraints while optimising the number of
available safe workspaces in a return to work scenario. We propose and
demonstrate a graph based approach that solves the optimisation problem via
modelling as a bipartite graph of disconnected components over a graph of
constraints. We compare results obtained with a constrained random walk and a
linear programming approach.
","['\nMichael Barry\n', '\nClaudio Gambella\n', '\nFabio Lorenzi\n', '\nJohn Sheehan\n', '\nJoern Ploennigs\n']",,,http://arxiv.org/abs/2105.05017v1,cs.OH,"['cs.OH', 'math.OC']",,,[]
"People Counting using Radio Irregularity in Wireless Sensor Networks --
  An Experimental Study",http://arxiv.org/abs/2106.16143v1,2021-05-13T22:35:04Z,2021-05-13T22:35:04Z,"  The Internet has grown into a large cyber-physical system centered that
connects not just computer systems but a plethora of systems, devices, and
objects, collectively referred to as ""Things"", giving rise to the term
""Internet of Things"" (IoT). It encompasses technologies for identification and
tracking, sensing and actuation, both wired and wireless communications, and
also, intelligence and cognition. Wireless communications, which is an integral
part of IoT, suffers from radio irregularity -- a phenomenon referring to radio
waves being selectively absorbed, reflected or scattered by objects in their
paths, e.g., human bodies that comprises liquid, bone and flesh. Radio
irregularity is often regarded as a problem in wireless communications but,
with the envisioned pervasiveness of IoT, we aim to exploit radio irregularity
as a means to detect and estimate the number of people. We demonstrate how
radio signal fluctuations arising from radio irregularity, combined with
discriminant analysis, can be used to provide a simple low-cost alternative to
dedicated sensing systems for indoor people counting.
","['\nWei-Chuan Lin\n', '\nWinston K. G. Seah\n', '\nWei Li\n']",,,http://arxiv.org/abs/2106.16143v1,cs.NI,"['cs.NI', 'cs.OH']",,,[]
"YAPS -- Your Open Examination System for Activating and emPowering
  Students",http://arxiv.org/abs/2105.06552v1,2021-04-27T09:52:43Z,2021-04-27T09:52:43Z,"  There are numerous e-assessment systems devoted to specific domains under
diverse license models. Cost, extensibility, and maintainability are relevant
issues for an institution. Ease of use and inclusion into courses are
educator's main concerns. For students the user experience and fast transparent
feedback plus ""better"" tests are most important. Many exams still focus on
testing memorized knowledge, instead of improving and testing skills with
competence-oriented learning support and examinations, respectively. We discuss
design decisions and present the resulting architecture of YAPS - Your open
Assessment system for emPowering Students. YAPS has been used for very diverse
lectures in logistics, computer engineering, and algorithms for exams, but also
for empowering students by fast feedback during the learning period. We report
on results in a basic lecture on Computer Science for Mechanical Engineers.
","['\nFin Hendrik Bahnsen\n', '\nGoerschwin Fey\n']",,,http://arxiv.org/abs/2105.06552v1,cs.CY,"['cs.CY', 'cs.OH', 'K.3.1']",,,[]
"Considerations for using reproduction data in
  toxicokinetic-toxicodynamic modelling",http://arxiv.org/abs/2105.03254v1,2021-05-04T10:13:04Z,2021-05-04T10:13:04Z,"  Toxicokinetic-toxicodynamic (TKTD) modelling is essential to make sense of
the time dependence of toxic effects, and to interpret and predict consequences
of time-varying exposure. These advantages have been recognised in the
regulatory arena, especially for environmental risk assessment (ERA) of
pesticides, where time-varying exposure is the norm. We critically evaluate the
link between the modelled variables in TKTD models and the observations from
laboratory ecotoxicity tests. For the endpoint reproduction, this link is far
from trivial. The relevant TKTD models for sub-lethal effects are based on
Dynamic-Energy Budget (DEB) theory, which specifies a continuous investment
flux into reproduction. In contrast, experimental tests score egg or offspring
release by the mother. The link between model and data is particularly
troublesome when a species reproduces in discrete clutches, and even more so
when eggs are incubated in the mother's brood pouch (and release of neonates is
scored in the test). This situation is quite common among aquatic invertebrates
(e.g., cladocerans, amphipods, mysids), including many popular test species. We
discuss these and other issues with reproduction data, reflect on their
potential impact on DEB-TKTD analysis, and provide preliminary recommendations
to correct them. Both modellers and users of model results need to be aware of
these complications, as ignoring them could easily lead to unnecessary failure
of DEB-TKTD models during calibration, or when validating them against
independent data for other exposure scenarios.
","['\nTjalling Jager\n', '\nMarie Trijau\n', '\nNeil Sherborne\n', '\nBenoit Goussen\n', '\nRoman Ashauer\n']",13 pages,Integr Environ Assess Manag (2021) 18(2):479-487,http://dx.doi.org/10.1002/ieam.4476,q-bio.QM,"['q-bio.QM', 'cs.OH', 'q-bio.PE']",10.1002/ieam.4476,,[]
Introduction to Big data Technology,http://arxiv.org/abs/2104.08062v1,2021-04-15T13:34:45Z,2021-04-15T13:34:45Z,"  Big data is no more ""all just hype"" but widely applied in nearly all aspects
of our business, governments, and organizations with the technology stack of
AI. Its influences are far beyond a simple technique innovation but involves
all rears in the world. This chapter will first have historical review of big
data; followed by discussion of characteristics of big data, i.e. from the 3V's
to up 10V's of big data. The chapter then introduces technology stacks for an
or-ganization to build a big data application, from
infrastruc-ture/platform/ecosystem to constructional units and components.
Finally, we provide some big data online resources for reference.
","['\nBilal Abu-Salih\n', '\nPornpit Wongthongtham\n', '\nDengya Zhu\n', '\nKit Yan Chan\n', '\nAmit Rudra\n']",,,http://dx.doi.org/10.1007/978-981-33-6652-7_2,cs.OH,['cs.OH'],10.1007/978-981-33-6652-7_2,,[]
FireFly Autonomous Drone Project,http://arxiv.org/abs/2104.07758v1,2021-04-15T20:32:46Z,2021-04-15T20:32:46Z,"  As a fire erupts, the first few minutes can be critical, and first
respondents must race to the scene to analyze the situation and act fast before
it gets out of hand. Factors such as road traffic condition and distance may
not allow quick rescue operation using traditional means and methods, leading
to unmanageable spreading of fire, injuries or even deaths that can be avoided.
FireFly drone-based rescue consists of a squad of highly equipped drones that
will be the first responders to the fire site. Their intervention will make the
task of the fire rescue team much more effective and will contribute to reduce
the overall damage. As soon as the fire is detected by in-building implanted
sensors, the fire department would deploy a set of FireFly drones that would
fly to the site, scan the building, and send live fire status information to
the Fire fighter team. The drones would have the ability to identify trapped
humans using AI based pattern recognition tools (using sensors and thermal
cameras) and then drop them rescue kits as appropriate. The drones will also be
equipped with fire detection and recognition capabilities and be able to drop
fire extinguishing balls as first attempts to put off seeds of fires before
they evolve. The integration of drones with firefighting will allow for ease of
access and control of fire outbreaks. Drones will also result in increased
response time, prevention of further damage, and allow relaying of vital
information to out of reach places regarding the characteristics of the fire
scene.
","['\nHajer Ben Mnaouer\n', '\nMohammad Faieq\n', '\nAdel Yousefi\n', '\nSarra Ben Mnaouer\n']",,,http://arxiv.org/abs/2104.07758v1,cs.RO,"['cs.RO', 'cs.OH']",,,[]
A Composable Glitch-Aware Delay Model,http://arxiv.org/abs/2104.10966v1,2021-04-22T10:02:21Z,2021-04-22T10:02:21Z,"  We introduce the Composable Involution Delay Model (CIDM) for fast and
accurate digital simulation. It is based on the Involution Delay Model (IDM)
[F\""ugger et al., IEEE TCAD 2020], which has been shown to be the only existing
candidate for faithful glitch propagation known so far. In its present form,
however, it has shortcomings that limit its practical applicability and
utility. First, IDM delay predictions are conceptually based on discretizing
the analog signal waveforms using specific matching input and output
discretization threshold voltages. Unfortunately, they are difficult to
determine and typically different for interconnected gates. Second,
metastability and high-frequency oscillations in a real circuit could be
invisible in the IDM signal predictions. Our CIDM reduces the characterization
effort by allowing independent discretization thresholds, improves
composability and increases the modeling power by exposing canceled pulse
trains at the gate interconnect. We formally show that, despite these
improvements, the CIDM still retains the IDM's faithfulness, which is a
consequence of the mathematical properties of involution delay functions.
","['\nJürgen Maier\n', '\nDaniel Öhlinger\n', '\nUlrich Schmid\n', '\nMatthias Függer\n', '\nThomas Nowak\n']","13 pages, 9 figures, extended version of conference submission",,http://arxiv.org/abs/2104.10966v1,cs.OH,"['cs.OH', 'eess.SP', 'B.8.1; B.8.2']",,,[]
Gross polluters and vehicles' emissions reduction,http://arxiv.org/abs/2107.03282v3,2021-04-21T09:46:16Z,2022-03-17T15:43:08Z,"  Vehicles' emissions produce a significant share of cities' air pollution,
with a substantial impact on the environment and human health. Traditional
emission estimation methods use remote sensing stations, missing vehicles' full
driving cycle, or focus on a few vehicles. We use GPS traces and a microscopic
model to analyse the emissions of four air pollutants from thousands of private
vehicles in three European cities. We find that the emissions across the
vehicles and roads are well approximated by heavy-tailed distributions and thus
discover the existence of gross polluters, vehicles responsible for the
greatest quantity of emissions, and grossly polluted roads, which suffer the
greatest amount of emissions. Our simulations show that emissions reduction
policies targeting gross polluters are way more effective than those limiting
circulation based on a non-informed choice of vehicles. Our study contributes
to shaping the discussion on how to measure emissions with digital data.
","['\nMatteo Böhm\n', '\nMirco Nanni\n', '\nLuca Pappalardo\n']","Version to be published in Nature Sustainability. Minor changes due
  to the last round of reviews",Nat. Sustain. (2022),http://dx.doi.org/10.1038/s41893-022-00903-x,physics.soc-ph,"['physics.soc-ph', 'cs.OH']",10.1038/s41893-022-00903-x,,[]
"Hypothesis Formalization: Empirical Findings, Software Limitations, and
  Design Implications",http://arxiv.org/abs/2104.02712v1,2021-04-06T21:11:28Z,2021-04-06T21:11:28Z,"  Data analysis requires translating higher level questions and hypotheses into
computable statistical models. We present a mixed-methods study aimed at
identifying the steps, considerations, and challenges involved in
operationalizing hypotheses into statistical models, a process we refer to as
hypothesis formalization. In a formative content analysis of research papers,
we find that researchers highlight decomposing a hypothesis into
sub-hypotheses, selecting proxy variables, and formulating statistical models
based on data collection design as key steps. In a lab study, we find that
analysts fixated on implementation and shaped their analysis to fit familiar
approaches, even if sub-optimal. In an analysis of software tools, we find that
tools provide inconsistent, low-level abstractions that may limit the
statistical models analysts use to formalize hypotheses. Based on these
observations, we characterize hypothesis formalization as a dual-search process
balancing conceptual and statistical considerations constrained by data and
computation, and discuss implications for future tools.
","['\nEunice Jun\n', '\nMelissa Birchfield\n', '\nNicole de Moura\n', '\nJeffrey Heer\n', '\nRene Just\n']",,,http://arxiv.org/abs/2104.02712v1,cs.OH,"['cs.OH', 'cs.HC', 'cs.SE']",,,[]
One-Time Pads from the Digits of Pi,http://arxiv.org/abs/2103.08783v1,2021-03-16T00:34:44Z,2021-03-16T00:34:44Z,"  I present a method for generating one-time pads from the digits of pi.
Computer code is given to generate such pads from passphrases in a method
having an extremely low probability (<10^-53) of a successful discovery of the
one-time pads by a brute-force attack. The advantages and disadvantages of this
method are discussed.
",['\nDevlin Gualtieri\n'],"12 page PDF file (5 page article with 7 pages of computer source
  code). Comments are welcome",,http://arxiv.org/abs/2103.08783v1,cs.OH,['cs.OH'],,,[]
"FBCNet: A Multi-view Convolutional Neural Network for Brain-Computer
  Interface",http://arxiv.org/abs/2104.01233v1,2021-03-17T08:27:01Z,2021-03-17T08:27:01Z,"  Lack of adequate training samples and noisy high-dimensional features are key
challenges faced by Motor Imagery (MI) decoding algorithms for
electroencephalogram (EEG) based Brain-Computer Interface (BCI). To address
these challenges, inspired from neuro-physiological signatures of MI, this
paper proposes a novel Filter-Bank Convolutional Network (FBCNet) for MI
classification. FBCNet employs a multi-view data representation followed by
spatial filtering to extract spectro-spatially discriminative features. This
multistage approach enables efficient training of the network even when limited
training data is available. More significantly, in FBCNet, we propose a novel
Variance layer that effectively aggregates the EEG time-domain information.
With this design, we compare FBCNet with state-of-the-art (SOTA) BCI algorithm
on four MI datasets: The BCI competition IV dataset 2a (BCIC-IV-2a), the
OpenBMI dataset, and two large datasets from chronic stroke patients. The
results show that, by achieving 76.20% 4-class classification accuracy, FBCNet
sets a new SOTA for BCIC-IV-2a dataset. On the other three datasets, FBCNet
yields up to 8% higher binary classification accuracies. Additionally, using
explainable AI techniques we present one of the first reports about the
differences in discriminative EEG features between healthy subjects and stroke
patients. Also, the FBCNet source code is available at
https://github.com/ravikiran-mane/FBCNet.
","['\nRavikiran Mane\n', '\nEffie Chew\n', '\nKaren Chua\n', '\nKai Keng Ang\n', '\nNeethu Robinson\n', '\nA. P. Vinod\n', '\nSeong-Whan Lee\n', '\nCuntai Guan\n']","This work has been submitted to the IEEE for possible publication.
  Copyright may be transferred without notice, after which this version may no
  longer be accessible",,http://arxiv.org/abs/2104.01233v1,cs.OH,"['cs.OH', 'cs.AI', 'cs.LG', 'eess.SP']",,,[]
"Exact and heuristic approaches for multi-objective garbage accumulation
  points location in real scenarios",http://arxiv.org/abs/2103.04826v2,2021-03-05T13:47:21Z,2021-03-11T18:05:15Z,"  Municipal solid waste management is a major challenge for nowadays urban
societies, because it accounts for a large proportion of public budget and,
when mishandled, it can lead to environmental and social problems. This work
focuses on the problem of locating waste bins in an urban area, which is
considered to have a strong influence in the overall efficiency of the reverse
logistic chain. This article contributes with an exact multiobjective approach
to solve the waste bin location in which the optimization criteria that are
considered are: the accessibility to the system (as quality of service
measure), the investment cost, and the required frequency of waste removal from
the bins (as a proxy of the posterior routing costs). In this approach,
different methods to obtain the objectives ideal and nadir values over the
Pareto front are proposed and compared. Then, a family of heuristic methods
based on the PageRank algorithm is proposed which aims to optimize the
accessibility to the system, the amount of collected waste and the installation
cost. The experimental evaluation was performed on real-world scenarios of the
cities of Montevideo, Uruguay, and Bah\'ia Blanca, Argentina. The obtained
results show the competitiveness of the proposed approaches for constructing a
set of candidate solutions that considers the different trade-offs between the
optimization criteria.
","['\nDiego Gabriel Rossit\n', '\nJamal Toutouh\n', '\nSergio Nesmachnow\n']","This article has been accepted for publication in the Waste
  Management journal",Waste Management. 105:467-481 (2020),http://dx.doi.org/10.1016/j.wasman.2020.02.016,cs.OH,"['cs.OH', 'cs.AI']",10.1016/j.wasman.2020.02.016,,[]
"Using Fault Injection on the Nanosatellite Subsystems Integration
  Testing",http://arxiv.org/abs/2102.11776v1,2021-02-23T16:22:32Z,2021-02-23T16:22:32Z,"  Since the 2000's, an increased number of nanosatellites have accessed space.
However, studies show that the number of unsuccessful nanosatellite missions is
very expressive. Moreover, these statistics are correlated to poor verification
and validation processes used by hobbyists satellite developers because major
space agencies keep high successful ratings even with small/nano satellites
missions due to its rigorous V\&V processes. Aiming to improve payloads
integration testing of NanosatC-BR-2, a 2-U Cubesat based nanosatellite under
development by INPE, the fault injection technique has been used. It is very
useful technique to test systems prototypes. We present the design and
implementation of a Failure Emulator Mechanism (FEM) on I2C communication bus
for testing the interaction among the NCBR2 subsystems, supporting
interoperability and robustness requirements verification. The FEM is modelled
to work at the communication bus emulating eventual faults of the communicating
subsystems in the messages exchanged. Using an Arduino board for the FEM and NI
LabView environment it is possible to program the mechanism to inject different
faults at the I2C bus during different operation modes. Based on a serial
architecture, the FEM will be able to intercept all messages and implement
different faults as service and timing faults. The FEM interface with the
tester is designed in LabView environment. Control and observation facilities
are available to generate and upload the faultload script to FEM Arduino board.
The proposed FEM architecture and its implementation are validated using two
subsystems under testing prototypes: the OnBoard Data Handling Computer and the
Langmuir Probe NCBR2 payload. For this analysis purpose, the prototypes
simulate in two different Arduinos boards the expected behavior of each
subsystem in the communication.
","['\nCarlos Leandro Gomes Batista\n', '\nAndré Corsetti\n', '\nFátima Mattiello-Francisco\n']","8 pages, 8 figures, 1st IAA Latin American Symposium on Small
  Satellites",,http://arxiv.org/abs/2102.11776v1,cs.OH,['cs.OH'],,,[]
"A Multi-Stage Stochastic Programming Approach to Epidemic Resource
  Allocation with Equity Considerations",http://arxiv.org/abs/2102.11814v1,2021-02-23T17:28:40Z,2021-02-23T17:28:40Z,"  Existing compartmental models in epidemiology are limited in terms of
optimizing the resource allocation to control an epidemic outbreak under
disease growth uncertainty. In this study, we address this core limitation by
presenting a multi-stage stochastic programming compartmental model, which
integrates the uncertain disease progression and resource allocation to control
an infectious disease outbreak. The proposed multi-stage stochastic program
involves various disease growth scenarios and optimizes the distribution of
treatment centers and resources while minimizing the total expected number of
new infections and funerals. We define two new equity metrics, namely infection
and capacity equity, and explicitly consider equity for allocating treatment
funds and facilities over multiple time stages. We also study the multi-stage
value of the stochastic solution (VSS), which demonstrates the superiority of
the proposed stochastic programming model over its deterministic counterpart.
We apply the proposed formulation to control the Ebola Virus Disease (EVD) in
Guinea, Sierra Leone, and Liberia of West Africa to determine the optimal and
fair resource-allocation strategies. Our model balances the proportion of
infections over all regions, even without including the infection equity or
prevalence equity constraints. Model results also show that allocating
treatment resources proportional to population is sub-optimal, and enforcing
such a resource allocation policy might adversely impact the total number of
infections and deaths, and thus resulting in a high cost that we have to pay
for the fairness. Our multi-stage stochastic epidemic-logistics model is
practical and can be adapted to control other infectious diseases in
meta-populations and dynamically evolving situations.
","['\nXuecheng Yin\n', '\nI. Esra Buyuktahtakin\n']","Accepted for publication in Health Care Management Science, Feb 19,
  2021, 1-58",,http://arxiv.org/abs/2102.11814v1,stat.AP,"['stat.AP', 'cs.OH', 'math.OC', 'q-bio.PE']",,,[]
"Practical application of the multi-model approach in the study of
  complex systems",http://arxiv.org/abs/2102.08153v1,2021-02-16T13:51:11Z,2021-02-16T13:51:11Z,"  Different kinds of models are used to study various natural and technical
phenomena. Usually, the researcher is limited to using a certain kind of model
approach, not using others (or even not realizing the existence of other model
approaches). The authors believe that a complete study of a certain phenomenon
should cover several model approaches. The paper describes several model
approaches which we used in the study of the random early detection algorithm
for active queue management. Both the model approaches themselves and their
implementation and the results obtained are described.
","['\nAnna V. Korolkova\n', '\nDmitry S. Kulyabov\n', '\nMichal Hnatič\n']",in English; in Russian,,http://dx.doi.org/10.1007/978-3-030-66471-8_40,cs.OH,['cs.OH'],10.1007/978-3-030-66471-8_40,,[]
Designing a Binary Clock using logic gates,http://arxiv.org/abs/2102.02845v1,2021-02-04T19:28:23Z,2021-02-04T19:28:23Z,"  Wristwatches have been a common fashion accessory addition for several
people. However, the concept of using a seven-segment digital display or
sometimes, even an analog indicator hasn't changed for a number of years. This
project aims to test and design a binary clock, also referred to as 32, 16, 8,
4, 2, 1 clock or even 8, 4, 2, 1 clock (due to their display configuration),
that could change this everlasting display for watches. Specifically, digital
logic and design engineers would find interest in this topic due to the
sophistication involved in reading-out the time. This project will do so using
by showing each decimal digit of sexagesimal time as a binary value. This
design will be primarily functioning on logic gates and would involve the use
of several basic components that include, but are not limited to, integrated
circuits (or ICs), Light-emitting diodes (LEDs), and resistors.
",['\nJacob John\n'],"5 pages, 9 figures, 7 references",,http://arxiv.org/abs/2102.02845v1,cs.OH,['cs.OH'],,,[]
"Analyzing and comparing door-to-door travel times for air transportation
  using aggregated Uber data",http://arxiv.org/abs/2101.08852v1,2021-01-21T21:02:43Z,2021-01-21T21:02:43Z,"  Improving the passenger air travel experience is one of the explicit goals
set by the Next Generation Air Transportation System in the United States and
by the Advisory Council for Aeronautics Research in Europe FlightPath 2050.
Both suggest door-to-door travel times as a potential metric for these
objectives. In this paper, we propose a data-driven model to estimate
door-to-door travel times and compare the reach and performance of different
access modes to a city, as well as conduct segment analysis of full
door-to-door trips. This model can also be used to compare cities with respect
to the integration of their airport within their road structure. We showcase
multiple applications of this full door-to-door travel time model to
demonstrate how the model can be used to locate where progress can be made.
","['\nPhilippe Monmousseau\n', '\nAude Marzuoli\n', '\nEric Feron\n', '\nDaniel Delahaye\n']","11 pages, 15 figures and 6 tables",,http://arxiv.org/abs/2101.08852v1,cs.OH,['cs.OH'],,,[]
"The Future of Artificial Intelligence and its Social, Economic and
  Ethical Consequences",http://arxiv.org/abs/2101.03366v1,2021-01-09T14:21:54Z,2021-01-09T14:21:54Z,"  Recent development in AI has enabled the expansion of its application to
multiple domains. From medical treatment, gaming, manufacturing to daily
business processes. A huge amount of money has been poured into AI research due
to its exciting discoveries. Technology giants like Google, Facebook, Amazon,
and Baidu are the driving forces in the field today. But the rapid growth and
excitement that the technology offers obscure us from looking at the impact it
brings on our society. This short paper gives a brief history of AI and
summarizes various social, economic and ethical issues that are impacting our
society today. We hope that this work will provide a useful starting point and
perhaps reference for newcomers and stakeholders of the field.
","['\nBurhan Rashid Hussein\n', '\nChongomweru Halimu\n', '\nMuhammad Tariq Siddique\n']","International Conference on Advances in Computing and Technology
  ICACT 2020 Proceedings",,http://arxiv.org/abs/2101.03366v1,cs.OH,['cs.OH'],,,[]
"The Study of Urban Residential's Public Space Activeness using
  Space-centric Approach",http://arxiv.org/abs/2101.03725v2,2021-01-11T06:47:38Z,2021-01-12T02:11:02Z,"  With the advancement of the Internet of Things (IoT) and communication
platform, large scale sensor deployment can be easily implemented in an urban
city to collect various information. To date, there are only a handful of
research studies about understanding the usage of urban public spaces.
Leveraging IoT, various sensors have been deployed in an urban residential area
to monitor and study public space utilization patterns. In this paper, we
propose a data processing system to generate space-centric insights about the
utilization of an urban residential region of multiple points of interest
(PoIs) that consists of 190,000m$^2$ real estate. We identify the activeness of
each PoI based on the spectral clustering, and then study their corresponding
static features, which are composed of transportation, commercial facilities,
population density, along with other characteristics. Through the heuristic
features inferring, the residential density and commercial facilities are the
most significant factors affecting public place utilization.
","['\nBilly Pik Lik Lau\n', '\nBenny Kai Kiat Ng\n', '\nChau Yuen\n', '\nBige Tuncer\n', '\nKeng Hua Chong\n']",Accepted at IEEE Internet of Things Journal 2021,,http://dx.doi.org/10.1109/JIOT.2021.3051343,stat.AP,"['stat.AP', 'cs.OH']",10.1109/JIOT.2021.3051343,,[]
Foresight AND Hindsight,http://arxiv.org/abs/2101.03111v1,2020-12-27T22:54:07Z,2020-12-27T22:54:07Z,"  To model is to represent. The threshold of decidability defines two
epistemological choices: one model (or a finite number of models) suffices for
representing the dynamics below the undecidable; above this threshold (defined
as G-complexity), every model is partial, no complete modeling is possible.
",['\nMihai Nadin\n'],"11 pages, 1 figure",,http://arxiv.org/abs/2101.03111v1,cs.OH,['cs.OH'],,,[]
"Analogy, Mind, and Life",http://arxiv.org/abs/2012.13803v1,2020-12-26T19:55:04Z,2020-12-26T19:55:04Z,"  I'll show that the kind of analogy between life and information [argue for by
authors such as Davies (2000), Walker and Davies (2013), Dyson (1979), Gleick
(2011), Kurzweil (2012), Ward (2009)], that seems to be central to the effect
that artificial mind may represents an expected advance in the life evolution
in Universe, is like the design argument and that if the design argument is
unfounded and invalid, the argument to the effect that artificial mind may
represents an expected advance in the life evolution in Universe is also
unfounded and invalid. However, if we are prepared to admit (though we should
not do) this method of reasoning as valid, I'll show that the analogy between
life and information to the effect that artificial mind may represents an
expected advance in the life evolution in Universe seems suggest some type of
reductionism of life to information, but biology respectively chemistry or
physics are not reductionist, contrary to what seems to be suggested by the
analogy between life and information.
",['\nVitor Manuel Dinis Pereira\n'],"27 pages, 0 figures, chapter book, (2015).Tran, Q-N. and Arabnia,
  H.R. (eds.). Emerging Trends in Computational Biology, Bioinformatics, and
  Systems Biology. Elsevier/Morgan Kaufmann",,http://dx.doi.org/10.1016/b978-0-12-802508-6.00020-x,cs.OH,['cs.OH'],10.1016/b978-0-12-802508-6.00020-x,,[]
SEH: Size Estimate Hedging for Single-Server Queues,http://arxiv.org/abs/2101.00007v5,2020-12-30T00:25:44Z,2023-01-21T02:53:35Z,"  For a single server system, Shortest Remaining Processing Time (SRPT) is an
optimal size-based policy. In this paper, we discuss scheduling a single-server
system when exact information about the jobs' processing times is not
available. When the SRPT policy uses estimated processing times, the
underestimation of large jobs can significantly degrade performance. We propose
a simple heuristic, Size Estimate Hedging (SEH), that only uses estimated
processing times for scheduling decisions. A job's priority is increased
dynamically according to an SRPT rule until it is determined that it is
underestimated, at which time the priority is frozen. Numerical results suggest
that SEH has desirable performance for estimation error variance that is
consistent with what is seen in practice.
","['\nMaryam Akbari-Moghaddam\n', '\nDouglas G. Down\n']","For a published version of this paper refer to Quantitative
  Evaluation of Systems: 18th International Conference, https:
  //doi.org/10.1007/978-3-030-85172-9_9","International Conference on Quantitative Evaluation of Systems,
  pp. 168-185. Springer, Cham, 2021",http://dx.doi.org/10.1007/978-3-030-85172-9_9,cs.OH,"['cs.OH', 'math.OC']",10.1007/978-3-030-85172-9_9,,[]
"Looking for non-compliant documents using error messages from multiple
  parsers",http://arxiv.org/abs/2012.10211v1,2020-12-15T19:54:58Z,2020-12-15T19:54:58Z,"  Whether a file is accepted by a single parser is not a reliable indication of
whether a file complies with its stated format. Bugs within both the parser and
the format specification mean that a compliant file may fail to parse, or that
a non-compliant file might be read without any apparent trouble. The latter
situation presents a significant security risk, and should be avoided. This
article suggests that a better way to assess format specification compliance is
to examine the set of error messages produced by a set of parsers rather than a
single parser. If both a sample of compliant files and a sample of
non-compliant files are available, then we show how a statistical test based on
a pseudo-likelihood ratio can be very effective at determining a file's
compliance. Our method is format agnostic, and does not directly rely upon a
formal specification of the format. Although this article focuses upon the case
of the PDF format (ISO 32000-2), we make no attempt to use any specific details
of the format. Furthermore, we show how principal components analysis can be
useful for a format specification designer to assess the quality and structure
of these samples of files and parsers. While these tests are absolutely
rudimentary, it appears that their use to measure file format variability and
to identify non-compliant files is both novel and surprisingly effective.
",['\nMichael Robinson\n'],"15 pages, 8 figures",,http://arxiv.org/abs/2012.10211v1,cs.OH,"['cs.OH', 'cs.PL', '62P30', 'D.3.4']",,,[]
A Quantum Edge Detection Algorithm,http://arxiv.org/abs/2012.11036v1,2020-12-20T22:10:05Z,2020-12-20T22:10:05Z,"  The application of quantum computing to the field of image processing has
produced several promising applications: quantum image representation
techniques have been developed showing how, by taking advantage of quantum
properties like entanglement and superposition, many image processing
algorithms could have an exponential speed-up in comparison to their
""classical"" counterparts. In this paper, after briefly discussing some of the
main quantum image representation methods, we propose an improved version of a
quantum edge detection algorithm.
","['\nGiacomo Cavalieri\n', '\nDario Maio\n']",,,http://arxiv.org/abs/2012.11036v1,quant-ph,"['quant-ph', 'cs.OH']",,,[]
"6-Layer Model for a Structured Description and Categorization of Urban
  Traffic and Environment",http://arxiv.org/abs/2012.06319v2,2020-12-09T16:11:32Z,2021-02-02T16:39:11Z,"  Verification and validation of automated driving functions impose large
challenges. Currently, scenario-based approaches are investigated in research
and industry, aiming at a reduction of testing efforts by specifying safety
relevant scenarios. To define those scenarios and operate in a complex
real-world design domain, a structured description of the environment is
needed. Within the PEGASUS research project, the 6-Layer Model (6LM) was
introduced for the description of highway scenarios. This paper refines the 6LM
and extends it to urban traffic and environment. As defined in PEGASUS, the 6LM
provides the possibility to categorize the environment and, therefore,
functions as a structured basis for subsequent scenario description. The model
enables a structured description and categorization of the general environment,
without incorporating any knowledge or anticipating any functions of actors.
Beyond that, there is a variety of other applications of the 6LM, which are
elaborated in this paper. The 6LM includes a description of the road network
and traffic guidance objects, roadside structures, temporary modifications of
the former, dynamic objects, environmental conditions and digital information.
The work at hand specifies each layer by categorizing its items. Guidelines are
formulated and explanatory examples are given to standardize the application of
the model for an objective environment description. In contrast to previous
publications, the model and its design are described in far more detail.
Finally, the holistic description of the 6LM presented includes remarks on
possible future work when expanding the concept to machine perception aspects.
","['\nMaike Scholtes\n', '\nLukas Westhofen\n', '\nLara Ruth Turner\n', '\nKatrin Lotto\n', '\nMichael Schuldes\n', '\nHendrik Weber\n', '\nNicolas Wagener\n', '\nChristian Neurohr\n', '\nMartin Bollmann\n', '\nFranziska Körtke\n', '\nJohannes Hiller\n', '\nMichael Hoss\n', '\nJulian Bock\n', '\nLutz Eckstein\n']","16 pages, 7 figures, submitted to IEEE Access",,http://arxiv.org/abs/2012.06319v2,cs.OH,"['cs.OH', 'cs.AI', 'cs.SE']",,,[]
Observement as Universal Measurement,http://arxiv.org/abs/2012.12095v1,2020-12-07T08:05:31Z,2020-12-07T08:05:31Z,"  Measurement theory is the cornerstone of science, but no equivalent theory
underpins the huge volumes of non-numerical data now being generated. In this
study, we show that replacing numbers with alternative mathematical models,
such as strings and graphs, generalises traditional measurement to provide
rigorous, formal systems (`observement') for recording and interpreting
non-numerical data. Moreover, we show that these representations are already
widely used and identify general classes of interpretive methodologies implicit
in representations based on character strings and graphs (networks). This
implies that a generalised concept of measurement has the potential to reveal
new insights as well as deep connections between different fields of research.
","['\nDavid G. Green\n', '\nKerri Morgan\n', '\nMarc Cheong\n']","28 pages, 10 figures",,http://arxiv.org/abs/2012.12095v1,cs.OH,"['cs.OH', 'cs.IT', 'cs.LG', 'math.IT']",,,[]
"Impacts of the Space Technology Evolution in the V\&V of Embedded
  Software-Intensive Systems",http://arxiv.org/abs/2011.14914v1,2020-11-26T19:24:04Z,2020-11-26T19:24:04Z,"  CubeSat-based nanosatellites are composed of COTS components and rely on its
structure and standardized interfaces. A challenge in the nanosatellites
context is to adapt the V\&V (Verification and Validation) process to answer to
the increase importance of the embedded software, to reduce the artefacts to be
delivered aiming at cutting cost and time and still complying with
international standards. This work presents an analysis of the strategy adopted
in a real nanosatellite for the development of the OBDH software embedded in
NanosatC-BR2 mission. The goal is to discuss the impact that the
standardization of the structure and interfaces of the CubeSat impose on the
V\&V process of the SiS and to highlight the challenges of ``New Space Age``
for the use of existing V\&V techniques and methods.
","['\nCarlos Leandro Gomes Batista\n', '\nTania Basso\n', '\nFátima Mattiello-Francisco\n', '\nRegina Moraes\n']","7 pages, 7 figures 2020 International Conference on Computational
  Science and Computational Intelligence (CSCI)",,http://arxiv.org/abs/2011.14914v1,cs.OH,"['cs.OH', 'cs.SE']",,,[]
"Calibrating Path Choices and Train Capacities for Urban Rail Transit
  Simulation Models Using Smart Card and Train Movement Data",http://arxiv.org/abs/2012.07731v3,2020-11-23T17:41:25Z,2022-12-11T03:37:49Z,"  Transit network simulation models are often used for performance and
retrospective analysis of urban rail systems, taking advantage of the
availability of extensive automated fare collection (AFC) and automated vehicle
location (AVL) data. Important inputs to such models, in addition to
origin-destination flows, include passenger path choices and train capacity.
Train capacity, which has often been overlooked in the literature, is an
important input that exhibits a lot of variabilities. The paper proposes a
simulation-based optimization (SBO) framework to simultaneously calibrate path
choices and train capacity for urban rail systems using AFC and AVL data. The
calibration is formulated as an optimization problem with a black-box objective
function. Seven algorithms from four branches of SBO solving methods are
evaluated. The algorithms are evaluated using an experimental design that
includes five scenarios, representing different degrees of path choice
randomness and crowding sensitivity. Data from the Hong Kong Mass Transit
Railway (MTR) system is used as a case study. The data is used to generate
synthetic observations used as ""ground truth"". The results show that the
response surface methods (particularly Constrained Optimization using Response
Surfaces) have consistently good performance under all scenarios. The proposed
approach drives large-scale simulation applications for monitoring and
planning.
","['\nBaichuan Mo\n', '\nZhenliang Ma\n', '\nHaris N. Koutsopoulos\n', '\nJinhua Zhao\n']",The paper's author names are wrong in the google scholar,,http://dx.doi.org/10.1155/2021/5597130,cs.OH,"['cs.OH', 'cs.SY', 'eess.SY']",10.1155/2021/5597130,,[]
"Comparison Analysis of Tree Based and Ensembled Regression Algorithms
  for Traffic Accident Severity Prediction",http://arxiv.org/abs/2010.14921v1,2020-10-27T11:52:39Z,2020-10-27T11:52:39Z,"  Rapid increase of traffic volume on urban roads over time has changed the
traffic scenario globally. It has also increased the ratio of road accidents
that can be severe and fatal in the worst case. To improve traffic safety and
its management on urban roads, there is a need for prediction of severity level
of accidents. Various machine learning models are being used for accident
prediction. In this study, tree based ensemble models (Random Forest, AdaBoost,
Extra Tree, and Gradient Boosting) and ensemble of two statistical models
(Logistic Regression Stochastic Gradient Descent) as voting classifiers are
compared for prediction of road accident severity. Significant features that
are strongly correlated with the accident severity are identified by Random
Forest. Analysis proved Random Forest as the best performing model with highest
classification results with 0.974 accuracy, 0.954 precision, 0.930 recall and
0.942 F-score using 20 most significant features as compared to other
techniques classification of road accidents severity.
","['\nMuhammad Umer\n', '\nSaima Sadiq\n', '\nAbid Ishaq\n', '\nSaleem Ullah\n', '\nNajia Saher\n', '\nHamza Ahmad Madni\n']",,,http://arxiv.org/abs/2010.14921v1,cs.OH,"['cs.OH', 'cs.LG']",,,[]
"To Lane or Not to Lane? Comparing On-Road Experiences in Developing and
  Developed Countries using a New Simulator ""RoadBird""",http://arxiv.org/abs/2010.08590v1,2020-10-15T16:23:39Z,2020-10-15T16:23:39Z,"  Even though the traffic systems in developed countries have been analyzed
with rigor and operated efficiently, the same does not generally hold for
developing countries due to inadequate planning, design, and operations of
their transportation systems. Because of inherent differences between internal
infrastructures, the systems deployed in developed countries may not be
amenable to developing ones. Besides, the traffic systems of developing
countries are not well-studied in the literature to the best of our knowledge.
For example, it is yet to explore how a developed country's lane-based traffic
flow would perform in the context of a developing country, which generally
experiences non-lane-based traffic. As such, by using our newly developed
traffic simulator 'RoadBird', we investigate outcomes of both lane-based and
non-lane-based traffic from the contexts of both developing and developed
countries. To do so, we run simulations over real road topologies (extracted
from the GIS maps of major cities such as Dhaka, Miami, and Riyadh) considering
different scenarios such as lane-based or non-lane-based flows, homogeneous or
heterogeneous traffic, with or without pedestrians, etc. We also incorporate
different car-following and lane-changing models to mimic traffic behaviors and
investigate their performances. While the lane changing dilemma remains an open
research question, our experimental evidences indicate: (i) lane-based
approaches will not necessarily perform better in the case of currently-adopted
non-lane-based scenarios; and (ii) non-lane-based strategies may benefit system
performance in lane-based scenarios while having heavy mixed traffic.
Nonetheless, we reveal several new insights for on-road experiences both in
developing and developed countries.
","['\nMd. Masum Mushfiq\n', '\nTarik Reza Toha\n', '\nSaiful Islam Salim\n', '\nAaiyeesha Mostak\n', '\nMasfiqur Rahaman\n', '\nNajla Abdulrahman Al-Nabhan\n', '\nArif Mohamin Sadri\n', '\nA. B. M. Alim Al Islam\n']",,,http://arxiv.org/abs/2010.08590v1,cs.OH,['cs.OH'],,,[]
On licenses for [Open] Hardware,http://arxiv.org/abs/2010.09039v1,2020-10-18T17:29:12Z,2020-10-18T17:29:12Z,"  This document explains the basic concepts related to software and hardware
licenses, and it summarizes the most popular licenses that are currently used
for hardware projects. Two case studies of hardware projects at different
levels of abstraction are also presented, together with a discussion of license
applicability, commercial issues, code protection, and related concerns. This
paper intends to help the reader understand how to release open hardware with
the most appropriate license, and to answer questions that are of current
interest. We have been mainly motivated by the growing influence of the open
RISC-V ISA, but trying to address a wider hardware point of view.
","['\nMàrius Montón\n', '\nXavier Salazar\n']","6 pages, 3 figures, Submitted to DCIS 2020 conference",,http://arxiv.org/abs/2010.09039v1,cs.OH,['cs.OH'],,,[]
"A microsimulation approach for the impact assessment of a
  Vehicle-to-Infrastructure based Road Hazard Warning system",http://arxiv.org/abs/2010.10262v1,2020-10-14T17:09:28Z,2020-10-14T17:09:28Z,"  Cooperative Intelligent Transportation Systems (C-ITS) constitute
technologies which enable vehicles to communicate with each other and with road
infrastructure. Verification or testing is required for C-ITS applications, in
order to assess their impact on traffic operation. In this work, a microscopic
traffic simulation approach is used, to evaluate the impact of
Vehicle-to-Infrastructure (V2I) technologies in the context of a road traffic
accident. Specifically, the methodology is implemented to explicitly models
vehicles collisions, Road Hazard Warning (RHW), Emergency Electronic Brake
Light (EEBL) warnings and the resulting driver behavior. Moreover, a new gap
control mechanism is adopted, to improve safety by advising vehicles in hazard
lane to increase their headways with respect to their preceding vehicle, so
that they can avoid a collision. Perfect communication links to all vehicles
are assumed. The study findings indicate that the proposed V2I hazard warning
strategy has a positive impact on traffic flow safety and efficiency.
","['\nKallirroi N. Porfyri\n', '\nAreti Kotsi\n', '\nEvangelos Mitsakis\n']",,,http://arxiv.org/abs/2010.10262v1,cs.OH,['cs.OH'],,,[]
C-ITS bundling for integrated traffic management,http://arxiv.org/abs/2011.03425v1,2020-10-14T17:11:06Z,2020-10-14T17:11:06Z,"  Cooperative Intelligent Transportation Systems (C-ITS) enable vehicles
communication with each other (Vehicle-to-Vehicle, V2V) and with roadside
infrastructure (Vehicle-to-Infrastructure, V2I). In the context of traffic
efficiency, C-ITS technologies could assist in road network status
visualization and monitoring, through data exchange, improving this way traffic
control organization and traffic management implementation. Bundling is the
provision of several C-ITS services as one combined service. The purpose of
bundling is to harvest the usability of C-ITS services by developing a strategy
for the operation and exploitation of services in real-time and within varying
geographical areas. Two different dimensions of bundling have been recognized
covering: 1) end-users, and 2) operators-managers. The objective of the
operators-managers dimension is the integration of C-ITS services in
operational traffic management. This work spotlights the operators-managers
bundling dimension, presenting a framework based on a step-by-step approach for
integrating C-ITS services in traditional traffic management.
","['\nEvangelos Mitsakis\n', '\nAreti Kotsi\n', '\nVasileios Psonis\n']",,,http://arxiv.org/abs/2011.03425v1,cs.OH,['cs.OH'],,,[]
"Arc Flow Formulations Based on Dynamic Programming: Theoretical
  Foundations and Applications",http://arxiv.org/abs/2010.00558v2,2020-10-01T17:23:14Z,2021-04-15T19:50:36Z,"  Network flow formulations are among the most successful tools to solve
optimization problems. Such formulations correspond to determining an optimal
flow in a network. One particular class of network flow formulations is the arc
flow, where variables represent flows on individual arcs of the network. For
$\mathcal{NP}$-hard problems, polynomial-sized arc flow models typically
provide weak linear relaxations and may have too much symmetry to be efficient
in practice. Instead, arc flow models with a pseudo-polynomial size usually
provide strong relaxations and are efficient in practice. The interest in
pseudo-polynomial arc flow formulations has grown considerably in the last
twenty years, in which they have been used to solve many open instances of hard
problems. A remarkable advantage of pseudo-polynomial arc flow models is the
possibility to solve practical-sized instances directly by a Mixed Integer
Linear Programming solver, avoiding the implementation of complex methods based
on column generation.
  In this survey, we present theoretical foundations of pseudo-polynomial arc
flow formulations, by showing a relation between their network and Dynamic
Programming (DP). This relation allows a better understanding of the strength
of these formulations, through a link with models obtained by Dantzig-Wolfe
decomposition. The relation with DP also allows a new perspective to relate
state-space relaxation methods for DP with arc flow models. We also present a
dual point of view to contrast the linear relaxation of arc flow models with
that of models based on paths and cycles. To conclude, we review the main
solution methods and applications of arc flow models based on DP in several
domains such as cutting, packing, scheduling, and routing.
","['\nVinícius L. de Lima\n', '\nCláudio Alves\n', '\nFrançois Clautiaux\n', '\nManuel Iori\n', '\nJosé M. Valério de Carvalho\n']",,,http://dx.doi.org/10.1016/j.ejor.2021.04.024,math.OC,"['math.OC', 'cs.OH']",10.1016/j.ejor.2021.04.024,,[]
"Investigating Cultural Aspects in the Fundamental Diagram using
  Convolutional Neural Networks and Simulation",http://arxiv.org/abs/2010.11995v1,2020-09-30T14:44:04Z,2020-09-30T14:44:04Z,"  This paper presents a study regarding group behavior in a controlled
experiment focused on differences in an important attribute that vary across
cultures -- the personal spaces -- in two Countries: Brazil and Germany. In
order to coherently compare Germany and Brazil evolutions with same population
applying same task, we performed the pedestrian Fundamental Diagram experiment
in Brazil, as performed in Germany. We use CNNs to detect and track people in
video sequences. With this data, we use Voronoi Diagrams to find out the
neighbor relation among people and then compute the walking distances to find
out the personal spaces. Based on personal spaces analyses, we found out that
people behavior is more similar, in terms of their behaviours, in high dense
populations and vary more in low and medium densities. So, we focused our study
on cultural differences between the two Countries in low and medium densities.
Results indicate that personal space analyses can be a relevant feature in
order to understand cultural aspects in video sequences. In addition to the
cultural differences, we also investigate the personality model in crowds,
using OCEAN. We also proposed a way to simulate the FD experiment from other
countries using the OCEAN psychological traits model as input. The simulated
countries were consistent with the literature.
","['\nRodolfo M. Favaretto\n', '\nRoberto R. Santos\n', '\nMarcio Ballotin\n', '\nPaulo Knob\n', '\nSoraia R. Musse\n', '\nFelipe Vilanova\n', '\nAngelo B. Costa\n']","Computer Animation and Virtual Worlds, 2019",,http://dx.doi.org/10.1002/cav.1899,cs.OH,"['cs.OH', 'cs.CV', 'cs.GR', 'physics.soc-ph']",10.1002/cav.1899,,[]
JXES: JSON Support for the XES Event Log Standard,http://arxiv.org/abs/2009.06363v1,2020-09-14T12:30:21Z,2020-09-14T12:30:21Z,"  Process mining assumes the existence of an event log where each event refers
to a case, an activity, and a point in time. XES is an XML based IEEE approved
standard format for event logs supported by most of the process mining tools.
JSON (JavaScript Object Notation) is a lightweight data interchange format. In
this paper, we present JXES, the JSON standard for the event logs and also
provide implementation in ProM for importing and exporting event logs in JSON
format using 4 different parsers. The evaluation results show notable
performance differences between the different parsers (Simple JSON, Jackson,
GSON, Jsoninter).
","['\nMadhavi Bangalore Shankara Narayana\n', '\nHossameldin Khalifa\n', '\nWil van der Aalst\n']","4 pages, IEEE format",,http://arxiv.org/abs/2009.06363v1,cs.OH,['cs.OH'],,,[]
"Gaining or Losing Perspective for Piecewise-Linear Under-Estimators of
  Convex Univariate Functions",http://arxiv.org/abs/2009.07178v1,2020-09-15T15:31:37Z,2020-09-15T15:31:37Z,"  We study MINLO (mixed-integer nonlinear optimization) formulations of the
disjunction $x\in\{0\}\cup[\ell,u]$, where $z$ is a binary indicator of
$x\in[\ell,u]$ ($0 \leq \ell <u$), and $y$ ""captures"" $f(x)$, which is assumed
to be convex and positive on its domain $[\ell,u]$, but otherwise $y=0$ when
$x=0$. This model is very useful in nonlinear combinatorial optimization, where
there is a fixed cost of operating an activity at level $x$ in the operating
range $[\ell,u]$, and then there is a further (convex) variable cost $f(x)$. In
particular, we study relaxations related to the perspective transformation of a
natural piecewise-linear under-estimator of $f$, obtained by choosing
linearization points for $f$. Using 3-d volume (in $(x,y,z)$) as a measure of
the tightness of a convex relaxation, we investigate relaxation quality as a
function of $f$, $\ell$, $u$, and the linearization points chosen. We make a
detailed investigation for convex power functions $f(x):=x^p$, $p>1$.
","['\nJon Lee\n', '\nDaphne Skipper\n', '\nEmily Speakman\n', '\nLuze Xu\n']",,,http://arxiv.org/abs/2009.07178v1,math.OC,"['math.OC', 'cs.OH', '90C26 90C25 65K05 49M15']",,,[]
"Investigating the Performance Gap between Testing on Real and Denoised
  Aggregates in Non-Intrusive Load Monitoring",http://arxiv.org/abs/2008.10985v2,2020-08-22T12:00:50Z,2020-10-03T12:50:12Z,"  Prudent and meaningful performance evaluation of algorithms is essential for
the progression of any research field. In the field of Non-Intrusive Load
Monitoring (NILM), performance evaluation can be conducted on real-world
aggregate signals, provided by smart energy meters or artificial superpositions
of individual load signals (i.e., denoised aggregates). It has long been
suspected that testing on these denoised aggregates provides better evaluation
results mainly due to the the fact that the signal is less complex. Complexity
in real-world aggregate signals increases with the number of unknown/untracked
load. Although this is a known performance reporting problem, an investigation
in the actual performance gap between real and denoised testing is still
pending. In this paper, we examine the performance gap between testing on
real-world and denoised aggregates with the aim of bringing clarity into this
matter. Starting with an assessment of noise levels in datasets, we find
significant differences in test cases. We give broad insights into our
evaluation setup comprising three load disaggregation algorithms, two of them
relying on neural network architectures. The results presented in this paper,
based on studies covering three scenarios with ascending noise levels, show a
strong tendency towards load disaggregation algorithms providing significantly
better performance on denoised aggregate signals. A closer look into the
outcome of our studies reveals that all appliance types could be subject to
this phenomenon. We conclude the paper by discussing aspects that could be
causing these considerable gaps between real and denoised testing in NILM.
","['\nChristoph Klemenjak\n', '\nStephen Makonin\n', '\nWilfried Elmenreich\n']",,,http://arxiv.org/abs/2008.10985v2,cs.OH,['cs.OH'],,,[]
"LED wristbands for Cell-based Crowd Evacuation: an Adaptive Exit-choice
  Guidance System Architecture",http://arxiv.org/abs/2008.11128v1,2020-08-19T09:49:15Z,2020-08-19T09:49:15Z,"  Cell-based crowd evacuation systems provide adaptive or static exit-choice
indications that favor a coordinated group dynamic, improving evacuation time
and safety. While a great effort has been made to modeling its control logic by
assuming an ideal communication and positioning infrastructure, the
architectural dimension and the influence of pedestrian positioning uncertainty
have been largely overlooked. In our previous research, a Cell-based crowd
evacuation system (CellEVAC) was proposed that dynamically allocates exit gates
to pedestrians in a cell-based pedestrian positioning infrastructure. This
system provides optimal exit-choice indications through color-based indications
and a control logic module built upon an optimized discrete-choice model. Here,
we investigate how location-aware technologies and wearable devices can be used
for a realistic deployment of CellEVAC. We consider a simulated real evacuation
scenario (Madrid Arena) and propose a system architecture for CellEVAC that
includes: a controller node, a radio-controlled LED wristband subsystem, and a
cell-node network equipped with active Radio Frequency Identification (RFID)
devices. These subsystems coordinate to provide control, display and
positioning capabilities. We quantitatively study the sensitivity of evacuation
time and safety to uncertainty in the positioning system. Results showed that
CellEVAC was operational within a limited range of positioning uncertainty.
Further analyses revealed that reprogramming the control logic module through a
simulation-optimization process, simulating the positioning system's expected
uncertainty level, improved the CellEVAC performance in scenarios with poor
positioning systems.
","['\nMiguel A. Lopez-Carmona\n', '\nAlvaro Paricio\n']","19 pages,11 figures",,http://arxiv.org/abs/2008.11128v1,cs.OH,"['cs.OH', 'I.6.4']",,,[]
Statistically Significant Pattern Mining with Ordinal Utility,http://arxiv.org/abs/2008.10747v1,2020-08-24T23:39:15Z,2020-08-24T23:39:15Z,"  Statistically significant patterns mining (SSPM) is an essential and
challenging data mining task in the field of knowledge discovery in databases
(KDD), in which each pattern is evaluated via a hypothesis test. Our study aims
to introduce a preference relation into patterns and to discover the most
preferred patterns under the constraint of statistical significance, which has
never been considered in existing SSPM problems. We propose an iterative
multiple testing procedure that can alternately reject a hypothesis and safely
ignore the hypotheses that are less useful than the rejected hypothesis. One
advantage of filtering out patterns with low utility is that it avoids
consumption of the significance budget by rejection of useless (that is,
uninteresting) patterns. This allows the significance budget to be focused on
useful patterns, leading to more useful discoveries.
  We show that the proposed method can control the familywise error rate (FWER)
under certain assumptions, that can be satisfied by a realistic problem class
in SSPM.\@We also show that the proposed method always discovers a set of
patterns that is at least equally or more useful than those discovered using
the standard Tarone-Bonferroni method SSPM.\@Finally, we conducted several
experiments with both synthetic and real-world data to evaluate the performance
of our method. As a result, in the experiments with real-world datasets, the
proposed method discovered a larger number of more useful patterns than the
existing method for all five conducted tasks.
","['\nThien Q. Tran\n', '\nKazuto Fukuchi\n', '\nYouhei Akimoto\n', '\nJun Sakuma\n']","Proceedings of the 26th ACM SIGKDD Conference on Knowledge Discovery
  and Data Mining (KDD '20), August 23--27, 2020, Virtual Event, CA, USA",,http://dx.doi.org/10.1145/3394486.3403215,stat.ME,"['stat.ME', 'cs.OH']",10.1145/3394486.3403215,,[]
How to Design While Loops,http://arxiv.org/abs/2008.12375v1,2020-08-21T01:21:47Z,2020-08-21T01:21:47Z,"  Beginning students find the syntactic construct known as a while loop
difficult to master. The difficulties revolve around guaranteeing loop
termination and around learning how to properly sequence mutations to solve a
problem. In fact, both of these are intertwined and students need to be taught
a model that helps them reason about how to design while loops. For students
that have been introduced to how to design programs using structural recursion,
generative recursion, accumulative recursion, and mutation, the task of
teaching them how to design while loops is made easier. These students are
familiar, for example, with state variables, termination arguments, and
accumulator invariants. All of these are fundamental in the design of while
loops. This articles presents a novel technique used at Seton Hall University
to introduce beginners to the design of while loops. It presents a design
recipe that students can follow step-by-step to establish such things as the
driver of the loop, the loop invariant, and the proper sequencing of mutations.
The article also presents an example of designing a while-loop based function
using the new design recipe.
",['\nMarco T. Morazán\nSeton Hall University\n'],"In Proceedings TFPIE 2019 and 2020, arXiv:2008.08923","EPTCS 321, 2020, pp. 1-18",http://dx.doi.org/10.4204/EPTCS.321.1,cs.OH,"['cs.OH', 'cs.PL']",10.4204/EPTCS.321.1,,['Seton Hall University']
"Analysis of Fleet Management and Network Design for On-Demand Urban Air
  Mobility Operations",http://arxiv.org/abs/2008.05535v1,2020-08-07T21:39:54Z,2020-08-07T21:39:54Z,"  A significant challenge in estimating operational feasibility of Urban Air
Mobility (UAM) missions lies in understanding how choices in design impact the
performance of a complex system-of-systems. This work examines the ability of
the UAM ecosystem and the operations within it to meet a variety of demand
profiles that may emerge in the coming years. We perform a set of simulation
driven feasibility and scalability analyses based on UAM operational models
with the goal of estimating capacity and throughput for a given set of
parameters that represent an operational UAM ecosystem. UAM ecosystem design
guidelines, vehicle constraints, and effective operational policies can be
drawn from our analysis. Results show that, while critical for enabling UAM,
the performance of the UAM ecosystem is robust to variations in ground
infrastructure and fleet design decisions, while being sensitive to decisions
for fleet and traffic management policies. We show that so long as the
ecosystem design parameters for ground infrastructure and fleet design fall
within a sensible range, the performance of the UAM ecosystem is affected by
the policies used to manage the UAM traffic.
","['\nSheng Li\n', '\nMaxim Egorov\n', '\nMykel J. Kochenderfer\n']",,,http://dx.doi.org/10.2514/6.2020-2907,cs.OH,['cs.OH'],10.2514/6.2020-2907,,[]
"Computational Framework for Behind-The-Meter DER Techno-Economic
  Modeling and Optimization -- REopt Lite",http://arxiv.org/abs/2008.05873v1,2020-08-12T17:37:58Z,2020-08-12T17:37:58Z,"  The global energy system is undergoing a major transformation. Renewable
energy generation is growing and is projected to accelerate further with the
global emphasis on decarbonization. Furthermore, distributed generation is
projected to play a significant role in the new energy system, and energy
models are playing a key role in understanding how distributed generation can
be integrated reliably and economically. The deployment of massive amounts of
distributed generation requires understanding the interface of technology,
economics, and policy in the energy modeling process. In this work, we present
an end-to-end computational framework for distributed energy resource (DER)
modeling, REopt Lite which addresses this need effectively. We describe the
problem space, the building blocks of the model, the scaling capabilities of
the design, the optimization formulation, and the accessibility of the model.
We present a framework for accelerating the techno-economic analysis of
behind-the-meter distributed energy resources to enable rapid planning and
decision-making, thereby significantly boosting the rate the renewable energy
deployment. Lastly, but equally importantly, this computation framework is
open-sourced to facilitate transparency, flexibility, and wider collaboration
opportunities within the worldwide energy modeling community.
","['\nSakshi Mishra\n', '\nJosiah Pohl\n', '\nNick Laws\n', '\nDylan Cutler\n', '\nTed Kwasnik\n', '\nWilliam Becker\n', '\nAlex Zolan\n', '\nKate Anderson\n', '\nDan Olis\n', '\nEmma Elgqvist\n']","18 pages, 6 figures, under journal review",,http://dx.doi.org/10.1007/s12667-021-00446-8,cs.OH,['cs.OH'],10.1007/s12667-021-00446-8,,[]
Probabilistic Cellular Automata for Granular Media in Video Games,http://arxiv.org/abs/2008.06341v1,2020-08-13T15:19:44Z,2020-08-13T15:19:44Z,"  Granular materials are very common in the everyday world. Media such as sand,
soil, gravel, food stuffs, pharmaceuticals, etc. all have similar irregular
flow since they are composed of numerous small solid particles. In video games,
simulating these materials increases immersion and can be used for various game
mechanics. Computationally, full scale simulation is not typically feasible
except on the most powerful hardware and tends to be reduced in priority to
favor other, more integral, gameplay features. Here we study the computational
and qualitative aspects of side profile flow of sand-like particles using
cellular automata (CA). Our CA uses a standard square lattice that updates via
a custom, modified Margolus neighborhood. Each update occurs using a set of
probabilistic transitions that can be tuned to simulate friction between
particles. We focus on the look of the sandpile structure created from an
hourglass shape over time using different transition probabilities and the
computational impact of such a simulation.
","['\nJonathan Devlin\n', '\nMicah D. Schuster\n']","Cellular Automata, Sandpile",,http://arxiv.org/abs/2008.06341v1,nlin.CG,"['nlin.CG', 'cs.OH']",,,[]
"City-Scale Agent-Based Simulators for the Study of Non-Pharmaceutical
  Interventions in the Context of the COVID-19 Epidemic",http://arxiv.org/abs/2008.04849v1,2020-08-11T16:49:04Z,2020-08-11T16:49:04Z,"  We highlight the usefulness of city-scale agent-based simulators in studying
various non-pharmaceutical interventions to manage an evolving pandemic. We
ground our studies in the context of the COVID-19 pandemic and demonstrate the
power of the simulator via several exploratory case studies in two
metropolises, Bengaluru and Mumbai. Such tools become common-place in any city
administration's tool kit in our march towards digital health.
","['\nShubhada Agrawal\n', '\nSiddharth Bhandari\n', '\nAnirban Bhattacharjee\n', '\nAnand Deo\n', '\nNarendra M. Dixit\n', '\nPrahladh Harsha\n', '\nSandeep Juneja\n', '\nPoonam Kesarwani\n', '\nAditya Krishna Swamy\n', '\nPreetam Patil\n', '\nNihesh Rathod\n', '\nRamprasad Saptharishi\n', '\nSharad Shriram\n', '\nPiyush Srivastava\n', '\nRajesh Sundaresan\n', '\nNidhin Koshy Vaidhiyan\n', '\nSarath Yasodharan\n']",56 pages,"Journal of the Indian Institute of Science, volume 100, pages
  809-847, 2020",http://dx.doi.org/10.1007/s41745-020-00211-3,q-bio.PE,"['q-bio.PE', 'cs.OH', 'physics.soc-ph', 'q-bio.QM']",10.1007/s41745-020-00211-3,,[]
On the Battery Consumption of Mobile Browsers,http://arxiv.org/abs/2009.03740v1,2020-08-06T22:01:41Z,2020-08-06T22:01:41Z,"  Mobile web browsing has recently surpassed desktop browsing both in term of
popularity and traffic. Following its desktop counterpart, the mobile browsers
ecosystem has been growing from few browsers (Chrome, Firefox, and Safari) to a
plethora of browsers, each with unique characteristics (battery friendly,
privacy preserving, lightweight, etc.). In this paper, we introduce a browser
benchmarking pipeline for Android browsers encompassing automation, in-depth
experimentation, and result analysis. We tested 15 Android browsers, using
Cappuccino a novel testing suite we built for third party Android applications.
We perform a battery-centric analysis of such browsers and show that: 1)
popular browsers tend also to consume the most, 2) adblocking produces
significant battery savings (between 20 and 40% depending on the browser), and
3) dark mode offers an extra 10% battery savings on AMOLED screens. We exploit
this observation to build AttentionDim, a screen dimming mechanism driven by
browser events. Via integration with the Brave browser and 10 volunteers, we
show potential battery savings up to 30%, on both devices with AMOLED and LCD
screens.
","['\nMatteo Varvello\n', '\nBenjamin Livshits\n']",,,http://arxiv.org/abs/2009.03740v1,cs.OH,"['cs.OH', 'cs.NI', 'eess.SP']",,,[]
3D city models for urban farming site identification in buildings,http://arxiv.org/abs/2007.14203v2,2020-07-27T12:04:16Z,2021-01-05T03:28:33Z,"  Studies have suggested that there is farming potential in residential
buildings. However, these studies are limited in scope, require field visits
and time-consuming measurements. Furthermore, they have not suggested ways to
identify suitable sites on a larger scale let alone means of surveying numerous
micro-locations across the same building. Using a case study area focused on
high-rise buildings in Singapore, this paper examines a novel application of 3D
city models to identify suitable farming micro-locations in buildings. We
specifically investigate whether the vertical spaces of these buildings
comprising outdoor corridors, fa\c{c}ades and windows receive sufficient
photosynthetically active radiation (PAR) for growing food crops and do so at a
high resolution. We also analyze the spatio-temporal characteristics of PAR,
and the impact of shadows and different weather conditions on PAR in the
building. Environmental simulations on the 3D model of the study area indicated
that the cumulative daily PAR or Daily Light Integral (DLI) at a location in
the building was dependent on its orientation and shape, sun's diurnal and
annual motion, weather conditions, and shadowing effects of the building's
fa\c{c}ades and surrounding buildings. The DLI in the study area generally
increased with building's levels and, depending on the particular
micro-location, was found suitable for growing moderately light-demanding crops
such as lettuce and sweet pepper. These variations in DLI at different
locations of the same building affirmed the need for such simulations. The
simulations were validated with field measurements of PAR, and correlation
coefficients between them exceeded 0.5 in most cases thus, making a case that
3D city models offer a promising practical solution to identifying suitable
farming locations in residential buildings, and have the potential for
urban-scale applications.
","['\nAnkit Palliwal\n', '\nShuang Song\n', '\nHugh Tiang Wah Tan\n', '\nFilip Biljecki\n']",,"Computers, Environment and Urban Systems 86 (2021) 101584",http://dx.doi.org/10.1016/j.compenvurbsys.2020.101584,cs.OH,['cs.OH'],10.1016/j.compenvurbsys.2020.101584,,[]
"BIDEAL: A Toolbox for Bicluster Analysis -- Generation, Visualization
  and Validation",http://arxiv.org/abs/2007.13737v1,2020-07-26T15:24:53Z,2020-07-26T15:24:53Z,"  This paper introduces a novel toolbox named BIDEAL for the generation of
biclusters, their analysis, visualization, and validation. The objective is to
facilitate researchers to use forefront biclustering algorithms embedded on a
single platform. A single toolbox comprising various biclustering algorithms
play a vital role to extract meaningful patterns from the data for detecting
diseases, biomarkers, gene-drug association, etc. BIDEAL consists of seventeen
biclustering algorithms, three biclusters visualization techniques, and six
validation indices. The toolbox can analyze several types of data, including
biological data through a graphical user interface. It also facilitates data
preprocessing techniques i.e., binarization, discretization, normalization,
elimination of null and missing values. The effectiveness of the developed
toolbox has been presented through testing and validations on Saccharomyces
cerevisiae cell cycle, Leukemia cancer, Mammary tissue profile, and Ligand
screen in B-cells datasets. The biclusters of these datasets have been
generated using BIDEAL and evaluated in terms of coherency, differential
co-expression ranking, and similarity measure. The visualization of generated
biclusters has also been provided through a heat map and gene plot.
","['\nNishchal K. Verma\n', '\nT. Sharma\n', '\nS. Dixit\n', '\nP. Agrawal\n', '\nS. Sengupta\n', '\nV. Singh\n']",,,http://arxiv.org/abs/2007.13737v1,cs.OH,"['cs.OH', 'cs.HC', 'q-bio.QM']",,,[]
"A Research Agenda on Pediatric Chest X-Ray: Is Deep Learning Still in
  Childhood?",http://arxiv.org/abs/2007.11369v2,2020-07-20T23:24:03Z,2020-10-07T16:41:03Z,"  Several reasons explain the significant role that chest X-rays play on
supporting clinical analysis and early disease detection in pediatric patients,
such as low cost, high resolution, low radiation levels, and high availability.
In the last decade, Deep Learning (DL) has been given special attention from
the computer-aided diagnosis research community, outperforming the state of the
art of many techniques, including those applied to pediatric chest X-rays
(PCXR). Due to this increasing interest, much high-quality secondary research
has also arisen, overviewing machine learning and DL algorithms on medical
imaging and PCXR, in particular. However, these secondary studies follow
different guidelines, hampering their reproduction or improvement by
third-parties regarding the identified trends and gaps. This paper proposes a
""deep radiography"" of primary research on DL techniques applied in PCXR images.
We elaborated on a Systematic Literature Mapping (SLM) protocol, including
automatic search on six sources for studies published from January 1, 2010, to
May 20, 2020, and selection criteria utilized on a hundred research papers. As
a result, this paper categorizes twenty-six relevant studies and provides a
research agenda highlighting limitations, gaps, and trends for further
investigations on DL usage in PCXR images. Besides the fact that there is no
systematic mapping study on this research topic, to the best of authors'
knowledge, this work organizes the process of finding and selecting relevant
studies and data gathering and synthesis in a reproducible way.
","['\nAfonso U. Fonseca\n', '\nGabriel S. Vieira\n', '\nFabrízzio A. A. M. N. Soares\n', '\nRenato F. Bulcão-Neto\n']","16 pages, 11 figures, 11 tables",,http://arxiv.org/abs/2007.11369v2,cs.OH,['cs.OH'],,,[]
"Observing the Invisible: Live Cache Inspection for High-Performance
  Embedded Systems",http://arxiv.org/abs/2007.12271v1,2020-07-23T21:37:21Z,2020-07-23T21:37:21Z,"  The vast majority of high-performance embedded systems implement multi-level
CPU cache hierarchies. But the exact behavior of these CPU caches has
historically been opaque to system designers. Absent expensive hardware
debuggers, an understanding of cache makeup remains tenuous at best. This
enduring opacity further obscures the complex interplay among applications and
OS-level components, particularly as they compete for the allocation of cache
resources. Notwithstanding the relegation of cache comprehension to proxies
such as static cache analysis, performance counter-based profiling, and cache
hierarchy simulations, the underpinnings of cache structure and evolution
continue to elude software-centric solutions. In this paper, we explore a novel
method of studying cache contents and their evolution via snapshotting. Our
method complements extant approaches for cache profiling to better formulate,
validate, and refine hypotheses on the behavior of modern caches. We leverage
cache introspection interfaces provided by vendors to perform live cache
inspections without the need for external hardware. We present CacheFlow, a
proof-of-concept Linux kernel module which snapshots cache contents on an
NVIDIA Tegra TX1 SoC (system on chip).
","['\nDharmesh Tarapore\n', '\nShahin Roozkhosh\n', '\nSteven Brzozowski\n', '\nRenato Mancuso\n']",,,http://dx.doi.org/10.1109/TC.2021.3060650,cs.OH,['cs.OH'],10.1109/TC.2021.3060650,,[]
"Herramientas tecnológicas en Android para la formación de mapeadores
  y promotores de Mapa Verde",http://arxiv.org/abs/2008.07458v1,2020-07-21T17:26:58Z,2020-07-21T17:26:58Z,"  When you talk about technologies and the environment, you usually imagine a
lot of equipment, techniques, technologies and tools polluting the natural
environment. The good and bad consequences of our development have been
projected on the planet for years, and part of that development is reflected in
the new technologies, among which is the mobile phone. In the municipality of
Consolaci\'on del Sur and from the Municipal University Center, the project
Implementation of the Green Map Methodology in the management of environmental
education in console communities for the formation of an environmental culture
for sustainable development is created, creating awareness of care and
protection of the environment. The present work is given to solve the following
problem: how to contribute in the construction of a package of computer tools
for the implementation of the Green Map methodology in environmental management
in console communities and the training of mappers and promoters of Green Map
for the development of green maps of the communities of the municipality
Consolaci\'on del Sur? For this purpose, two Android applications for mobile
devices based on the Green Map methodology were developed, thus responding to
the following objective: Develop a package of computer applications for the
implementation of the Green Map methodology in the management of environmental
education in console communities and the formation of mappers and promoters of
the Green Map that allows the development of the green maps of the communities
of the Consolaci\'on del Sur municipality.
","['\nYosvany Medina Carbó\n', '\nC. Álvaro Celestino Alonso Vázquez\n', '\nReina María Rodríguez García\n']",in Spanish,,http://arxiv.org/abs/2008.07458v1,cs.OH,['cs.OH'],,,[]
IoT Applications in Urban Sustainability,http://arxiv.org/abs/2008.10656v1,2020-07-23T12:50:33Z,2020-07-23T12:50:33Z,"  Internet of Things is one of the driving technologies behind the concept of
Smart Cities and is capable of playing a significant role in facilitating urban
sustainable development. This chapter explores the relationship between three
core concepts namely Smart Cities, Internet of Things and Sustainability;
thereby identifying the challenges and opportunities that exist in the
synergistic use of Internet of Things for sustainability, in the Smart Cities
context. Moreover, this chapter also presents some of the existing use cases
that apply Internet of Things for urban sustainable development, also
presenting the vision for these applications as they continue to evolve in and
adapt to the real world scenario. It is because of the interdisciplinary nature
of these applications that a clear comprehension of the associated challenges
becomes quintessential. Study of challenges and opportunities in this area
shall facilitate collaboration between different sectors of urban planning and
optimize the utilization of Internet of Things for sustainability.
","['\nSamiya Khan\n', '\nMohammad Moazum Wani\n', '\nMansaf Alam\n']",,,http://arxiv.org/abs/2008.10656v1,cs.OH,['cs.OH'],,,[]
"A Deep Learning-Based FPGA Function Block Detection Method with
  Bitstream to Image Transformation",http://arxiv.org/abs/2007.11434v3,2020-07-20T07:45:05Z,2021-08-04T14:11:57Z,"  In the context of various application scenarios and/or for the sake of
strengthening field-programmable gate array (FPGA) security, the system
functions of an FPGA design need to be analyzed, which can be achieved by
systematically partitioning the FPGA's bitstream into manageable functional
blocks and detecting their functionalities thereafter. In this paper, we
propose a novel deep learning-based FPGA function block detection method with
three major steps. In specific, we first analyze the format of the bitstream to
obtain the mapping relationship between the configuration bits and configurable
logic blocks because of the discontinuity of the configuration bits in the
bitstream for one element. In order to reap the maturity of object detection
techniques based on deep learning, our next step is to convert an FPGA
bitstream to an image, following the proposed transformation method that takes
account of both the adjacency nature of the programmable logic and the high
degree of redundancy of configuration information. Once the image is obtained,
a deep learning-based object detection algorithm is applied to this transformed
image, and the objects detected can be reflected back to determine the function
blocks of the original FPGA design. The deep neural network used for function
block detection is trained and validated with a specially crafted
bitstream/image dataset. Experiments have confirmed high detection accuracy of
the proposed function detection method, showing a 98.11% of mean Average
Precision (IoU=0.5) for 10 function blocks within a YOLOv3 detector implemented
on Xilinx Zynq-7000 SoCs and Zynq UltraScale+ MPSoCs.
","['\nMinzhen Chen\nZhejiang University, Hangzhou, China\n', '\nPeng Liu\nZhejiang University, Hangzhou, China\n']",12 pages and 10 figures,IEEE Access 9(2021) 99794-99804,http://dx.doi.org/10.1109/ACCESS.2021.3096664,cs.OH,"['cs.OH', 'eess.IV', 'eess.SP']",10.1109/ACCESS.2021.3096664,,"['Zhejiang University, Hangzhou, China', 'Zhejiang University, Hangzhou, China']"
Business Email Compromise (BEC) and Cyberpsychology,http://arxiv.org/abs/2007.02415v1,2020-07-05T18:40:43Z,2020-07-05T18:40:43Z,"  The paper gives a brief introduction about what BEC (Business Email
Compromise) is and why we should be concerned about. In addition, it presents 2
examples, Ubiquity and Peebles Media Group, which have been chosen to analyse
the phenomena of BEC and underpin how universal BEC threat is for all
companies. The psychology behind this scam has been, then, studied. In
particular, the Big Five Framework has been analysed to understand how
personality traits play an important role in Social Engineering-based attacks.
Furthermore, the 6 basic principles of influence, by Cialdini, have been
presented to show which strategies are adopted in such scam. The paper follows
with the analysis of the BEC impacts, the incidents evaluation and, finally,
with the description of some precautions, that companies should undertake in
order to mitigate the likelihood of a Business Email Compromise.
",['\nAlessandro Ecclesie Agazzi\n'],,,http://arxiv.org/abs/2007.02415v1,cs.OH,['cs.OH'],,,[]
Value driven Analysis Framework of Service Ecosystem Evolution Mechanism,http://arxiv.org/abs/2008.01055v1,2020-07-09T03:40:03Z,2020-07-09T03:40:03Z,"  With the development of cloud computing, service computing, IoT(Internet of
Things) and mobile Internet, the diversity and sociality of services are
increasingly apparent. To meet the customized user demands, Service Ecosystem
is emerging as a complex social-technology system, which is formed with various
IT services through cross-border integration. However, how to analyze and
promote the evolution mechanism of service ecosystem is still a serious
challenge in the field, which is of great significance to achieve the expected
system evolution trends. Based on this, this paper proposes a value-driven
analysis framework of service ecosystem, including value creation, value
operation, value realization and value distribution. In addition, a
computational experiment system is established to verify the effectiveness of
the analysis framework, which stimulates the effect of different operation
strategies on the value network in the service ecosystem. The result shows that
our analysis framework can provide new means and ideas for the analysis of
service ecosystem evolution, and can also support the design of operation
strategies. Index
","['\nXiao Xue\n', '\nDeyu Zhou\n', '\nYaodan Guo\n', '\nZhiyong Feng\n', '\nLejun Zhang\n', '\nLin Meng\n']",14pages,,http://arxiv.org/abs/2008.01055v1,cs.OH,['cs.OH'],,,[]
Value Entropy Model: Metric Method of Service Ecosystem Evolution,http://arxiv.org/abs/2008.02247v1,2020-07-09T01:25:02Z,2020-07-09T01:25:02Z,"  With the development of cloud computing, service computing, IoT(Internet of
Things) and mobile Internet, the diversity and sociality of services are
increasingly apparent. To meet the customized user demands, service ecosystems
begins to emerge with the formation of various IT services collaboration
network. However, service ecosystem is a complex social-technology system with
the characteristics of natural ecosystems, economic systems and complex
networks. Hence, how to realize the multi-dimensional evaluation of service
ecosystem is of great significance to promote its sound development. Based on
this, this paper proposes a value entropy model to analyze the performance of
service ecosystem, which is conducive to integrate evaluation indicators of
different dimensions. In addition, a computational experiment system is
constructed to verify the effectiveness of value entropy model. The result
shows that our model can provide new means and ideas for the analysis of
service ecosystem.
","['\nXiao Xue\n', '\nZhaojie Chen\n', '\nShufang Wang\n', '\nZhiyong Feng\n', '\nYucong Duan\n', '\nZhangbing Zhou\n']",14,2020,http://dx.doi.org/10.1109/TSC_2020_3016660,cs.OH,['cs.OH'],10.1109/TSC_2020_3016660,,[]
Quality Classification of Defective Parts from Injection Moulding,http://arxiv.org/abs/2008.02872v1,2020-07-08T06:24:48Z,2020-07-08T06:24:48Z,"  This report examines machine learning algorithms for detecting short forming
and weaving in plastic parts produced by injection moulding. Transfer learning
was implemented by using pretrained models and finetuning them on our dataset
of 494 samples of 150 by 150 pixels images. The models tested were Xception,
InceptionV3 and Resnet-50. Xception showed the highest overall accuracy
(86.66%), followed by InceptionV3 (82.47%) and Resnet-50 (80.41%). Short
forming was the easiest fault to identify, with the highest F1 score for each
model.
",['\nAdithya Venkatadri Hulagadri\n'],"4 pages, 11 figures",,http://arxiv.org/abs/2008.02872v1,cs.OH,['cs.OH'],,,[]
Privacy vs National Security,http://arxiv.org/abs/2007.12633v1,2020-07-10T19:35:10Z,2020-07-10T19:35:10Z,"  There are growing concerns and anxiety about privacy among the general public
especially after the revelations of former NSA contractor and whistleblowers
like Edward Snowden and others. While privacy is the fundamental concept of
being human, the growing tug-of-war between an individuals privacy and freedom
vs national security has renewed the concerns about where the fine balance
should lie between the two. For the first time in history the technological
advancement has made the mass data gathering, analysis, and storage a
financially and technologically feasible option for the governments and private
businesses. This has led to the growing interest of governments and security
agencies around the globe to develop sophisticated algorithms using the power
of Big-Data, Machine-Learning and Artificial Intelligence. The technology has
enabled governments and private businesses to collect and store thousands of
data points on every individual, which has put an individuals privacy under
constant threat. This article analyses the individual's privacy concepts and
its perceived link with national security. The article will also discuss the
various aspects of privacy and national-security, arguments of both sides and
where a boundary should be drawn between privacy and national security.
",['\nTajdar Jawaid\n'],,,http://dx.doi.org/10.14445/22312803/IJCTT-V68I7P101,cs.OH,"['cs.OH', 'cs.CR']",10.14445/22312803/IJCTT-V68I7P101,,[]
"SAXSDOG: open software for real-time azimuthal integration of 2D
  scattering images",http://arxiv.org/abs/2007.02022v1,2020-07-04T06:58:15Z,2020-07-04T06:58:15Z,"  In-situ small- and wide-angle scattering experiments at synchrotrons often
result in massive amounts of data within seconds only. Especially during such
beamtimes, processing of the acquired data online, so without mentionable
delay, is key to obtain feedback on failure or success of the experiment. We
thus developed SAXSDOG, a python based environment for real-time azimuthal
integration of large-area scattering-images. The software is primarily designed
for dedicated data-pipelines: once a scattering image is transferred from the
detector onto the storage-unit, it is automatically integrated and
pre-evaluated using integral parameters within milliseconds. The control and
configuration of the underlying server-based processes is done via a graphical
user interface SAXSLEASH, which visualizes the resulting 1D data together with
integral classifiers in real time. SAXSDOG further includes a portable
'take-home' version for users that runs on standalone computers, enabling its
use in labs or at the preferred workspace.
","['\nMax Burian\n', '\nChristian Meisenbichler\n', '\nDenys Naumenko\n', '\nHeinz Amenitsch\n']",Source-code available at https://github.com/maxburian/SAXS_py3,,http://arxiv.org/abs/2007.02022v1,cs.OH,"['cs.OH', 'eess.IV', 'physics.ins-det']",,,[]
The Metastable Behavior of a Schmitt-Trigger,http://arxiv.org/abs/2006.08319v1,2020-06-15T12:21:33Z,2020-06-15T12:21:33Z,"  Schmitt-Trigger circuits are the method of choice for converting general
signal shapes into clean, well-behaved digital ones. In this context these
circuits are often used for metastability handling, as well. However, like any
other positive feedback circuit, a Schmitt-Trigger can become metastable
itself. Therefore, its own metastable behavior must be well understood; in
particular the conditions that may cause its metastability. In this paper we
will build on existing results from Marino to show that (a) a monotonic input
signal can cause late transitions but never leads to a non-digital voltage at
the Schmitt-Trigger output, and (b) a non-monotonic input can pin the
Schmitt-Trigger output to a constant voltage at any desired (also non-digital)
level for an arbitrary duration. In fact, the output can even be driven to any
waveform within the dynamic limits of the system. We will base our analysis on
a mathematical model of a Schmitt-Trigger's dynamic behavior and perform SPICE
simulations to support our theory and confirm its validity for modern CMOS
implementations. Furthermore, we will discuss several use cases of a
Schmitt-Trigger in the light of our results.
","['\nAndreas Steininger\n', '\nJürgen Maier\n', '\nRobert Najvirt\n']","8 pages, 12 figures, 2016 22nd IEEE International Symposium on
  Asynchronous Circuits and Systems (ASYNC)",,http://dx.doi.org/10.1109/ASYNC.2016.19,cs.OH,"['cs.OH', 'B.8.1']",10.1109/ASYNC.2016.19,,[]
A Faithful Binary Circuit Model with Adversarial Noise,http://arxiv.org/abs/2006.08485v2,2020-06-15T15:41:54Z,2021-12-07T12:27:13Z,"  Accurate delay models are important for static and dynamic timing analysis of
digital circuits, and mandatory for formal verification. However, F\""ugger et
al. [IEEE TC 2016] proved that pure and inertial delays, which are employed for
dynamic timing analysis in state-of-the-art tools like ModelSim, NC-Sim and
VCS, do not yield faithful digital circuit models. Involution delays, which are
based on delay functions that are mathematical involutions depending on the
previous-output-to-input time offset, were introduced by F\""ugger et al.
[DATE'15] as a faithful alternative (that can easily be used with existing
tools). Although involution delays were shown to predict real signal traces
reasonably accurately, any model with a deterministic delay function is
naturally limited in its modeling power. In this paper, we thus extend the
involution model, by adding non-deterministic delay variations (random or even
adversarial), and prove analytically that faithfulness is not impaired by this
generalization. Albeit the amount of non-determinism must be considerably
restricted to ensure this property, the result is surprising: the involution
model differs from non-faithful models mainly in handling fast glitch trains,
where small delay shifts have large effects. This originally suggested that
adding even small variations should break the faithfulness of the model, which
turned out not to be the case. Moreover, the results of our simulations also
confirm that this generalized involution model has larger modeling power and,
hence, applicability.
","['\nMatthias Függer\n', '\nJürgen Maier\n', '\nRobert Najvirt\n', '\nThomas Nowak\n', '\nUlrich Schmid\n']","9 pages, 9 figures, extended version of a paper which was accepted at
  DATE'18",,http://arxiv.org/abs/2006.08485v2,cs.OH,"['cs.OH', 'B.8.1; B.8.2']",,,[]
"Artificial Buildings: Safety, Complexity and a Quantifiable Measure of
  Beauty",http://arxiv.org/abs/2006.11113v1,2020-06-17T19:40:20Z,2020-06-17T19:40:20Z,"  A place to live is one of the most crucial necessities for all living
organisms since the advent of life on planet Earth. The nature of homes has
changed considerably over time. At the very early stages, human begins lived in
natural places such as caves. Later on, they started to use their intelligence
to build places with special purposes. Nowadays, modern technologies such as
robotics and artificial intelligence have made their ways into the construction
process and opened up a whole new area of opportunities and concerns that may
be of interest to both technologists and philosophers. In this article, I
review the evolution of buildings from fully natural to fully artificial and
discuss philosophical thoughts that a fully automated construction technology
may raise. I elaborate on the safety concerns of a fully automated
architectural process. Then, I'll borrow Kolmogorov complexity from algorithmic
information theory to define a complexity measure for buildings. The proposed
measure is then used to provide a quantifiable measure of beauty.
",['\nArash Mehrjou\n'],8 pages,,http://arxiv.org/abs/2006.11113v1,cs.OH,['cs.OH'],,,[]
"Run-Time Power Modelling in Embedded GPUs with Dynamic Voltage and
  Frequency Scaling",http://arxiv.org/abs/2006.12176v1,2020-06-19T00:41:05Z,2020-06-19T00:41:05Z,"  This paper investigates the application of a robust CPU-based power modelling
methodology that performs an automatic search of explanatory events derived
from performance counters to embedded GPUs. A 64-bit Tegra TX1 SoC is
configured with DVFS enabled and multiple CUDA benchmarks are used to train and
test models optimized for each frequency and voltage point. These optimized
models are then compared with a simpler unified model that uses a single set of
model coefficients for all frequency and voltage points of interest. To obtain
this unified model, a number of experiments are conducted to extract
information on idle, clock and static power to derive power usage from a single
reference equation. The results show that the unified model offers competitive
accuracy with an average 5\% error with four explanatory variables on the test
data set and it is capable to correctly predict the impact of voltage,
frequency and temperature on power consumption. This model could be used to
replace direct power measurements when these are not available due to hardware
limitations or worst-case analysis in emulation platforms.
","['\nJose Nunez-Yanez\n', '\nKris Nikov\n', '\nKerstin Eder\n', '\nMohammad Hosseinabady\n']",,,http://arxiv.org/abs/2006.12176v1,cs.OH,['cs.OH'],,,[]
"Environmental Impact of Bundling Transport Deliveries Using SUMO:
  Analysis of a cooperative approach in Austria",http://arxiv.org/abs/2006.12965v2,2020-06-16T09:59:08Z,2020-08-16T11:57:07Z,"  Urban Traffic is recognized as one of the major CO2 contributors that puts a
high burden on the environment. Different attempts have been made for reducing
the impacts ranging from traffic management actions to shared-vehicle concepts
to simply reducing the number of vehicles on the streets. By relying on
cooperative approaches between different logistics companies, such as sharing
and pooling resources for bundling deliveries in the same zone, an increased
environmental benefit can be attained. To quantify this benefit we compare the
CO2 emissions, fuel consumption and total delivery time resulting from
deliveries performed by one cargo truck with two trailers versus by two
single-trailer cargo trucks under real conditions in a simulation scenario in
the city of Linz in Austria. Results showed a fuel consumption and CO2
emissions reduction of 28% and 34% respectively in the scenario in which
resources were bundled in one single truck.
","['\nAso Validi\n', '\nNicole Polasek\n', '\nLeonie Alabi\n', '\nMichael Leitner\n', '\nCristina Olaverri-Monreal\n']","5 pages, 7 figures, 2 tables, paper accepted for the proceedings of
  the CISTI'2020 - 15th Iberian Conference on Information Systems and
  Technologies",,http://dx.doi.org/10.23919/CISTI49556.2020.9141129,cs.OH,['cs.OH'],10.23919/CISTI49556.2020.9141129,,[]
Does Cascading Schmitt-Trigger Stages Improve the Metastable Behavior?,http://arxiv.org/abs/2006.08415v1,2020-06-15T14:08:00Z,2020-06-15T14:08:00Z,"  Schmitt-Trigger stages are the method of choice for robust discretization of
input voltages with excessive transition times or significant noise. However,
they may suffer from metastability. Based on the experience that the cascading
of flip-flop stages yields a dramatic improvement of their overall
metastability hardness, in this paper we elaborate on the question whether the
cascading of Schmitt-Trigger stages can obtain a similar gain. We perform a
theoretic analysis that is backed up by an existing metastability model for a
single Schmitt-Trigger stage and elaborate some claims about the behavior of a
Schmitt-Trigger cascade. These claims suggest that the occurrence of
metastability is indeed reduced from the first stage to the second which
suggests an improvement. On the downside, however, it becomes clear that
metastability can still not be completely ruled out, and in some cases the
behavior of the cascade may be less beneficial for a given application, e.g. by
introducing seemingly acausal transitions. We validate our findings by
extensive HSPICE simulations in which we directly cover our most important
claims.
","['\nAndreas Steininger\n', '\nRobert Najvirt\n', '\nJürgen Maier\n']","8 pages, 14 figures, 2016 Euromicro Conference on Digital System
  Design (DSD)",,http://dx.doi.org/10.1109/DSD.2016.56,cs.OH,"['cs.OH', 'eess.SP', 'B.8.1']",10.1109/DSD.2016.56,,[]
Converting Biomechanical Models from OpenSim to MuJoCo,http://arxiv.org/abs/2006.10618v2,2020-06-17T15:16:51Z,2020-08-25T13:58:48Z,"  OpenSim is a widely used biomechanics simulator with several anatomically
accurate human musculo-skeletal models. While OpenSim provides useful tools to
analyse human movement, it is not fast enough to be routinely used for emerging
research directions, e.g., learning and simulating motor control through deep
neural networks and Reinforcement Learning (RL). We propose a framework for
converting OpenSim models to MuJoCo, the de facto simulator in machine learning
research, which itself lacks accurate musculo-skeletal human models. We show
that with a few simple approximations of anatomical details, an OpenSim model
can be automatically converted to a MuJoCo version that runs up to 600 times
faster. We also demonstrate an approach to computationally optimize MuJoCo
model parameters so that forward simulations of both simulators produce similar
results.
","['\nAleksi Ikkala\n', '\nPerttu Hämäläinen\n']","Submitted to 5th International Conference on NeuroRehabilitation
  (ICNR2020)",,http://dx.doi.org/10.1007/978-3-030-70316-5_45,q-bio.QM,"['q-bio.QM', 'cs.OH']",10.1007/978-3-030-70316-5_45,,[]
Efficient Metastability Characterization for Schmitt-Triggers,http://arxiv.org/abs/2006.14001v1,2020-06-16T06:28:54Z,2020-06-16T06:28:54Z,"  Despite their attractiveness as metastability filters, Schmitt-Triggers can
suffer from metastability themselves. Therefore, in the selection or
construction of a suitable Schmitt-Trigger implementation, it is a necessity to
accurately determine the metastable behavior. Only then one is able to compare
different designs and thus guide proper optimizations, and only then one can
assess the potential for residual metastable upsets. However, while the state
of the art provides a lot of research and practical characterization approaches
for flip-flops, comparatively little is known about Schmitt-Trigger
characterization. Unlike the flip-flop with its single metastable point, the
Schmitt-Trigger exhibits a whole range of metastable points depending on the
input voltage. Thus the task of characterization gets much more challenging.
  In this paper we present different approaches to determine the metastable
behavior of Schmitt-Triggers using novel methods and mechanisms. We compare
their accuracy and runtime by applying them to three common circuit
implementations. The achieved results are then used to reason about the
metastable behavior of the chosen designs which turns out to be problematic in
some cases. Overall the approaches proposed in this paper are generic and can
be extended beyond the Schmitt-Trigger, i.e., to efficiently characterize
metastable states in other circuits as well.
","['\nJürgen Maier\n', '\nAndreas Steininger\n']","10 pages, 15 figures","2019 25th IEEE International Symposium on Asynchronous Circuits
  and Systems (ASYNC)",http://dx.doi.org/10.1109/ASYNC.2019.00024,cs.OH,"['cs.OH', 'eess.SP', 'B.8.1']",10.1109/ASYNC.2019.00024,,[]
Automatic Parking in Smart Cities,http://arxiv.org/abs/2007.13491v1,2020-06-17T21:54:21Z,2020-06-17T21:54:21Z,"  The objective behind this project is to maximize the efficiency of land
space, to decrease the driver stress and frustration, along with a considerable
reduction in air pollution. Our contribution is in the form of an automatic
parking system that is controlled by cellular phones. The structure is a
hexagon shape that uses conveyor belts, to transport the vehicles from the
entrance into the parking spaces over an elevating platform. The entrance gate
includes length-measuring sensors to determine whether the approaching vehicle
is eligible to enter. Our system is controlled through a microcontroller, and
using cellular communications to connect to the customer. The project can be
applied to different locations and is capable of capacity extensions.
","['\nArezou Abyaneh\nQatar University\n', '\nVanessa Fakhoury\nLund University\n', '\nNizar Zorba\nQatar University\n']","5 pages, 6 figures, To be published in conference proceedings of 10th
  IEEE GCC conference and exhibition",,http://arxiv.org/abs/2007.13491v1,cs.OH,"['cs.OH', 'cs.SY', 'eess.SP', 'eess.SY']",,,"['Qatar University', 'Lund University', 'Qatar University']"
"Genetic Algorithm: Reviews, Implementations, and Applications",http://arxiv.org/abs/2007.12673v1,2020-06-05T14:34:33Z,2020-06-05T14:34:33Z,"  Nowadays genetic algorithm (GA) is greatly used in engineering pedagogy as an
adaptive technique to learn and solve complex problems and issues. It is a
meta-heuristic approach that is used to solve hybrid computation challenges. GA
utilizes selection, crossover, and mutation operators to effectively manage the
searching system strategy. This algorithm is derived from natural selection and
genetics concepts. GA is an intelligent use of random search supported with
historical data to contribute the search in an area of the improved outcome
within a coverage framework. Such algorithms are widely used for maintaining
high-quality reactions to optimize issues and problems investigation. These
techniques are recognized to be somewhat of a statistical investigation process
to search for a suitable solution or prevent an accurate strategy for
challenges in optimization or searches. These techniques have been produced
from natural selection or genetics principles. For random testing, historical
information is provided with intelligent enslavement to continue moving the
search out from the area of improved features for processing of the outcomes.
It is a category of heuristics of evolutionary history using behavioral
science-influenced methods like an annuity, gene, preference, or combination
(sometimes refers to as hybridization). This method seemed to be a valuable
tool to find solutions for problems optimization. In this paper, the author has
explored the GAs, its role in engineering pedagogies, and the emerging areas
where it is using, and its implementation.
","['\nTanweer Alam\n', '\nShamimul Qamar\n', '\nAmit Dixit\n', '\nMohamed Benaida\n']",,"International Journal of Engineering Pedagogy (iJEP), 2020",http://arxiv.org/abs/2007.12673v1,cs.OH,['cs.OH'],,,[]
"Generation of Complex Road Networks Using a Simplified Logical
  Description for the Validation of Automated Vehicles",http://arxiv.org/abs/2006.03403v1,2020-06-04T12:35:30Z,2020-06-04T12:35:30Z,"  Simulation is a valuable building block for the verification and validation
of automated driving functions (ADF). When simulating urban driving scenarios,
simulation maps are one important component. Often, the generation of those
road networks is a time consuming and manual effort. Furthermore, typically
many variations of a distinct junction or road section are demanded to ensure
that an ADF can be validated in the process of releasing those functions to the
public. Therefore, in this paper, we present a prototypical solution for a
logical road network description which is easy to maintain and modify. The
concept aims to be non-redundant so that changes of distinct quantities do not
affect other places in the code and thus the variation of maps is
straightforward. In addition, the simple definition of junctions is a focus of
the work. Intersecting roads are defined separately, are then set in relation
and the junction is finally generated automatically. The idea is to derive the
description from a commonly used, standardized format for simulation maps in
order to generate this format from the introduced logical description.
Consequently, we developed a command-line tool that generates the standardized
simulation map format OpenDRIVE.
","['\nDaniel Becker\n', '\nFabian Ruß\n', '\nChristian Geller\n', '\nLutz Eckstein\n']","Accepted to be published as part of the 23rd IEEE International
  Conference on Intelligent Transportation Systems (ITSC), Rhodes, Greece,
  September 20-23, 2020",,http://arxiv.org/abs/2006.03403v1,cs.OH,['cs.OH'],,,[]
Smart Motion Detection System using Raspberry Pi,http://arxiv.org/abs/2006.06442v1,2020-06-09T21:52:27Z,2020-06-09T21:52:27Z,"  This paper throws light on the security issues that modern day homes and
businesses face and describes the implementation of a motion detection system
using Raspberry Pi which could be an effective solution to address the security
concerns. The goal of the solution is to provide an implementation that uses
PIR motion sensors for motion detection and sends notifications to users via
emails. Furthermore, the system is verified using a verification tool named
UPPAAL.
",['\nVenkat Margapuri\n'],,,http://arxiv.org/abs/2006.06442v1,cs.OH,['cs.OH'],,,[]
"CAN-D: A Modular Four-Step Pipeline for Comprehensively Decoding
  Controller Area Network Data",http://arxiv.org/abs/2006.05993v2,2020-06-09T22:32:22Z,2021-06-23T03:50:48Z,"  CANs are a broadcast protocol for real-time communication of critical vehicle
subsystems. Original equipment manufacturers of passenger vehicles hold secret
their mappings of CAN data to vehicle signals, and these definitions vary
according to make, model, and year. Without these mappings, the wealth of
real-time vehicle information hidden in the CAN packets is uninterpretable,
impeding vehicle-related research. Guided by the 4-part CAN signal definition,
we present CAN-D (CAN-Decoder), a modular, 4-step pipeline for identifying each
signal's boundaries (start bit, length), endianness (byte order), signedness
(bit-to-integer encoding), and by leveraging diagnostic standards, augmenting a
subset of the extracted signals with physical interpretation. We provide a
comprehensive review of the CAN signal reverse engineering research. Previous
methods ignore endianness and signedness, rendering them incapable of decoding
many standard CAN signal definitions. Incorporating endianness grows the search
space from 128 to 4.72E21 signal tokenizations and introduces a web of changing
dependencies. We formulate, formally analyze, and provide an efficient solution
to an optimization problem, allowing identification of the optimal set of
signal boundaries and byte orderings. We provide two novel, state-of-the-art
signal boundary classifiers-both superior to previous approaches in precision
and recall in three different test scenarios-and the first signedness
classification algorithm which exhibits a $>$97\% F-score. CAN-D is the only
solution with the potential to extract any CAN signal. In evaluation on 10
vehicles, CAN-D's average $\ell^1$ error is 5x better than all previous methods
and exhibits lower ave. error, even when considering only signals that meet
prior methods' assumptions. CAN-D is implemented in lightweight hardware,
allowing for an OBD-II plugin for real-time in-vehicle CAN decoding.
","['\nMiki E. Verma\n', '\nRobert A. Bridges\n', '\nJordan J. Sosnowski\n', '\nSamuel C. Hollifield\n', '\nMichael D. Iannacone\n']",,,http://arxiv.org/abs/2006.05993v2,cs.OH,"['cs.OH', 'eess.SP']",,,[]
COVID-19 Epidemic Study II: Phased Emergence From the Lockdown in Mumbai,http://arxiv.org/abs/2006.03375v1,2020-06-05T11:22:16Z,2020-06-05T11:22:16Z,"  The nation-wide lockdown starting 25 March 2020, aimed at suppressing the
spread of the COVID-19 disease, was extended until 31 May 2020 in three
subsequent orders by the Government of India. The extended lockdown has had
significant social and economic consequences and `lockdown fatigue' has likely
set in. Phased reopening began from 01 June 2020 onwards. Mumbai, one of the
most crowded cities in the world, has witnessed both the largest number of
cases and deaths among all the cities in India (41986 positive cases and 1368
deaths as of 02 June 2020). Many tough decisions are going to be made on
re-opening in the next few days. In an earlier IISc-TIFR Report, we presented
an agent-based city-scale simulator(ABCS) to model the progression and spread
of the infection in large metropolises like Mumbai and Bengaluru. As discussed
in IISc-TIFR Report 1, ABCS is a useful tool to model interactions of city
residents at an individual level and to capture the impact of
non-pharmaceutical interventions on the infection spread. In this report we
focus on Mumbai. Using our simulator, we consider some plausible scenarios for
phased emergence of Mumbai from the lockdown, 01 June 2020 onwards. These
include phased and gradual opening of the industry, partial opening of public
transportation (modelling of infection spread in suburban trains), impact of
containment zones on controlling infections, and the role of compliance with
respect to various intervention measures including use of masks, case
isolation, home quarantine, etc. The main takeaway of our simulation results is
that a phased opening of workplaces, say at a conservative attendance level of
20 to 33\%, is a good way to restart economic activity while ensuring that the
city's medical care capacity remains adequate to handle the possible rise in
the number of COVID-19 patients in June and July.
","['\nPrahladh Harsha\n', '\nSandeep Juneja\n', '\nPreetam Patil\n', '\nNihesh Rathod\n', '\nRamprasad Saptharishi\n', '\nA. Y. Sarath\n', '\nSharad Sriram\n', '\nPiyush Srivastava\n', '\nRajesh Sundaresan\n', '\nNidhin Koshy Vaidhiyan\n']",34 pages,,http://arxiv.org/abs/2006.03375v1,q-bio.PE,"['q-bio.PE', 'cs.OH', 'physics.soc-ph', 'q-bio.QM']",,,[]
"Optimizing Visual Cortex Parameterization with Error-Tolerant
  Teichmuller Map in Retinotopic Mapping",http://arxiv.org/abs/2005.11908v1,2020-05-25T03:21:02Z,2020-05-25T03:21:02Z,"  The mapping between the visual input on the retina to the cortical surface,
i.e., retinotopic mapping, is an important topic in vision science and
neuroscience. Human retinotopic mapping can be revealed by analyzing cortex
functional magnetic resonance imaging (fMRI) signals when the subject is under
specific visual stimuli. Conventional methods process, smooth, and analyze the
retinotopic mapping based on the parametrization of the (partial) cortical
surface. However, the retinotopic maps generated by this approach frequently
contradict neuropsychology results. To address this problem, we propose an
integrated approach that parameterizes the cortical surface, such that the
parametric coordinates linearly relates the visual coordinate. The proposed
method helps the smoothing of noisy retinotopic maps and obtains
neurophysiological insights in human vision systems. One key element of the
approach is the Error-Tolerant Teichmuller Map, which uniforms the angle
distortion and maximizes the alignments to self-contradicting landmarks. We
validated our overall approach with synthetic and real retinotopic mapping
datasets. The experimental results show the proposed approach is superior in
accuracy and compatibility. Although we focus on retinotopic mapping, the
proposed framework is general and can be applied to process other human sensory
maps.
","['\nYanshuai Tu\n', '\nDuyan Ta\n', '\nZhong-Lin Lu\n', '\nYalin Wang\n']",submitted to MICCAI,,http://arxiv.org/abs/2005.11908v1,q-bio.NC,"['q-bio.NC', 'cs.OH']",,,[]
"No Substitute for Functionalism -- A Reply to 'Falsification &
  Consciousness'",http://arxiv.org/abs/2006.13664v3,2020-05-28T08:12:07Z,2021-04-30T21:54:25Z,"  In their paper 'Falsification and Consciousness' [1], Kleiner and Hoel
introduced a formal mathematical model of the process of generating observable
data from experiments and using that data to generate inferences and
predictions onto an experience space. The resulting substitution argument built
on this framework was used to show that any theory of consciousness with
independent inference and prediction data are pre-falsified, if the inference
reports are considered valid. If this argument does indeed pre-falsify many of
the leading theories of consciousness, it would indicate a fundamental problem
affecting the field of consciousness as a whole that would require radical
changes to how consciousness science is performed. In this reply, the author
will identify avenues of expansion for the model proposed in [1], allowing us
to distinguish between different types of variation. Motivated by examples from
neural networks, state machines and Turing machines, we will prove that
substitutions do not exist for a very broad class of Level-1 functionalist
theories, rendering them immune to the aforementioned substitution argument.
",['\nNatesh Ganesh\n'],"This paper replaced an earlier version that was on arXiv under the
  title -- C-Wars: The Unfolding Argument Strikes Back - A Reply to
  'Falsification & Consciousness'",,http://arxiv.org/abs/2006.13664v3,cs.OH,"['cs.OH', 'cs.AI']",,,[]
"Chook -- A comprehensive suite for generating binary optimization
  problems with planted solutions",http://arxiv.org/abs/2005.14344v2,2020-05-28T15:55:38Z,2021-03-22T00:09:15Z,"  We present Chook, an open-source Python-based tool to generate discrete
optimization problems of tunable complexity with a priori known solutions.
Chook provides a cross-platform unified environment for solution planting using
a number of techniques, such as tile planting, Wishart planting, equation
planting, and deceptive cluster loop planting. Chook also incorporates planted
solutions for higher-order (beyond quadratic) binary optimization problems. The
support for various planting schemes and the tunable hardness allows the user
to generate problems with a wide range of complexity on different graph
topologies ranging from hypercubic lattices to fully-connected graphs.
","['\nDilina Perera\n', '\nInimfon Akpabio\n', '\nFiras Hamze\n', '\nSalvatore Mandra\n', '\nNathan Rose\n', '\nMaliheh Aramon\n', '\nHelmut G. Katzgraber\n']","8 pages, 2 figures, 3 tables. Python source code under ancillary
  files (v 0.2 uses an updated k-local scheme)",,http://arxiv.org/abs/2005.14344v2,quant-ph,"['quant-ph', 'cond-mat.dis-nn', 'cs.OH']",,,[]
"AGI and the Knight-Darwin Law: why idealized AGI reproduction requires
  collaboration",http://arxiv.org/abs/2005.08801v1,2020-05-09T18:45:18Z,2020-05-09T18:45:18Z,"  Can an AGI create a more intelligent AGI? Under idealized assumptions, for a
certain theoretical type of intelligence, our answer is: ""Not without outside
help"". This is a paper on the mathematical structure of AGI populations when
parent AGIs create child AGIs. We argue that such populations satisfy a certain
biological law. Motivated by observations of sexual reproduction in
seemingly-asexual species, the Knight-Darwin Law states that it is impossible
for one organism to asexually produce another, which asexually produces
another, and so on forever: that any sequence of organisms (each one a child of
the previous) must contain occasional multi-parent organisms, or must
terminate. By proving that a certain measure (arguably an intelligence measure)
decreases when an idealized parent AGI single-handedly creates a child AGI, we
argue that a similar Law holds for AGIs.
",['\nSamuel Allen Alexander\n'],"10 pages, accepted for the International Conference on Artificial
  General Intelligence",,http://arxiv.org/abs/2005.08801v1,cs.OH,"['cs.OH', '03F15, 68T01']",,,[]
"SciANN: A Keras/Tensorflow wrapper for scientific computations and
  physics-informed deep learning using artificial neural networks",http://arxiv.org/abs/2005.08803v2,2020-05-11T22:55:15Z,2020-09-16T03:18:44Z,"  In this paper, we introduce SciANN, a Python package for scientific computing
and physics-informed deep learning using artificial neural networks. SciANN
uses the widely used deep-learning packages Tensorflow and Keras to build deep
neural networks and optimization models, thus inheriting many of Keras's
functionalities, such as batch optimization and model reuse for transfer
learning. SciANN is designed to abstract neural network construction for
scientific computations and solution and discovery of partial differential
equations (PDE) using the physics-informed neural networks (PINN) architecture,
therefore providing the flexibility to set up complex functional forms. We
illustrate, in a series of examples, how the framework can be used for curve
fitting on discrete data, and for solution and discovery of PDEs in strong and
weak forms. We summarize the features currently available in SciANN, and also
outline ongoing and future developments.
","['\nEhsan Haghighat\n', '\nRuben Juanes\n']",,,http://dx.doi.org/10.1016/j.cma.2020.113552,cs.OH,"['cs.OH', 'cs.LG', 'cs.MS', '74S30 (primary), 74S05, 74B05, 74L05, 74L10 (secondary)', 'J.2']",10.1016/j.cma.2020.113552,,[]
White Paper on Business of 6G,http://arxiv.org/abs/2005.06400v2,2020-04-30T08:31:31Z,2020-07-16T12:34:06Z,"  Developing products, services and vertical applications for the future
digitized society in the 6G era requires a multidisciplinary approach and a
re-definition of how we create, deliver and consume network resources, data and
services for both communications and sensing purposes. This development will
change and disrupt the traditional business models and ecosystem roles of
digital service providers, as well as open the market for key stakeholders in
the 6G era like digital service operators, cloud operators and resource
brokers. White paper discusses unprecedented opportunities of enabling and
empowering multiple stakeholders to have a more active participation in the
future 6G ecosystem via novel sustainable open ecosystemic business models with
flexible integration of long tail services with tailored performance
attributes. This research adopts a qualitative scenario planning method and
portrays three scenario themes resulting in a total of 12 scenarios for the
futures of the 6G business. By focusing on key trends, their interactions, and
irreducible uncertainties, scenario building generates perspectives for the
futures within which alternative 6G business strategies were developed and
assessed for a traditional incumbent mobile network operator and a novel 6G
digital service provider stemming from redefined sustainable economics.
Value-capture in the 6G era requires understanding the dynamics of platforms
and ecosystems. Results indicate that, to reach some of the preferred futures,
we should pay attention to the privacy and security issues related to business
and regulation needs; public/governmental, corporate, community and user(s)
perspectives to and aims of governance; ecosystem configuration related to
users, decentralized business models and platforms; user empowerment; and the
role of location-specificity of services.
","['\nSeppo Yrjola\n', '\nPetri Ahokangas\n', '\nMarja Matinmikko-Blue\n', '\nRisto Jurva\n', '\nVivek Kant\n', '\nPasi Karppinen\n', '\nMarianne Kinnula\n', '\nHarilaos Koumaras\n', '\nMika Rantakokko\n', '\nVolker Ziegler\n', '\nAbhishek Thakur\n', '\nHans-Jurgen Zepernick\n']","This draft white paper has been written by an international expert
  group, led by the Finnish 6G Flagship program (6gflagship.com) at the
  University of Oulu, within a series of twelve 6G white papers to be published
  in their final format in June 2020",,http://arxiv.org/abs/2005.06400v2,cs.OH,['cs.OH'],,,[]
Acceptance of e-procurement in organisations,http://arxiv.org/abs/2005.10094v1,2020-05-06T23:38:36Z,2020-05-06T23:38:36Z,"  This research is concerned with the development of a realistic model for
e-procurement adoption by organisations and groups observing the Rules of
Islamic Sharia (RIS). This model is intended to be based on the behavioural
control, subjective norms, and the recognition of the benefits and risks of e
procurement adoption. The developed model,(E-PAM), combined and extended two
existing models previously used for information technology adoption. Central to
the design of the E-PAM is the principle that a realistic model should consider
all relevant psychological, social, cultural, demography, and religious
factors. .
","['\nMuhammed S. Maddi\n', '\nPaul Davis\n', '\nJohn Geraghty\n']",E-procurement and Structural Equation Modeling (SEM),,http://arxiv.org/abs/2005.10094v1,cs.OH,"['cs.OH', '14J26 (Secondary)', 'F.2']",,,[]
AI in society and culture: decision making and values,http://arxiv.org/abs/2005.02777v1,2020-04-29T07:09:39Z,2020-04-29T07:09:39Z,"  With the increased expectation of artificial intelligence, academic research
face complex questions of human-centred, responsible and trustworthy technology
embedded into society and culture. Several academic debates, social
consultations and impact studies are available to reveal the key aspects of the
changing human-machine ecosystem. To contribute to these studies, hundreds of
related academic sources are summarized below regarding AI-driven decisions and
valuable AI. In details, sociocultural filters, taxonomy of human-machine
decisions and perspectives of value-based AI are in the focus of this
literature review. For better understanding, it is proposed to invite
stakeholders in the prepared large-scale survey about the next generation AI
that investigates issues that go beyond the technology.
","['\nKatalin Feher\n', '\nAsta Zelenkauskaite\n']","5 pages, 2 figures",,http://arxiv.org/abs/2005.02777v1,cs.OH,"['cs.OH', 'cs.AI', '94', 'H.1; I.2; J.4; K.6']",,,[]
Neighbourhood Evaluation Criteria for Vertex Cover Problem,http://arxiv.org/abs/2005.05065v1,2020-05-07T05:30:01Z,2020-05-07T05:30:01Z,"  Neighbourhood Evaluation Criteria is a heuristical approximate algorithm that
attempts to solve the Minimum Vertex Cover. degree count is kept in check for
each vertex and the highest count based vertex is included in our cover set. In
the case of multiple equivalent vertices, the one with the lowest neighbourhood
influence is selected. In the case of still existing multiple equivalent
vertices, the one with the lowest remaining active vertex count (the highest
Independent Set enabling count) is selected as a tie-breaker.
",['\nKaustubh K Joshi\n'],"8 pages, 4 figures, 2 tables, 1 algorithm section",,http://arxiv.org/abs/2005.05065v1,cs.OH,"['cs.OH', 'cs.AI']",,,[]
"System of Computer Modeling and Features of their use in the Educational
  Process of General Secondary Eeducation",http://arxiv.org/abs/2005.07552v1,2020-05-03T10:26:12Z,2020-05-03T10:26:12Z,"  The article analyzes the historical aspect of the formation of computer
modeling as one of the perspective directions of educational process
development. The notion of ""system of computer modeling"", conceptual model of
system of computer modeling (SCMod), its components (mathematical, animation,
graphic, strategic), functions, principles and purposes of use are grounded.
The features of the organization of students work using SCMod, individual and
group work, the formation of subject competencies are described; the aspect of
students' motivation to learning is considered. It is established that
educational institutions can use SCMod at different levels and stages of
training and in different contexts, which consist of interrelated physical,
social, cultural and technological aspects. It is determined that the use of
SCMod in general secondary school would increase the capacity of teachers to
improve the training of students in natural and mathematical subjects and
contribute to the individualization of the learning process, in order to meet
the pace, educational interests and capabilities of each particular student. It
is substantiated that the use of SCMod in the study of natural-mathematical
subjects contributes to the formation of subject competencies, develops the
skills of analysis and decision-making, increases the level of digital
communication, develops vigilance, raises the level of knowledge, increases the
duration of attention of students. Further research requires the justification
of the process of forming students' competencies in natural-mathematical
subjects and designing cognitive tasks using SCMod.
",['\nSvitlana H. Lytvynova\n'],"17 pages, in Ukrainian",,http://dx.doi.org/10.33407/itlt.v64i2.2111,cs.OH,"['cs.OH', 'cs.CY', '6802', 'K.3; K.4']",10.33407/itlt.v64i2.2111,,[]
"Convolutional Neural Networks vs. Deformable Image Registration For
  Medical Slice Interpolation",http://arxiv.org/abs/2004.13784v1,2020-04-28T19:28:26Z,2020-04-28T19:28:26Z,"  Medical image slice interpolation is an active field of research. The methods
for this task can be categorized into two broad groups: intensity-based and
object-based interpolation methods. While intensity-based methods are generally
easier to perform and less computationally expensive, object-based methods are
capable of producing more accurate results and account for deformable changes
in the objects within the slices. In this paper, performance of two well-known
object-based interpolation methods is analyzed and compared. Here, a deformable
registration-based method specifically designed for medical applications and a
learning-based method, trained for video frame interpolation, are considered.
While the deformable registration-based technique is capable of accurate
modeling of the changes in the shapes of the objects within slices, the
learning-based method is able to produce results with similar accuracy, but
with a much sharper appearance in a fraction of the time. This is despite the
fact that the learning-based approach is not trained on medical images and
rather is trained using regular video footage. However, experiments show that
the method is capable of accurate slice interpolation results.
","['\nDilip Kumar Verma\n', '\nAhmadreza Baghaie\n']",Submitted to LISAT 2020,,http://arxiv.org/abs/2004.13784v1,eess.IV,"['eess.IV', 'cs.OH']",,,[]
Dataset for anomalies detection in 3D printing,http://arxiv.org/abs/2004.08817v1,2020-04-19T11:20:21Z,2020-04-19T11:20:21Z,"  Nowadays, Internet of Things plays a significant role in many domains.
Especially, Industry 4.0 is making a great usage of concepts like smart sensors
and big data analysis. IoT devices are commonly used to monitor industry
machines and detect anomalies in their work. In this paper we present and
describe a set of data streams coming from working 3D printer. Among others, it
contains accelerometer data of printer head, intrusion power and temperatures
of the printer elements. In order to gain data we lead to several printing
malfunctions applied to the 3D model. Resulting dataset can therefore be used
for anomalies detection research.
","['\nJoanna Sendorek\n', '\nTomasz Szydlo\n', '\nMateusz Windak\n', '\nRobert Brzoza-Woch\n']",,,http://arxiv.org/abs/2004.08817v1,cs.OH,['cs.OH'],,,[]
Knowledge Management Systems Requirements Specifications,http://arxiv.org/abs/2004.08961v1,2020-04-19T21:08:58Z,2020-04-19T21:08:58Z,"  In recent years, Knowledge Management Systems (KMS) have drawn remarkable
attention. However, there is no common understanding of how a knowledge
management system should look like or where the corresponding research should
be directed at. Based on a number of essential requirements that a KMS should
satisfy, this report introduces some possible requirements for the
commonwealth's KMS components forming the KMS architecture. Also, these
requirements will be analysed through evaluating and measuring there
functionality to produce a tangible outcome.
",['\nOmar S. Al-Kadi\n'],Online report - University of Canberra (May 2003),,http://arxiv.org/abs/2004.08961v1,cs.OH,['cs.OH'],,,[]
Correlating Unlabeled Events at Runtime,http://arxiv.org/abs/2004.09971v1,2020-04-19T05:02:32Z,2020-04-19T05:02:32Z,"  Process mining is of great importance for both data-centric and
process-centric systems. Process mining receives so-called process logs which
are collections of partially-ordered events. An event has to possess at least
three attributes, case ID, task ID and a timestamp for mining approaches to
work. When a case ID is unknown, the event is called unlabeled. Traditionally,
process mining is an offline task, where events are collected from different
sources are usually manually correlated. That is, events belonging to the same
instance are assigned the same case ID. With today's high-volume/high-speed
nature of, e.g., IoT applications, process mining shifts to be an online task.
For this, event correlation has to be automated and has to occur as the data is
generated. In this paper, we introduce an approach that correlates unlabeled
events at runtime. Given a process model, a stream of unlabeled events and
other information about task duration, our approach can induce a case
identifier to a set of unlabeled events with a trust percentage. It can also
check the conformance of the identified cases with the process model. A
prototype of the proposed approach was implemented and evaluated against
real-life and synthetic logs.
","['\nIman M. A. Helal\n', '\nAhmed Awad\n']","10 pages, 3 figures",,http://arxiv.org/abs/2004.09971v1,cs.OH,['cs.OH'],,,[]
"Current Practices in the Information Collection for Enterprise
  Architecture Management",http://arxiv.org/abs/2004.05087v1,2020-04-07T11:38:35Z,2020-04-07T11:38:35Z,"  The digital transformation influences business models, processes, and
enterprise IT landscape as a whole. Therefore, business-IT alignment is
becoming more important than ever before. Enterprise architecture management
(EAM) is designed to support and improve this business-IT alignment. The
success of EAM crucially depends on the information available about a company's
enterprise architecture, such as infrastructure components, applications, and
business processes. This paper discusses the results of a qualitative expert
survey with 26 experts in the field of EAM. The goal of this survey was to
highlight current practices in the information collection for EAM and identify
relevant information from enterprise-external data sources. The results provide
a comprehensive overview of collected and utilized information in the industry,
including an assessment of the relevance of such information. Furthermore, the
results highlight challenges in practice and point out investments that
organizations plan in the field of EAM.
","['\nRobert Ehrensperger\n', '\nClemens Sauerwein\n', '\nRuth Breu\n']",11 pages,,http://arxiv.org/abs/2004.05087v1,cs.OH,['cs.OH'],,,[]
"Distributed Resources for the Earth System Grid Advanced Management
  (DREAM)",http://arxiv.org/abs/2004.09599v1,2020-04-13T19:13:33Z,2020-04-13T19:13:33Z,"  The DREAM project was funded more than 3 years ago to design and implement a
next-generation ESGF (Earth System Grid Federation [1]) architecture which
would be suitable for managing and accessing data and services resources on a
distributed and scalable environment. In particular, the project intended to
focus on the computing and visualization capabilities of the stack, which at
the time were rather primitive. At the beginning, the team had the general
notion that a better ESGF architecture could be built by modularizing each
component, and redefining its interaction with other components by defining and
exposing a well defined API. Although this was still the high level principle
that guided the work, the DREAM project was able to accomplish its goals by
leveraging new practices in IT that started just about 3 or 4 years ago: the
advent of containerization technologies (specifically, Docker), the development
of frameworks to manage containers at scale (Docker Swarm and Kubernetes), and
their application to the commercial Cloud. Thanks to these new technologies,
DREAM was able to improve the ESGF architecture (including its computing and
visualization services) to a level of deployability and scalability beyond the
original expectations.
","['\nLuca Cinquini\n', '\nSteve Petruzza\n', '\nJason Jerome Boutte\n', '\nSasha Ames\n', '\nGhaleb Abdulla\n', '\nVenkatramani Balaji\n', '\nRobert Ferraro\n', '\nAparna Radhakrishnan\n', '\nLaura Carriere\n', '\nThomas Maxwell\n', '\nGiorgio Scorzelli\n', '\nValerio Pascucci\n']",,,http://arxiv.org/abs/2004.09599v1,cs.OH,['cs.OH'],,,[]
ROOT I/O compression improvements for HEP analysis,http://arxiv.org/abs/2004.10531v1,2020-04-08T16:35:43Z,2020-04-08T16:35:43Z,"  We overview recent changes in the ROOT I/O system, increasing performance and
enhancing it and improving its interaction with other data analysis ecosystems.
Both the newly introduced compression algorithms, the much faster bulk I/O data
path, and a few additional techniques have the potential to significantly to
improve experiment's software performance. The need for efficient lossless data
compression has grown significantly as the amount of HEP data collected,
transmitted, and stored has dramatically increased during the LHC era. While
compression reduces storage space and, potentially, I/O bandwidth usage, it
should not be applied blindly: there are significant trade-offs between the
increased CPU cost for reading and writing files and the reduce storage space.
","['\nOksana Shadura\nUniversity of Nebraska-Lincoln\n', '\nBrian Paul Bockelman\nMorgridge Institute for Research\n', '\nPhilippe Canal\nFermilab\n', '\nDanilo Piparo\nCERN\n', '\nZhe Zhang\nUniversity of Nebraska-Lincoln\n']",Submitted as a proceeding for CHEP 2019,,http://dx.doi.org/10.1051/epjconf/202024502017,cs.OH,['cs.OH'],10.1051/epjconf/202024502017,,"['University of Nebraska-Lincoln', 'Morgridge Institute for Research', 'Fermilab', 'CERN', 'University of Nebraska-Lincoln']"
"Interactive distributed cloud-based web-server systems for the smart
  healthcare industry",http://arxiv.org/abs/2005.01442v1,2020-04-14T21:58:03Z,2020-04-14T21:58:03Z,"  The work aims to investigate the possible contemporary interactive cloud
based solutions in the fields of the applied medicine for the smart Healthcare
as the data visualization open-source free system distributed under the MIT
license. A comparative study of a number of the well-known implementations of
the Ray Casting algorithms was studied. A new method of numerical calculus is
proposed for calculating the volume -- the method of spheres, as well as a
proposal for paralleling the algorithm on graphic accelerators in a linearly
homogeneous computing environment using the block decomposition methods. For
the artifacts control -- algorithm of the cubic interpolation was used. The
cloud server architecture was proposed.
",['\nAlmagul Baurzhanovna Kondybayeva\n'],"This work is dedicated to the questions of the contemporary medical
  image visualization, the architecture design of the cloud server systems and
  the using of methods for the .DICOM data representation for the distributed
  smart healthcare industry systems. The work is done as a part of the PhD
  thesis of the author for non-profit/non-commercial, educational/research only
  reasons under the MIT License",,http://arxiv.org/abs/2005.01442v1,cs.OH,"['cs.OH', 'cs.CY']",,,[]
Quantum Approximation for Wireless Scheduling,http://arxiv.org/abs/2004.11229v2,2020-04-14T13:29:22Z,2020-09-04T05:00:57Z,"  This paper proposes a quantum approximate optimization algorithm (QAOA)
method for wireless scheduling problems. The QAOA is one of the promising
hybrid quantum-classical algorithms for many applications and it provides
highly accurate optimization solutions in NP-hard problems. QAOA maps the given
problems into Hilbert spaces, and then it generates Hamiltonian for the given
objectives and constraints. Then, QAOA finds proper parameters from classical
optimization approaches in order to optimize the expectation value of generated
Hamiltonian. Based on the parameters, the optimal solution to the given problem
can be obtained from the optimum of the expectation value of Hamiltonian.
Inspired by QAOA, a quantum approximate optimization for scheduling (QAOS)
algorithm is proposed. First of all, this paper formulates a wireless
scheduling problem using maximum weight independent set (MWIS). Then, for the
given MWIS, the proposed QAOS designs the Hamiltonian of the problem. After
that, the iterative QAOS sequence solves the wireless scheduling problem. This
paper verifies the novelty of the proposed QAOS via simulations implemented by
Cirq and TensorFlow-Quantum.
","['\nJaeho Choi\n', '\nSeunghyeok Oh\n', '\nJoongheon Kim\n']",,,http://arxiv.org/abs/2004.11229v2,cs.OH,"['cs.OH', 'quant-ph']",,,[]
Dyslexia and Dysgraphia prediction: A new machine learning approach,http://arxiv.org/abs/2005.06401v1,2020-04-15T09:31:51Z,2020-04-15T09:31:51Z,"  Learning disabilities like dysgraphia, dyslexia, dyspraxia, etc. interfere
with academic achievements but have also long terms consequences beyond the
academic time. It is widely admitted that between 5% to 10% of the world
population is subject to this kind of disabilities. For assessing such
disabilities in early childhood, children have to solve a battery of tests.
Human experts score these tests, and decide whether the children require
specific education strategy on the basis of their marks. The assessment can be
lengthy, costly and emotionally painful. In this paper, we investigate how
Artificial Intelligence can help in automating this assessment. Gathering a
dataset of handwritten text pictures and audio recordings, both from standard
children and from dyslexic and/or dysgraphic children, we apply machine
learning techniques for classification in order to analyze the differences
between dyslexic/dysgraphic and standard readers/writers and to build a model.
The model is trained on simple features obtained by analysing the pictures and
the audio files. Our preliminary implementation shows relatively high
performances on the dataset we have used. This suggests the possibility to
screen dyslexia and dysgraphia via non-invasive methods in an accurate way as
soon as enough data are available.
","['\nGilles Richard\n', '\nMathieu Serrurier\n']",,,http://arxiv.org/abs/2005.06401v1,cs.OH,"['cs.OH', 'cs.LG', 'stat.ML']",,,[]
"NDE 4.0: Digital Twin, Semantics, Interfaces, Networking, Feedback, New
  Markets and Integration into the Industrial Internet of Things",http://arxiv.org/abs/2004.05193v1,2020-04-02T16:40:55Z,2020-04-02T16:40:55Z,"  The industrial revolution is divided into three phases by historians: The
invention of the steam engine (mechanization), electricity (mass production)
and the microelectric revolution (automation). There was a similar development
in non-destructive evaluation: tools such as lenses or stethoscopes allowed the
human senses to be sharpened, the conversion of waves makes the invisible
visible and thus offers a ""look"" into the components and finally automation,
digitization and reconstruction. During the entire industrial development NDE
was decisively responsible for the quality and thus for the success of the
manufactured goods. Industry is now talking about a fourth revolution: The
informatization, digitization and networking of industrial production. As
always, NDE will be critical to the success of this fourth revolution by
providing the database needed for feedback in a networked production
environment. For NDE, this will lead to change. The test results must be made
available to a networked production environment in such a way that they can be
evaluated for feedback loops, the testability must be considered in the design
and the reliability of the test statements will become increasingly important.
This publication presents first an orientation to NDE 4.0, including the
development of Industry and NDE, a definition of its revolutions, a collection
of several current-day challenges of NDE, and a discussion whether and how
those can be solved with NDE 4.0. Second this publication presents concepts on
how NDE can be integrated into Industry 4.0 landscapes: The Reference
Architecture Model Industry 4.0 (RAMI 4.0) shows the complete Industry 4.0
space and allows every Industry 4.0 standard and interface to be located. The
Industry 4.0 Asset Administration Shell (AAS) implements the digital twin and
is the interface between Industry 4.0 communication and the physical device.
The ...
",['\nJohannes Vrana\n'],"24 pages, 12 figures",Mater. Eval. 78 (2020) 835-851,http://dx.doi.org/10.32548/2020.me-04131,cs.OH,['cs.OH'],10.32548/2020.me-04131,,[]
"Objective Multi-variable Classification and Inference of Biological
  Neuronal Networks",http://arxiv.org/abs/2003.12670v1,2020-03-28T00:25:49Z,2020-03-28T00:25:49Z,"  Classification of biological neuron types and networks poses challenges to
the full understanding of the brain's organisation and functioning. In this
paper, we develop a novel objective classification model of biological neuronal
types and networks based on the communication metrics of neurons. This presents
advantages against the existing approaches since the mutual information or the
delay between neurons obtained from spike trains are more abundant data compare
to conventional morphological data. We firstly designed two open-access
supporting computational platforms of various neuronal circuits from the Blue
Brain Project realistic models, named Neurpy and Neurgen. Then we investigate
how the concept of network tomography could be achieved with cortical neuronal
circuits for morphological, topological and electrical classification of
neurons. We extract the simulated data to many different classifiers (including
SVM, Decision Trees, Random Forest, and Artificial Neuron Networks) classifying
the specific cell type (and sub-group types) achieving accuracies of up to
70\%. Inference of biological network structures using network tomography
reached up to 65\% of accuracy. We also analysed recall, precision and F1score
of the classification of five layers, 25 cell m-types, and 14 cell e-types. Our
research not only contributes to existing classification efforts but sets the
road-map for future usage of cellular-scaled brain-machine interfaces for
in-vivo objective classification of neurons as a sensing mechanism of the
brain's structure.
","['\nMichael Taynnan Barros\n', '\nHarun Siljak\n', '\nPeter Mullen\n', '\nConstantinos Papadias\n', '\nJari Hyttinen\n', '\nNicola Marchetti\n']",,,http://arxiv.org/abs/2003.12670v1,q-bio.NC,"['q-bio.NC', 'cs.OH']",,,[]
"Software-Based Monitoring and Analysis of a USB Host Controller Subject
  to Electrostatic Discharge",http://arxiv.org/abs/2004.06647v1,2020-03-16T16:08:52Z,2020-03-16T16:08:52Z,"  Observing, understanding, and mitigating the effects of failure in embedded
systems is essential for building dependable control systems. We develop a
software-based monitoring methodology to further this goal. This methodology
can be applied to any embedded system peripheral and allows the system to
operate normally while the monitoring software is running. We use software to
instrument the operating system kernel and record indicators of system
behavior. By comparing those indicators against baseline indicators of normal
system operation, faults can be detected and appropriate action can be taken.
  We implement this methodology to detect faults caused by electrostatic
discharge in a USB host controller. As indicators, we select specific control
registers that provide a manifestation of the internal execution of the host
controller. Analysis of the recorded register values reveals differences in
system execution when the system is subject to interference. %We also develop a
classifier capable of predicting whether or not the system's behavior is being
affected by such shocks. This improved understanding of system behavior may
lead to better hardware and software mitigation of electrostatic discharge and
assist in root-cause analysis and repair of failures.
","['\nNatasha Jarus\n', '\nAntonio Sabatini\n', '\nPratik Maheshwari\n', '\nSahra Sedigh Sarvestani\n']",To appear in proceedings of RTEST2020,,http://arxiv.org/abs/2004.06647v1,cs.OH,['cs.OH'],,,[]
"Modeling and solving a vehicle-sharing problem considering multiple
  alternative modes of transport",http://arxiv.org/abs/2003.08207v2,2020-03-17T11:12:20Z,2022-09-28T12:20:54Z,"  Motivated by the change in mobility patterns, we present a scheduling
approach for a vehicle-sharing problem, considering several alternative modes
of transport, from a company viewpoint with centralized planning. We consider
vehicle-sharing in a company having one or more depots and a fixed number of
users, i.e. employees. The users have appointments with a fixed location and
fixed start and end times. A vehicle must be used for a full trip of a user
from depot to depot. We aim at assigning vehicles to user trips so as to
maximize savings compared to other modes of transport. We first consider that
only one type of vehicle is used, and second that multiple vehicle types can be
used. For the first case, we show that the vehicle-sharing problem can be
formulated as a minimum-cost flow problem. Secondly, if multiple types of
vehicles are available the problem can be formulated as a multi-commodity flow
problem. These formulations make the problem applicable in daily operations due
to efficient solution methods. We provide a comprehensive computational study
for both cases on instances based on demographic, spatial, and economic data of
Vienna. We show that our formulations for this problem solve these instances in
a few seconds, which makes them usable in an online booking system. In the
analysis we discuss different potential settings. We study the optimal
composition of a shared fleet, restricted sets of modes of transport, and
variations of the objective function.
","['\nMiriam Enzi\n', '\nSophie N. Parragh\n', '\nDavid Pisinger\n']",,,http://arxiv.org/abs/2003.08207v2,cs.OH,"['cs.OH', 'cs.AI']",,,[]
"High Performance Interference Suppression in Multi-User Massive MIMO
  Detector",http://arxiv.org/abs/2005.03466v1,2020-03-20T15:37:26Z,2020-03-20T15:37:26Z,"  In this paper, we propose a new nonlinear detector with improved interference
suppression in Multi-User Multiple Input, Multiple Output (MU-MIMO) system. The
proposed detector is a combination of the following parts: QR decomposition
(QRD), low complexity users sorting before QRD, sorting-reduced (SR) K-best
method and minimum mean square error (MMSE) pre-processing. Our method
outperforms a linear interference rejection combining (IRC, i.e. MMSE
naturally) method significantly in both strong interference and additive white
noise scenarios with both ideal and real channel estimations. This result has
wide application importance for scenarios with strong interference, i.e. when
co-located users utilize the internet in stadium, highway, shopping center,
etc. Simulation results are presented for the non-line of sight 3D-UMa model of
5G QuaDRiGa 2.0 channel for 16 highly correlated single-antenna users with
QAM16 modulation in 64 antennas of Massive MIMO system. The performance was
compared with MMSE and other detection approaches.
","['\nAndrey Ivanov\n', '\nAlexander Osinsky\n', '\nDmitry Lakontsev\n', '\nDmitry Yarotsky\n']",Accepted for presentation at the VTC2020-Spring conference,,http://dx.doi.org/10.1109/VTC2020-Spring48590.2020.9128653,cs.OH,"['cs.OH', 'cs.IT', 'eess.SP', 'math.IT']",10.1109/VTC2020-Spring48590.2020.9128653,,[]
NDE 4.0 From Design Thinking to Strategy,http://arxiv.org/abs/2003.07773v2,2020-03-05T12:54:19Z,2020-11-17T13:50:51Z,"  Cyber technologies are offering new horizons for quality control in
manufacturing and safety assurance in-service of physical assets. The line
between non-destructive evaluation (NDE) and Industry 4.0 is getting blurred
since both are sensory data-driven domains. This multidisciplinary approach has
led to the emergence of a new capability: NDE 4.0. The NDT community is coming
together once again to define the purpose, chart the process, and address the
adoption of emerging technologies. In this paper, the authors have taken a
design thinking approach to spotlight proper objectives for research on this
subject. It begins with qualitative research on twenty different perceptions of
stakeholders and misconceptions around the current state of NDE. The
interpretation is used to define ten value propositions or use cases under ""NDE
for Industry 4.0"" and ""Industry 4.0 for NDE"" leading up to the clarity of
purpose for NDE 4.0, enhanced safety and economic value for stakeholders. To
pursue this worthy cause, the paper delves into some of the top adoption
challenges, and proposes a journey of managed innovation, conscious skills
development, and a new form of leadership required to succeed in the
cyber-physical world.
","['\nJohannes Vrana\n', '\nRipudaman Singh\n']",,"J Nondestruct Eval 40, 8 (2021)",http://dx.doi.org/10.1007/s10921-020-00735-9,cs.OH,['cs.OH'],10.1007/s10921-020-00735-9,,[]
"Predicting Memory Compiler Performance Outputs using Feed-Forward Neural
  Networks",http://arxiv.org/abs/2003.03269v1,2020-03-05T13:11:47Z,2020-03-05T13:11:47Z,"  Typical semiconductor chips include thousands of mostly small memories. As
memories contribute an estimated 25% to 40% to the overall power, performance,
and area (PPA) of a chip, memories must be designed carefully to meet the
system's requirements. Memory arrays are highly uniform and can be described by
approximately 10 parameters depending mostly on the complexity of the
periphery. Thus, to improve PPA utilization, memories are typically generated
by memory compilers. A key task in the design flow of a chip is to find optimal
memory compiler parametrizations which on the one hand fulfill system
requirements while on the other hand optimize PPA. Although most compiler
vendors also provide optimizers for this task, these are often slow or
inaccurate. To enable efficient optimization in spite of long compiler run
times, we propose training fully connected feed-forward neural networks to
predict PPA outputs given a memory compiler parametrization. Using an
exhaustive search-based optimizer framework which obtains neural network
predictions, PPA-optimal parametrizations are found within seconds after chip
designers have specified their requirements. Average model prediction errors of
less than 3%, a decision reliability of over 99% and productive usage of the
optimizer for successful, large volume chip design projects illustrate the
effectiveness of the approach.
","['\nFelix Last\n', '\nMax Haeberlein\n', '\nUlf Schlichtmann\n']","23 pages, 8 figures, 4 tables; accepted for publication in the ACM
  TODAES special issue on machine learning for CAD (ML-CAD)","ACM Trans. Des. Autom. Electron. Syst. 25, 5 (2020) 39",http://dx.doi.org/10.1145/3385262,cs.OH,"['cs.OH', 'cs.LG']",10.1145/3385262,,[]
"Toward a Wearable RFID System for Real-Time Activity Recognition Using
  Radio Patterns",http://arxiv.org/abs/2003.07719v1,2020-03-08T08:08:05Z,2020-03-08T08:08:05Z,"  Elderly care is one of the many applications supported by real-time activity
recognition systems. Traditional approaches use cameras, body sensor networks,
or radio patterns from various sources for activity recognition. However, these
approaches are limited due to ease-of-use, coverage, or privacy preserving
issues. In this paper, we present a novel wearable Radio Frequency
Identification (RFID) system aims at providing an easy-to-use solution with
high detection coverage. Our system uses passive tags which are
maintenance-free and can be embedded into the clothes to reduce the wearing and
maintenance efforts. A small RFID reader is also worn on the user's body to
extend the detection coverage as the user moves. We exploit RFID radio patterns
and extract both spatial and temporal features to characterize various
activities. We also address the issues of false negative of tag readings and
tag/antenna calibration, and design a fast online recognition system. Antenna
and tag selection is done automatically to explore the minimum number of
devices required to achieve target accuracy. We develop a prototype system
which consists of a wearable RFID system and a smartphone to demonstrate the
working principles, and conduct experimental studies with four subjects over
two weeks. The results show that our system achieves a high recognition
accuracy of 93.6 percent with a latency of 5 seconds. Additionally, we show
that the system only requires two antennas and four tagged body parts to
achieve a high recognition accuracy of 85 percent.
","['\nLiang Wang\n', '\nTao Gu\n', '\nXianping Tao\n', '\nJian Lu\n']",,,http://arxiv.org/abs/2003.07719v1,cs.OH,"['cs.OH', 'eess.SP']",,,[]
"Reconfigurable Computing Applied to Latency Reduction for the Tactile
  Internet",http://arxiv.org/abs/2003.12463v1,2020-03-12T03:45:43Z,2020-03-12T03:45:43Z,"  Tactile internet applications allow robotic devices to be remotely controlled
over a communication medium with an unnoticeable time delay. In a bilateral
communication, the acceptable round trip latency is usually in the order of 1ms
up to 10ms depending on the application requirements. It is estimated that 70%
of the total latency is generated by the communication network, and the
remaining 30% is produced by master and slave devices. Thus, this paper aims to
propose a strategy to reduce 30% of the total latency that is produced by such
devices. The strategy is to apply reconfigurable computation using FPGAs to
minimize the execution time of device-associated algorithms. With this in mind,
this work presents a hardware reference model for modules that implement
nonlinear positioning and force calculations as well as a tactile system formed
by two robotic manipulators. In addition to presenting the implementation
details, simulations and experimental tests are performed in order to validate
the proposed model. Results associated with the FPGA sampling rate, throughput,
latency, and post-synthesis occupancy area are analyzed.
","['\nJosé C. V. S. Junior\n', '\nMatheus F. Torquato\n', '\nToktam Mahmoodi\n', '\nMischa Dohler\n', '\nMarcelo A. C. Fernandes\n']","20 pages, 32 Figures",,http://dx.doi.org/10.3390/s22207851,cs.OH,"['cs.OH', 'eess.SP']",10.3390/s22207851,,[]
"TiLA: Twin-in-the-Loop Architecture for Cyber-Physical Production
  Systems",http://arxiv.org/abs/2003.09370v1,2020-03-11T03:31:44Z,2020-03-11T03:31:44Z,"  Digital twin is a virtual replica of a real-world object that lives
simultaneously with its physical counterpart. Since its first introduction in
2003 by Grieves, digital twin has gained momentum in a wide range of
applications such as industrial manufacturing, automotive and artificial
intelligence. However, many digital-twin-related approaches, found in
industries as well as literature, mainly focus on modelling individual physical
things with high-fidelity methods with limited scalability. In this paper, we
introduce a digital-twin architecture called TiLA (Twin-in-the-Loop
Architecture). TiLA employs heterogeneous models and online data to create a
digital twin, which follows a Globally Asynchronous Locally Synchronous (GALS)
model of computation. It facilitates the creation of a scalable digital twin
with different levels of modelling abstraction as well as giving GALS formalism
for execution strategy. Furthermore, TiLA provides facilities to develop
applications around the twin as well as an interface to synchronise the twin
with the physical system through an industrial communication protocol. A
digital twin for a manufacturing line has been developed as a case study using
TiLA. It demonstrates the use of digital twin models together with online data
for monitoring and analysing failures in the physical system.
","['\nHeejong Park\n', '\nArvind Easwaran\n', '\nSidharta Andalam\n']",,"IEEE International Conference on Computer Design (ICCD), Abu
  Dhabi, United Arab Emirates, 2019, pages 82-90",http://dx.doi.org/10.1109/ICCD46524.2019.00019,cs.OH,"['cs.OH', 'cs.SY', 'eess.SY']",10.1109/ICCD46524.2019.00019,,[]
Toward Predicting Success and Failure in CS2: A Mixed-Method Analysis,http://arxiv.org/abs/2002.11813v1,2020-02-25T01:49:43Z,2020-02-25T01:49:43Z,"  Factors driving success and failure in CS1 are the subject of much study but
less so for CS2. This paper investigates the transition from CS1 to CS2 in
search of leading indicators of success in CS2. Both CS1 and CS2 at the
University of North Carolina Wilmington (UNCW) are taught in Python with annual
enrollments of 300 and 150 respectively. In this paper, we report on the
following research questions: 1) Are CS1 grades indicators of CS2 grades? 2)
Does a quantitative relationship exist between CS2 course grade and a modified
version of the SCS1 concept inventory? 3) What are the most challenging aspects
of CS2, and how well does CS1 prepare students for CS2 from the student's
perspective? We provide a quantitative analysis of 2300 CS1 and CS2 course
grades from 2013--2019. In Spring 2019, we administered a modified version of
the SCS1 concept inventory to 44 students in the first week of CS2. Further, 69
students completed an exit questionnaire at the conclusion of CS2 to gain
qualitative student feedback on their challenges in CS2 and on how well CS1
prepared them for CS2. We find that 56% of students' grades were lower in CS2
than CS1, 18% improved their grades, and 26% earned the same grade. Of the
changes, 62% were within one grade point. We find a statistically significant
correlation between the modified SCS1 score and CS2 grade points. Students
identify linked lists and class/object concepts among the most challenging.
Student feedback on CS2 challenges and the adequacy of their CS1 preparations
identify possible avenues for improving the CS1-CS2 transition.
","['\nLucas Layman\n', '\nYang Song\n', '\nCurry Guinn\n']","The definitive Version of Record was published in 2020 ACM Southeast
  Conference (ACMSE 2020), April 2-4, 2020, Tampa, FL, USA. 8 pages",,http://dx.doi.org/10.1145/3374135.3385277,cs.OH,['cs.OH'],10.1145/3374135.3385277,,[]
Engaging Users through Social Media in Public Libraries,http://arxiv.org/abs/2003.04204v2,2020-02-25T00:08:33Z,2020-10-30T20:34:21Z,"  The participatory library is an emerging concept which refers to the idea
that an integrated library system must allow users to take part in core
functions of the library rather than engaging on the periphery. To embrace the
participatory idea, libraries have employed many technologies, such as social
media to help them build participatory services and engage users. To help
librarians understand the impact of emerging technologies on a participatory
service building, this paper takes social media as an example to explore how to
use different engagement strategies that social media provides to engage more
users. This paper provides three major contributions to the library system. The
libraries can use the resultant engagement strategies to engage its users.
Additionally, the best-fit strategy can be inferred and designed based on the
preferences of users. Lastly, the preferences of users can be understood based
on data analysis of social media. Three such contributions put together to
fully address the proposed research question of how to use different engagement
strategies on social media to build participatory library services and better
engage more users visiting the library?
","['\nHongbo Zou\n', '\nHsuanwei Michelle Chen\n', '\nSharmistha Dey\n']",11 pages,,http://arxiv.org/abs/2003.04204v2,cs.OH,"['cs.OH', 'cs.HC', 'cs.LG', 'cs.SI']",,,[]
"On Orthogonal Projections on the Space of Consistent Pairwise
  Comparisons Matrices",http://arxiv.org/abs/2002.06607v1,2020-02-16T16:02:19Z,2020-02-16T16:02:19Z,"  In this study, the orthogonalization process for different inner products is
applied to pairwise comparisons. Properties of consistent approximations of a
given inconsistent pairwise comparisons matrix are examined. A method of a
derivation of a priority vector induced by a pairwise comparison matrix for a
given inner product has been introduced. The mathematical elegance of
orthogonalization and its universal use in most applied sciences has been the
motivating factor for this study. However, the finding of this study that
approximations depend on the inner product assumed, is of considerable
importance.
","['\nW. W. Koczkodaj\n', '\nR. Smarzewski\n', '\nJ. Szybowski\n']","Followup to: Koczkodaj, WW; Orlowski, M, An orthogonal basis for
  computing a consistent approximation to a pairwise comparisons matrix,
  Computers and Mathematics with Applications, 34(10): 41-47. 1997","Fundamenta Informaticae, vol. 172, no. 4, pp. 379-397, 2020",http://arxiv.org/abs/2002.06607v1,cs.OH,"['cs.OH', 'I.2']",,,[]
"Proceedings of the VI International Workshop on Locational Analysis and
  Related Problems",http://arxiv.org/abs/2002.08287v1,2020-02-19T17:01:46Z,2020-02-19T17:01:46Z,"  The International Workshop on Locational Analysis and Related Problems will
take place during November 25-27, 2015 in Barcelona (Spain). It is organized by
the Spanish Location Network and Location Group GELOCA (SEIO). GELOCA is a
working group on location belonging to the Statistics and Operations Research
Spanish Society. The Spanish Location Network is a group of more than 140
researchers distributed into 16 nodes corresponding to several Spanish
universities. The Network has been funded by the Spanish Government. Every
year, the Network organizes a meeting to promote the communication among its
members and between them and other researchers, and to contribute to the
development of the location field and related problems. Previous meetings took
place in Sevilla (October 1-3, 2014), Torremolinos (M\'alaga, June 19-21,
2013), Granada (May 10-12, 2012), Las Palmas de Gran Canaria (February 2-5,
2011) and Sevilla (February 1-3, 2010). The topics of interest are location
analysis and related problems. This includes location, routing, networks,
transportation and logistics models; exact and heuristic solution methods, and
computational geometry, among others.
","['\nMaria Albareda-Sambola\nEditors\n', '\nLuisa I. Martínez-Merino\nEditors\n', '\nAntonio M. Rodríguez-Chía\nEditors\n']",,,http://dx.doi.org/10.3926/redloca15,cs.OH,['cs.OH'],10.3926/redloca15,,"['Editors', 'Editors', 'Editors']"
"Proceedings of the VIII International Workshop on Locational Analysis
  and Related Problems",http://arxiv.org/abs/2002.08293v1,2020-02-19T17:10:43Z,2020-02-19T17:10:43Z,"  The International Workshop on Locational Analysis and Related Problems will
take place during September 27-29, 2017 in Segovia (Spain). It is organized by
the Spanish Location Network and Location Group GELOCA (SEIO). GELOCA is a
working group on location belonging to the Statistics and Operations Research
Spanish Society. The Spanish Location Network is a group of more than 100
researchers distributed into 16 nodes corresponding to several Spanish
universities. The Network has been funded by the Spanish Government. Every
year, the Network organizes a meeting to promote the communication between its
members and between them and other researchers, and to contribute to the
development of the location field and related problems. Previous meetings took
place in M\'alaga (September 14-16, 2016), Barcelona (November 25-28, 2015),
Sevilla (October 1-3, 2014), Torremolinos (M\'alaga, June 19-21, 2013), Granada
(May 10-12, 2012), Las Palmas de Gran Canaria (February 2-5, 2011) and Sevilla
(February 1-3, 2010). The topics of interest are location analysis and related
problems. It includes location, networks, transportation, routing, logistics
models, as well as, exact and heuristic solution methods, and computational
geometry, among others.
","['\nMarta Baldomero-Naranjo\nEditors\n', '\nInmaculada Espejo-Miranda\nEditors\n', '\nLuisa I. Martínez-Merino\nEditors\n', '\nAntonio M. Rodríguez-Chía\nEditors\n', '\nDiego Ruiz-Hernández\nEditors\n']",,,http://arxiv.org/abs/2002.08293v1,cs.OH,['cs.OH'],,,"['Editors', 'Editors', 'Editors', 'Editors', 'Editors']"
"Proceedings of the IX International Workshop on Locational Analysis and
  Related Problems",http://arxiv.org/abs/2002.08300v1,2020-02-19T17:19:31Z,2020-02-19T17:19:31Z,"  The International Workshop on Locational Analysis and Related Problems will
take place during January 30-February 1, 2019 in C\'adiz (Spain). It is
organized by the Spanish Location Network and Location Group GELOCA (SEIO).
GELOCA is a working group on location belonging to the Statistics and
Operations Research Spanish Society. The Spanish Location Network is a group of
more than 140 researchers distributed into 16 nodes corresponding to several
Spanish universities. The Network has been funded by the Spanish Government.
Every year, the Network organizes a meeting to promote the communication
between its members and between them and other researchers, and to contribute
to the development of the location field and related problems. Previous
meetings took place in Segovia (September 27-29, 2017), M\'alaga (September
14-16, 2016), Barcelona (November 25-28, 2015), Sevilla (October 1-3, 2014),
Torremolinos (M\'alaga, June 19-21, 2013), Granada (May 10-12, 2012), Las
Palmas de Gran Canaria (February 2-5, 2011) and Sevilla (February 1-3, 2010).
The topics of interest are location analysis and related problems. It includes
location models, networks, transportation, logistics, exact and heuristic
solution methods, and computational geometry, among others.
","['\nMarta Baldomero-Naranjo\nEditors\n', '\nInmaculada Espejo-Miranda\nEditors\n', '\nLuisa I. Martínez-Merino\nEditors\n', '\nJuan Manuel Muñoz-Ocaña\nEditors\n', '\nAntonio M. Rodríguez-Chía\nEditors\n']",,,http://arxiv.org/abs/2002.08300v1,cs.OH,['cs.OH'],,,"['Editors', 'Editors', 'Editors', 'Editors', 'Editors']"
"Proceedings of the X International Workshop on Locational Analysis and
  Related Problems",http://arxiv.org/abs/2002.01702v1,2020-02-05T10:07:32Z,2020-02-05T10:07:32Z,"  The International Workshop on Locational Analysis and Related Problems will
take place during January 23-24, 2020 in Seville (Spain). It is organized by
the Spanish Location Network and the Location Group GELOCA from the Spanish
Society of Statistics and Operations Research(SEIO). The Spanish Location
Network is a group of more than 140 researchers from several Spanish
universities organized into 7 thematic groups. The Network has been funded by
the Spanish Government since 2003.
  One of the main activities of the Network is a yearly meeting aimed at
promoting the communication among its members and between them and other
researchers, and to contribute to the development of the location field and
related problems. The last meetings have taken place in C\'adiz (January
20-February 1, 2019), Segovia (September 27-29, 2017), M\'alaga (September
14-16, 2016), Barcelona (November 25-28, 2015), Sevilla (October 1-3, 2014),
Torremolinos (M\'alaga, June 19-21, 2013), Granada (May 10-12, 2012), Las
Palmas de Gran Canaria (February 2-5, 2011) and Sevilla (February 1-3, 2010).
  The topics of interest are location analysis and related problems. This
includes location models, networks, transportation, logistics, exact and
heuristic solution methods, and computational geometry, among others.
","['\nMaria Albareda-Sambola\nEditors\n', '\nMarta Baldomero-Naranjo\nEditors\n', '\nLuisa I. Martínez-Merino\nEditors\n', '\nDiego Ponce\nEditors\n', '\nMiguel A. Pozo\nEditors\n', '\nJusto Puerto\nEditors\n', '\nVictoria Rebillas-Loredo.\nEditors\n']",,,http://arxiv.org/abs/2002.01702v1,cs.OH,['cs.OH'],,,"['Editors', 'Editors', 'Editors', 'Editors', 'Editors', 'Editors', 'Editors']"
"The Separator, a Two-Phase Oil and Water Gravity CPS Separator Testbed",http://arxiv.org/abs/2002.00945v1,2020-02-01T19:27:46Z,2020-02-01T19:27:46Z,"  Industrial Control Systems (ICS) are evolving with advances in new
technology. The addition of wireless sensors and actuators and new control
techniques means that engineering practices from communication systems are
being integrated into those used for control systems. The two are engineered in
very different ways. Neither engineering approach is capable of accounting for
the subtle interactions and interdependence that occur when the two are
combined. This paper describes our first steps to bridge this gap, and push the
boundaries of both computer communication system and control system design. We
present The Separator testbed, a Cyber-Physical testbed enabling our search for
a suitable way to engineer systems that combine both computer networks and
control systems.
","['\nMichael Breza\n', '\nLaksh Bhatia\n', '\nIvana Tomic\n', '\nAnqi Fu\n', '\nWaqas Ikram\n', '\nValentinos Kongezos\n', '\nJulie A. McCann\n']","6 pages, 4 figures",,http://arxiv.org/abs/2002.00945v1,cs.OH,"['cs.OH', 'cs.SY', 'eess.SP', 'eess.SY']",,,[]
"Proposal of a standard of Knowledge Management and Technological
  Innovation for Mexico",http://arxiv.org/abs/2001.11379v1,2020-01-28T04:17:20Z,2020-01-28T04:17:20Z,"  The purpose of this work is to offer a methodology that allows to construct a
standard in Knowledge Management and Technological Innovation which may be used
in various organizations in M\'exico to improve the operation of their
resources and productivity. Based on the review of the existing literature, a
model is offered including several elements to enable organizations to
establish their position in relation to both concepts. The following proposal
is based on a systematic effort to understand and integrate models of Knowledge
Management and Innovation published in recent years as well as the results of
the experiences to propose standards of Knowledge Management and Technological
Innovation. In order to elaborate the proposal, factors and their associated
components have been analyzed through a review of the literature in order to
build and validate a standard proposal. To test the research study, a six-stage
research model has been constructed. For this purpose, an in-depth exploratory
research study has been carried out in a public sector organization, in an area
that allows the replicability of the model. The results have been analyzed to
construct and empirically validate the Mexican Standard of Knowledge Management
an Technological Innovation. Finally, after the statistical analysis, results
obtained from the application of the validated instrument are shown , which
supports the definition of the model.
",['\nJorge Romero-Hidalgo\n'],"in Spanish, Keywords: Knowledge management, technological innovation,
  standardization. arXiv admin note: substantial text overlap with
  arXiv:1609.02995 by other authors",,http://arxiv.org/abs/2001.11379v1,cs.OH,['cs.OH'],,,[]
"Integrating data science ethics into an undergraduate major: A case
  study",http://arxiv.org/abs/2001.07649v6,2020-01-21T17:01:08Z,2022-01-31T14:49:56Z,"  We present a programmatic approach to incorporating ethics into an
undergraduate major in statistical and data sciences. We discuss
departmental-level initiatives designed to meet the National Academy of
Sciences recommendation for integrating ethics into the curriculum from
top-to-bottom as our majors progress from our introductory courses to our
senior capstone course, as well as from side-to-side through co-curricular
programming. We also provide six examples of data science ethics modules used
in five different courses at our liberal arts college, each focusing on a
different ethical consideration. The modules are designed to be portable such
that they can be flexibly incorporated into existing courses at different
levels of instruction with minimal disruption to syllabi. We connect our
efforts to a growing body of literature on the teaching of data science ethics,
present assessments of our effectiveness, and conclude with next steps and
final thoughts.
","['\nBenjamin S. Baumer\n', '\nRandi L. Garcia\n', '\nAlbert Y. Kim\n', '\nKatherine M. Kinnaird\n', '\nMiles Q. Ott\n']",,,http://arxiv.org/abs/2001.07649v6,stat.OT,"['stat.OT', 'cs.OH', '00A05', 'K.7.4; K.3.2']",,,[]
"On the loss of learning capability inside an arrangement of neural
  networks",http://arxiv.org/abs/2001.11880v1,2020-01-09T05:09:22Z,2020-01-09T05:09:22Z,"  We analyze the loss of information and the loss of learning capability inside
an arrangement of neural networks. Our method is new and based on the
formulation of non-unitary Bogoliubov transformations in order to connect the
information between different points of the arrangement. This can be done after
expanding the activation function in a Fourier series and then assuming that
its information is stored inside a Quantum scalar field.
","['\nIvan Arraut\n', '\nDiana Diaz\n']","6 pages, work presented at Neurips 2019 inside the Workshop Machine
  Learning and the Physical Sciences","Symmetry 2020, 12, 1484",http://dx.doi.org/10.3390/sym12091484,cs.OH,['cs.OH'],10.3390/sym12091484,,[]
"Efficient Programmable Random Variate Generation Accelerator from Sensor
  Noise",http://arxiv.org/abs/2001.05400v2,2020-01-10T13:43:29Z,2020-04-23T11:31:20Z,"  We introduce a method for non-uniform random number generation based on
sampling a physical process in a controlled environment. We demonstrate one
proof-of-concept implementation of the method that reduces the error of Monte
Carlo integration of a univariate Gaussian by 1068 times while doubling the
speed of the Monte Carlo simulation. We show that the supply voltage and
temperature of the physical process must be controlled to prevent the mean and
standard deviation of the random number generator from drifting.
","['\nJames Timothy Meech\n', '\nPhillip Stanley-Marbell\n']","5 pages, 5 figures",,http://arxiv.org/abs/2001.05400v2,cs.OH,"['cs.OH', 'eess.SP', 'stat.ML']",,,[]
"An Approach Towards Intelligent Accident Detection, Location Tracking
  and Notification System",http://arxiv.org/abs/2001.00453v1,2019-12-29T14:15:18Z,2019-12-29T14:15:18Z,"  Advancement in transportation system has boosted speed of our lives.
Meantime, road traffic accident is a major global health issue resulting huge
loss of lives, properties and valuable time. It is considered as one of the
reasons of highest rate of death nowadays. Accident creates catastrophic
situation for victims, especially accident occurs in highways imposes great
adverse impact on large numbers of victims. In this paper, we develop an
intelligent accident detection, location tracking and notification system that
detects an accident immediately when it takes place. Global Positioning System
(GPS) device finds the exact location of accident. Global System for Mobile
(GSM) module sends a notification message including the link of location in the
google map to the nearest police control room and hospital so that they can
visit the link, find out the shortest route of the accident spot and take
initiatives to speed up the rescue process.
","['\nSupriya Sarker\n', '\nMd. Sajedur Rahman\n', '\nMohammad Nazmus Sakib\n']","The 3rd IEEE International Conference on Telecommunications and
  Photonics (ICTP) 2019",,http://arxiv.org/abs/2001.00453v1,cs.OH,"['cs.OH', 'cs.HC', 'B.1']",,,[]
"Delineation of the flow and mixing induced by Rayleigh-Taylor
  instability through tracers",http://arxiv.org/abs/1912.13181v2,2019-12-31T05:40:34Z,2021-03-23T10:20:45Z,"  Rayleigh-Taylor-instability(RTI) induced flow and mixing are of great
importance in both nature and engineering scenarios. To capture the
underpinning physics, tracers are introduced to make a supplement to discrete
Boltzmann simulation of RTI in compressible flows. Via marking two types of
tracers with different colors, the tracer distribution provides a clear
boundary of two fluids during the RTI evolution. Fine structures of the flow
and thermodynamic nonequilibrium behavior around the interface in a miscible
two-fluid system are delineated. Distribution of tracers in its velocity phase
space makes a charming pattern showing quite dense information on the flow
behavior, which opens a new perspective for analyzing and accessing
significantly deep insights into the flow system. RTI mixing is further
investigated via tracer defined local mixedness. The appearance of
Kelvin-Helmholtz instability is quantitatively captured by mixedness averaged
align the direction of the pressure gradient. The role of compressibility and
viscosity on mixing are investigated separately, both of which show two-stage
effect. The underlying mechanism of the two-stage effect is interpreted as the
development of large structures at the initial stage and the generation of
small structures at the late stage. At the late stage, for a fixed time, a
saturation phenomenon of viscosity is found that further increase of viscosity
cannot see an evident decline in mixedness. The mixing statues of heavy and
light fluids are not synchronous and the mixing of a RTI system is
heterogenous. The results are helpful for understanding the mechanism of flow
and mixing induced by RTI.
","['\nGe Zhang\n', '\nAiguo Xu\n', '\nDejia Zhang\n', '\nYingjun Li\n', '\nHuilin Lai\n', '\nXiaomian Hu\n']",,"Phys. Fluids 33, 076105 (2021)",http://dx.doi.org/10.1063/5.0051154,physics.flu-dyn,"['physics.flu-dyn', 'cond-mat.stat-mech', 'cs.OH', 'physics.app-ph', 'physics.comp-ph']",10.1063/5.0051154,,[]
A Promise Theoretic Account of the Boeing 737 Max MCAS Algorithm Affair,http://arxiv.org/abs/2001.01543v1,2019-12-24T10:48:57Z,2019-12-24T10:48:57Z,"  Many public controversies involve the assessment of statements about which we
have imperfect information. Without a structured approach, it is quite
difficult to develop an approach to reasoning which is not based on ad hoc
choices. Forms of logic have been used in the past to try to bring such
clarity, but these fail for a variety of reasons. We demonstrate a simple
approach to bringing a standardized approach to semantics, in certain
discourse, using Promise Theory. As a case, we use Promise Theory (PT) to
collect and structure publicly available information about the case of the MCAS
software component for the Boeing 737 Max flight control system.
","['\nJ. A. Bergstra\n', '\nM. Burgess\n']",,,http://arxiv.org/abs/2001.01543v1,cs.OH,"['cs.OH', 'D.2.m; K.4.0; A.1']",,,[]
The Epistemic Landscape: a Computability Perspective,http://arxiv.org/abs/2001.10800v1,2019-12-27T15:21:11Z,2019-12-27T15:21:11Z,"  By nature, transmissible human knowledge is enumerable: every sentence,
movie, audio record can be encoded in a sufficiently long string of 0's and
1's. The works of G\""odel, Turing and others showed that there are inherent
limits and properties associated with the fact that language technology is
enumerable. G\""odel's numbering technique is universal for enumerable
structures and shows strong limits of the language technology. Computability
theory is a particular example: programs can be numbered and all sorts of
limits can be studied from there. Computability is also at the heart of science
since any experimental validation of a theory supposes that theoretical results
have been computed, then checked against concrete experiments. It implies that
limitations on what is computable ultimately are also limits of what we
understand as ""scientific theory"", and more generally to all the transmissible
knowledge. We argue that it is fruitful to look a epistemology from a
computability perspective. We show that it allows to precisely define different
kinds of knowledge acquisition techniques, and helps the study of how they are
related to one another.
",['\nFrédéric Prost\n'],,,http://arxiv.org/abs/2001.10800v1,cs.OH,['cs.OH'],,,[]
"Estudo comparativo de meta-heurísticas para problemas de
  colorações de grafos",http://arxiv.org/abs/1912.11533v1,2019-12-18T22:46:23Z,2019-12-18T22:46:23Z,"  A classic graph coloring problem is to assign colors to vertices of any graph
so that distinct colors are assigned to adjacent vertices. Optimal graph
coloring colors a graph with a minimum number of colors, which is its chromatic
number. Finding out the chromatic number is a combinatorial optimization
problem proven to be computationally intractable, which implies that no
algorithm that computes large instances of the problem in a reasonable time is
known. For this reason, approximate methods and metaheuristics form a set of
techniques that do not guarantee optimality but obtain good solutions in a
reasonable time. This paper reports a comparative study of the Hill-Climbing,
Simulated Annealing, Tabu Search, and Iterated Local Search metaheuristics for
the classic graph coloring problem considering its time efficiency for
processing the DSJC125 and DSJC250 instances of the DIMACS benchmark.
",['\nFlávio José Mendes Coelho\n'],in Portuguese,,http://arxiv.org/abs/1912.11533v1,cs.OH,"['cs.OH', 'cs.AI']",,,[]
CAD Tool Design Space Exploration via Bayesian Optimization,http://arxiv.org/abs/1912.06460v1,2019-12-12T08:16:11Z,2019-12-12T08:16:11Z,"  The design complexity is increasing as the technology node keeps scaling
down. As a result, the electronic design automation (EDA) tools also become
more and more complex. There are lots of parameters involved in EDA tools,
which results in a huge design space. What's worse, the runtime cost of the EDA
flow also goes up as the complexity increases, thus exhaustive exploration is
prohibitive for modern designs. Therefore, an efficient design space
exploration methodology is of great importance in advanced designs. In this
paper we target at an automatic flow for reducing manual tuning efforts to
achieve high quality circuits synthesis outcomes. It is based on Bayesian
optimization which is a promising technique for optimizing black-box functions
that are expensive to evaluate. Gaussian process regression is leveraged as the
surrogate model in Bayesian optimization framework. In this work, we use 64-bit
prefix adder design as a case study. We demonstrate that the Bayesian
optimization is efficient and effective for performing design space exploration
on EDA tool parameters, which has great potential for accelerating the design
flow in advanced technology nodes.
","['\nYuzhe Ma\n', '\nZiyang Yu\n', '\nBei Yu\n']","6 pages, 5 figures",,http://arxiv.org/abs/1912.06460v1,cs.OH,['cs.OH'],,,[]
A Noxious Market for Personal Data,http://arxiv.org/abs/2001.00457v1,2019-12-12T05:05:06Z,2019-12-12T05:05:06Z,"  Many policymakers, academics and governments have advocated for exchangeable
property rights over information as it presents a market solution to what could
be considered a market failure. Particularly in jurisdictions such as Africa,
Asia or South America, where weaker legal protections and fleeting regulatory
enforcement leaves data subjects vulnerable or exploited regardless of the
outcome. We argue that whether we could achieve this personal data economy in
which individuals have ownership rights akin to property rights over their data
should be approached with caution as a solution to ensuring individuals have
agency over their data across different legal landscapes. We present an
objection to the use of property rights, a market solution, due to the
\textit{noxious} nature of personal data - which is founded on Satz and
Sandel's objection to markets.
","['\nAbdul Abdulrahim\n', '\nMichael Famoroti\n']","Presented at NeurIPS 2019 Workshop on Machine Learning for the
  Developing World",,http://arxiv.org/abs/2001.00457v1,cs.OH,['cs.OH'],,,[]
"High-Freedom Inverse Design with Deep Neural Network for Metasurface
  Filter in the Visible",http://arxiv.org/abs/1912.03696v1,2019-12-08T15:28:36Z,2019-12-08T15:28:36Z,"  In order to obtain a metasurface structure capable of filtering the light of
a specific wavelength in the visible band, traditional method usually traverses
the space consisting of possible designs, searching for a potentially
satisfying device by performing iterative calculations to solve Maxwell's
equations. In this paper, we propose a neural network that can complete an
inverse design process to solve the problem. Compared with the traditional
method, our method is much faster while competent of generating better devices
with the desired spectrum. One of the most significant advantages is that it
can handle a real spectrum as well as an artificial one. Besides, our method
encompasses a high degree of freedom to generate devices, ensuring their
generated spectra resemble desired ones and meeting the accuracy requirements
without losing practicability in the manufacturing process.
","['\nXiao Han\n', '\nZiyang Fan\n', '\nChao Li\n', '\nZeyang Liu\n', '\nL. Jay Guo\n']",,,http://arxiv.org/abs/1912.03696v1,cs.OH,"['cs.OH', 'eess.IV']",,,[]
"Non-linearity identification for construction workers'
  personality-safety behaviour predictive relationship using neural network and
  linear regression modelling",http://arxiv.org/abs/1912.05944v3,2019-12-11T07:51:56Z,2020-08-26T13:05:32Z,"  The prediction of workers' safety behaviour can help identify vulnerable
workers who intend to undertake unsafe behaviours and be useful in the design
of management practices to minimise the occurrence of accidents. The latest
literature has evidenced that there is within-population diversity that leads
people's intended safety behaviours in the workplace, which are found to vary
among individuals as a function of their personality traits. In this study, an
innovative forecasting model, which employs neural network algorithms, is
developed to numerically simulate the predictive relationship between
construction workers' personality traits and their intended safety behaviour.
The data-driven nature of neural network enabled a reliable estimate of the
relationship, which allowed this research to find that a nonlinear effect
exists in the relationship. This research has practical implications. The
neural network developed is shown to have highly satisfactory prediction
accuracy and is thereby potentially useful for assisting project
decision-makers to assess how prone workers are to carry out unsafe behaviours
in the workplace.
","['\nYifan Gao\n', '\nVicente A. Gonzalez\n', '\nTak Wing Yiu\n', '\nGuillermo Cabrera-Guerrerod\n']","The manuscript is currently undergoing a major revision as some
  contents in its current form are not scientifically rigorous and can be
  misleading to potential readers. Thus, we apply for withdrawal of the
  manuscript",,http://arxiv.org/abs/1912.05944v3,cs.OH,"['cs.OH', 'cs.LG', 'stat.ML']",,,[]
Nonintrusive Load Monitoring for Machines used in Manufacturing,http://arxiv.org/abs/1912.01500v1,2019-12-01T12:34:19Z,2019-12-01T12:34:19Z,"  In order to increase the electric energy efficiency of production machines,
it is necessary to determine the energy demand of the constituent electric
loads. Therefore, a new measurement system based on nonintrusive load
monitoring is proposed in this paper. It only measures the voltage and current
of the aggregate load and then uses automatic disaggregation methods to
estimate the energy demand of the constituent loads. In two case studies, the
energy demand of most loads could be determined with an accuracy of 85~\% or
more in this way.
",['\nChristian Gebbe\n'],"Keywords: Energy monitoring; energy efficiency; nonintrusive load
  monitoring; disaggregation; measurement method;",,http://arxiv.org/abs/1912.01500v1,cs.OH,"['cs.OH', 'eess.SP']",,,[]
Business Process Variant Analysis: Survey and Classification,http://arxiv.org/abs/1911.07582v2,2019-11-18T12:21:54Z,2019-12-23T02:16:53Z,"  Process variant analysis aims at identifying and addressing the differences
existing in a set of process executions enacted by the same process model. A
process model can be executed differently in different situations for various
reasons, e.g., the process could run in different locations or seasons, which
gives rise to different behaviors. Having intuitions about the discrepancies in
process behaviors, though challenging, is beneficial for managers and process
analysts since they can improve their process models efficiently, e.g., via
interactive learning or adapting mechanisms. Several methods have been proposed
to tackle the problem of uncovering discrepancies in process executions.
However, because of the interdisciplinary nature of the challenge, the methods
and sorts of analysis in the literature are very heterogeneous. This article
not only presents a systematic literature review and taxonomy of methods for
variant analysis of business processes but also provides a methodology
including the required steps to apply this type of analysis for the
identification of variants in business process executions.
","['\nFarbod Taymouri\n', '\nMarcello La Rosa\n', '\nMarlon Dumas\n', '\nFabrizio Maria Maggi\n']",,,http://arxiv.org/abs/1911.07582v2,cs.OH,['cs.OH'],,,[]
Inflationary Constant Factors and Why Python is Faster Than C++,http://arxiv.org/abs/1911.12338v2,2019-11-23T20:29:49Z,2020-10-08T10:59:31Z,"  Constant-factor differences are frequently ignored when analyzing the
complexity of algorithms and implementations, as they appear to be
insignificant in practice. In this paper, we demonstrate that this assumption
can in fact have far more profound implications on time complexity than is
obvious at first glance, and that a poor consideration of trade-offs can result
in polynomially slower algorithms whose roots can be deeply and fundamentally
ingrained into a programming language itself. While the general observation may
not be novel from a theoretical standpoint, it is rarely (if ever) presented in
traditional computer science curricula or other settings, and appears to be far
from common knowledge in practical software engineering. We thus hope bring
awareness to this issue and urge careful consideration of significant
trade-offs that can result from trivial decisions made while programming.
",['\nMehrdad Niknami\n'],,,http://arxiv.org/abs/1911.12338v2,cs.OH,['cs.OH'],,,[]
Context Adaptivity as Enabler for Meaningful Pervasive Advertising,http://arxiv.org/abs/1912.01490v1,2019-11-17T20:31:10Z,2019-11-17T20:31:10Z,"  Socio-demographic user profiles are currently regarded as the most convenient
base for successful personalized advertising. However, signs point to the
dormant power of context recognition. While technologies that can sense the
environment are increasingly advanced, questions such as what to sense and how
to adapt to a consumer's context are largely unanswered. Research in the field
is scattered and frequently prototype-driven. What the community lacks is a
thorough methodology to provide the basis for any context-adaptive system:
conceptualizing context. This position paper describes our current research of
conceptualizing context for pervasive advertising. It summarizes findings from
literature analysis and proposes a methodology for context conceptualization,
which is currently work-in-progress.
",['\nChristine Bauer\n'],"9 pages, 2 figures, 4th Workshop on Pervasive Advertising, in
  conjunction with Pervasive 2011, San Francisco, CA, USA, 12 June",,http://arxiv.org/abs/1912.01490v1,cs.OH,['cs.OH'],,,[]
Improvements of the REDCRAFT Software Package,http://arxiv.org/abs/1911.08612v1,2019-11-19T22:18:24Z,2019-11-19T22:18:24Z,"  Traditional approaches to elucidation of protein structures by NMR
spectroscopy rely on distance restraints also known as nuclear Overhauser
effects (NOEs). The use of NOEs as the primary source of structure
determination by NMR spectroscopy is time consuming and expensive. Residual
Dipolar Couplings (RDCs) have become an alternate approach for structure
calculation by NMR spectroscopy. In previous works, the software package
REDCRAFT has been presented as a means of harnessing the information containing
in RDCs for structure calculation of proteins. In this work, we present
significant improvements to the REDCRAFT package including: refinement of the
decimation procedure, the inclusion of graphical user interface, adoption of
NEF standards, and addition of scripts for enhanced protein modeling options.
The improvements to REDCRAFT have resulted in the ability to fold proteins that
the previous versions were unable to fold. For instance, we report the results
of folding of the protein 1A1Z in the presence of highly erroneous data.
","['\nCasey A Cole\n', '\nCaleb Parks\n', '\nJulian Rachele\n', '\nHomayoun Valafar\n']","7 pages, 5 figures, Int'l Conf. Bioinformatics and Computational
  Biology (BIOCOMP'19), Las Vegas, NV, August 2019",,http://arxiv.org/abs/1911.08612v1,q-bio.BM,"['q-bio.BM', 'cs.OH']",,,[]
"Cybernetical Concepts for Cellular Automaton and Artificial Neural
  Network Modelling and Implementation",http://arxiv.org/abs/2001.02037v3,2019-11-24T21:02:34Z,2020-08-31T21:41:04Z,"  As a discipline cybernetics has a long and rich history. In its first
generation it not only had a worldwide span, in the area of computer modelling,
for example, its proponents such as John von Neumann, Stanislaw Ulam, Warren
McCulloch and Walter Pitts, also came up with models and methods such as
cellular automata and artificial neural networks, which are still the
foundation of most modern modelling approaches. At the same time, cybernetics
also got the attention of philosophers, such as the Frenchman Gilbert Simondon,
who made use of cybernetical concepts in order to establish a metaphysics and a
natural philosophy of individuation, giving cybernetics thereby a philosophical
interpretation, which he baptised allagmatic. In this paper, we emphasise this
allagmatic theory by showing how Simondon's philosophical concepts can be used
to formulate a generic computer model or metamodel for complex systems
modelling and its implementation in program code, according to generic
programming. We also present how the developed allagmatic metamodel is capable
of building simple cellular automata and artificial neural networks.
","['\nPatrik Christen\n', '\nOlivier Del Fabbro\n']","12 pages, 1 figure","2019 IEEE International Conference on Systems, Man and Cybernetics
  (SMC), 4124-4130, 2019",http://dx.doi.org/10.1109/SMC.2019.8913839,cs.OH,"['cs.OH', 'cs.AI', 'cs.NE']",10.1109/SMC.2019.8913839,,[]
Value-Added Chemical Discovery Using Reinforcement Learning,http://arxiv.org/abs/1911.07630v1,2019-11-10T07:36:37Z,2019-11-10T07:36:37Z,"  Computer-assisted synthesis planning aims to help chemists find better
reaction pathways faster. Finding viable and short pathways from sugar
molecules to value-added chemicals can be modeled as a retrosynthesis planning
problem with a catalyst allowed. This is a crucial step in efficient biomass
conversion. The traditional computational chemistry approach to identifying
possible reaction pathways involves computing the reaction energies of hundreds
of intermediates, which is a critical bottleneck in silico reaction discovery.
Deep reinforcement learning has shown in other domains that a well-trained
agent with little or no prior human knowledge can surpass human performance.
While some effort has been made to adapt machine learning techniques to the
retrosynthesis planning problem, value-added chemical discovery presents unique
challenges. Specifically, the reaction can occur in several different sites in
a molecule, a subtle case that has never been treated in previous works. With a
more versatile formulation of the problem as a Markov decision process, we
address the problem using deep reinforcement learning techniques and present
promising preliminary results.
","['\nPeihong Jiang\n', '\nHieu Doan\n', '\nSandeep Madireddy\n', '\nRajeev Surendran Assary\n', '\nPrasanna Balaprakash\n']",,,http://arxiv.org/abs/1911.07630v1,cs.OH,"['cs.OH', 'cs.LG', 'stat.ML']",,,[]
"Fast Safety Assessment and Correction Framework for Maintenance Work
  Zones",http://arxiv.org/abs/1911.01179v1,2019-11-01T03:05:52Z,2019-11-01T03:05:52Z,"  A framework is proposed to assess the safety of maintenance work zones in a
timely manner, show whether there are safety hazards, whether adjustments need
to be made and how to adjust it. By means of advanced data acquisition
technologies such as multi video detection and portable device based
naturalistic driving, the microscopic vehicle behaviour data can be collected.
Based on this data, a method for expressing and displaying the distribution of
unsafe vehicle behaviour is used to show whether safety hazards exist. Using
Vissim, the impacts of the length and speed limit of the warning area, the
length and type of the upstream transition area and the length of the work area
of the maintenance work zone on the distribution of unsafe vehicle behaviour
are simulated to establish the safety correction matrix, which can tell
maintenance departments the direction of adjustment when safety hazards exist
in maintenance work zones.
","['\nZhepu Xu\n', '\nQun Yang\n']",,,http://arxiv.org/abs/1911.01179v1,cs.OH,['cs.OH'],,,[]
"Gene expression and pathway bioinformatics analysis detect a potential
  predictive value of MAP3K8 in thyroid cancer progression",http://arxiv.org/abs/1910.12055v1,2019-10-26T12:42:06Z,2019-10-26T12:42:06Z,"  Thyroid cancer is the commonest endocrine malignancy. Mutation in the BRAF
serine/threonine kinase is the most frequent genetic alteration in thyroid
cancer. Target therapy for advanced and poorly differentiated thyroid
carcinomas include BRAF pathway inhibitors. Here, we evaluated the role of
MAP3K8 expression as a potential driver of resistance to BRAF inhibition in
thyroid cancer. By analyzing Gene Expression Omnibus data repository, across
all thyroid cancer histotypes, we found that MAP3K8 is up-regulated in poorly
differentiated thyroid carcinomas and its expression is related to a stem cell
like phenotype and a poorer prognosis and survival. Taken together these data
unravel a novel mechanism for thyroid cancer progression and chemo-resistance
and confirm previous results obtained in cultured thyroid cancer stem cells
","['\nValentina Di Salvatore\n', '\nFiorenza Gianì\n', '\nGiulia Russo\n', '\nMarzio Pennisi\n', '\nPasqualino Malandrino\n', '\nFrancesco Frasca\n', '\nFrancesco Pappalardo\n']",5 pages,,http://arxiv.org/abs/1910.12055v1,q-bio.MN,"['q-bio.MN', 'cs.OH']",,,[]
"Smart Monitoring: remote-monitoring technology of power, gas, and water
  consumption in Smart Cities",http://arxiv.org/abs/1910.08759v1,2019-10-19T12:01:52Z,2019-10-19T12:01:52Z,"  This paper describes the remote-collection technology of detailed data (Smart
Monitoring) on the consumption and quality of energy resources in public
services. In this article, under ""energy resources"" (hereinafter referred to as
resources) we outline electrical power, water (hot and cold), heat, and gas.
Data on resource quality refer to the parameters characterizing the consumed
resource. We also present an option of the data-acquisition system structure
based on Smart Monitoring technology. Particular attention is paid to security
in the system and the centralized management of its elements. The data flow in
such system carries information about the behavior of energy consumers and the
household equipment they use. Data on energy consumption for billing purposes
in such a system is just one of many kinds, and not the most important feature.
The development of Smart Monitoring technology is aimed at developing the
market of IT services and mass services based on analysis of collected detailed
data on energy-resource consumption.
","['\nSergey Surnov\n', '\nIgor Bychkovskiy\n', '\nGrigory Surnov\n', '\nSergey Krasnov\n']","7 pages, 3 figures",,http://arxiv.org/abs/1910.08759v1,cs.OH,['cs.OH'],,,[]
Priority Quality Attributes for Engineering AI-enabled Systems,http://arxiv.org/abs/1911.02912v1,2019-10-15T14:33:12Z,2019-10-15T14:33:12Z,"  Deploying successful software-reliant systems that address their mission
goals and user needs within cost, resource, and expected quality constraints
require design trade-offs. These trade-offs dictate how systems are structured
and how they behave and consequently can effectively be evolved and sustained.
Software engineering practices address this challenge by centering system
design and evolution around delivering key quality attributes, such as
security, privacy, data centricity, sustainability, and explainability. These
concerns are more urgent requirements for software-reliant systems that also
include AI components due to the uncertainty introduced by data elements.
Moreover, systems employed by the public sector exhibit unique design time and
runtime challenges due to the regulatory nature of the domains. We assert that
the quality attributes of security, privacy, data centricity, sustainability,
and explainability pose new challenges to AI engineering and will drive the
success of AI-enabled systems in the public sector. In this position paper, we
enumerate with examples from healthcare domain concerns related to these
requirements to mitigate barriers to architecting and fielding AI-enabled
systems in the public sector.
","['\nLena Pons\n', '\nIpek Ozkaya\n']","Presented at AAAI FSS-19: Artificial Intelligence in Government and
  Public Sector, Arlington, Virginia, USA",,http://arxiv.org/abs/1911.02912v1,cs.OH,['cs.OH'],,,[]
"Data-driven charging strategies for grid-beneficial, customer-oriented
  and battery-preserving electric mobility",http://arxiv.org/abs/1910.07503v1,2019-10-14T08:10:22Z,2019-10-14T08:10:22Z,"  Electric Vehicle (EV) penetration and renewable energies enables synergies
between energy supply, vehicle users, and the mobility sector. However, also
new issues arise for car manufacturers: During charging and discharging of EV
batteries a degradation (battery aging) occurs that correlates with a value
depreciation of the entire EV. As EV users' satisfaction depends on reliable
and value-stable products, car manufacturers offer charging assistants for
simplified and sustainable EV usage by considering individual customer needs
and battery aging. Hitherto models to quantify battery aging have limited
practicability due to a complex execution. Data-driven methods hold feasible
alternatives for SOH estimation. However, the existing approaches barely use
user-related data. By means of a linear and a neural network regression model,
we first estimate the energy consumption for driving considering individual
driving styles and environmental conditions. In following work, the consumption
model trained on data from batteries without degradation can be used to
estimate the energy consumption for EVs with aged batteries. A discrepancy
between the estimation and the real consumption indicates a battery aging
caused by increased internal losses. We then target to evaluate the influence
of charging strategies on battery degradation.
","['\nKarl Schwenk\n', '\nTim Harr\n', '\nRené Großmann\n', '\nRiccardo Remo Appino\n', '\nVeit Hagenmeyer\n', '\nRalf Mikut\n']",,,http://arxiv.org/abs/1910.07503v1,cs.OH,"['cs.OH', 'cs.SY', 'eess.SP', 'eess.SY']",,,[]
"Benchmark Dataset for Timetable Optimization of Bus Routes in the City
  of New Delhi",http://arxiv.org/abs/1910.08903v1,2019-10-20T05:22:47Z,2019-10-20T05:22:47Z,"  Public transport is one of the major forms of transportation in the world.
This makes it vital to ensure that public transport is efficient. This research
presents a novel real-time GPS bus transit data for over 500 routes of buses
operating in New Delhi. The data can be used for modeling various timetable
optimization tasks as well as in other domains such as traffic management,
travel time estimation, etc. The paper also presents an approach to reduce the
waiting time of Delhi buses by analyzing the traffic behavior and proposing a
timetable. This algorithm serves as a benchmark for the dataset. The algorithm
uses a constrained clustering algorithm for classification of trips. It further
analyses the data statistically to provide a timetable which is efficient in
learning the inter- and intra-month variations.
","['\nAnubhav Jain\n', '\nAvdesh Kumar\n', '\nSaumya Balodi\n', '\nPravesh Biyani\n']",,,http://arxiv.org/abs/1910.08903v1,cs.OH,"['cs.OH', 'cs.LG', 'stat.ML']",,,[]
"A Survey of Benchmarks to Evaluate Data Analytics for Smart-*
  Applications",http://arxiv.org/abs/1910.02004v1,2019-10-04T15:56:14Z,2019-10-04T15:56:14Z,"  The growth of ubiquitous sensor networks at an accelerating pace cuts across
many areas of modern day life. They enable measuring, inferring, understanding
and acting upon a wide variety of indicators, in fields ranging from
agriculture to healthcare or to complex urban environments. The applications
devoted to this task are designated as Smart-* Applications. They hide a
staggering complexity, relying on multiple layers of data collection,
transmission, aggregation, analysis and also storage, both at the network edge
and on the cloud. Furthermore, Smart-* Applications raise additional specific
challenges, such as the need to process and extract knowledge from diverse
data, which is flowing at high velocity in near real-time or in the heavily
distributed environment they rely on. How to assess the performance of such a
complex stack, when faced with the specifics of \mbox{Smart-*} Applications,
remains an open research question. In this article, the key specific
characteristics and requirements of Smart-* Applications are initially
detailed. Afterwards, for each of these requirements, there is a description of
the benchmarks one can use to precisely evaluate the performance of the
underlying systems and technologies. Finally, an identification of future
research directions related to identified open issues for benchmarking Smart-*
Applications is performed.
","['\nAthanasios Kiatipis\n', '\nAlvaro Brandon\n', '\nRizkallah Touma\n', '\nPierre Matri\n', '\nMichal Zasadzinski\n', '\nLinh Thuy Nhuyen\n', '\nAdrien Lebre\n', '\nAlexandru Costan\n']",,,http://arxiv.org/abs/1910.02004v1,cs.OH,['cs.OH'],,,[]
Simulation Reproducibility of a Chaotic Circuit,http://arxiv.org/abs/1910.04551v1,2019-10-08T19:00:51Z,2019-10-08T19:00:51Z,"  An evergreen scientific feature is the ability for scientific works to be
reproduced. This feature allows researchers to understand, enhance, or even
question works that have been developed by other scientists. In control theory
the importance of modeling and simulation of systems is widely recognized.
Despite this recognition, less attention is paid to the effects of finite
precision of computers on the simulation reproducibility of nonlinear dynamic
systems. In this work, a case study of reproducibility is presented in the
simulation of a chaotic Jerk circuit, using the software LtSpice. In order to
do so, we performed simulations of the circuit in the same version of the
software on different computers, in order to collect the data and compare them
with experimental results. The comparison was made with the NRMSE (Normalized
Root Mean Square Error), in order to identify the computer with the highest
prediction horizon. Tests performed in 4 different configurations showed the
difficulties of simulation reproducibility in LtSpice. The methodology
developed was efficient in identifying the computer with better performance,
which allows applying it to other cases in the literature.
","['\nT. E. Nazare\n', '\nE. G. Nepomuceno\n']","SBAI 2019 - Simposio Brasileiro de Automacao Inteligente - Ouro
  Preto. 6 pages. In Portuguese",,http://arxiv.org/abs/1910.04551v1,cs.OH,['cs.OH'],,,[]
Scalability of TTool's AMS extensions: a case study,http://arxiv.org/abs/1910.06091v1,2019-10-11T12:33:15Z,2019-10-11T12:33:15Z,"  Embedded cyber-physical systems (CPS) are commonly built upon heterogeneous
digital and analog integrated circuits, including sensors and actuators. Less
common is their deployment on parallel, NoC based designs based on general
purpose processor cores of a Multi-processor System-on-chip (MPSoC).
Application code has to be run on the MPSoC for the digital part, and interact
with the analog sensors. We recently proposed a major extension to the design
and exploration tool named TTool, now allowing the design of CPS on a high
level of abstraction and the generation of cycle-bit accurate simulations. We
explore the scalability of our approach with an automotive case study.
",['\nDaniela Genius\n'],"International Workshop on Reconfigurable and Communication-centric
  Cyber- Physical Systems ReCoCyPS 2019 (arXiv:1909.05617)",,http://arxiv.org/abs/1910.06091v1,cs.OH,['cs.OH'],,,[]
A note on 'Collaborative hub location problem under cost uncertainty',http://arxiv.org/abs/1910.02044v1,2019-10-04T17:17:04Z,2019-10-04T17:17:04Z,"  Three models were presented in M.K. Khakim Habibi, Hamid Allaoui, Gilles
Goncalves, Collaborative hub location problem under cost uncertainty, Computers
& Industrial Engineering Volume 124, October 2018, Pages 393-410 as models for
collaborative Capacitated Multiple Allocation Hub Location Problem. In this
note, we point out a few flaws in modeling. In particular, we elaborate and
explain that none of the those models incorporates any element of a
collaborative activity.
",['\nShahin Gelareh\n'],,,http://arxiv.org/abs/1910.02044v1,math.OC,"['math.OC', 'cs.OH']",,,[]
The Open Porous Media Flow Reservoir Simulator,http://arxiv.org/abs/1910.06059v1,2019-10-04T09:31:52Z,2019-10-04T09:31:52Z,"  The Open Porous Media (OPM) initiative is a community effort that encourages
open innovation and reproducible research for simulation of porous media
processes. OPM coordinates collaborative software development, maintains and
distributes open-source software and open data sets, and seeks to ensure that
these are available under a free license in a long-term perspective.
  In this paper, we present OPM Flow, which is a reservoir simulator developed
for industrial use, as well as some of the individual components used to make
OPM Flow. The descriptions apply to the 2019.10 release of OPM.
","['\nAtgeirr Flø Rasmussen\n', '\nTor Harald Sandve\n', '\nKai Bao\n', '\nAndreas Lauser\n', '\nJoakim Hove\n', '\nBård Skaflestad\n', '\nRobert Klöfkorn\n', '\nMarkus Blatt\n', '\nAlf Birger Rustad\n', '\nOve Sævareid\n', '\nKnut-Andreas Lie\n', '\nAndreas Thune\n']","43 pages, 22 figures",,http://arxiv.org/abs/1910.06059v1,cs.OH,"['cs.OH', 'physics.comp-ph', '76S05, 68N01, 97N80']",,,[]
GeoSES -- um Índice Socioeconômico para Estudos de Saúde no Brasil,http://arxiv.org/abs/1910.06155v1,2019-10-09T21:49:09Z,2019-10-09T21:49:09Z,"  Objective: to define an index that summarizes the main dimensions of the
socioeconomic context for research purposes, evaluation and monitoring health
inequalities. Methods: the index was created from the 2010 Brazilian
Demographic Census, whose variables selection was guided by theoretical
references for health studies, including seven socioeconomic dimensions:
education, mobility, poverty, wealth, income, segregation and deprivation of
resources and services. The index was developed using principal component
analysis, and was evaluated for its construct, content and applicability
components. Results: GeoSES-BR dimensions showed good association with HDI-M
(above 0.85). The model with the poverty dimension best explained the relative
risk of avoidable cause mortality in Brazil. In the intraurban scale, the model
with GeoSES-IM was the one that best explained the relative risk of mortality
from circulatory system diseases. Conclusion: GeoSES showed significant
explanatory potential in the studied scales.
","['\nLigia Vizeu Barrozo\n', '\nMichel Fornaciali\n', '\nCarmen Diva Saldiva de André\n', '\nGuilherme Augusto Zimeo Morais\n', '\nGiselle Mansur\n', '\nWilliam Cabral-Miranda\n', '\nJoão Ricardo Sato\n', '\nEdson Amaro Júnior\n']",in Portuguese,,http://arxiv.org/abs/1910.06155v1,cs.OH,"['cs.OH', 'stat.AP']",,,[]
Overview of Fault Tolerant Techniques in Underwater Sensor Networks,http://arxiv.org/abs/1910.00889v1,2019-10-02T11:49:26Z,2019-10-02T11:49:26Z,"  Sensor networks provide services to a broad range of applications ranging
from intelligence service surveillance to weather forecasting. Most of the
sensor networks are terrestrial, however much of our planet is covered by water
and Underwater Sensor Networks (USN) are an emerging research area. One of the
unavoidable increasing challenge for modern technology is tolerating faults -
accepting that hardware is imperfect and cope with it. Fault tolerance may have
more impact underwater than in terrestrial environment as terrestrial
environment is more forgiving, reaching the malfunctioning devices for
replacement underwater is harder and may be more costly. Current paper is the
first to investigate fault tolerance, particularly cross layer fault tolerance,
in USN-s.
","['\nLauri Vihman\n', '\nMaarja Kruusmaa\n', '\nJaan Raik\n']",,,http://arxiv.org/abs/1910.00889v1,eess.SY,"['eess.SY', 'cs.OH', 'cs.SY']",,,[]
Stochastic model of business process decomposition,http://arxiv.org/abs/1909.09954v1,2019-09-22T07:30:35Z,2019-09-22T07:30:35Z,"  Decomposition is the basis of works dedicated to business process modelling
at the stage of information and management systems analysis and design. The
article shows that the business process decomposition can be represented as a
Galton Watson branching stochastic process. This representation allows
estimating the decomposition tree depth and the total amount of its elements,
as well as explaining the empirical requirement for the business function
decomposition (not more than 7 elements). The problem is deemed relevant as the
obtained results allow objectively estimating the labor input in business
process modelling.
",['\nGrigory Tsiperman\n'],Published in two languages (English and Russian),,http://arxiv.org/abs/1909.09954v1,cs.OH,['cs.OH'],,,[]
Exact Calculation of Expected Values for Splitting Pairs in Blackjack,http://arxiv.org/abs/1909.13710v1,2019-09-22T23:13:03Z,2019-09-22T23:13:03Z,"  Computer calculations for most exact expected values in blackjack have been
available since the 1960's, but exact results for pair splitting and
resplitting have previously been too computer intensive. This paper describes a
new algorithm for exact pair-splitting. By using dealer probability caching
methods and revising the method for recursively generating possible player
hands, the estimated calculation time compared to standard methods was reduced
by five orders of magnitude. The resulting algorithm was used to calculate the
first exact and complete pair splitting results for a single deck game. The
exact results were compared to prior approximate theories for resplitting. The
prior theories are accurate for many calculations, but inaccurate for
resplitting tens. A new approximation method was developed that is accurate for
all resplitting calculations.
",['\nJohn A. Nairn\n'],"23 pages, 3 figures, 4 tables (two of them sideways, full page
  tables)",,http://arxiv.org/abs/1909.13710v1,cs.OH,['cs.OH'],,,[]
Information collection for fraud detection in P2P financial market,http://arxiv.org/abs/1910.02009v1,2019-09-24T09:01:26Z,2019-09-24T09:01:26Z,"  Fintech companies have been facing challenges from fraudulent behavior for a
long time. Fraud rate in Chinese P2P financial market could go as high as 10%.
It is crucial to collect sufficient information of the user as input to the
anti-fraud process. Data collection framework for Fintech companies are
different fro m conventional internet firms. With individual-based crawling
request , we need to deal with new challenges negligible elsewhere . In this
paper , we give an outline of how we collect data from the web to facilitate
our anti-fraud process. We also overview the challenges and solutions to our
problems. Our team at HC Financial Service Group is one of the few companies
that are capable of developing full-fledged crawlers on our own.
","['\nHao Wang\n', '\nZonghu Wang\n', '\nBin Zhang\n', '\nJun Zhou\n']",,,http://arxiv.org/abs/1910.02009v1,cs.OH,['cs.OH'],,,[]
"A Self-Healing Hardware Architecture for Safety-Critical Digital
  Embedded Devices",http://arxiv.org/abs/1910.00064v1,2019-09-30T19:32:17Z,2019-09-30T19:32:17Z,"  Digital Embedded Devices of next-generation safety-critical industrial
automation systems require high levels of survivability and resilience against
the hardware and software failure. One of the concepts for achieving this
requirement is the design of resilient and survivable digital embedded systems.
In the last two decades, development of self-healing digital systems based on
molecular and cellular biology have received attention for the design of robust
digital systems. However, many of these approaches have not been architected
from the outset with safety in mind, nor have they been targeted for the
applications of automation community where a significant need exists. This
paper presents a new self-healing hardware architecture, inspired from the way
nature responds, defends and heals: the stem cells in the immune system of
living organisms, the life cycle of the living cell, and the pathway from
Deoxyribonucleic acid (DNA) to protein. The proposed architecture is
integrating cellular-based biological concepts, traditional fault tolerance
techniques, and operational schematics for the international standard IEC
61131-3 to facilitate adoption in the automation industry and safety-critical
applications. To date, two industrial applications have been mapped on the
proposed architecture, which are capable of tolerating a significant number of
faults that can stem from harsh environmental changes and external disturbances
and we believe the nexus of its concepts can positively impact the next
generation of critical systems in the automation industry
",['\nShawkat Sabah Khairullah\n'],"7 pages, 4 figures, 2 tables","International Journal of Innovative Research in Computer and
  Communication Engineering 2019",http://arxiv.org/abs/1910.00064v1,cs.OH,"['cs.OH', 'eess.SP']",,,[]
Detection of fraudulent users in P2P financial market,http://arxiv.org/abs/1910.02010v1,2019-09-24T07:58:04Z,2019-09-24T07:58:04Z,"  Financial fraud detection is one of the core technological assets of Fintech
companies. It saves tens of millions of money fro m Chinese Fintech companies
since the bad loan rate is more than 10%. HC Financial Service Group is the 3rd
largest company in the Chinese P2P financial market. In this paper we
illustrate how we tackle the fraud detection problem at HC Financial. We
utilize two powerful workhorses in the machine learning field - random forest
and gradient boosting decision tree to detect fraudulent users . We demonstrate
that by carefully select features and tune model parameters , we could
effectively filter out fraudulent users in the P2P market.
",['\nHao Wang\n'],,,http://arxiv.org/abs/1910.02010v1,cs.OH,"['cs.OH', 'cs.LG']",,,[]
Completely uniformly distributed sequences based on de Bruijn sequences,http://arxiv.org/abs/1909.11156v1,2019-09-24T20:12:11Z,2019-09-24T20:12:11Z,"  We study a construction published by Donald Knuth in 1965 yielding a
completely uniformly distributed sequence of real numbers. Knuth's work is
based on de Bruijn sequences of increasing orders and alphabet sizes, which
grow exponentially in each of the successive segments composing the generated
sequence. In this work we present a similar albeit simpler construction using
linearly increasing alphabet sizes, and give an elementary proof showing that
the sequence it yields is also completely uniformly distributed. In addition,
we present an alternative proof of the same result based on Weyl's criterion.
","['\nEmilio Almansi\n', '\nVerónica Becher\n']",,,http://arxiv.org/abs/1909.11156v1,math.NT,"['math.NT', 'cs.OH', 'math.PR']",,,[]
Authentication Modeling with Five Generic Processes,http://arxiv.org/abs/1910.01597v1,2019-09-30T21:49:53Z,2019-09-30T21:49:53Z,"  Conceptual modeling is an essential tool in many fields of study, including
security specification in information technology systems. As a model, it
restricts access to resources and identifies possible threats to the system. We
claim that current modeling languages (e.g., Unified Modeling Language,
Business Process Model and Notation) lack the notion of genericity, which
refers to a limited set of elementary processes. This paper proposes five
generic processes for modeling the structural behavior of a system: creating,
releasing, transferring, receiving, and processing. The paper demonstrates
these processes within the context of public key infrastructure, biometric, and
multifactor authentication. The results indicate that the proposed generic
processes are sufficient to represent these authentication schemes.
","['\nSabah Al-Fedaghi\n', '\nMennatAllah Bayoumi\n']","10 pages, 18 figures","(IJACSA) International Journal of Advanced Computer Science and
  Applications, Vol. 10, No. 9, 2019",http://dx.doi.org/10.14569/IJACSA.2019.0100947,cs.OH,"['cs.OH', 'cs.CR', 'cs.SE']",10.14569/IJACSA.2019.0100947,,[]
"A Method of EV Detour-to-Recharge Behavior Modeling and Charging Station
  Deployment",http://arxiv.org/abs/1910.02138v4,2019-09-27T02:36:37Z,2022-11-13T05:00:36Z,"  Electric vehicles (EVs) are increasingly used in transportation. Worldwide
use of EVs, for their limited battery capacity, calls for effective planning of
EVs charging stations to enhance the efficiency of using EVs. This paper
provides a methodology of describing EV detouring behavior for recharging, and
based on this, we adopt the extra driving length caused by detouring and the
length of uncompleted route as the indicators of evaluating an EV charging
station deployment plan. In this way, we can simulate EV behavior based on
travel data (demand). Then, a genetic algorithm (GA) based EV charging station
sitting optimization method is developed to obtain an effective plan. A
detailed case study based on a 100-node 203-branch transportation network
within a 30 km * 30 km region is included to test the effectiveness of our
method. Insights from our method may be applicable for charging station
planning in various transportation networks.
","['\nTianshu Ouyang\n', '\nJiahong Cai\n', '\nYuxuan Gao\n', '\nXinyan He\n', '\nHuimiao Chen\n', '\nKexin Hang\n']",,,http://arxiv.org/abs/1910.02138v4,cs.OH,"['cs.OH', 'cs.SY', 'eess.SY']",,,[]
"Modelagem de um Problema de Dimensionamento de Lotes com Demanda
  Variavel e Deterministica e Efeitos de Learning e Forgetting",http://arxiv.org/abs/1908.10293v1,2019-08-20T13:39:39Z,2019-08-20T13:39:39Z,"  The main goal of this paper was to analyze the importance that the effects of
learning and forgetting might have in a lot-sizing problem. It assumes that the
learning curve and the economies of scale are present in several industries yet
are, in most cases, not considered when dealing with a lot-sizing problem. The
importance of the effects was demonstrated and quantified, showing that there
is still space for developments in this field. However, as the problem becomes
quadratic, there is a possibility that the current algorithms are not able to
solve the problem to optimality. Thus, future improvements in the algorithms
may further improve the results. However, the overall results found with
current algorithms show that the contribution of a discount from a learning
curve can be very considerable, even if it is a minimal amount.
",['\nPedro Cesar Lopes Gerum\n'],in Portuguese,,http://arxiv.org/abs/1908.10293v1,cs.OH,['cs.OH'],,,[]
How Downwards Causation Occurs in Digital Computers,http://arxiv.org/abs/1908.10186v2,2019-08-15T14:49:53Z,2019-11-11T11:19:33Z,"  Digital computers carry out algorithms coded in high level programs. These
abstract entities determine what happens at the physical level: they control
whether electrons flow through specific transistors at specific times or not,
entailing downward causation in both the logical and implementation
hierarchies. This paper explores how this is possible in the light of the
alleged causal completeness of physics at the bottom level, and highlights the
mechanism that enables strong emergence (the manifest causal effectiveness of
application programs) to occur. Although synchronic emergence of higher levels
from lower levels is manifestly true, diachronic emergence is generically not
the case; indeed we give specific examples where it cannot occur because of the
causal effectiveness of higher level variables.
","['\nGeorge Ellis\n', '\nBarbara Drossel\n']","Final version, as accepted for publication","Foundations of Physics, 49(11), 1253-1277 (2019)",http://dx.doi.org/10.1007/s10701-019-00307-6,cs.OH,"['cs.OH', 'quant-ph']",10.1007/s10701-019-00307-6,,[]
An adaptive architecture for portability of greenhouse models,http://arxiv.org/abs/1908.01643v2,2019-07-29T13:09:41Z,2019-08-12T09:49:17Z,"  This work deals with the portability of greenhouse models, as we believe that
this is a challenge to their practical usage in control strategies under
production conditions. We address this task by means of adaptive neural
networks, which re-adjust their weights when transferred to new conditions.
Such an adaptive account for computational models is typical of the field of
developmental robotics, which investigates learning of motor control in
artificial systems inspired on infants development. Similarly to robots,
greenhouses are complex systems comprising technical and biological elements,
whose state can be measured and modified through control actions. We present an
adaptive model architecture to perform online learning on greenhouse models.
This learning process makes use of an episodic memory and of online
re-training. This allows for adaptation without the need for a complete new
training, which might be prohibitive if the data under the new conditions is
scarce. Current experiments focus on how a model of tomato photosynthesis,
developed in a research facility, can adapt itself to a new environment in a
production greenhouse. Further research will focus on model plasticity by means
of adaptive learning rates and management of the episodic memory described in
this paper. The models presented as a proof-of-concept estimate the
transpiration and photosynthesis of a hydroponic tomato crop by using
measurements of the climate as inputs. The models are trained and tested using
data from a greenhouse in Berlin, Germany. Thereafter, the adaptive
architecture is fed with data from a production greenhouse in southern Germany,
where other tomato varieties were grown under different irrigation and climate
strategies. The proposed adaptive architecture represents a promising tool for
spreading the use of models produced by high-tech research centers to the
greenhouse production sector.
","['\nLuis Miranda\n', '\nGuido Schillaci\n']","10 pages, 3 figures",,http://arxiv.org/abs/1908.01643v2,cs.OH,['cs.OH'],,,[]
"Simulation and Learning for Urban Mobility: City-scale Traffic
  Reconstruction and Autonomous Driving",http://arxiv.org/abs/1908.06131v1,2019-08-05T21:48:34Z,2019-08-05T21:48:34Z,"  Traffic congestion has become one of the most critical issues worldwide. The
costs due to traffic gridlock and jams are approximately $160 billion in the
United States, more than {\pounds}13 billion in the United Kingdom, and over
one trillion dollars across the globe annually. As more metropolitan areas will
experience increasingly severe traffic conditions, the ability to analyze,
understand, and improve traffic dynamics becomes critical. This dissertation is
an effort towards achieving such an ability. I propose various techniques
combining simulation and machine learning to tackle the problem of traffic from
two perspectives: city-scale traffic reconstruction and autonomous driving.
",['\nWeizi Li\n'],"PhD Thesis, Department of Computer Science, The University of North
  Carolina at Chapel Hill, July 2019",,http://arxiv.org/abs/1908.06131v1,cs.OH,"['cs.OH', 'eess.SP']",,,[]
"Inferring Accurate Bus Trajectories from Noisy Estimated Arrival Time
  Records",http://arxiv.org/abs/1907.08483v1,2019-07-19T12:23:40Z,2019-07-19T12:23:40Z,"  Urban commuting data has long been a vital source of understanding population
mobility behaviour and has been widely adopted for various applications such as
transport infrastructure planning and urban anomaly detection. While
individual-specific transaction records (such as smart card (tap-in, tap-out)
data or taxi trip records) hold a wealth of information, these are often
private data available only to the service provider (e.g., taxicab operator).
In this work, we explore the utility in harnessing publicly available, albeit
noisy, transportation datasets, such as noisy ""Estimated Time of Arrival"" (ETA)
records (commonly available to commuters through transit Apps or electronic
signages). We first propose a framework to extract accurate individual bus
trajectories from such ETA records, and present results from both a primary
city (Singapore) and a secondary city (London) to validate the techniques.
Finally, we quantify the upper bound on the spatiotemporal resolution, of the
reconstructed trajectory outputs, achieved by our proposed technique.
","['\nLakmal Meegahapola\n', '\nNoel Athaide\n', '\nKasthuri Jayarajah\n', '\nShili Xiang\n', '\nArchan Misra\n']","To appear in 22nd IEEE Intelligent Transportation Systems Conference
  (ITSC) 2019","IEEE Intelligent Transportation Systems Conference (ITSC),
  Auckland, New Zealand, 2019, pp. 4517-4524",http://dx.doi.org/10.1109/ITSC.2019.8916939,cs.OH,['cs.OH'],10.1109/ITSC.2019.8916939,,[]
Prototype Software Monitoring Sarana dan Prasarana Perguruan Tinggi,http://arxiv.org/abs/1907.13527v1,2019-07-25T02:29:55Z,2019-07-25T02:29:55Z,"  This study aims to facilitate the management system of monitoring
infrastructure program of university facilities and infrastructure, through
software engineering technology approach as an effort to improve productivity
and quality of monitoring process become more efficient and effective. The
software in this research is built in a systematic and organized approach to
monitoring infrastructure of facilities and infrastructure using appropriate
tools and techniques. Through this research, universities are expected to be
able to develop the necessary quality measures to support the process of
planning and controlling infrastructure infrastructure monitoring. The research
was conducted using survey method, development of monitoring management and
software. Up to the design stage of this program prototype, research has
produced a special picture of the software requirements to be built in the next
year. Software development process starting from the analysis phase of system
and software requirements, designing data structures up to the architecture
stage of the program has produced a list of needs/requirements, the design of
program prototype contained in the design of input/output for the monitoring
process facilities and infrastructure.
","['\nLeon Andretti Abdillah\n', '\nLinda Atika\n', '\n Kurniawan\n', '\nFitri Purwaningtias\n']","7 pages, accredited journal article, in Indonesian","JSINBIS (Jurnal Sistem Informasi Bisnis), 9(1), 18-24 (2019)",http://dx.doi.org/10.21456/vol9iss1pp18-24,cs.OH,['cs.OH'],10.21456/vol9iss1pp18-24,,[]
DNA based Network Model and Blockchain,http://arxiv.org/abs/1908.07829v1,2019-07-20T14:44:20Z,2019-07-20T14:44:20Z,"  Biological cells can transmit, process and receive chemically encoded data in
the same way as network devices transmit, process, and receive digitally
encoded data. Communication protocols have led to the rapid development of
computer networks. Therefore, we need to develop communication protocols for
biological cell networks, which will lead to significant development,
especially in medical applications where surgery or delivery of drugs can be
performed using nanoscale devices. Blockchain is a peer-to-peer network that
contains a series of clusters to make a valid and secure transaction. Blockhain
technology is used in many areas such as e-commerce, public services, security,
finance, Internet stuff, etc. Although blockchain has a major impact on
Internet technology, it suffers from time problems and scalability. DNA
computing is the execution of computations using natural molecules, especially
DNA. DNA gaps above silicon because of massive parallelism, size and storage
density. In this paper, biological cells and DNA are used to create the
necessary protocols for the networks to be used in the performance of the
cell-based communication system. The proposed hybrid solution involves DNA as
well as calculated on an enzymatic basis, where each contributes to the
function of a given protocol. Also a correspondence between blockchain and DNA
is proposed that can be utilized to create DNA based blockchain.
","['\nA. M. El-Edkawy\n', '\nM. A. El-Dosuky\n', '\nTaher Hamza\n']",,,http://arxiv.org/abs/1908.07829v1,cs.OH,"['cs.OH', 'cs.CR']",,,[]
"Wise Data: A Novel Approach in Data Science from a Network Science
  Perspective",http://arxiv.org/abs/1906.10686v2,2019-06-26T03:17:43Z,2019-07-28T01:21:25Z,"  Human beings have been generating data since very long times ago. We ask the
following common-sense and wise questions (WizQuestions):
  1. Why do we refer to some pieces of data more often than referring to other
pieces? 2. What does make those commonly-referred pieces of data so unique and
different? 3. What are the characteristics of data that sometimes make the data
so unique and different?
  In this article, we introduce a novel approach (model) that helps us answer
these questions from data science and network science perspectives. WizWordily
speaking, our proposed approach enables us to model the data (as a network),
measure the quality of data, and study the network of data deeply and
thoroughly.
",['\nMike Raeini\nMike WiseMan\n'],"16 pages, 3 figures",,http://arxiv.org/abs/1906.10686v2,cs.OH,['cs.OH'],,,['Mike WiseMan']
Autonomous CPS mobility securely designed,http://arxiv.org/abs/1907.00967v1,2019-07-02T11:22:34Z,2019-07-02T11:22:34Z,"  In the last years the interconnection and ongoing development of physical
systems combined with cyber resources has led to increasing automation. Through
this progress in technology, autonomous vehicles, especially autonomous trains
are getting more attention from industry and are already under test. The use of
autonomous trains is known for increasing operation efficiency and reduction of
personnel and infrastructure costs, which is mostly considered for main tracks.
However, for less-used secondary lines, autonomous trains and their underlying
sensor infrastructure are not yet considered. Thus, a system needs to be
developed, which is less expensive for installation and operation of these
trains and underlying infrastructure for secondary lines. Therefore, this
position paper describes the process of how to derive an approach to help
develop a digital interlocking system at design time for the use with secondary
railway lines. In this work, we motivate the necessary research by
investigating gaps in existing work as well as presenting a possible solution
for this problem, a meta-model. The model considers safety, security as well as
interoperability like 5G and socio-technical aspects to provide a holistic
modeling approach for the development of the interlocking system for industrial
secondary line use cases.
","['\nDavid Hofbauer\n', '\nChristoph Schmittner\n', '\nManuela Brandstetter\n', '\nMarkus Tauber\n']","The 5th IEEE International workshop on Communication, Computing, and
  Networking in Cyber Physical Systems (CCNCPS 2019) in association with 20th
  IEEE International Symposium on a World of Wireless, Mobile and Multimedia
  Networks (IEEE WoWMoM 2019) - Washington D.C., USA;",,http://arxiv.org/abs/1907.00967v1,cs.OH,['cs.OH'],,,[]
Fashion Retail: Forecasting Demand for New Items,http://arxiv.org/abs/1907.01960v1,2019-06-27T09:31:42Z,2019-06-27T09:31:42Z,"  Fashion merchandising is one of the most complicated problems in forecasting,
given the transient nature of trends in colours, prints, cuts, patterns, and
materials in fashion, the economies of scale achievable only in bulk
production, as well as geographical variations in consumption. Retailers that
serve a large customer base spend a lot of money and resources to stay prepared
for meeting changing fashion demands, and incur huge losses in unsold inventory
and liquidation costs [2]. This problem has been addressed by analysts and
statisticians as well as ML researchers in a conventional fashion - of building
models that forecast for future demand given a particular item of fashion with
historical data on its sales. To our knowledge, none of these models have
generalized well to predict future demand at an abstracted level for a new
design/style of fashion article. To address this problem, we present a study of
large scale fashion sales data and directly infer which clothing/footwear
attributes and merchandising factors drove demand for those items. We then
build generalised models to forecast demand given new item attributes, and
demonstrate robust performance by experimenting with different neural
architectures, ML methods, and loss functions.
","['\nPawan Kumar Singh\n', '\nYadunath Gupta\n', '\nNilpa Jha\n', '\nAruna Rajan\n']",KDD - AI4Fashion Workshop,,http://arxiv.org/abs/1907.01960v1,cs.OH,['cs.OH'],,,[]
"A Theoretical Model For Artificial Learning, Memory Management And
  Decision Making System",http://arxiv.org/abs/1907.04698v1,2019-07-02T02:51:20Z,2019-07-02T02:51:20Z,"  Human beings are considered as the most intelligent species on Earth. The
ability to think, to create, to innovate, are the key elements which make
humans superior over other existing species on Earth. Machines lack all those
elements, although machines are faster than human in aspects like computing,
equating etc. But humans are still more valuable than machines, due to all
those previously discussed elements. Various models have been developed in last
few years to create models that can think like human beings, but are not
completely successful. This paper presents a new theoretical system for
learning, memory management and decision making that can be used to develop
highly complex systems, and shows the potential to be used for development of
systems that can be used to provide the essential features to the machines to
act like human beings.
",['\nRavin Kumar\n'],"3 pages (2 columns per page), 5 figures",,http://dx.doi.org/10.31219/osf.io/b4wsr,cs.OH,['cs.OH'],10.31219/osf.io/b4wsr,,[]
"Modeling and analysis of alternative distribution and Physical Internet
  schemes in urban area",http://arxiv.org/abs/1907.10593v1,2019-07-04T10:00:21Z,2019-07-04T10:00:21Z,"  Urban logistics is becoming more complicated and costlier due to new
challenges in recent years. Since the main problem lies on congestion, the
clean vehicle is not necessarily the most effective solution. There is thus a
need to redesign the logistics networks in the city. This paper proposes a
methodology to evaluate different distribution schemes in the city among which
we find the most efficient and sustainable one. External impacts are added to
the analysis of schemes, including accident, air pollution, climate change,
noise, and congestion. An optimization model based on an analytical model is
developed to optimize transportation means and distribution schemes. Results
based on Bordeaux city show that PI scheme improves the performances of
distribution.
","['\nHao Jiang\nRSM\n', '\nEric Ballot\nCGS i3\n', '\nShenle Pan\nCGS i3\n']",,"6TH International Physical Internet Conference, Jul 2019, London,
  United Kingdom",http://arxiv.org/abs/1907.10593v1,cs.OH,['cs.OH'],,,"['RSM', 'CGS i3', 'CGS i3']"
SysMART Outdoor Services: A System of Connected and Smart Supermarkets,http://arxiv.org/abs/1907.12407v1,2019-07-01T17:21:03Z,2019-07-01T17:21:03Z,"  Smart cities are today's modern trend. Many high-tech industrial firms are
exploring different approaches to implement smart cities. Various projects aim
at internet-of-things and smart solutions. Current implementations are mostly
localized to a specific building or area; however, the growth is crossing space
and geographic location limits. Shopping is a central activity that is frequent
and typically a time-consuming task. SysMART is system of connected and smart
supermarkets. SysMART enables a plausible shopping experience for customers.
The aim of SysMART is to provide an advanced lifestyle with its ease of use
functionality. SysMART outdoor services support distant parking availability,
traffic status, and remote inventory checks for supermarkets in a chain.
SysMART implementation relies on cutting edge technologies that support rapid
prototyping and precision data acquisition, such as, National Instrument
devices. The selected development environment is LabView with its world-class
interfacing libraries. The paper comprises a detailed system description,
development strategy, interface design, software engineering, and a thorough
analysis and evaluation.
","['\nYazan Mohamad\nAmerican University of Kuwait\n', '\nMajd Makdessi\nAmerican University of Kuwait\n', '\nOmar Raad\nAmerican University of Kuwait\n', '\nIssam Damaj\nAmerican University of Kuwait\n']","5 pages, 6 figures, 3 tables","The 9th IEEE-GCC Conference and Exhibition, Manama, Bahrain, May
  8-11, (2017) 98-102",http://dx.doi.org/10.1109/IEEEGCC.2017.8448000,cs.OH,"['cs.OH', 'eess.SP', 'B.4.1; H.4.3; K.8.1']",10.1109/IEEEGCC.2017.8448000,,"['American University of Kuwait', 'American University of Kuwait', 'American University of Kuwait', 'American University of Kuwait']"
EU H2020 Gauss project. Geo-Fencing Software System,http://arxiv.org/abs/1907.04154v1,2019-06-25T15:30:17Z,2019-06-25T15:30:17Z,"  The Geofencing system is the key to operate the Unmanned Aerial Vehicle (UAV)
within the safe and appropriate zone to avoid public concerns and other privacy
issues. The system is designed to keep the UAV away from geofenced obstacles
using the onboard GNSS and IMU location. The Geofencing system is part of the
H2020 GAUSS project and facilities other subsystems, for instance, to support
the command and control link, which is the security measure to secure the UAV
from hijacking and signal spoofing. The regulatory authorities expressed the
concern of having UAVs flying in the no-fly zone and causing troubles from
offending private privacy to hazards at airport airspace. Hence the geofence
system shall provide guidance message, which enables the UAV to evacuate from
no-fly-zone, based on real-time updated location. This thesis aims to first
illustrate the generation of geofence and then apply the geofence system on UAV
operation. This application enables UAV to fly in the designated area without
human intervention. The project is built with JAVA using GIS-enabled Database
Management System and Open Soured Map data powered by OpenStreetMap and OS map.
This method has been tested by simulations which had results of high accuracy.
",['\nHao Xu\n'],"96 pages, Master Thesis at Cranfield University",,http://arxiv.org/abs/1907.04154v1,cs.OH,"['cs.OH', 'cs.CG', 'cs.CR']",,,[]
"Assessment of Urban Ecological Service value used in Urban Rail Transit
  Project",http://arxiv.org/abs/1906.06572v1,2019-06-15T14:36:29Z,2019-06-15T14:36:29Z,"  Ecosystem services refer to the ones human beings often obtain from the
natural environment ecosystem. In order to solve the problem of environmental
degradation, based on the Integrated Valuation of Ecosystem Services and
Trade-offs (InVEST model), this paper makes innovation by adding the urban
module that was not in the previous models, which can better deal with the
evaluation of ecosystem services in urban scenarios.
","['\nYijie Li\n', '\nJing Chen\n']",arXiv admin note: text overlap with arXiv:1903.07889 by other authors,,http://arxiv.org/abs/1906.06572v1,cs.OH,['cs.OH'],,,[]
Characterizing IoT Data and its Quality for Use,http://arxiv.org/abs/1906.10497v1,2019-06-23T18:39:10Z,2019-06-23T18:39:10Z,"  The Internet of Things (IoT) is a cyber physical social system that
encompasses science, enterprise and societal domains. Data is the most
important commodity in IoT, enabling the ""smarts"" through analytics and
decision making. IoT environments can generate and consume vast amounts of
data. But managing this data effectively and gaining meaningful insights from
it requires us to understand its characteristics. Traditional scientific,
enterprise and big data management approaches may not be adequate, and have to
evolve. Further, these characteristics and the physical deployment environments
also impact the quality of the data for use. In this paper, we offer a taxonomy
of IoT data characteristics, along with data quality considerations, that are
constructed from the ground-up based on the diverse IoT domains and
applications we review. We emphasize on the essential features, rather than a
vast array of attributes. We also indicate factors that influence the data
quality. Such a review is of value to IoT managers, data handlers and
application composers in managing and making meaningful use of data, and for
big data platform developers to offer meaningful solutions to address these
considerations.
","['\nNashez Zubair\n', '\nNiranjan A\n', '\nKiran Hebbar\n', '\nYogesh Simmhan\n']","Tech Report on IoT Data and its Quality in light of various
  applications reviewed",,http://arxiv.org/abs/1906.10497v1,cs.OH,['cs.OH'],,,[]
Algorithmic measurement procedures,http://arxiv.org/abs/1906.11028v1,2019-06-18T19:06:09Z,2019-06-18T19:06:09Z,"  Measurements are shown to be processes designed to return figures: they are
effective. This effectivity allows for a formalization as Turing machines,
which can be described employing computation theory. Inspired in the halting
problem we draw some limitations for measurement procedures: procedures that
verify if a quantity is measured cannot work in every case.
","['\nAldo F. G. Solis-Labastida\n', '\nJorge G. Hirsch\n']","9 pages, 1 figure",,http://dx.doi.org/10.1007/s10701-020-00354-4,cs.OH,['cs.OH'],10.1007/s10701-020-00354-4,,[]
Emergency Management Systems and Algorithms: a Comprehensive Survey,http://arxiv.org/abs/1907.04136v1,2019-06-21T15:10:14Z,2019-06-21T15:10:14Z,"  Owing to the increasing frequency and destruction of natural and manmade
disasters to modern highly-populated societies, emergency management, which
provides solutions to prevent or address disasters, have drawn considerable
research over the last few decades and become a multidisciplinary area. Because
of its open and inclusive nature, new technologies always tend to influence,
change or even revolutionise this research area. Hence, it is imperative to
consolidate the state-of-the-art studies and knowledge to meet the research
needs and identify the future research directions. The paper presents a
comprehensive and systemic review of the existing research in the field of
emergency management from both the system design aspect and algorithm
engineering aspect. We begin with the history and evolution of the emergency
management research. Then the two main research topics of this area, ""emergency
navigation"" and ""emergency search and rescue planning"", are introduced and
discussed. Finally, we suggest the emerging challenges and opportunities from
system optimisation, evacuee behaviour modelling and optimisation, computing
patterns, data analysis, energy and cyber security aspects.
","['\nHuibo Bi\n', '\nErol Gelenbe\n']","33 pages, 3 figures",,http://arxiv.org/abs/1907.04136v1,cs.OH,"['cs.OH', 'cs.SY', 'eess.SY']",,,[]
"Customizing Pareto Simulated Annealing for Multi-objective Optimization
  of Control Cabinet Layout",http://arxiv.org/abs/1906.04825v1,2019-06-04T10:05:00Z,2019-06-04T10:05:00Z,"  Determining the optimal location of control cabinet components requires the
exploration of a large configuration space. For real-world control cabinets it
is impractical to evaluate all possible cabinet configurations. Therefore, we
need to apply methods for intelligent exploration of cabinet configuration
space that enable to find a near-optimal configuration without evaluation of
all possible configurations. In this paper, we describe an approach for
multi-objective optimization of control cabinet layout that is based on Pareto
Simulated Annealing. Optimization aims at minimizing the total wire length used
for interconnection of components and the heat convection within the cabinet.
We simulate heat convection to study the warm air flow within the control
cabinet and determine the optimal position of components that generate heat
during the operation. We evaluate and demonstrate the effectiveness of our
approach empirically for various control cabinet sizes and usage scenarios.
","['\nSabri Pllana\n', '\nSuejb Memeti\n', '\nJoanna Kolodziej\n']","Preprint, CSCS22, (C) 2019 IEEE",,http://arxiv.org/abs/1906.04825v1,cs.OH,"['cs.OH', 'cs.AI']",,,[]
Definitively Identifying an Inherent Limitation to Actual Cognition,http://arxiv.org/abs/1905.13010v2,2019-05-29T17:55:38Z,2019-05-31T17:38:27Z,"  A century ago, discoveries of a serious kind of logical error made separately
by several leading mathematicians led to acceptance of a sharply enhanced
standard for rigor within what ultimately became the foundation for Computer
Science. By 1931, Godel had obtained a definitive and remarkable result: an
inherent limitation to that foundation. The resulting limitation is not
applicable to actual human cognition, to even the smallest extent, unless both
of these extremely brittle assumptions hold: humans are infallible reasoners
and reason solely via formal inference rules. Both assumptions are contradicted
by empirical data from well-known Cognitive Science experiments. This article
investigates how a novel multi-part methodology recasts computability theory
within Computer Science to obtain a definitive limitation whose application to
human cognition avoids assumptions contradicting empirical data. The limitation
applies to individual humans, to finite sets of humans, and more generally to
any real-world entity.
",['\nArthur Charlesworth\n'],"45 pages, 5 figures; changed author's email address",,http://arxiv.org/abs/1905.13010v2,cs.OH,"['cs.OH', 'cs.AI', 'F.1.1; I.2; J.4']",,,[]
Deep Fuzzy Systems,http://arxiv.org/abs/1906.08222v1,2019-05-23T08:48:37Z,2019-05-23T08:48:37Z,"  An investigation of deep fuzzy systems is presented in this paper. A deep
fuzzy system is represented by recursive fuzzy systems from an input terminal
to output terminal. Recursive fuzzy systems are sequences of fuzzy grade
memberships obtained using fuzzy transmition functions and recursive calls to
fuzzy systems. A recursive fuzzy system which calls a fuzzy system n times
includes fuzzy chains to evaluate the final grade membership of this recursive
system. A connection matrix which includes recursive calls are used to
represent recursive fuzzy systems.
",['\nKhaled Ahmed Nagaty\n'],,,http://arxiv.org/abs/1906.08222v1,cs.OH,"['cs.OH', 'cs.AI']",,,[]
Using AI for Economic Upliftment of Handicraft Industry,http://arxiv.org/abs/1907.02014v1,2019-05-31T07:33:42Z,2019-05-31T07:33:42Z,"  The handicraft industry is a strong pillar of Indian economy which provides
large-scale employment opportunities to artisans in rural and underprivileged
communities. However, in this era of globalization, diverse modern designs have
rendered traditional designs old and monotonous, causing an alarming decline of
handicraft sales. For this age-old industry to survive the global competition,
it is imperative to integrate contemporary designs with Indian handicrafts. In
this paper, we use novel AI techniques to generate contemporary designs for two
popular Indian handicrafts - Ikat and Block Print. These techniques were
successfully employed by communities across India to manufacture and sell
products with greater appeal and revenue. The designs are evaluated to be
significantly more likeable and marketable than the current designs used by
artisans.
","['\nNitya Raviprakash\n', '\nSonam Damani\n', '\nAnkush Chatterjee\n', '\nMeghana Joshi\n', '\nPuneet Agrawal\n']",,,http://arxiv.org/abs/1907.02014v1,cs.OH,"['cs.OH', 'cs.CV', 'cs.CY']",,,[]
"Earthquake Prediction With Artificial Neural Network Method: The
  Application Of West Anatolian Fault In Turkey",http://arxiv.org/abs/1907.02209v1,2019-05-26T13:54:13Z,2019-05-26T13:54:13Z,"  A method that exactly knows the earthquakes beforehand and can generalize
them cannot still been developed. However, earthquakes are tried to be
predicted through numerous methods. One of these methods, artificial neural
networks give appropriate outputs to different patterns by learning the
relationship between the determined inputs and outputs. In this study, a
feedforward back propagation artificial neural network that is connected to
Gutenberg-Richter relationship and that bases on b value used in earthquake
predictions was developed. The artificial neural network was trained employing
earthquake data belonging to four different regions which have intensive
seismic activity in the west of Turkey. After the training process, the
earthquake data belonging to later dates of the same regions were used for
testing and the performance of the network was put forward. When the prediction
results of the developed network are examined, the prediction results that the
network predicts that an earthquake is not going to occur are quite high in all
regions. Furthermore, the earthquake prediction results that the network
predicts that an earthquake is going to occur are different to some extent for
the studied regions.
","['\nHandan Cam\n', '\nOsman Duman\n']",,"GUEJISS, Gumushane University Electronic Journal of The Institute
  of Social Sciences Volume: 7, Number: 17, Year: 2016",http://dx.doi.org/10.17823/gusb.352,cs.OH,"['cs.OH', 'cs.LG', 'eess.SP']",10.17823/gusb.352,,[]
Simulation Typology and Termination Risks,http://arxiv.org/abs/1905.05792v1,2019-05-12T16:31:50Z,2019-05-12T16:31:50Z,"  The goal of the article is to explore what is the most probable type of
simulation in which humanity lives (if any) and how this affects simulation
termination risks. We firstly explore the question of what kind of simulation
in which humanity is most likely located based on pure theoretical reasoning.
We suggest a new patch to the classical simulation argument, showing that we
are likely simulated not by our own descendants, but by alien civilizations.
Based on this, we provide classification of different possible simulations and
we find that simpler, less expensive and one-person-centered simulations,
resurrectional simulations, or simulations of the first artificial general
intelligence's (AGI's) origin (singularity simulations) should dominate. Also,
simulations which simulate the 21st century and global catastrophic risks are
probable. We then explore whether the simulation could collapse or be
terminated. Most simulations must be terminated after they model the
singularity or after they model a global catastrophe before the singularity.
Undeniably observed glitches, but not philosophical speculations could result
in simulation termination. The simulation could collapse if it is overwhelmed
by glitches. The Doomsday Argument in simulations implies termination soon. We
conclude that all types of the most probable simulations except resurrectional
simulations are prone to termination risks in a relatively short time frame of
hundreds of years or less from now.
","['\nAlexey Turchin\n', '\nMichael Batin\n', '\nDavid Denkenberger\n', '\nRoman Yampolskiy\n']",,,http://arxiv.org/abs/1905.05792v1,cs.OH,['cs.OH'],,,[]
"Quartierstrom -- Implementation of a real world prosumer centric local
  energy market in Walenstadt, Switzerland",http://arxiv.org/abs/1905.07242v2,2019-05-17T13:03:53Z,2019-07-29T06:28:35Z,"  Prosumers in many regions are facing reduced feed-in tariffs and currently
have no possibility to influence the level of remuneration for the locally
produced solar energy. Peer-to-peer communities may offer an alternative to the
feed-in tariff model by enabling prosumers to directly sell their solar energy
to local consumers (possibly at a rate that is beneficial for both consumer and
prosumer). The Quartierstrom project investigates a transactional energy system
that manages the exchange and remuneration of electricity between consumers,
prosumers and the local electric grid provider in the absence of
intermediaries. This whitepaper describes the prototypical real-world system
being implemented in the town of Walenstadt, Switzerland, with 37 participating
households. The community members of this pilot project pay a reduced tariff
for grid usage if the electricity produced by a prosumer is sold to another
community member, which is located on the same voltage or grid level downstream
a substation1. Such a tariff structure incentivizes local balancing, i.e.
locally produced energy can be consumed locally whenever possible to avoid
costs from higher grid levels. The blockchain is a novel technology suitable to
log the produced and consumed units of energy within a community, making it
possible to implement market places. In those marketplaces, both prosumers and
consumers can indicate a price at which they are willing to sell / buy locally
produced solar energy without the intermediation of a utility. The key goals of
this project are the assessment of A) the technical, economical and ecological
feasibility of a blockchain-based community energy system regarding local
utilization of solar energy, grid quality and energy efficiency and B)
resulting dynamics regarding local market prices and user acceptance.
","['\nLiliane Ableitner\n', '\nArne Meeuw\n', '\nSandro Schopfer\n', '\nVerena Tiefenbeck\n', '\nFelix Wortmann\n', '\nAnselma Wörner\n']","Energy, solar, local market, auction, blockchain, user-interaction",,http://arxiv.org/abs/1905.07242v2,cs.OH,['cs.OH'],,,[]
Programmable Logic Arrays,http://arxiv.org/abs/1905.02074v1,2019-05-03T15:32:16Z,2019-05-03T15:32:16Z,"  Programmable logic arrays (PLAs) are traditional digital electronic devices.
A PLA is a simple programmable logic device (SPLD) used to implement
combinational logic circuits. A PLA has a set of programmable AND gates, which
link to a set of programmable OR gates to produce an output. The AND-OR layout
of a PLA allows for implementing logic functions that are in a sum-of-products
form. PLAs are available in the market in different types. PLAs could be stand
alone chips, or parts of bigger processing systems. Stand alone PLAs are
available as mask programmable (MPLAs) and field programmable (FPLAs) devices.
The attractions of PLAs that brought them to mainstream engineers include their
simplicity, relatively small circuit area, predictable propagation delay, and
ease of development. The powerful-but-simple property brought PLAs to rapid
prototyping, synthesis, design optimization techniques, embedded systems,
traditional computer systems, hybrid high-performance computing systems, etc.
Indeed, there has been renewable interests in working with the simple AND-to-OR
PLAs.
",['\nIssam Damaj\nDhofar University\n'],"19 pages, 18 figures. arXiv admin note: text overlap with
  arXiv:1905.02075, arXiv:1905.02076",Wiley. Enc. of. Comp. Sc. & Eng. 4(2008) 2272-2280,http://dx.doi.org/10.1002/9780470050118.ecse316,cs.OH,"['cs.OH', 'B.6.1']",10.1002/9780470050118.ecse316,,['Dhofar University']
Logic Design,http://arxiv.org/abs/1905.02075v1,2019-05-03T15:34:07Z,2019-05-03T15:34:07Z,"  Electronic circuits can be separated into two groups, digital and analog
circuits. Analog circuits operate on analog quantities that are continuous in
value, whereas digital circuits operate on digital quantities that are discrete
in value and limited in precision. In practice, most digital systems contain
combinational circuits along with memory; these systems are known as sequential
circuits. Sequential circuits are of two types: synchronous and asynchronous.
In a synchronous sequential circuit, a clock signal is used at discrete
instants of time to synchronize desired operations. Asynchronous sequential
circuits do not require synchronizing clock pulses; however, the completion of
an operation signals the start of the next operation in sequence. The basic
logic design steps are generally identical for sequential and combinational
circuits; these are specification, formulation, optimization, and the
implementation of the optimized equations using a suitable hardware technology.
The differences between sequential and combinational design steps appear in the
details of each step. The minimization (optimization) techniques used in logic
design range from simple (manual) to complex (automated). An example of manual
optimization methods is the Karnough map (K-map). Indeed, hardware
implementation technology has been growing faster than the ability of designers
to produce hardware designs. Hence, there has been a growing interest in
developing techniques and tools that facilitate the process of logic design.
",['\nIssam Damaj\nDhofar University\n'],"20 pages, 17 figures. arXiv admin note: text overlap with
  arXiv:1905.02074, arXiv:1905.02076",Wiley. En. of. Comp. Sc. & Eng. 3(2008) 1495-1504,http://dx.doi.org/10.1002/9780470050118.ecse177,cs.OH,"['cs.OH', 'B.6']",10.1002/9780470050118.ecse177,,['Dhofar University']
High-level Synthesis,http://arxiv.org/abs/1905.02076v1,2019-05-03T15:33:03Z,2019-05-03T15:33:03Z,"  Hardware synthesis is a general term used to refer to the processes involved
in automatically generating a hardware design from its specification.
High-level synthesis (HLS) could be defined as the translation from a
behavioral description of the intended hardware circuit into a structural
description similar to the compilation of programming languages (such as C and
Pascal into assembly language. The chained synthesis tasks at each level of the
design process include system synthesis, register-transfer synthesis, logic
synthesis, and circuit synthesis. The development of hardware solutions for
complex applications is no more a complicated task with the emergence of
various HLS tools. Many areas of application have benefited from the modern
advances in hardware design, such as automotive and aerospace industries,
computer graphics, signal and image processing, security, complex simulations
like molecular modeling, and DND matching. The field of HLS is continuing its
rapid growth to facilitate the creation of hardware and to blur more and more
the border separating the processes of designing hardware and software.
",['\nIssam Damaj\nDhofar University\n'],"19 Pages, 16 Figures. arXiv admin note: text overlap with
  arXiv:1905.02075, arXiv:1905.02074",Wiley. Enc. of. Comp. Sc. & Eng. 3(2008) 1495-1504,http://dx.doi.org/10.1002/9780470050118.ecse177,cs.OH,"['cs.OH', 'B.1.2']",10.1002/9780470050118.ecse177,,['Dhofar University']
Analytical review of medical mobile diagnostic systems,http://arxiv.org/abs/1905.03212v1,2019-05-03T16:25:47Z,2019-05-03T16:25:47Z,"  This article analyzes the mobile medical diagnostic systems and compare them
with the proposed HealthTracker system based on smartwatch Apple Watch. Before
the development of the system HealthTracker, there was conducted a review and
analysis of existing similar systems to identify common and distinctive
features of the future system. This analysis will improve HealthTracker system,
based on the strengths and weaknesses of existing systems and help identify and
justify the key benefits and unique system HealthTracker. The main goal is to
provide a system HealthTracker convenient way to interact with the patient the
doctor based on the vital signs of the patient. Apple Watch is an excellent
watch presented in 2014 that has the capacity to collect and compile data on
the health of the user and can be used for medical purposes.
","['\nDarii Kordiyak\n', '\nNataliya Shakhovska\n']","6 pages, 0 figures",,http://arxiv.org/abs/1905.03212v1,cs.OH,['cs.OH'],,,[]
"New-Generation Design-Technology Co-Optimization (DTCO):
  Machine-Learning Assisted Modeling Framework",http://arxiv.org/abs/1904.10269v1,2019-04-23T12:14:35Z,2019-04-23T12:14:35Z,"  In this paper, we propose a machine-learning assisted modeling framework in
design-technology co-optimization (DTCO) flow. Neural network (NN) based
surrogate model is used as an alternative of compact model of new devices
without prior knowledge of device physics to predict device and circuit
electrical characteristics. This modeling framework is demonstrated and
verified in FinFET with high predicted accuracy in device and circuit level.
Details about the data handling and prediction results are discussed. Moreover,
same framework is applied to new mechanism device tunnel FET (TFET) to predict
device and circuit characteristics. This work provides new modeling method for
DTCO flow.
","['\nZhe Zhang\n', '\nRunsheng Wang\n', '\nCheng Chen\n', '\nQianqian Huang\n', '\nYangyuan Wang\n', '\nCheng Hu\n', '\nDehuang Wu\n', '\nJoddy Wang\n', '\nRu Huang\n']",,,http://arxiv.org/abs/1904.10269v1,cs.OH,['cs.OH'],,,[]
"Transfer and Online Reinforcement Learning in STT-MRAM Based Embedded
  Systems for Autonomous Drones",http://arxiv.org/abs/1905.06314v1,2019-04-22T00:18:09Z,2019-04-22T00:18:09Z,"  In this paper we present an algorithm-hardware codesign for camera-based
autonomous flight in small drones. We show that the large write-latency and
write-energy for nonvolatile memory (NVM) based embedded systems makes them
unsuitable for real-time reinforcement learning (RL). We address this by
performing transfer learning (TL) on metaenvironments and RL on the last few
layers of a deep convolutional network. While the NVM stores the meta-model
from TL, an on-die SRAM stores the weights of the last few layers. Thus all the
real-time updates via RL are carried out on the SRAM arrays. This provides us
with a practical platform with comparable performance as end-to-end RL and
83.4% lower energy per image frame
","['\nInsik Yoon\n', '\nAqeel Anwar\n', '\nTitash Rakshit\n', '\nArijit Raychowdhury\n']",,,http://arxiv.org/abs/1905.06314v1,cs.OH,"['cs.OH', 'cs.AR']",,,[]
"A Method for Expressing and Displaying the Vehicle Behavior Distribution
  in Maintenance Work Zones",http://arxiv.org/abs/1904.11786v1,2019-04-25T10:41:38Z,2019-04-25T10:41:38Z,"  Maintenance work zones on the road network have impacts on the normal
travelling of vehicles, which increase the risk of traffic accidents. The
traffic characteristic analysis in maintenance work zones is a basis for
maintenance work zone related research such as layout design, traffic control
and safety assessment. Due to the difficulty in vehicle microscopic behaviour
data acquisition, traditional traffic characteristic analysis mainly focuses on
macroscopic characteristics. With the development of data acquisition
technology, it becomes much easier to obtain a large amount of microscopic
behaviour data nowadays, which lays a good foundation for analysing the traffic
characteristics from a new point of view. This paper puts forward a method for
expressing and displaying the vehicle behaviour distribution in maintenance
work zones. Using portable vehicle microscopic behaviour data acquisition
devices, lots of data can be obtained. Based on this data, an endpoint
detection technology is used to automatically extract the segments in behaviour
data with violent fluctuations, which are segments where vehicles take
behaviours such as acceleration or turning. Using the support vector machine
classification method, the specific types of behaviours of the segments
extracted can be identified, and together with a data combination method, a
total of ten types of behaviours can be identified. Then the kernel density
analysis is used to cluster different types of behaviours of all passing
vehicles to show the distribution on maps. By this method, how vehicles travel
through maintenance work zones, and how different vehicle behaviours distribute
in maintenance work zones can be displayed intuitively on maps, which is a
novel traffic characteristic and can shed light to maintenance work zone
related researches such as safety assessment and design method.
","['\nQun Yang\n', '\nZhepu Xu\n', '\nSaravanan Gurupackiam\n', '\nPing Wang\n']","14 pages, 12 figures, 1 table",,http://arxiv.org/abs/1904.11786v1,cs.OH,"['cs.OH', 'cs.LG']",,,[]
PowerNet: Neural Power Demand Forecasting in Smart Grid,http://arxiv.org/abs/1904.11979v1,2019-04-27T11:59:39Z,2019-04-27T11:59:39Z,"  Power demand forecasting is a critical task for achieving efficiency and
reliability in power grid operation. Accurate forecasting allows grid operators
to better maintain the balance of supply and demand as well as to optimize
operational cost for generation and transmission. This article proposes a novel
neural network architecture PowerNet, which can incorporate multiple
heterogeneous features, such as historical energy consumption data, weather
data, and calendar information, for the power demand forecasting task. Compared
to two recent works based on Gradient Boosting Tree (GBT) and Support Vector
Regression (SVR), PowerNet demonstrates a decrease of 33.3% and 14.3% in
forecasting error, respectively. We further provide empirical results the two
operational considerations that are crucial when using PowerNet in practice,
i.e., how far in the future the model can forecast with a decent accuracy and
how often we should re-train the forecasting model to retain its modeling
capability. Finally, we briefly discuss a multilayer anomaly detection approach
based on PowerNet.
","['\nYao Cheng\n', '\nChang Xu\n', '\nDaisuke Mashima\n', '\nVrizlynn L. L. Thing\n', '\nYongdong Wu\n']",,,http://arxiv.org/abs/1904.11979v1,cs.OH,"['cs.OH', 'cs.SY']",,,[]
Galaxy Learning -- A Position Paper,http://arxiv.org/abs/1905.00753v1,2019-04-22T11:05:26Z,2019-04-22T11:05:26Z,"  The recent rapid development of artificial intelligence (AI, mainly driven by
machine learning research, especially deep learning) has achieved phenomenal
success in various applications. However, to further apply AI technologies in
real-world context, several significant issues regarding the AI ecosystem
should be addressed. We identify the main issues as data privacy, ownership,
and exchange, which are difficult to be solved with the current centralized
paradigm of machine learning training methodology. As a result, we propose a
novel model training paradigm based on blockchain, named Galaxy Learning, which
aims to train a model with distributed data and to reserve the data ownership
for their owners. In this new paradigm, encrypted models are moved around
instead, and are federated once trained. Model training, as well as the
communication, is achieved with blockchain and its smart contracts. Pricing of
training data is determined by its contribution, and therefore it is not about
the exchange of data ownership. In this position paper, we describe the
motivation, paradigm, design, and challenges as well as opportunities of Galaxy
Learning.
","['\nChao Wu\n', '\nJun Xiao\n', '\nGang Huang\n', '\nFei Wu\n']",,,http://arxiv.org/abs/1905.00753v1,cs.OH,"['cs.OH', 'cs.AI', 'cs.CR', 'cs.LG']",,,[]
"Clifford algebras, Spin groups and qubit trees",http://arxiv.org/abs/1904.09912v6,2019-04-22T15:15:31Z,2022-12-05T04:15:30Z,"  Representations of Spin groups and Clifford algebras derived from the
structure of qubit trees are introduced in this work. For ternary trees the
construction is more general and reduction to binary trees is formally defined
by deletion of superfluous branches. The usual Jordan--Wigner construction also
may be formally obtained in this approach by bringing the process up to trivial
qubit chain (trunk). The methods can also be used for effective simulation of
some quantum circuits corresponding to the binary tree structure. The modeling
of more general qubit trees, as well as the relationship with the mapping used
in the Bravyi--Kitaev transformation, are also briefly discussed.
",['\nAlexander Yu. Vlasov\n'],"16 pages, 9 figures",Quanta 2022; 11: 97-114,http://dx.doi.org/10.12743/quanta.v11i1.199,quant-ph,"['quant-ph', 'cs.OH', 'math-ph', 'math.MP']",10.12743/quanta.v11i1.199,,[]
"Appliance Event Detection -- A Multivariate, Supervised Classification
  Approach",http://arxiv.org/abs/1904.11580v1,2019-04-24T15:17:55Z,2019-04-24T15:17:55Z,"  Non-intrusive load monitoring (NILM) is a modern and still expanding
technique, helping to understand fundamental energy consumption patterns and
appliance characteristics. Appliance event detection is an elementary step in
the NILM pipeline. Unfortunately, several types of appliances (e.g., switching
mode power supply (SMPS) or multi-state) are known to challenge
state-of-the-art event detection systems due to their noisy consumption
profiles. Classical rule-based event detection system become infeasible and
complex for these appliances. By stepping away from distinct event definitions,
we can learn from a consumer-configured event model to differentiate between
relevant and irrelevant event transients.
  We introduce a boosting oriented adaptive training, that uses false positives
from the initial training area to reduce the number of false positives on the
test area substantially. The results show a false positive decrease by more
than a factor of eight on a dataset that has a strong focus on SMPS-driven
appliances. To obtain a stable event detection system, we applied several
experiments on different parameters to measure its performance. These
experiments include the evaluation of six event features from the spectral and
time domain, different types of feature space normalization to eliminate
undesired feature weighting, the conventional and adaptive training, and two
common classifiers with its optimal parameter settings. The evaluations are
performed on two publicly available energy datasets with high sampling rates:
BLUED and BLOND-50.
","['\nMatthias Kahl\n', '\nThomas Kriechbaumer\n', '\nDaniel Jorde\n', '\nAnwar Ul Haq\n', '\nHans-Arno Jacobsen\n']",,,http://arxiv.org/abs/1904.11580v1,cs.OH,"['cs.OH', 'cs.SY', 'eess.SP']",,,[]
"Charging control of electric vehicles using contextual bandits
  considering the electrical distribution grid",http://arxiv.org/abs/1905.01163v1,2019-04-10T12:32:04Z,2019-04-10T12:32:04Z,"  With the proliferation of electric vehicles, the electrical distribution
grids are more prone to overloads. In this paper, we study an intelligent
pricing and power control mechanism based on contextual bandits to provide
incentives for distributing charging load and preventing network failure. The
presented work combines the microscopic mobility simulator SUMO with electric
network simulator SIMONA and thus produces reliable electrical distribution
load values. Our experiments are carefully conducted under realistic conditions
and reveal that conditional bandit learning outperforms context-free
reinforcement learning algorithms and our approach is suitable for the given
problem. As reinforcement learning algorithms can be adapted rapidly to include
new information we assume these to be suitable as part of a holistic traffic
control scenario.
","['\nChristian Römer\n', '\nJohannes Hiry\n', '\nChris Kittl\n', '\nThomas Liebig\n', '\nChristian Rehtanz\n']","appeared in KNOWme workshop @ECMLPKDD2018, 19 pages",,http://arxiv.org/abs/1905.01163v1,cs.OH,['cs.OH'],,,[]
"Combining Conformance Checking and Classification of XES Log Data for
  the Manufacturing Domain",http://arxiv.org/abs/1904.05883v1,2019-04-11T20:09:39Z,2019-04-11T20:09:39Z,"  Currently, data collection on the shop floor is based on individual resources
such as machines, robots, and Autonomous Guided Vehicles (AGVs). There is a gap
between this approach and manufacturing orchestration software that supervises
the process of creating single products and controls the ressources'
interactions. This creates the need to save resource-based data streams in
databases, clean it, and then re-contextualize it, i.e., by connecting it to
orders, batches, and single products. Looking at this data from a
process-oriented analysis point of view enables new analysis prospects. This
paper utilises these prospects in an experimental way by creating BPMN models
for the manufacturing of two real-world products: (1) a low volume, high
complexity lower-housing for a gas-turbine and (2) a high volume, low
complexity, small tolerance valve lifter for a gas turbine. In contrast to the
resource-based data collection, 30+ values are modeled into the BPMN models and
enacted by a workflow engine, creating execution logs in the XES standard
format. Conformance checks are carried out and interpreted for both scenarios
and it is shown how existing classification and clustering techniques can be
applied on the collected data in order to predict good and bad parts, ex-post
and potentially at run-time.
","['\nMatthias Ehrendorfer\n', '\nJuergen-Albrecht Fassmann\n', '\nJuergen Mangler\n', '\nStefanie Rinderle-Ma\n']","42 pages, 26 figures, 16 tables, 6 listings. 2 datasets with each ~1
  GiB process log (IEEE 1849-2016 XES) data",,http://arxiv.org/abs/1904.05883v1,cs.OH,['cs.OH'],,,[]
"Evaluation of IoT-Based Computational Intelligence Tools for DNA
  Sequence Analysis in Bioinformatics",http://arxiv.org/abs/1904.09268v1,2019-04-16T09:47:52Z,2019-04-16T09:47:52Z,"  In contemporary age, Computational Intelligence (CI) performs an essential
role in the interpretation of big biological data considering that it could
provide all of the molecular biology and DNA sequencing computations. For this
purpose, many researchers have attempted to implement different tools in this
field and have competed aggressively. Hence, determining the best of them among
the enormous number of available tools is not an easy task, selecting the one
which accomplishes big data in the concise time and with no error can
significantly improve the scientist's contribution in the bioinformatics field.
This study uses different analysis and methods such as Fuzzy, Dempster-Shafer,
Murphy and Entropy Shannon to provide the most significant and reliable
evaluation of IoT-based computational intelligence tools for DNA sequence
analysis. The outcomes of this study can be advantageous to the bioinformatics
community, researchers and experts in big biological data.
","['\nZainab Alansari\n', '\nNor Badrul Anuar\n', '\nAmirrudin Kamsin\n', '\nSafeeullah Soomro\n', '\nMohammad Riyaz Belgaum\n']",,,http://dx.doi.org/10.1007/978-981-13-0224-4_31,cs.OH,['cs.OH'],10.1007/978-981-13-0224-4_31,,[]
"The Rise of Internet of Things (IoT) in Big Healthcare Data: Review and
  Open research Issues",http://arxiv.org/abs/1904.09270v1,2019-04-16T09:32:30Z,2019-04-16T09:32:30Z,"  Health is one of the sustainable development areas in all of the countries.
Internet of Things has a variety of use in this sector which was not studied
yet. The aim of this research is to prioritize IoT usage in the healthcare
sector to achieve sustainable development. The study is an applied descriptive
research according to data collection. As per the research methodology which is
FAHP, it is a single cross sectional survey research. After data collection,
the agreed paired comparison matrices, allocated to weighted criteria and the
priority of IoT usage were determined. Based on the research findings, the two
criteria of Economic Prosperity and Quality of Life achieved the highest
priority for IoT sustainable development in the healthcare sector. Moreover,
the top priorities for IoT in the area of health, according to the usage, were
identified as Ultraviolet Radiation, Dental Health and Fall Detection.
","['\nZainab Alansari\n', '\nSafeeullah Soomro\n', '\nMohammad Riyaz Belgaum\n', '\nShahaboddin Shamshirband\n']",,,http://dx.doi.org/10.1007/978-981-10-6875-1_66,cs.OH,['cs.OH'],10.1007/978-981-10-6875-1_66,,[]
"A Review on Energy Consumption Optimization Techniques in IoT Based
  Smart Building Environments",http://arxiv.org/abs/1904.09821v1,2019-04-16T12:37:45Z,2019-04-16T12:37:45Z,"  In recent years, due to the unnecessary wastage of electrical energy in
residential buildings, the requirement of energy optimization and user comfort
has gained vital importance. In the literature, various techniques have been
proposed addressing the energy optimization problem. The goal of each technique
was to maintain a balance between user comfort and energy requirements such
that the user can achieve the desired comfort level with the minimum amount of
energy consumption. Researchers have addressed the issue with the help of
different optimization algorithms and variations in the parameters to reduce
energy consumption. To the best of our knowledge, this problem is not solved
yet due to its challenging nature. The gap in the literature is due to the
advancements in the technology and drawbacks of the optimization algorithms and
the introduction of different new optimization algorithms. Further, many newly
proposed optimization algorithms which have produced better accuracy on the
benchmark instances but have not been applied yet for the optimization of
energy consumption in smart homes. In this paper, we have carried out a
detailed literature review of the techniques used for the optimization of
energy consumption and scheduling in smart homes. The detailed discussion has
been carried out on different factors contributing towards thermal comfort,
visual comfort, and air quality comfort. We have also reviewed the fog and edge
computing techniques used in smart homes.
","['\nAbdul Salam Shah\n', '\nHaidawati Nasir\n', '\nMuhammad Fayaz\n', '\nAdidah Lajis\n', '\nAsadullah Shah\n']",,"Information 2019, 10, 108",http://dx.doi.org/10.3390/info10030108,cs.OH,['cs.OH'],10.3390/info10030108,,[]
Smart Laptop Bag with Machine Learning for Activity Recognition,http://arxiv.org/abs/1904.11882v1,2019-04-14T06:29:29Z,2019-04-14T06:29:29Z,"  In todays world of smart living, the smart laptop bag, presented in this
paper, provides a better solution to keep track of our precious possessions and
monitoring them in real time. As the world moves towards a much tech-savvy
direction, the novel laptop bag discussed here facilitates the user to perform
location tracking, ambiance monitoring, user-state monitoring etc. in one
device. The innovative design uses cloud computing and machine learning
algorithms to monitor the health of the user and many parameters of the bag.
The emergency alert system in this bag could be trained to send appropriate
notifications to emergency contacts of the user, in case of abnormal health
conditions or theft of the bag. The experimental smart laptop bag uses deep
neural network, which was trained and tested over the various parameters from
the bag and produces above 95% accurate results.
","['\nDwij Sukeshkumar Sheth\n', '\nShantanu Singh\n', '\nPrakhar S Mathur\n', '\nVydeki D\n']",,,http://arxiv.org/abs/1904.11882v1,cs.OH,"['cs.OH', 'cs.LG', 'eess.SP']",,,[]
A Blockchain-based Educational Record Repository,http://arxiv.org/abs/1904.00315v1,2019-03-31T01:15:36Z,2019-03-31T01:15:36Z,"  The Blockchain technology was initially adopted to implement various
cryptocurrencies. Currently, Blockchain is foreseen as a general purpose
technology with a huge potential in many areas. Blockchain-based applications
have inherent characteristics like authenticity, immutability and consensus.
Beyond that, records stored on Blockchain ledger can be accessed any time and
from any location. Blockchain has a great potential for managing and
maintaining educational records. This paper presents a Blockchain-based
Educational Record Repository (BcER2) that manages and distributes educational
assets for academic and industry professionals. The BcER2 system allows
educational records like e-diplomas and e-certificates to be securely and
seamless transferred, shared and distributed by parties.
","['\nEmanuel E. Bessa\n', '\nJoberto S. B. Martins\n']","ADVANCE 2019 - International Workshop on ADVANCEs in ICT
  Infrastructures and Services",,http://dx.doi.org/10.5281/zenodo.2567524,cs.OH,['cs.OH'],10.5281/zenodo.2567524,,[]
Algorithms Clearly Beat Gamers at Quantum Moves. A Verification,http://arxiv.org/abs/1904.01008v4,2019-04-01T11:56:39Z,2019-07-02T13:06:01Z,"  The paper [S{\o}rensen et al., Nature 532] considers how human players
compare to algorithms for solving the Quantum Moves game BringHomeWater and
design new algorithms based on the intuition extracted from players. The claim
by [S{\o}rensen et al., Nature 532] is that players outperform widely used
algorithms, in particular the KASS algorithm, based on the Krotov algorithm,
and that player intuition is crucial to develop improved methods. However, as
initially discussed by D. Sels [D. Sels, Phys. Rev. A 97], a standard
Coordinate Ascent algorithm outperforms all players by a large margin. Albeit
D. Sels only compare to player solutions, the simple algorithm outperforms all
algorithms based on player solutions and Krotov, and it does so using much less
time and iterations. In this paper we elaborate on the methods discussed by D.
Sels and verify that the presented algorithm, solves the problem better than
all players and algorithms derived from player solutions in [S{\o}rensen et
al., Nature 532]. We also verify the theoretical analysis presented by D. Sels,
that gives a theoretically derived protocol that outperforms all players. We
add a comparison with gradient ascent or GRAPE. Starting from uniform random
values, GRAPE outperforms all players by a large margin. GRAPE works at least
as well as the methods from [S{\o}rensen et al., Nature 532] initialized with
player solutions. A standard analysis of the results from GRAPE provides a
starting point for GRAPE, that outperform all algorithms from [S{\o}rensen et
al., Nature 532]. We compare with a basic Krotov algorithm, and get results
similar to GRAPE, clearly outperforming players and the KASS algorithm. These
experiments verify and underline the result in [D. Sels, Phys. Rev. A 97] that
the conclusions from [S{\o}rensen et al., Nature 532] regarding algorithms are
untenable. In fact the opposite conclusions are true.
",['\nAllan Grønlund\n'],,,http://arxiv.org/abs/1904.01008v4,cs.OH,"['cs.OH', 'quant-ph']",,,[]
Anti-Turing Machine,http://arxiv.org/abs/1903.09653v1,2019-03-22T18:03:05Z,2019-03-22T18:03:05Z,"  The invention of CPU-centric computing paradigm was incredible breakthrough
of computer science that revolutionized our everyday life dramatically.
However, the CPU- centric paradigm is based on the Turing machine concept and,
as a result, expensive and power-hungry data transferring between the memory
and CPU core is inevitable operation. Anti-Turing machine paradigm can be based
on two fundamental principles: (1) data-centric computing, and (2)
decentralized computing. Anti-Turing machine is able to execute a special type
of programs. The commands of such program have to be addressed to the 2D or 3D
persistent memory space is able to process data in-place. This program should
not define the position or structure of data but it has to define the goal of
data processing activity. Generally speaking, it needs to consider the whole
memory space like the data transformation space. But the data placement,
particular algorithm implementation, and strategy of algorithm execution are
out of scope of the program.
",['\nViacheslav Dubeyko\n'],,,http://arxiv.org/abs/1903.09653v1,cs.OH,['cs.OH'],,,[]
"Numerical Algorithmic Science and Engineering within Computer Science:
  Rationale, Foundations and Organization",http://arxiv.org/abs/1903.08647v1,2019-03-20T00:44:25Z,2019-03-20T00:44:25Z,"  A re-calibration is proposed for ""numerical analysis"" as it arises
specifically within the broader, embracing field of modern computer science
(CS). This would facilitate research into theoretical and practicable models of
real-number computation at the foundations of CS, and it would also advance the
instructional objectives of the CS field. Our approach is premised on the key
observation that the great ""watershed"" in numerical computation is much more
between finite- and infinite-dimensional numerical problems than it is between
discrete and continuous numerical problems. A revitalized discipline for
numerical computation within modern CS can more accurately be defined as
""numerical algorithmic science & engineering (NAS&E), or more compactly, as
""numerical algorithmics,"" its focus being the algorithmic solution of numerical
problems that are either discrete, or continuous over a space of finite
dimension, or a combination of the two. It is the counterpart within modern CS
of the numerical analysis discipline, whose primary focus is the algorithmic
solution of continuous, infinite-dimensional numerical problems and their
finite-dimensional approximates, and whose specialists today have largely been
repatriated to departments of mathematics. Our detailed overview of NAS&E from
the viewpoints of rationale, foundations, and organization is preceded by a
recounting of the role played by numerical analysts in the evolution of
academic departments of computer science, in order to provide background for
NAS&E and place the newly-emerging discipline within its larger historical
context.
",['\nJohn Lawrence Nazareth\n'],29 pages,,http://arxiv.org/abs/1903.08647v1,cs.NA,"['cs.NA', 'cs.OH']",,,[]
Substation One-Line Diagram Automatic Generation and Visualization,http://arxiv.org/abs/1903.09495v1,2019-03-20T20:41:50Z,2019-03-20T20:41:50Z,"  In Energy Management System (EMS) applications and many other off-line
planning and study tools, one-line diagram (OLND) of the whole system and
stations is a straightforward view for planners and operators to design,
monitor, analyze, and control the power system. Large-scale power system OLND
is usually manually developed and maintained. The work is tedious,
time-consuming and ease to make mistake. Meanwhile, the manually created
diagrams are hard to be shared among the on-line and off-line systems. To save
the time and efforts to draw and maintain OLNDs, and provide the capability to
share the OLNDs, a tool to automatically develop substation based upon Common
Information Model (CIM) standard is needed. Currently, there is no standard
rule to draw the substation OLND. Besides, the substation layouts can be
altered from the typical formats in textbooks based on factors of economy,
efficiency, engineering practice, etc. This paper presents a tool on substation
OLND automatic generation and visualization. This tool takes the substation
CIM/E model as input, then automatically computes the coordinates of all
components and generates the substation OLND based on its components attributes
and connectivity relations. Evaluation of the proposed approach is presented
using a real provincial power system. Over 95\% of substation OLNDs are
decently presented and the rest are corner cases, needing extra effort to do
specific reconfiguration.
","['\nJing Hong\n', '\nYue Li\n', '\nYiran Xu\n', '\nChen Yuan\n', '\nHong Fan\n', '\nGuangyi Liu\n', '\nRenchang Dai\n']","6 pages, 6 figures, 1 table, accepted by 2019 IEEE PES ISGT ASIA",,http://arxiv.org/abs/1903.09495v1,cs.OH,"['cs.OH', 'cs.SY']",,,[]
"Simulation-Based Analytics for Fabrication Quality-Associated Decision
  Support",http://arxiv.org/abs/1903.10565v1,2019-03-18T16:09:41Z,2019-03-18T16:09:41Z,"  Automated, data-driven quality management systems, which facilitate the
transformation of data into useable information, are desired to enhance
decision-making processes. Integration of accurate, reliable, and
straightforward approaches that measure uncertainty of inspection processes are
instrumental for the successful implementation of automated, data-driven
quality management systems. This research has addressed these needs by
exploring and adapting Bayesian statistics-based approaches for fraction
nonconforming posterior distribution derivation purposes. Using these accurate
and reliable inputs, this research further develops novel, analytically-based
approaches to improve the practical function of traditional construction
fabrication quality management systems. Multiple descriptive and predictive
analytical functionalities are developed to support and augment
quality-associated decision-making processes. Multi-relational databases (e.g.,
quality management system, engineering design system, and cost management
system) from an industrial company in Edmonton, Canada, are investigated and
mapped to implement the novel system proposed. This research has contributed to
academic literature and practice by: (1) advancing decision-support systems for
construction management by developing a dynamic simulation environment that
uses real-time data to enhance simulation predictability; (2) developing
integrated analytical methods for improved modeling in fabrication
quality-associated decision making; and (3) creating reliable and interpretable
decision-support metrics for quality performance measurement, complexity
analysis, and rework cost management to reduce the data interpretation load of
practitioners and to uncover valuable knowledge and information from available
data sources.
",['\nWenying Ji\n'],,,http://dx.doi.org/10.7939/R3HX16598,cs.OH,"['cs.OH', 'stat.AP']",10.7939/R3HX16598,,[]
"MoA Interpretation of the Iterative Conjugate Gradient Method with Psi
  Reduction - A Tutorial to teach the Mathematically literate in Linear and
  Tensor Algebra: Part I",http://arxiv.org/abs/1904.02612v1,2019-03-24T17:09:00Z,2019-03-24T17:09:00Z,"  It is often difficult to learn new mathematics semantically and
syntactically, even when there are similarities in the words and meaning when
discussed aloud. The goal of this document is to facilitate learning through
explanations and definitions relating our common mathematical knowledge and
highlighting what is new. It is meant to be a working document that will evolve
based on feedback from target audiences, those mathematically literate in
linear and tensor algebra, those that want to learn MoA, Psi Calculus, and its
uses, those that want and need the ability to prove a design, either in
hardware or software through the ONF, Operational Normal Form, and those
wanting to exploit all resources optimally, especially when Tensor Algebra,
i.e. algorithms foundational to their application,are needed: Knowledge
Representation, Machine Learning, Signal Processing, AI, HPC, etc.
","['\nLenore Mullin\n', '\nPaul Sebexen\n']",,,http://arxiv.org/abs/1904.02612v1,cs.OH,"['cs.OH', 'cs.PF']",,,[]
"Power and Thermal Analysis of Commercial Mobile Platforms: Experiments
  and Case Studies",http://arxiv.org/abs/1904.09814v1,2019-03-19T16:26:20Z,2019-03-19T16:26:20Z,"  State-of-the-art mobile processors can deliver fast response time and high
throughput to maximize the user experience. However, high performance comes at
the expense of larger power density, which leads to higher skin temperatures.
Since this can degrade the user experience, there is a strong need for power
consumption and thermal analysis in mobile processors. In this paper, we first
perform experiments on the Nexus 6P phone to study the power, performance and
thermal behavior of modern smartphones. Using the insight from these
experiments, we propose a control algorithm that throttles select applications
without affecting other apps. We demonstrate our governor on the Exynos 5422
processor employed in the Odroid-XU3 board.
","['\nGanapati Bhat\n', '\nSuat Gumussoy\n', '\nUmit Y. Ogras\n']",To appear in proceedings of IEEE DATE 2019,,http://arxiv.org/abs/1904.09814v1,cs.OH,"['cs.OH', 'cs.DC']",,,[]
"Epistemological and Bibliometric Analysis of Ethics and Shared
  Responsibility Health Policy and IoT Systems",http://arxiv.org/abs/1903.12582v2,2019-03-08T13:42:53Z,2022-12-26T01:39:57Z,"  The focus in this paper is placed on shared responsibility and ethics in
health policy, specific to Internet of Things (IoT) devices in healthcare
systems. The article assesses how the introduction of IoT brings risks to the
security of medical systems. The justification for this research emerges from
the opportunities emerging from digital technologies for medical services, but
also creating a range of new cyber risks in the shared healthcare
infrastructure. Such concerns are often not visible to individual departments
in an integrated healthcare system. In addition, many healthcare organisations
do not possess cyber skills and are faced with barriers to the adoption of
smart manufacturing technologies, e.g., cost. These barriers trigger ethical
concerns related to responsibility of cyber risks in shared healthcare systems.
","['\nPetar Radanliev\n', '\nDavid De Roure\n']",,,http://dx.doi.org/10.3390/su13158355,cs.OH,['cs.OH'],10.3390/su13158355,,[]
"Grounds for trust: Essential Epistemic Opacity and Computational
  Reliabilism",http://arxiv.org/abs/1904.01052v1,2019-03-09T15:38:58Z,2019-03-09T15:38:58Z,"  Several philosophical issues in connection with computer simulations rely on
the assumption that results of simulations are trustworthy. Examples of these
include the debate on the experimental role of computer simulations
\cite{Parker2009, Morrison2009}, the nature of computer data
\cite{Barberousse2013, Humphreys2013}, and the explanatory power of computer
simulations \cite{Krohs2008, Duran2017}. The aim of this article is to show
that these authors are right in assuming that the results of computer
simulations are to be trusted when computer simulations are reliable processes.
After a short reconstruction of the problem of \textit{epistemic opacity}, the
article elaborates extensively on \textit{computational reliabilism}, a
specified form of process reliabilism with computer simulations located at the
center. The article ends with a discussion of four sources for computational
reliabilism, namely, verification and validation, robustness analysis for
computer simulations, a history of (un)successful implementations, and the role
of expert knowledge in simulations.
","['\nJuan M. Durán\n', '\nNico Formanek\n']",,,http://dx.doi.org/10.1007/s11023-018-9481-6,cs.OH,['cs.OH'],10.1007/s11023-018-9481-6,,[]
"Computer simulations in science and engineering - Concepts - Practices -
  Perspectives",http://arxiv.org/abs/1904.01053v1,2019-03-09T15:26:05Z,2019-03-09T15:26:05Z,"  The ubiquitous presence of computer simulations in all kinds of research
areas evidence their role as the new driving force for the advancement of
science and engineering research. Nothing seems to escape the image of success
that computer simulations project onto the research community and the general
public. One simple way to illustrate this consists of asking ourselves how
would contemporary science and engineering look like without the use of
computer simulations. The answer would certainly diverge from the current image
we have of scientific and engineering research.
  As much as computer simulations are successful, they are also methods that
fail in their purpose of inquiring about the world; and as much as researchers
make use of them, computer simulations raise important questions that are at
the heart of contemporary science and engineering practice. In this respect,
computer simulations make a fantastic subject of research for the natural
sciences, the social sciences, engineering and, as in our case, also for
philosophy. Studies on computer simulations touch upon many different facets of
scientific and engineering research and evoke philosophically inclined
questions of interpretation with close ties to problems in experimental
settings and engineering applications (...)
",['\nJuan M. Durán\n'],,,http://arxiv.org/abs/1904.01053v1,cs.OH,['cs.OH'],,,[]
"Varying the explanatory span: scientific explanation for computer
  simulations",http://arxiv.org/abs/1904.01054v1,2019-03-09T09:23:10Z,2019-03-09T09:23:10Z,"  This article aims to develop a new account of scientific explanation for
computer simulations. To this end, two questions are answered: what is the
explanatory relation for computer simulations? and what kind of epistemic gain
should be expected? For several reasons tailored to the benefits and needs of
computer simulations, these questions are better answered within the
unificationist model of scientific explanation. Unlike previous efforts in the
literature, I submit that the explanatory relation is between the simulation
model and the results of the simulation. I also argue that our epistemic gain
goes beyond the unificationist account, encompassing a practical dimension as
well.
",['\nJuan M. Durán\n'],,,http://dx.doi.org/10.1080/02698595.2017.1370929,cs.OH,['cs.OH'],10.1080/02698595.2017.1370929,,[]
"Pragmatic inference and visual abstraction enable contextual flexibility
  during visual communication",http://arxiv.org/abs/1903.04448v2,2019-03-11T17:18:16Z,2019-03-28T01:06:20Z,"  Visual modes of communication are ubiquitous in modern life --- from maps to
data plots to political cartoons. Here we investigate drawing, the most basic
form of visual communication. Participants were paired in an online environment
to play a drawing-based reference game. On each trial, both participants were
shown the same four objects, but in different locations. The sketcher's goal
was to draw one of these objects so that the viewer could select it from the
array. On `close' trials, objects belonged to the same basic-level category,
whereas on `far' trials objects belonged to different categories. We found that
people exploited shared information to efficiently communicate about the target
object: on far trials, sketchers achieved high recognition accuracy while
applying fewer strokes, using less ink, and spending less time on their
drawings than on close trials. We hypothesized that humans succeed in this task
by recruiting two core faculties: visual abstraction, the ability to perceive
the correspondence between an object and a drawing of it; and pragmatic
inference, the ability to judge what information would help a viewer
distinguish the target from distractors. To evaluate this hypothesis, we
developed a computational model of the sketcher that embodied both faculties,
instantiated as a deep convolutional neural network nested within a
probabilistic program. We found that this model fit human data well and
outperformed lesioned variants. Together, this work provides the first
algorithmically explicit theory of how visual perception and social cognition
jointly support contextual flexibility in visual communication.
","['\nJudith Fan\n', '\nRobert Hawkins\n', '\nMike Wu\n', '\nNoah Goodman\n']",29 pages; 5 figures; submitted draft of manuscript,,http://dx.doi.org/10.1007/s42113-019-00058-7,cs.OH,"['cs.OH', 'cs.AI']",10.1007/s42113-019-00058-7,,[]
Hyperspectral Calibration of Art: Acquisition and Calibration Workflows,http://arxiv.org/abs/1903.04651v1,2019-03-11T23:23:45Z,2019-03-11T23:23:45Z,"  Hyperspectral imaging has become an increasingly used tool in the analysis of
works of art. However, the quality of the acquired data and the processing of
that data to produce accurate and reproducible spectral image cubes can be a
challenge to many cultural heritage users. The calibration of data that is both
spectrally and spatially accurate is an essential step in order to obtain
useful and relevant results from hyperspectral imaging. Data that is too noisy
or inaccurate will produce sub-optimal results when used for pigment mapping,
the detection of hidden features, change detection or for quantitative spectral
documentation. To help address this, therefore, we will examine the specific
acquisition and calibration workflows necessary for works of art. These
workflows includes the key parameters that must be addressed during acquisition
and the essential steps and issues at each of the stages required during
post-processing in order to fully calibrate hyperspectral data. In addition we
will look in detail at the key issues that affect data quality and propose
practical solutions that can make significant differences to overall
hyperspectral image quality.
","['\nRuven Pillay\n', '\nJon Y Hardeberg\n', '\nSony George\n']",,,http://dx.doi.org/10.1080/01971360.2018.1549919,eess.IV,"['eess.IV', 'cs.OH']",10.1080/01971360.2018.1549919,,[]
"Research on the pixel-based and object-oriented methods of urban feature
  extraction with GF-2 remote-sensing images",http://arxiv.org/abs/1903.03412v1,2019-03-08T15:19:36Z,2019-03-08T15:19:36Z,"  During the rapid urbanization construction of China, acquisition of urban
geographic information and timely data updating are important and fundamental
tasks for the refined management of cities. With the development of domestic
remote sensing technology, the application of Gaofen-2 (GF-2) high-resolution
remote sensing images can greatly improve the accuracy of information
extraction. This paper introduces an approach using object-oriented
classification methods for urban feature extraction based on GF-2 satellite
data. A combination of spectral, spatial attributes and membership functions
was employed for mapping the urban features of Qinhuai District, Nanjing. The
data preprocessing is carried out by ENVI software, and the subsequent data is
exported into the eCognition software for object-oriented classification and
extraction of urban feature information. Finally, the obtained raster image
classification results are vectorized using the ARCGIS software, and the vector
graphics are stored in the library, which can be used for further analysis and
modeling. Accuracy assessment was performed using ground truth data acquired by
visual interpretation and from other reliable secondary data sources. Compared
with the result of pixel-based supervised (neural net) classification, the
developed object-oriented method can significantly improve extraction accuracy,
and after manual interpretation, an overall accuracy of 95.44% can be achieved,
with a Kappa coefficient of 0.9405, which objectively confirmed the superiority
of the object-oriented method and the feasibility of the utilization of GF-2
satellite data.
","['\nDong-dong Zhang\n', '\nLei Zhang\n', '\nVladimir Zaborovsky\n', '\nFeng Xie\n', '\nYan-wen Wu\n', '\nTing-ting Lu\n']",,,http://arxiv.org/abs/1903.03412v1,cs.OH,"['cs.OH', 'cs.CV', 'cs.LG', 'physics.data-an']",,,[]
"Optimal Clustering of Energy Consumers based on Entropy of the
  Correlation Matrix between Clusters",http://arxiv.org/abs/1903.01159v1,2019-03-04T10:21:45Z,2019-03-04T10:21:45Z,"  Increased deployment of residential smart meters has made it possible to
record energy consumption data on short intervals. These data, if used
efficiently, carry valuable information for managing power demand and
increasing energy consumption efficiency. However, analyzing smart meter data
of millions of customers in a timely manner is quite challenging. An efficient
way to analyze these data is to first identify clusters of customers, and then
focus on analyzing these clusters. Deciding on the optimal number of clusters
is a challenging task. In this manuscript, we propose a metric to efficiently
find the optimal number of clusters. A genetic algorithm based feature
selection is used to reduce the number of features, which are then fed into
self-organizing maps for clustering. We apply the proposed clustering technique
on two electricity consumption datasets from Victoria, Australia and Ireland.
The numerical simulations reveal effectiveness of the proposed method in
finding the optimal clusters.
","['\nNameer Al Khafaf\n', '\nMahdi Jalili\n', '\nPeter Sokolowski\n']","8 pages, 18 figures",,http://arxiv.org/abs/1903.01159v1,cs.OH,['cs.OH'],,,[]
"TinBiNN: Tiny Binarized Neural Network Overlay in about 5,000 4-LUTs and
  5mW",http://arxiv.org/abs/1903.06630v1,2019-03-05T14:51:36Z,2019-03-05T14:51:36Z,"  Reduced-precision arithmetic improves the size, cost, power and performance
of neural networks in digital logic. In convolutional neural networks, the use
of 1b weights can achieve state-of-the-art error rates while eliminating
multiplication, reducing storage and improving power efficiency. The
BinaryConnect binary-weighted system, for example, achieves 9.9% error using
floating-point activations on the CIFAR-10 dataset. In this paper, we introduce
TinBiNN, a lightweight vector processor overlay for accelerating inference
computations with 1b weights and 8b activations. The overlay is very small --
it uses about 5,000 4-input LUTs and fits into a low cost iCE40 UltraPlus FPGA
from Lattice Semiconductor. To show this can be useful, we build two embedded
'person detector' systems by shrinking the original BinaryConnect network. The
first is a 10-category classifier with a 89% smaller network that runs in
1,315ms and achieves 13.6% error. The other is a 1-category classifier that is
even smaller, runs in 195ms, and has only 0.4% error. In both classifiers, the
error can be attributed entirely to training and not reduced precision.
","['\nGuy G. F. Lemieux\n', '\nJoe Edwards\n', '\nJoel Vandergriendt\n', '\nAaron Severance\n', '\nRyan De Iaco\n', '\nAbdullah Raouf\n', '\nHussein Osman\n', '\nTom Watzka\n', '\nSatwant Singh\n']","Presented at 3rd International Workshop on Overlay Architectures for
  FPGAs (OLAF 2017) arXiv:1704.08802",,http://arxiv.org/abs/1903.06630v1,cs.DC,"['cs.DC', 'cs.CV', 'cs.OH']",,,[]
Trial of an AI: Empowering people to explore law and science challenges,http://arxiv.org/abs/1903.09518v1,2019-03-05T07:22:29Z,2019-03-05T07:22:29Z,"  Artificial Intelligence represents many things: a new market to conquer or a
quality label for tech companies, a threat for traditional industries, a menace
for democracy, or a blessing for our busy everyday life. The press abounds in
examples illustrating these aspects, but one should draw not hasty and
premature conclusions. The first successes in AI have been a surprise for
society at large-including researchers in the field. Today, after the initial
stupefaction, we have examples of the system reactions: traditional companies
are heavily investing in AI, social platforms are monitored during elections,
data collection is more and more regulated, etc. The resilience of an
organization (i.e. its capacity to resist to a shock) relies deeply on the
perception of its environment. Future problems have to be anticipated, while
unforeseen events occurring have to be quickly identified in order to be
mitigated as fast as possible. The author states that this clear perception
starts with a common definition of AI in terms of capacities and limits. AI
practitioners should make notions and concepts accessible to the general public
and the impacted fields (e.g. industries, law, education). It is a truism that
only law experts would have the potential to estimate IA impacts on judicial
system. However, questions remain on how to connect different kind of expertise
and what is the appropriate level of detail required for the knowledge
exchanges. And the same consideration is true for dissemination towards
society. Ultimately, society will live with decisions made by the ""experts"". It
sounds wise to involve society in the decision process rather than risking to
pay consequences later. Therefore, society also needs the key concepts to
understand AI impact on their life. This was the purpose of the trial of an IA
that took place in October 2018 at the Court of Appeal of Paris: gathering
experts from various fields to expose challenges in law and science towards a
general public.
",['\nGaudron Arthur\nCAOR\n'],,"IFIM's International Journal on Law & Regulation of Artificial
  Intelligence & Robotics, 2019, 1 (1)",http://arxiv.org/abs/1903.09518v1,cs.OH,"['cs.OH', 'cs.AI', 'cs.CY']",,,['CAOR']
On Double-Sided QR-Codes,http://arxiv.org/abs/1902.05722v1,2019-02-15T08:34:55Z,2019-02-15T08:34:55Z,"  Due to the widespread adoption of the smart mobile devices, QR codes have
become one of the most-known types of 2D codes around the world. However, the
data capacity properties of modern QR codes are still not perfect. To address
this issue, in this paper, we propose a novel approach to make double-sided QR
codes, which could carry two different messages in a straight and mirrored
position. To facilitate the process of creation of such codes we propose two
methods of their construction: the brute-force method and the analytic
solution.
",['\nAlexey Tikhonov\n'],SIGBOVIK 2019,,http://arxiv.org/abs/1902.05722v1,cs.OH,['cs.OH'],,,[]
Monitorology the art of observing the world,http://arxiv.org/abs/1902.09459v1,2019-02-22T09:30:08Z,2019-02-22T09:30:08Z,"  In the age of ever increasing demand for big data and data analytics, a
question of collecting the data becomes fundamental. What and how to collect
the data is essential as it has direct impact on decision making, system
operation and control. Specifically, we focus on the art of observing the world
by electronic devices such as sensors and meters that, in general, we call
monitors. We define five challenges to ensure effective and efficient
monitoring that still need a lot of research. Additionally, we illustrate each
challenge by example. Since reliance on big data and data analytics is
continuously increasing, these challenges will become ever more relevant to
save the world from flood of meaningless, dumb data, leading frequently to
false conclusions and wrong decisions whose impact may range from a minor
inconvenience to major disasters and even loss of lives.
",['\nMiroslaw Malek\n'],,,http://arxiv.org/abs/1902.09459v1,cs.OH,['cs.OH'],,,[]
Was ist eine Professur fuer Kuenstliche Intelligenz?,http://arxiv.org/abs/1903.09516v1,2019-02-17T14:35:11Z,2019-02-17T14:35:11Z,"  The Federal Government of Germany aims to boost the research in the field of
Artificial Intelligence (AI). For instance, 100 new professorships are said to
be established. However, the white paper of the government does not answer what
an AI professorship is at all. In order to give colleagues, politicians, and
citizens an idea, we present a view that is often followed when appointing
professors for AI at German and international universities. We hope that it
will help to establish a guideline with internationally accepted measures and
thus make the public debate more informed.
","['\nKristian Kersting\n', '\nJan Peters\n', '\nConstantin Rothkopf\n']",in German,,http://arxiv.org/abs/1903.09516v1,cs.OH,"['cs.OH', 'cs.AI']",,,[]
Trustworthy Experimentation Under Telemetry Loss,http://arxiv.org/abs/1903.12470v1,2019-01-22T01:36:01Z,2019-01-22T01:36:01Z,"  Failure to accurately measure the outcomes of an experiment can lead to bias
and incorrect conclusions. Online controlled experiments (aka AB tests) are
increasingly being used to make decisions to improve websites as well as mobile
and desktop applications. We argue that loss of telemetry data (during upload
or post-processing) can skew the results of experiments, leading to loss of
statistical power and inaccurate or erroneous conclusions. By systematically
investigating the causes of telemetry loss, we argue that it is not practical
to entirely eliminate it. Consequently, experimentation systems need to be
robust to its effects. Furthermore, we note that it is nontrivial to measure
the absolute level of telemetry loss in an experimentation system. In this
paper, we take a top-down approach towards solving this problem. We motivate
the impact of loss qualitatively using experiments in real applications
deployed at scale, and formalize the problem by presenting a theoretical
breakdown of the bias introduced by loss. Based on this foundation, we present
a general framework for quantitatively evaluating the impact of telemetry loss,
and present two solutions to measure the absolute levels of loss. This
framework is used by well-known applications at Microsoft, with millions of
users and billions of sessions. These general principles can be adopted by any
application to improve the overall trustworthiness of experimentation and
data-driven decision making.
","['\nJayant Gupchup\n', '\nYasaman Hosseinkashi\n', '\nPavel Dmitriev\n', '\nDaniel Schneider\n', '\nRoss Cutler\n', '\nAndrei Jefremov\n', '\nMartin Ellis\n']","Proceedings of the 27th ACM International Conference on Information
  and Knowledge Management, October 2018",,http://dx.doi.org/10.1145/3269206.3271747,cs.OH,['cs.OH'],10.1145/3269206.3271747,,[]
"Detecting Multiple Communities Using Quantum Annealing on the D-Wave
  System",http://arxiv.org/abs/1901.09756v1,2019-01-28T15:57:04Z,2019-01-28T15:57:04Z,"  A very important problem in combinatorial optimization is partitioning a
network into communities of densely connected nodes; where the connectivity
between nodes inside a particular community is large compared to the
connectivity between nodes belonging to different ones. This problem is known
as community detection, and has become very important in various fields of
science including chemistry, biology and social sciences. The problem of
community detection is a twofold problem that consists of determining the
number of communities and, at the same time, finding those communities. This
drastically increases the solution space for heuristics to work on, compared to
traditional graph partitioning problems. In many of the scientific domains in
which graphs are used, there is the need to have the ability to partition a
graph into communities with the ``highest quality'' possible since the presence
of even small isolated communities can become crucial to explain a particular
phenomenon. We have explored community detection using the power of quantum
annealers, and in particular the D-Wave 2X and 2000Q machines. It turns out
that the problem of detecting at most two communities naturally fits into the
architecture of a quantum annealer with almost no need of reformulation. This
paper addresses a systematic study of detecting two or more communities in a
network using a quantum annealer.
","['\nChristian F. A. Negre\n', '\nHayato Ushijima-Mwesigwa\n', '\nSusan M. Mniszewski\n']",,,http://dx.doi.org/10.1371/journal.pone.0227538,cs.OH,['cs.OH'],10.1371/journal.pone.0227538,,[]
"Information Operations Recognition: from Nonlinear Analysis to
  Decision-making",http://arxiv.org/abs/1901.10876v2,2019-01-22T14:20:00Z,2019-02-07T09:45:42Z,"  The book is dedicated to the issues of information operations recognition
based on analysis of information space, particularly, web-resources, social
networks, and blogs. In this context, open source intelligence technology
(OSINT) solves the problem of initial analysis of modern-time information
flows. The book provides a detailed description of mathematical principles of
information operations recognition, based on mathematical statistics, nonlinear
dynamics, complex networks theory, information and mathematical modeling,
sociology. A separate chapter covers the applications of approaches from expert
estimation theory and decision-making support to information operation
recognition. The book is addressed to a broad circle of specialists from
information technology and security domains.
","['\nAleksandr G. Dodonov\n', '\nDmitry V. Lande\n', '\nVitaliy V. Tsyganok\n', '\nOleh V. Andriichuk\n', '\nSergii V. Kadenko\n', '\nAnastasia N. Graivoronskaya\n']",Preprint of the Book. 280 pages,,http://arxiv.org/abs/1901.10876v2,cs.OH,['cs.OH'],,,[]
Teleporting digital images,http://arxiv.org/abs/1904.02066v7,2019-01-23T19:01:42Z,2019-08-18T21:20:22Z,"  During the last 25 years the scientific community has coexisted with the most
fascinating protocol due to Quantum Physics: quantum teleportation (QTele),
which would have been impossible if quantum entanglement, so questioned by
Einstein, did not exist. In this work, a complete architecture for the
teleportation of Computational Basis States (CBS) is presented. Such CBS will
represent each of the possible 24 classical bits commonly used to encode every
pixel of a 3-color-channel-image (red-green-blue, or cyan-yellow-magenta). For
this purpose, a couple of interfaces: classical-to-quantum (Cl2Qu) and
quantum-to-classical (Qu2Cl) are presented with two versions of the
teleportation protocol: standard and simplified.
",['\nMario Mastriani\n'],"25 pages, 14 figures. It was accepted for 104 Reunion de la
  Asociacion Fisica Argentina - Santa Fe - 2019",,http://arxiv.org/abs/1904.02066v7,cs.OH,['cs.OH'],,,[]
"Realize special instructions on clustering VLIW DSP:
  multiplication-accumulation instruction",http://arxiv.org/abs/1902.05982v1,2019-01-16T07:07:03Z,2019-01-16T07:07:03Z,"  BWDSP is a 32bit static scalar digital signal processor with VLIW and SIMD
features, which is designed for high-performance computing. Associated special
instructions are designed for its special architecture and application
scenarios. However, the existing compilation framework doesn't meet these
special instructions. Therefore, in the context of traditional Open64 compiler,
proposed a special instruction algorithm. Through this algorithm implements the
multiplication-accumulation operation with BWDSP structure, to improve the
performance of algorithms with multiply-accumulate requirements. Experimental
results show that the algorithm, which can make an maximum of 8.85 speedup on
BWDSP.
","['\nBinbin Liu\n', '\nQilong Zheng\n']","7 pages, in Chinese, 4 figures, 1 table",,http://arxiv.org/abs/1902.05982v1,cs.OH,"['cs.OH', 'eess.SP']",,,[]
Putting Natural Time into Science,http://arxiv.org/abs/1901.07357v1,2019-01-11T16:52:36Z,2019-01-11T16:52:36Z,"  This contribution argues that the notion of time used in the scientific
modeling of reality deprives time of its real nature. Difficulties from logic
paradoxes to mathematical incompleteness and numerical uncertainty ensue. How
can the emergence of novelty in the Universe be explained? How can the
creativity of the evolutionary process leading to ever more complex forms of
life be captured in our models of reality? These questions are deeply related
to our understanding of time. We argue here for a computational framework of
modeling that seems to us the only currently known type of modeling available
in Science able to capture aspects of the nature of time required to better
model and understand real phenomena.
","['\nRoger White\n', '\nWolfgang Banzhaf\n']","12 pages, book chapter",,http://arxiv.org/abs/1901.07357v1,cs.OH,"['cs.OH', 'cs.LO', 'math.LO']",,,[]
"Real-time Structural Health Monitoring System Using Internet of Things
  and Cloud Computing",http://arxiv.org/abs/1901.00670v1,2019-01-03T10:26:38Z,2019-01-03T10:26:38Z,"  Real-time monitoring of various structural behaviors, particularly
displacement and acceleration, serves important and valuable information for
people; for example, they can be used for active control or damage warning.
With recent advancement of the Internet of Things and client-side web
technologies, wireless integrated sensor devices nowadays can process real-time
raw sensor signal data into target measurements, such as displacement, and then
send the results through a standard protocol to the servers on the Internet.
The monitoring results are further processed for visualization purpose in the
servers and the computed results are pushed to connected clients like browsers
or mobile applications in real-time. We build a real-time cloud-based system
that can receive heterogeneous IoT data, allow users to create a
three-dimensional model online according to the real world structure, and the
monitoring results can be visualized in that model. In this paper, we
illustrate the software architecture of the proposed system and focus on the
technologies that are used, like client-side scripting, NoSql database, and
socket communication. We also present the challenges of displaying the overall
movement and shape transformation of the 3D structural model. Thus, each
internal-connected element's rotations and translations are obtained by
converting the monitoring results of each sensor device measured in the global
coordinate system. To overcome this, we create an inverted movement calculation
method. A simple 3D two-level structural model and simulated sensor
displacements are used to demonstrate system function and validate the inverted
movement calculation method.
","['\nHung-Fu Chang\n', '\nTzu-Kang Lin\n']",10 pages,,http://arxiv.org/abs/1901.00670v1,cs.OH,['cs.OH'],,,[]
Open Source Software Opportunities and Risks,http://arxiv.org/abs/1812.11697v1,2018-12-31T04:38:10Z,2018-12-31T04:38:10Z,"  Open Source Software (OSS) history is traced to initial efforts in 1971 at
Massachusetts Institute of Technology (MIT) Artificial Intelligence (AI) Lab,
the initial goals of OSS around Free vs. Freedom, and its evolution and impact
on commercial and custom applications. Through OSS history, much of the
research and has been around contributors (suppliers) to OSS projects, the
commercialization, and overall success of OSS as a development process. In
conjunction with OSS growth, intellectual property issues and licensing issues
still remain. The consumers of OSS, application architects, in developing
commercial or internal applications based upon OSS should consider license risk
as they compose their applications using Component Based Software Development
(CBSD) approaches, either through source code, binary, or standard protocols
such as HTTP.
","['\nJohn Sherlock\n', '\nManoj Muniswamaiah\n', '\nLauren Clarke\n', '\nShawn Cicoria\n']",,,http://arxiv.org/abs/1812.11697v1,cs.OH,['cs.OH'],,,[]
Towards Approximate Mobile Computing,http://arxiv.org/abs/1901.08972v1,2019-01-08T13:10:00Z,2019-01-08T13:10:00Z,"  Mobile computing is one of the main drivers of innovation, yet the future
growth of mobile computing capabilities remains critically threatened by
hardware constraints, such as the already extremely dense transistor packing
and limited battery capacity. The breakdown of Dennard scaling and stagnating
energy storage improvements further amplify these threats. However, the
computational burden we put on our mobile devices is not always justified. In a
myriad of situations the result of a computation is further manipulated,
interpreted, and finally acted upon. This allows for the computation to be
relaxed, so that the result is calculated with ""good enough"", not perfect
accuracy. For example, results of a Web search may be perfectly acceptable even
if the order of the last few listed items is shuffled, as an end user decides
which of the available links to follow. Similarly, the quality of a
voice-over-IP call may be acceptable, despite being imperfect, as long as the
two involved parties can clearly understand each other. This novel way of
thinking about computation is termed Approximate Computing (AC) and promises to
reduce resource usage, while ensuring that satisfactory performance is
delivered to end-users. AC is already experimented with on various levels of
desktop computer architecture, from the hardware level where incorrect adders
have been designed to sacrifice result correctness for reduced energy
consumption, to compiler-level optimisations that omit certain lines of code to
speed up video encoding. AC is yet to be attempted on mobile devices and in
this article we examine the potential benefits of mobile AC and present an
overview of AC techniques applicable in the mobile domain.
",['\nVeljko Pejovic\n'],,,http://arxiv.org/abs/1901.08972v1,cs.OH,"['cs.OH', 'cs.PL']",,,[]
Towards Modernising Data Collection and Archive for the Tor Network,http://arxiv.org/abs/1812.08429v1,2018-12-20T09:17:45Z,2018-12-20T09:17:45Z,"  CollecTor is developed by Tor Project's Metrics Team for the purpose of
archiving data relating to the public Tor network and applications developed by
Tor Project. This report distills the requirements for a prototype modernized
replacement of the CollecTor service, and evaluates frameworks and libraries
that are available to reduce code maintenance costs for the CollecTor service.
","['\nIain R. Learmonth\n', '\nKarsten Loesing\n']",27pp,,http://arxiv.org/abs/1812.08429v1,cs.OH,"['cs.OH', 'cs.CR']",,,[]
Markov chain aggregation and its application to rule-based modelling,http://arxiv.org/abs/1812.09774v1,2018-12-23T20:50:11Z,2018-12-23T20:50:11Z,"  Rule-based modelling allows to represent molecular interactions in a compact
and natural way. The underlying molecular dynamics, by the laws of stochastic
chemical kinetics, behaves as a continuous-time Markov chain. However, this
Markov chain enumerates all possible reaction mixtures, rendering the analysis
of the chain computationally demanding and often prohibitive in practice. We
here describe how it is possible to efficiently find a smaller, aggregate
chain, which preserves certain properties of the original one. Formal methods
and lumpability notions are used to define algorithms for automated and
efficient construction of such smaller chains (without ever constructing the
original ones). We here illustrate the method on an example and we discuss the
applicability of the method in the context of modelling large signalling
pathways.
",['\nTatjana Petrov\n'],,,http://arxiv.org/abs/1812.09774v1,cs.OH,"['cs.OH', 'q-bio.MN']",,,[]
"An Interactive, Graphical CPU Scheduling Simulator for Teaching
  Operating Systems",http://arxiv.org/abs/1812.05160v2,2018-12-12T21:24:49Z,2019-11-01T21:19:12Z,"  We present a graphical simulation tool for visually and interactively
exploring the processing of various events handled by an operating system when
running a program. Our graphical simulator is available for use on the web and
locally by both instructors and students for purposes of pedagogy. Instructors
can use it for live demonstrations of course concepts in class, while students
can use it outside of class to explore the concepts. The graphical simulation
tool is implemented using the React library for the fancy ui elements of the
Node.js framework and is available as a single page web application at
https://cpudemo.azurewebsites.net. Assigning the development of the underling
text-based simulation engine, on which the graphical simulator runs, to
students as a course project is also an effective approach to teach students
the concepts. The goals of this paper are to showcase the demonstrative
capabilities of the tool for instruction, share student experiences in
developing the engine underlying the simulation, and to inspire its use by
other educators.
","['\nJoshua W. Buck\n', '\nSaverio Perugini\n']","25 pages, 18 figures","Journal of Computing Sciences in Colleges, 35(5), 76-87 (2016).
  USA: Consortium for Computing Sciences in Colleges",http://arxiv.org/abs/1812.05160v2,cs.OH,['cs.OH'],,,[]
Schrödinger's Man,http://arxiv.org/abs/1812.05839v1,2018-12-14T09:32:12Z,2018-12-14T09:32:12Z,"  What if someone built a ""box"" that applies quantum superposition not just to
quantum bits in the microscopic but also to macroscopic everyday ""objects"",
such as Schr\""odinger's cat or a human being? If that were possible, and if the
different ""copies"" of a man could exploit quantum interference to synchronize
and collapse into their preferred state, then one (or they?) could in a sense
choose their future, win the lottery, break codes and other security devices,
and become king of the world, or actually of the many-worlds. We set up the
plot-line of a new episode of Black Mirror to reflect on what might await us if
one were able to build such a technology.
","['\nLuca Viganò\n', '\nDiego Sempreboni\n']","4 pages, 0 figures, Accepted at the ""Re-Coding Black Mirror"" workshop
  of the International Conference Data Protection and Democracy (CPDP)",,http://arxiv.org/abs/1812.05839v1,cs.OH,['cs.OH'],,,[]
"An intelligent household greenhouse system design based on Internet of
  Things",http://arxiv.org/abs/1812.11230v1,2018-12-12T07:30:45Z,2018-12-12T07:30:45Z,"  In order to combine indoor greenery conservation with Internet of Things
(IOT) Technologies, this paper designs an intelligent household greenhouse
project with the features of comprehensive sensing, reliable transmission and
intelligent processing. Through the analysis of functional requirements of the
intelligent household greenhouse system, an intelligent household greenhouse
system is designed with the functions of greenhouse environmental data
detection, greenhouse environmental control regulation, data remote
transmission and human-computer interaction. Its sensor layer collects
environmental data in real time based on the ZigBee wireless sensor network.
The network layer STM32 intelligent gateway coordinates with network server, so
as to exchange data from sensor layer to application layer, and solve the
problems of non-blocking of data sending and receiving as well as concurrent
requests of multiple mobile terminals. The application layer is designed into
two types. One is a desktop management system as a data storage and analysis
center, and the other is a mobile terminal APP. At the same time, we design a
communication protocol that is applicable to the interaction of the three-layer
structure of the Internet of Things, with the characteristics of simplicity,
stability, readability, and scalability. It can avoid the mutual influence of
multi-level data exchange and ensure the correctness of data circulation. In
the design, the system sensor layer ensures stable transmission of various data
and instructions, and the network layer has a high degree of concurrency and
real time. And various measurement and control data of the sensor layer can
interact with the data of mobile-terminal equipment of the application layer.
The desktop management system and mobile terminal APP can monitor greenhouse
data in real time and control various actuators in the greenhouse.
","['\nZhonghua Han\n', '\nZhenbo Wu\n', '\nShuo Lin\n', '\nFangjun Luan\n']",,,http://dx.doi.org/10.1088/1757-899X/399/1/012024,cs.OH,['cs.OH'],10.1088/1757-899X/399/1/012024,,[]
"Solving High Volume Capacitated Vehicle Routing Problem with Time
  Windows using Recursive-DBSCAN clustering algorithm",http://arxiv.org/abs/1812.02300v2,2018-12-05T14:01:21Z,2019-03-23T08:18:03Z,"  This paper introduces a new approach to improve the performance of the
Capacitated Vehicle Routing Problem with Time Windows (CVRPTW) solvers for a
high number of nodes. It proposes to cluster nodes together using
Recursive-DBSCAN - an algorithm that recursively applies DBSCAN until clusters
below the preset maximum number of nodes are obtained. That approach leads to
61% decrease in runtimes of the CVRPTW solver as benchmarked against Google
Optimization Tools, while the difference of total distance and number of
vehicles used by found solutions is below 7%. The improvement of runtimes with
the Recursive-DBSCAN method is because of splitting the node-set into
constituent clusters, which limits the number of solutions checked by the
solver, consequently reducing the runtime. The proposed method consumes less
memory and is able to find solutions for problems up to 5000 nodes, while the
baseline Google Optimisation Tools solves problems up to 2000 nodes.
","['\nKamil Bujel\n', '\nFeiko Lai\n', '\nMichal Szczecinski\n', '\nWinnie So\n', '\nMiguel Fernandez\n']",Draft,,http://arxiv.org/abs/1812.02300v2,cs.OH,['cs.OH'],,,[]
"Climate Anomalies vs Air Pollution: Carbon Emissions and Anomaly
  Networks",http://arxiv.org/abs/1812.02634v1,2018-12-06T16:14:44Z,2018-12-06T16:14:44Z,"  This project aims to shed light on how man-made carbon emissions are
affecting global wind patterns by looking for temporal and geographical
correlations between carbon emissions, surface temperatures anomalies, and wind
speed anomalies at high altitude. We use a networks-based approach and daily
data from 1950 to 2010 [1-3] to model and draw correlations between disparate
regions of the globe.
","['\nAnshul Goyal\n', '\nKartikeya Bhardwaj\n', '\nRadu Marculescu\n']","This is a class project report for CMU course 18-755 in Fall 2016. 7
  pages, 19 figures",,http://arxiv.org/abs/1812.02634v1,cs.OH,['cs.OH'],,,[]
"RoboCup Junior in the Hunter Region: Driving the Future of Robotic STEM
  Education",http://arxiv.org/abs/1901.03229v1,2018-12-04T05:22:52Z,2018-12-04T05:22:52Z,"  RoboCup Junior is a project-oriented educational initiative that sponsors
regional, national and international robotic events for young students in
primary and secondary school. It leads children to the fundamentals of teamwork
and complex problem solving through step-by-step logical thinking using
computers and robots. The Faculty of Engineering and Built Environment at the
University of Newcastle in Australia has hosted and organized the Hunter
regional tournament since 2012. This paper presents an analysis of data
collected from RoboCup Junior in the Hunter Region, New South Wales, Australia,
for a period of six years 2012-2017 inclusive. Our study evaluates the
effectiveness of the competition in terms of geographical spread, participation
numbers, and gender balance. We also present a case study about current
university students who have previously participated in RoboCup Junior.
","['\nAaron S. W. Wong\n', '\nRyan Jeffery\n', '\nPeter Turner\n', '\nScott Sleap\n', '\nStephan K. Chalup\n']","12 pages, 3 Figures, RoboCup Symposium 2018 (Accepted, in Press)",,http://arxiv.org/abs/1901.03229v1,cs.OH,"['cs.OH', 'cs.CY', 'cs.RO']",,,[]
ACTT: Automotive CAN Tokenization and Translation,http://arxiv.org/abs/1811.07897v1,2018-11-19T17:02:16Z,2018-11-19T17:02:16Z,"  Modern vehicles contain scores of Electrical Control Units (ECUs) that
broadcast messages over a Controller Area Network (CAN). Vehicle manufacturers
rely on security through obscurity by concealing their unique mapping of CAN
messages to vehicle functions which differs for each make, model, year, and
even trim. This poses a major obstacle for after-market modifications notably
performance tuning and in-vehicle network security measures. We present ACTT:
Automotive CAN Tokenization and Translation, a novel, vehicle-agnostic,
algorithm that leverages available diagnostic information to parse CAN data
into meaningful messages, simultaneously cutting binary messages into tokens,
and learning the translation to map these contiguous bits to the value of the
vehicle function communicated.
","['\nMiki E. Verma\n', '\nRobert A. Bridges\n', '\nSamuel C. Hollifield\n']","5th Annual Conference on Computational Science & Computational
  Intelligence (CSCI'18)",,http://arxiv.org/abs/1811.07897v1,cs.OH,"['cs.OH', 'stat.AP']",,,[]
Design paradigms of intelligent control systems on a chip,http://arxiv.org/abs/1811.08426v1,2018-11-20T12:16:26Z,2018-11-20T12:16:26Z,"  This paper focuses on the Field Programmable Gate Array (FPGA) design and
implementation of intelligent control system applications on a chip,
specifically fuzzy logic and genetic algorithm processing units. Initially, an
overview of the FPGA technology is presented, followed by design methodologies,
development tools and the use of hardware description languages (HDL). Two FPGA
design examples with the use of Hardware Description Languages (HDLs) of
parameterized fuzzy logic controller cores are discussed. Thereinafter, a
System-on-a-Chip (SoC) designed by the authors in previous work and realized on
FPGA featuring a Digital Fuzzy Logic Controller (DFLC) and a soft processor
core for the path tracking problem of mobile robots is discussed. Finally a
Genetic Algorithm implementation (previously published by the authors) in FPGA
chip for the Traveling Salesman Problem (TSP) is also discussed.
","['\nK. M. Deliparaschos\n', '\nS. G. Tzafestas\n']",,"Panhellenic Conf. on Electronics and Telecommunications (PACET
  2009), Patras, Greece, 20-22 Mar. 2009",http://arxiv.org/abs/1811.08426v1,cs.OH,"['cs.OH', 'cs.AR']",,,[]
"Exploring the Scope of Unconstrained Via Minimization by Recursive
  Floorplan Bipartitioning",http://arxiv.org/abs/1811.05161v1,2018-11-13T08:32:48Z,2018-11-13T08:32:48Z,"  Random via failure is a major concern for post-fabrication reliability and
poor manufacturing yield. A demanding solution to this problem is redundant via
insertion during post-routing optimization. It becomes very critical when a
multi-layer routing solution already incurs a large number of vias. Very few
global routers addressed unconstrained via minimization (UVM) problem, while
using minimal pattern routing and layer assignment of nets. It also includes a
recent floorplan based early global routability assessment tool STAIRoute
\cite{karb2}.
  This work addresses an early version of unconstrained via minimization
problem during early global routing by identifying a set of minimal bend
routing regions in any floorplan, by a new recursive bipartitioning framework.
These regions facilitate monotone pattern routing of a set of nets in the
floorplan by STAIRoute. The area/number balanced floorplan bipartitionining is
a multi-objective optimization problem and known to be NP-hard \cite{majum2}.
No existing approaches considered bend minimization as an objective and some of
them incurred higher runtime overhead. In this paper, we present a Greedy as
well as randomized neighbor search based staircase wave-front propagation
methods for obtaining optimal bipartitioning results for minimal bend routing
through multiple routing layers, for a balanced trade-off between routability,
wirelength and congestion.
  Experiments were conducted on MCNC/GSRC floorplanning benchmarks for studying
the variation of early via count obtained by STAIRoute for different values of
the trade-off parameters ($\gamma, \beta$) in this multi-objective optimization
problem, using $8$ metal layers. We studied the impact of ($\gamma, \beta$)
values on each of the objectives as well as their linear combination function
$Gain$ of these objectives.
","['\nBapi Kar\n', '\nSusmita Sur-Kolay\n', '\nChittaranjan Mandal\n']","A draft aimed at ACM TODAES journal, 25 pages with 16 figures and 2
  tables",,http://arxiv.org/abs/1811.05161v1,cs.OH,['cs.OH'],,,[]
Future Perspectives of Co-Simulation in the Smart Grid Domain,http://arxiv.org/abs/1811.06862v1,2018-11-15T09:46:21Z,2018-11-15T09:46:21Z,"  The recent attention towards research and development in cyber-physical
energy systems has introduced the necessity of emerging multi-domain
co-simulation tools. Different educational, research and industrial efforts
have been set to tackle the co-simulation topic from several perspectives. The
majority of previous works has addressed the standardization of models and
interfaces for data exchange, automation of simulation, as well as improving
performance and accuracy of co-simulation setups. Furthermore, the domains of
interest so far have involved communication, control, markets and the
environment in addition to physical energy systems. However, the current
characteristics and state of co-simulation testbeds need to be re-evaluated for
future research demands. These demands vary from new domains of interest, such
as human and social behavior models, to new applications of co-simulation, such
as holistic prognosis and system planning. This paper aims to formulate these
research demands that can then be used as a road map and guideline for future
development of co-simulation in cyber-physical energy systems.
","['\nCornelius Steinbrink\n', '\nFlorian Schlögl\n', '\nDavood Babazadeh\n', '\nSebastian Lehnhoff\n', '\nSebastian Rohjans\n', '\nAnand Narajan\n']",,IEEE International Energy Conference (2018),http://arxiv.org/abs/1811.06862v1,cs.OH,['cs.OH'],,,[]
Smart Grid Co-Simulation with MOSAIK and HLA: A Comparison Study,http://arxiv.org/abs/1811.06877v1,2018-11-15T08:25:29Z,2018-11-15T08:25:29Z,"  Evaluating new technological developments for energy systems is becoming more
and more complex. The overall application environment is a continuously growing
and interconnected cyber-physical system so that analytical assessment is
practically impossible to realize. Consequently, new solutions must be
evaluated in simulation studies. Due to the interdisciplinarity of the
simulation scenarios, various heterogeneous tools must be connected. This
approach is known as co-simulation. During the last years, different approaches
have been developed or adapted for applications in energy systems. In this
paper, two co-simulation approaches are compared that follow generic, versatile
concepts. The tool mosaik, which has been explicitly developed for the purpose
of co-simulation in complex energy systems, is compared to the High Level
Architecture (HLA), which possesses a domain-independent scope but is often
employed in the energy domain. The comparison is twofold, considering the
tools' conceptual architectures as well as results from the simulation of
representative test cases. It suggests that mosaik may be the better choice for
entry-level, prototypical co-simulation while HLA is more suited for complex
and extensive studies.
","['\nCornelius Steinbrink\n', '\nArjen A. van der Meer\n', '\nMilos Cvetkovic\n', '\nDavood Babazadeh\n', '\nSebastian Rohjans\n', '\nPeter Palensky\n', '\nSebastian Lehnhoff\n']",,Computer Science-Research and Development (2018),http://dx.doi.org/10.1007/s00450-017-0379-y,cs.OH,['cs.OH'],10.1007/s00450-017-0379-y,,[]
"Estimating Traffic Conditions At Metropolitan Scale Using Traffic Flow
  Theory",http://arxiv.org/abs/1810.12295v1,2018-10-29T15:43:46Z,2018-10-29T15:43:46Z,"  The rapid urbanization and increasing traffic have serious social, economic,
and environmental impact on metropolitan areas worldwide. It is of a great
importance to understand the complex interplay of road networks and traffic
conditions. The authors propose a novel framework to estimate traffic
conditions at the metropolitan scale using GPS traces. Their approach begins
with an initial estimation of network travel times by solving a convex
optimization program based on traffic flow theory. Then, they iteratively
refine the estimated network travel times and vehicle traversed paths. Last,
the authors perform a bilevel optimization process to estimate traffic
conditions on road segments that are not covered by GPS data. The evaluation
and comparison of the authors' approach over two state-of-the-art methods show
up to 96.57% relative improvements. The authors have further conducted field
tests by coupling road networks of San Francisco and Beijing with real-world
GIS data, which involve 128,701 nodes, 148,899 road segments, and over 26
million GPS traces.
","['\nWeizi Li\n', '\nMeilei Jiang\n', '\nYaoyu Chen\n', '\nMing C. Lin\n']","Accepted in Transportation Research Board 97th Annual Meeting, 2018",,http://arxiv.org/abs/1810.12295v1,cs.OH,['cs.OH'],,,[]
"Early Routability Assessment in VLSI Floorplans: A Generalized Routing
  Model",http://arxiv.org/abs/1810.12789v1,2018-10-30T14:55:38Z,2018-10-30T14:55:38Z,"  Multiple design iterations are inevitable in nanometer Integrated Circuit
(IC) design flow until desired printability and performance metrics are
achieved. This starts with placement optimization aimed at improving
routability, wirelength, congestion and timing in the design. Contrarily, no
such practice exists on a floorplanned layout, during the early stage of the
design flow. Recently, STAIRoute \cite{karb2} aimed to address that by
identifying the shortest routing path of a net through a set of routing regions
in the floorplan in multiple metal layers. Since the blocks in hierarchical
ASIC/SoC designs do not use all the permissible routing layers for the internal
routing corresponding to standard cell connectivity, the proposed STAIRoute
framework is not an effective for early global routability assessment. This
leads to improper utilization of routing area, specifically in higher routing
layers with fewer routing blockages, as the lack of placement of standard cells
does not facilitates any routing of their interconnections.
  This paper presents a generalized model for early global routability
assessment, HGR, by utilizing the free regions over the blocks beyond certain
metal layers. The proposed (hybrid) routing model comprises of (a) the junction
graph model in STAIRoute routing through the block boundary regions in lower
routing layers, and (ii) the grid graph model for routing in higher layers over
the free regions of the blocks.
  Experiment with the latest floorplanning benchmarks exhibit an average
reduction of $4\%$, $54\%$ and $70\%$ in netlength, via count, and congestion
respectively when HGR is used over STAIRoute. Further, we conducted another
experiment on an industrial design flow targeted for $45nm$ process, and the
results are encouraging with $~3$X runtime boost when early global routing is
used in conjunction with the existing physical design flow.
","['\nBapi Kar\n', '\nSusmita Sur-Kolay\n', '\nChittaranjan Mandal\n']","A draft of 24 pages aimed at ACM-TODAES Journal, with 10 figures and
  5 tables",,http://arxiv.org/abs/1810.12789v1,cs.OH,['cs.OH'],,,[]
"Real-Time Fine-Grained Air Quality Sensing Networks in Smart City:
  Design, Implementation and Optimization",http://arxiv.org/abs/1810.08514v2,2018-10-18T07:25:28Z,2019-02-27T02:36:51Z,"  Driven by the increasingly serious air pollution problem, the monitoring of
air quality has gained much attention in both theoretical studies and practical
implementations. In this paper, we present the architecture, implementation and
optimization of our own air quality sensing system, which provides real-time
and fine-grained air quality map of the monitored area. As the major component,
the optimization problem of our system is studied in detail. Our objective is
to minimize the average joint error of the established real-time air quality
map, which involves data inference for the unmeasured data values. A deep
Q-learning solution has been proposed for the power control problem to
reasonably plan the sensing tasks of the power-limited sensing devices online.
A genetic algorithm has been designed for the location selection problem to
efficiently find the suitable locations to deploy limited number of sensing
devices. The performance of the proposed solutions are evaluated by
simulations, showing a significant performance gain when adopting both
strategies.
","['\nZhiwen Hu\n', '\nZixuan Bai\n', '\nKaigui Bian\n', '\nTao Wang\n', '\nLingyang Song\n']","17 pages, 13 figures, IEEE Internet of Things Journal, accepted",,http://arxiv.org/abs/1810.08514v2,cs.OH,['cs.OH'],,,[]
Human-Competitive Awards 2018,http://arxiv.org/abs/1810.09416v1,2018-10-22T17:36:09Z,2018-10-22T17:36:09Z,"  Report on Humies competition at GECCO 2018 in Japan
",['\nW. B. Langdon\n'],To appear in SIGEVOlution,,http://arxiv.org/abs/1810.09416v1,cs.OH,['cs.OH'],,,[]
"STAIRoute: Early Global Routing using Monotone Staircases for Congestion
  Reduction",http://arxiv.org/abs/1810.10412v2,2018-10-24T14:14:44Z,2020-04-06T13:57:34Z,"  With aggressively shrinking process nodes, physical design methods face
severe challenges due to poor convergence and uncertainty in getting an optimal
solution. An early detection of potential failures is thus mandated. This has
encouraged to devise a feedback mechanism from a lower abstraction level of the
design flow to the higher ones, such as placement driven synthesis, routability
(timing) driven placement etc.
  Motivated by this, we propose an early global routing framework using pattern
routing following the floorplanning stage. We assess feasibility of a floorplan
topology of a given design by estimating routability, routed wirelength and
vias count while addressing the global congestion scenario across the layout.
Different capacity profiles for the routing regions, such as uniform or
non-uniform different cases of metal pitch variation across the metals layers
ensures adaptability to technology scaling. The proposed algorithm STAIRoute
takes $O(n^2kt)$ time for a given design with $n$ blocks and $k$ nets having at
most $t$ terminals. Experimental results on a set of floorplanning benchmark
circuits show $100\%$ routing completion, with no over-congestion in the
routing regions reported. The wirelength for the $t$-terminal ($t\geq$ 2) nets
is comparable with the Steiner length computed by FLUTE. An estimation on the
number of vias for different capacity profiles is also presented, along with
congestion and runtime results.
","['\nBapi Kar\n', '\nSusmita Sur-Kolay\n', '\nChittaranjan Mandal\n']","26 pages, 12 Figures, Springer Book chapter format",,http://arxiv.org/abs/1810.10412v2,cs.OH,['cs.OH'],,,[]
FSS++ Workshop Report: Handling Uncertainty for Data Quality Management,http://arxiv.org/abs/1810.02091v1,2018-10-04T08:15:15Z,2018-10-04T08:15:15Z,"  This report describes the results of the eSCF Awareness Workshop on Handling
Uncertainty for Data Quality Management - Challenges from Transport and Supply
Chain Management that was held on June 5, 2018 in Heeze, The Netherlands. The
goal of this workshop was to create and enhance awareness into data quality
management issues that are encountered in practice, for business organizations
that aim to integrate a data-analytical mind set into their operations.
",['\nAnna Wilbik\n'],,,http://arxiv.org/abs/1810.02091v1,cs.OH,['cs.OH'],,,[]
Ten Simple Rules for Reproducible Research in Jupyter Notebooks,http://arxiv.org/abs/1810.08055v1,2018-10-13T07:42:14Z,2018-10-13T07:42:14Z,"  Reproducibility of computational studies is a hallmark of scientific
methodology. It enables researchers to build with confidence on the methods and
findings of others, reuse and extend computational pipelines, and thereby drive
scientific progress. Since many experimental studies rely on computational
analyses, biologists need guidance on how to set up and document reproducible
data analyses or simulations.
  In this paper, we address several questions about reproducibility. For
example, what are the technical and non-technical barriers to reproducible
computational studies? What opportunities and challenges do computational
notebooks offer to overcome some of these barriers? What tools are available
and how can they be used effectively?
  We have developed a set of rules to serve as a guide to scientists with a
specific focus on computational notebook systems, such as Jupyter Notebooks,
which have become a tool of choice for many applications. Notebooks combine
detailed workflows with narrative text and visualization of results. Combined
with software repositories and open source licensing, notebooks are powerful
tools for transparent, collaborative, reproducible, and reusable data analyses.
","['\nAdam Rule\n', '\nAmanda Birmingham\n', '\nCristal Zuniga\n', '\nIlkay Altintas\n', '\nShih-Cheng Huang\n', '\nRob Knight\n', '\nNiema Moshiri\n', '\nMai H. Nguyen\n', '\nSara Brin Rosenthal\n', '\nFernando Pérez\n', '\nPeter W. Rose\n']",,,http://arxiv.org/abs/1810.08055v1,cs.OH,"['cs.OH', 'cs.CY']",,,[]
"UVM Based Reusable Verification IP for Wishbone Compliant SPI Master
  Core",http://arxiv.org/abs/1809.10845v1,2018-09-28T03:48:50Z,2018-09-28T03:48:50Z,"  The System on Chip design industry relies heavily on functional verification
to ensure that the designs are bug-free. As design engineers are coming up with
increasingly dense chips with much functionality, the functional verification
field has advanced to provide modern verification techniques. In this paper, we
present verification of a wishbone compliant Serial Peripheral Interface (SPI)
Master core using a System Verilog based standard verification methodology, the
Universal Verification Methodology (UVM). The reason for using UVM factory
pattern with parameterized classes is to develop a robust and reusable
verification IP. SPI is a full duplex communication protocol used to interface
components most likely in embedded systems. We have verified an SPI Master IP
core design that is wishbone compliant and compatible with SPI protocol and bus
and furnished the results of our verification. We have used QuestaSim for
simulation and analysis of waveforms, Integrated Metrics Center, Cadence for
coverage analysis. We also propose interesting future directions for this work
in developing reliable systems.
","['\nLakhan Shiva Kamireddy\n', '\nLakhan Saiteja Kamireddy\n']","5 pages, 6 figures",,http://arxiv.org/abs/1809.10845v1,cs.OH,['cs.OH'],,,[]
DATC RDF: An Open Design Flow from Logic Synthesis to Detailed Routing,http://arxiv.org/abs/1810.01078v2,2018-10-02T05:43:44Z,2018-10-10T15:02:48Z,"  In this paper, we present DATC Robust Design Flow (RDF) from logic synthesis
to detailed routing. Our goals are 1) to provide an open-source academic design
flow from logic synthesis to detailed routing based on existing contest
results, 2) to construct a database for design benchmarks and point tool
libraries, and 3) to interact with industrial designs by using industrial
standard design input/output formats. We also demonstrate RDF in a scalable
cloud infrastructure. Design methodology and cross-stage optimization research
can be conducted via RDF.
","['\nJinwook Jung\n', '\nIris Hui-Ru Jiang\n', '\nJianli Chen\n', '\nShih-Ting Lin\n', '\nYih-Lang Li\n', '\nVictor N. Kravets\n', '\nGi-Joon Nam\n']",,,http://arxiv.org/abs/1810.01078v2,cs.OH,['cs.OH'],,,[]
"An Automated System for Checking Lithography Friendliness of Standard
  Cells",http://arxiv.org/abs/1810.01446v1,2018-10-02T18:29:05Z,2018-10-02T18:29:05Z,"  At advanced process nodes, lithography weakpoints can exist in physical
layouts of integrated circuit designs even if the layouts pass design rule
checking (DRC). Existence of lithography weakpoints in a physical layout can
cause manufacturability issues, which in turn can result in yield losses. In
our experiments, we have found that specific standard cells have tendencies to
create lithography weakpoints after their cell instances are placed and routed,
even though each of these cells does not contain any lithography weakpoint
before performing placement and routing. In addition, our experiments have
shown that abutted standard cell instances can induce lithography weakpoints.
Therefore, in this paper, we propose methodologies that are used in a novel
software system for checking standard cells in terms of the aforementioned
lithography issues. Specifically, the software system is capable of detecting
and sorting problematic standard cells which are prone to generate lithography
weakpoints, as well as reporting standard cells that should not be abutted.
Methodologies proposed in this paper allow us to reduce or even prevent the
generation of undesirable lithography weakpoints during the physical synthesis
phase of designing a digital integrated circuit.
","['\nI-Lun Tseng\n', '\nYongfu Li\n', '\nValerio Perez\n', '\nVikas Tripathi\n', '\nZhao Chuan Lee\n', '\nJonathan Yoong Seang Ong\n']",,,http://arxiv.org/abs/1810.01446v1,cs.OH,['cs.OH'],,,[]
Community Detection Across Emerging Quantum Architectures,http://arxiv.org/abs/1810.07765v1,2018-10-01T15:28:30Z,2018-10-01T15:28:30Z,"  One of the roadmap plans for quantum computers is an integration within HPC
ecosystems assigning them a role of accelerators for a variety of
computationally hard tasks. However, in the near term, quantum hardware will be
in a constant state of change. Heading towards solving real-world problems, we
advocate development of portable, architecture-agnostic hybrid
quantum-classical frameworks and demonstrate one for the community detection
problem evaluated using quantum annealing and gate-based universal quantum
computation paradigms.
","['\nRuslan Shaydulin\n', '\nHayato Ushijima-Mwesigwa\n', '\nIlya Safro\n', '\nSusan Mniszewski\n', '\nYuri Alexeev\n']",,,http://arxiv.org/abs/1810.07765v1,quant-ph,"['quant-ph', 'cs.OH']",,,[]
"A Conceptual Approach to Complex Model Management with Generalized
  Modelling Patterns and Evolutionary Identification",http://arxiv.org/abs/1809.04656v1,2018-09-12T20:13:28Z,2018-09-12T20:13:28Z,"  Complex systems' modeling and simulation are powerful ways to investigate a
multitude of natural phenomena providing extended knowledge on their structure
and behavior. However, enhanced modeling and simulation require integration of
various data and knowledge sources, models of various kinds (data-driven
models, numerical models, simulation models, etc.), intelligent components in
one composite solution. Growing complexity of such composite model leads to the
need of specific approaches for management of such model. This need extends
where the model itself becomes a complex system. One of the important aspects
of complex model management is dealing with the uncertainty of various kinds
(context, parametric, structural, input/output) to control the model. In the
situation where a system being modeled, or modeling requirements change over
time, specific methods and tools are needed to make modeling and application
procedures (meta-modeling operations) in an automatic manner. To support
automatic building and management of complex models we propose a general
evolutionary computation approach which enables managing of complexity and
uncertainty of various kinds. The approach is based on an evolutionary
investigation of model phase space to identify the best model's structure and
parameters. Examples of different areas (healthcare, hydrometeorology, social
network analysis) were elaborated with the proposed approach and solutions.
","['\nSergey V. Kovalchuk\n', '\nOleg G. Metsker\n', '\nAnastasia A. Funkner\n', '\nIlia O. Kisliakovskii\n', '\nNikolay O. Nikitin\n', '\nAnna V. Kalyuzhnaya\n', '\nDanila A. Vaganov\n', '\nKlavdiya O. Bochenina\n']",,,http://dx.doi.org/10.1155/2018/5870987,cs.OH,['cs.OH'],10.1155/2018/5870987,,[]
OpenMPL: An Open Source Layout Decomposer,http://arxiv.org/abs/1809.07554v3,2018-09-20T10:20:46Z,2019-09-16T08:27:37Z,"  Multiple patterning lithography has been widely adopted in advanced
technology nodes of VLSI manufacturing. As a key step in the design flow,
multiple patterning layout decomposition (MPLD) is critical to design closure.
Due to the NP-hardness of the general decomposition problem, various efficient
algorithms have been proposed with high quality solutions. However, with
increasingly complicated design flow and peripheral processing steps,
developing a high-quality layout decomposer becomes more and more difficult,
slowing down the further advancement in this field. This paper presents OpenMPL
[1], an open-source layout decomposition framework, with well-separated
peripheral processing and the core solving steps. We demonstrate the
flexibility of the framework with efficient implementations of various
state-of-the-art algorithms, which enable us to reproduce most of the recent
results on widely-recognized benchmarks. We believe OpenMPL can pave the road
for developing layout decomposition engines and stimulate further researches on
this problem.
","['\nWei Li\n', '\nYuzhe Ma\n', '\nQi Sun\n', '\nYibo Lin\n', '\nIris Hui-Ru Jiang\n', '\nBei Yu\n', '\nDavid Z. Pan\n']",,,http://arxiv.org/abs/1809.07554v3,cs.OH,['cs.OH'],,,[]
PlayNPort: A Portable Wireless Music Player and Text Reader System,http://arxiv.org/abs/1809.00406v1,2018-09-02T22:47:29Z,2018-09-02T22:47:29Z,"  Portable Consumer Electronics has made a mark in the industry. With the ease
of use at an accessible price range, they have experienced significant growth
in the market. Our idea is to develop a portable wireless music player and text
reader using a Cortex-M series microcontroller and bare-metal programming
techniques. We chose to use an SD card as the storage device. The resulting
electronic device is similar to a consumer grade music player available in a
car. The system comprises an MCU, an MP3 encoder/decoder, an LCD, an audio
output jack, an SD card and a remote control. We also present various
challenges involved in developing the system and solutions we used to overcome
the challenges. The intricacy of the work lies in the fact that the system was
developed to be consumer-centric by providing a rich User Experience. It can be
used as a personal entertainment system in a car.
","['\nLakhan Shiva Kamireddy\n', '\nDharmik Thakkar\n', '\nLakhan Saiteja K\n']","5 pages, Accepted for Presentation at 37th IEEE International
  Conference On Consumer Electronics (ICCE 2019)",,http://arxiv.org/abs/1809.00406v1,cs.OH,['cs.OH'],,,[]
"Theoretical analysis and propositions for ""ontology citation""",http://arxiv.org/abs/1809.01462v1,2018-09-05T12:47:32Z,2018-09-05T12:47:32Z,"  Ontology citation, the practice of referring the ontology in a similar
fashion the scientific community routinely follows in providing the
bibliographic references to other scholarly works, has not received enough
attention it supposed to. Interestingly, so far none of the existing standard
citation styles (e.g., APA, CMOS, and IEEE) have included ontology as a citable
information source in the list of citable information sources such as journal
article, book, website, etc. Also, not much work can be found in the literature
on this topic though there are various issues and aspects of it that demand a
thorough study. For instance, what to cite? Is it the publication that
describes the ontology, or the ontology itself? The citation format, style,
illustration of motivations of ontology citation, the citation principles,
ontology impact factor, citation analysis, and so forth. In this work, we
primarily analyse the current ontology citation practices and the related
issues. We illustrate the various motivations and the basic principles of
ontology citation. We also propose a template for referring the source of
ontologies.
",['\nBiswanath Dutta\n'],"In Proc. of the Int. Conf. on Exploring the Horizons of Library and
  Information Sciences: From Libraries to Knowledge Hubs, 7-9 August, 2018
  Bangalore, India, pp. 451-458. ISBN 978-93-5311-726-9",,http://arxiv.org/abs/1809.01462v1,cs.OH,['cs.OH'],,,[]
Web Based Information System for Heat Supply Monitoring,http://arxiv.org/abs/1809.01640v1,2018-09-05T17:35:05Z,2018-09-05T17:35:05Z,"  The paper presents web based information system for heat supply monitoring.
The proposed model and information system for gathering data from heating
station heat-flow meters and regulators is software realized. The novel system
with proved functionality can be commercialized at the cost of minimal
investments, finding wildly use on Bulgarian market as cheap and quality
alternative of the western systems.
","['\nBorislav Stoyanov\n', '\nStrahil Strahilov\n']",,"Mathematical and Software Engineering, vol. 1, no. 2, 2015, 37-42",http://arxiv.org/abs/1809.01640v1,cs.OH,['cs.OH'],,,[]
"Current potentials and challenges using Sentinel-1 for broadacre field
  remote sensing",http://arxiv.org/abs/1809.01652v1,2018-09-04T18:17:20Z,2018-09-04T18:17:20Z,"  ESA operates the Sentinel-1 satellites, which provides Synthetic Aperture
Radar (SAR) data of Earth. Recorded Sentinel-1 data have shown a potential for
remotely observing and monitoring local conditions on broad acre fields. Remote
sensing using Sentinel-1 have the potential to provide daily updates on the
current conditions in the individual fields and at the same time give an
overview of the agricultural areas in the region. Research depends on the
ability of independent validation of the presented results. In the case of the
Sentinel-1 satellites, every researcher has access to the same base dataset,
and therefore independent validation is possible. Well documented research
performed with Sentinel-1 allow other research the ability to redo the
experiments and either validate or falsify presented findings. Based on current
state-of-art research we have chosen to provide a service for researchers in
the agricultural domain. The service allows researchers the ability to monitor
local conditions by using the Sentinel-1 information combined with a priori
knowledge from broad acre fields. Correlating processed Sentinel-1 to the
actual conditions is still a task the individual researchers must perform to
benefit from the service. In this paper, we presented our methodology in
translating sentinel-1 data to a level that is more accessible to researchers
in the agricultural field. The goal here was to make the data more easily
available, so the primary focus can be on correlating and comparing to
measurements collected in the broadacre fields. We illustrate the value of the
service with three examples of the possible application areas. The presented
application examples are all based on Denmark, where we have processed all
sentinel-1 scan from since 2016.
","['\nMartin Peter Christiansen\n', '\nMorten Stigaard Laursen\n', '\nBirgitte Feld Mikkelsen\n', '\nNima Teimouri\n', '\nRasmus Nyholm Jørgensen\n', '\nClaus Aage Grøn Sørensen\n']","9 pages, 5 figures, conference (AGENG2018)",EurAgEng 2018,http://arxiv.org/abs/1809.01652v1,cs.OH,['cs.OH'],,,[]
"CrowdExpress: A Probabilistic Framework for On-Time Crowdsourced Package
  Deliveries",http://arxiv.org/abs/1809.02897v1,2018-09-08T23:48:47Z,2018-09-08T23:48:47Z,"  Speed and cost of logistics are two major concerns to on-line shoppers, but
they generally conflict with each other in nature. To alleviate the
contradiction, we propose to exploit existing taxis that are transporting
passengers on the street to relay packages collaboratively, which can
simultaneously lower the cost and accelerate the speed. Specifically, we
propose a probabilistic framework containing two phases called CrowdExpress for
the on-time package express deliveries. In the first phase, we mine the
historical taxi GPS trajectory data offline to build the package transport
network. In the second phase, we develop an online adaptive taxi scheduling
algorithm to find the path with the maximum arriving-on-time probability
""on-the-fly"" upon real- time requests, and direct the package routing
accordingly. Finally, we evaluate the system using the real-world taxi data
generated by over 19,000 taxis in a month in the city of New York, US. Results
show that around 9,500 packages can be delivered successfully on time per day
with the success rate over 94%, moreover, the average computation time is
within 25 milliseconds.
","['\nChao Chen\n', '\nSen Yang\n', '\nWeichen Liu\n', '\nYasha Wang\n', '\nBin Guo\n', '\nDaqing Zhang\n']",,,http://arxiv.org/abs/1809.02897v1,cs.OH,['cs.OH'],,,[]
"PhaseMAC: A 14 TOPS/W 8bit GRO based Phase Domain MAC Circuit for
  In-Sensor-Computed Deep Learning Accelerators",http://arxiv.org/abs/1808.09335v1,2018-08-23T19:59:53Z,2018-08-23T19:59:53Z,"  PhaseMAC (PMAC), a phase domain Gated-Ring-Oscillator (GRO) based 8bit MAC
circuit, is proposed to minimize both area and power consumption of deep
learning accelerators. PMAC composes of only digital cells and consumes
significantly smaller power than standard digital designs, owing to its
efficient analog accumulation nature. It occupies 26.6 times smaller area than
conventional analog designs, which is competitive to digital MAC circuits. PMAC
achieves a peak efficiency of 14 TOPS/W, which is best reported and 48% higher
than conventional arts. Results in anomaly detection tasks are demonstrated,
which is the hottest application in the industrial IoT scene.
","['\nKentaro Yoshioka\n', '\nYosuke Toyama\n', '\nKoichiro Ban\n', '\nDaisuke Yashima\n', '\nShigeru Maya\n', '\nAkihide Sai\n', '\nKohei Onizuka\n']",Presented at Symp. VLSI 2018,,http://arxiv.org/abs/1808.09335v1,cs.OH,['cs.OH'],,,[]
MMDF2018 Workshop Report,http://arxiv.org/abs/1808.10721v1,2018-08-30T15:10:52Z,2018-08-30T15:10:52Z,"  Driven by the recent advances in smart, miniaturized, and mass produced
sensors, networked systems, and high-speed data communication and computing,
the ability to collect and process larger volumes of higher veracity real-time
data from a variety of modalities is expanding. However, despite research
thrusts explored since the late 1990's, to date no standard, generalizable
solutions have emerged for effectively integrating and processing multimodal
data, and consequently practitioners across a wide variety of disciplines must
still follow a trial-and-error process to identify the optimum procedure for
each individual application and data sources. A deeper understanding of the
utility and capabilities (as well as the shortcomings and challenges) of
existing multimodal data fusion methods as a function of data and challenge
characteristics has the potential to deliver better data analysis tools across
all sectors, therein enabling more efficient and effective automated
manufacturing, patient care, infrastructure maintenance, environmental
understanding, transportation networks, energy systems, etc. There is therefore
an urgent need to identify the underlying patterns that can be used to
determine a priori which techniques will be most useful for any specific
dataset or application. This next stage of understanding and discovery (i.e.,
the development of generalized solutions) can only be achieved via a high level
cross-disciplinary aggregation of learnings, and this workshop was proposed at
an opportune time as many domains have already started exploring use of
multimodal data fusion techniques in a wide range of application-specific
contexts.
","['\nChun-An Chou\n', '\nXiaoning Jin\n', '\nAmy Mueller\n', '\nSarah Ostadabbas\n']",https://www.northeastern.edu/mmdf2018/wp-content/uploads/2018/08/MMDF_2018_Report.pdf,,http://arxiv.org/abs/1808.10721v1,cs.OH,['cs.OH'],,,[]
Creation and Fixing of Lithography Hotspots with Synopsys Tools,http://arxiv.org/abs/1808.05998v1,2018-08-16T15:58:49Z,2018-08-16T15:58:49Z,"  At advanced process nodes, pattern matching techniques have been used in the
detection of lithography hotspots, which can affect yields of manufactured
integrated circuits. Although commercial pattern matching and in-design hotspot
fixing tools have been developed, engineers still need to verify that specific
hotspot patterns in routed designs can indeed be detected or even repaired by
software tools. Therefore, there is the need to create test cases with which
targeted hotspot patterns can be generated in routed layouts by using an APR
(automatic placement and routing) tool. In this paper, we propose a methodology
of creating hotspot patterns in the routing space by using Synopsys tools.
Also, methods for repairing hotspots during the physical design phase are
presented. With the use of the proposed hotspot creation methodology, we can
generate routed designs containing targeted hotspot patterns. As a result, the
effectiveness of hotspot detection rules, hotspot fixing guidance rules, and
relevant software tool functions can be verified.
","['\nI-Lun Tseng\n', '\nValerio Perez\n', '\nYongfu Li\n', '\nZhao Chuan Lee\n', '\nVikas Tripathi\n', '\nJonathan Yoong Seang Ong\n']",,,http://arxiv.org/abs/1808.05998v1,cs.OH,['cs.OH'],,,[]
Context-Aware DFM Rule Analysis and Scoring Using Machine Learning,http://arxiv.org/abs/1808.05999v1,2018-08-16T16:17:38Z,2018-08-16T16:17:38Z,"  To evaluate the quality of physical layout designs in terms of
manufacturability, DFM rule scoring techniques have been widely used in
physical design and physical verification phases. However, one major drawback
of conventional DFM rule scoring methodologies is that resultant DFM rule
scores may not accurate since the scores may not highly correspond to
lithography simulation results. For instance, conventional DFM rule scoring
methodologies usually use rule-based techniques to compute scores without
considering neighboring geometric scenarios of targeted layout shapes. That can
lead to inaccurate scoring results since computed DFM rule scores can be either
too optimistic or too pessimistic. Therefore, in this paper, we propose a novel
approach with the use of machine learning technology to analyze the context of
targeted layouts and predict their lithography impacts on manufacturability.
","['\nVikas Tripathi\n', '\nValerio Perez\n', '\nYongfu Li\n', '\nZhao Chuan Lee\n', '\nI-Lun Tseng\n', '\nJonathan Ong\n']",,,http://arxiv.org/abs/1808.05999v1,cs.OH,['cs.OH'],,,[]
Smart Grids Data Analysis: A Systematic Mapping Study,http://arxiv.org/abs/1808.00156v2,2018-07-31T17:16:52Z,2019-11-28T12:05:18Z,"  Data analytics and data science play a significant role in nowadays society.
In the context of Smart Grids (SG), the collection of vast amounts of data has
seen the emergence of a plethora of data analysis approaches. In this paper, we
conduct a Systematic Mapping Study (SMS) aimed at getting insights about
different facets of SG data analysis: application sub-domains (e.g., power load
control), aspects covered (e.g., forecasting), used techniques (e.g.,
clustering), tool-support, research methods (e.g., experiments/simulations),
replicability/reproducibility of research. The final goal is to provide a view
of the current status of research. Overall, we found that each sub-domain has
its peculiarities in terms of techniques, approaches and research methodologies
applied. Simulations and experiments play a crucial role in many areas. The
replicability of studies is limited concerning the provided implemented
algorithms, and to a lower extent due to the usage of private datasets.
","['\nBruno Rossi\n', '\nStanislav Chren\n']",,,http://dx.doi.org/10.1109/TII.2019.2954098,cs.OH,['cs.OH'],10.1109/TII.2019.2954098,,[]
Sunlight Enabled Vehicle Detection by LED Street Lights,http://arxiv.org/abs/1808.01980v1,2018-08-06T16:16:41Z,2018-08-06T16:16:41Z,"  We propose and demonstrate a preliminary traffic sensing system based on the
widely distributed LED street lights. The system utilizes and discriminates the
photoelectric responses of the LEDs to sunlight when a vehicle moves through
the LEDs' field of view aiming at the road. A data vector is constructed from
the consecutively collected time samples of a moving observation window, and a
support vector machine (SVM) based learning algorithm is subsequently developed
to classify the presence of a vehicle. Finally, we build a simulated platform
to experimentally evaluate the performance of the vehicle detection algorithm.
","['\nWeicheng Xue\n', '\nShangbin Li\n', '\nZhengyuan Xu\n']","3 pages, 5 figures",,http://arxiv.org/abs/1808.01980v1,cs.OH,['cs.OH'],,,[]
"The alternative bases of Boolean functions as a means of improving the
  structure of digital blocks",http://arxiv.org/abs/1808.03325v1,2018-07-31T07:47:43Z,2018-07-31T07:47:43Z,"  This paper analyzes three forms of representation of Boolean functions, such
as Classical, Algebraic and Reed-Muller. The concept of intersection and
subsets of representation forms have been introduced, moreover suitable
criteria for creating these subsets have been established. Later, these subsets
have been quantitatively compared by the number of parameters, in order to
assess the effectiveness of using each of the forms of representations proposed
in the work. Definitions of the specific weight of subsets of priority forms of
the representation of Boolean functions showed that the classical form is the
least optimal, in comparison with the parameters of other forms Also, it has
been shown that the use of alternative forms of representation of Boolean
functions, in some cases, allows to reduce twice the number of incoming PLA
buses. Estimating the average loss from the exclusive use of the Classical Form
Representation also shows that the use of alternatives yields significant
benefits in some parameters, this can be used to optimize devices in the logic
design process and reduce the chip area, what also contributes to reductions in
the cost of such devices.
",['\nSergii Kushch\n'],,,http://dx.doi.org/10.13140/RG.2.2.25912.03842/1,cs.OH,['cs.OH'],10.13140/RG.2.2.25912.03842/1,,[]
Graph Compact Orthogonal Layout Algorithm,http://arxiv.org/abs/1807.09368v1,2018-07-24T21:42:29Z,2018-07-24T21:42:29Z,"  There exist many orthogonal graph drawing algorithms that minimize edge
crossings or edge bends, however they produce unsatisfactory drawings in many
practical cases. In this paper we present a grid-based algorithm for drawing
orthogonal graphs with nodes of prescribed size. It distinguishes by creating
pleasant and compact drawings in relatively small running time. The main idea
is to minimize the total edge length that implicitly minimizes crossings and
makes the drawing easy to comprehend. The algorithm is based on combining local
and global improvements. Local improvements are moving each node to a new place
and swapping of nodes. Global improvement is based on constrained quadratic
programming approach that minimizes the total edge length while keeping node
relative positions.
","['\nKarlis Freivalds\n', '\nJans Glagolevs\n']",,"Freivalds K., Glagolevs J. (2014) Graph Compact Orthogonal Layout
  Algorithm. In: Fouilhoux P., Gouveia L., Mahjoub A., Paschos V. (eds)
  Combinatorial Optimization. ISCO 2014. Lecture Notes in Computer Science, vol
  8596. Springer, Cham",http://dx.doi.org/10.1007/978-3-319-09174-7_22,cs.OH,['cs.OH'],10.1007/978-3-319-09174-7_22,,[]
"A Localization Method Avoiding Flip Ambiguities for micro-UAVs with
  Bounded Distance Measurement Errors",http://arxiv.org/abs/1807.09590v2,2018-07-24T01:25:51Z,2018-09-18T10:35:39Z,"  Localization is a fundamental function in cooperative control of micro
unmanned aerial vehicles (UAVs), but is easily affected by flip ambiguities
because of measurement errors and flying motions. This study proposes a
localization method that can avoid the occurrence of flip ambiguities in
bounded distance measurement errors and constrained flying motions; to
demonstrate its efficacy, the method is implemented on bilateration and
trilateration. For bilateration, an improved bi-boundary model based on the
unit disk graph model is created to compensate for the shortage of distance
constraints, and two boundaries are estimated as the communication range
constraint. The characteristic of the intersections of the communication range
and distance constraints is studied to present a unique localization criterion
which can avoid the occurrence of flip ambiguities. Similarly, for
trilateration, another unique localization criterion for avoiding flip
ambiguities is proposed according to the characteristic of the intersections of
three distance constraints. The theoretical proof shows that these proposed
criteria are correct. A localization algorithm is constructed based on these
two criteria. The algorithm is validated using simulations for different
scenarios and parameters, and the proposed method is shown to provide excellent
localization performance in terms of average estimated error. Our code can be
found at: https://github.com/QingbeiGuo/AFALA.git.
","['\nQingbei Guo\n', '\nYuan Zhang\n', '\nJaime Lloretz\n', '\nBurak Kantarcix\n', '\nWinston K. G. Seah\n']","14 pages, 8 figures, IEEE Transactions on Mobile Computing(Accepted)","Qingbei Guo, Yuan Zhang, Jaime Lloretz, Burak Kantarcix and
  Winston K.G. Seah, A Localization Method Avoiding Flip Ambiguities for
  micro-UAVs with Bounded Distance Measurement Errors, IEEE Transactions on
  Mobile Computing, 2018",http://dx.doi.org/10.1109/TMC.2018.2865462,eess.SP,"['eess.SP', 'cs.OH']",10.1109/TMC.2018.2865462,,[]
Towards a Circular Economy via Intelligent Metamaterials,http://arxiv.org/abs/1807.06006v1,2018-07-16T09:37:35Z,2018-07-16T09:37:35Z,"  The present study proposes the use of intelligent metasurfaces in the design
of products, as enforcers of circular economy principles. Intelligent
metasurfaces can tune their physical properties (electromagnetic, acoustic,
mechanical) by receiving software commands. When incorporated within products
and spaces they can mitigate the resource waste caused by inefficient,
partially optimized designs and security concerns. Thus, circular economy and
fast-paced product design become compatible. The study begins by considering
electromagnetic metamaterials, and proposes a complete methodology for their
deployment. Finally, it is shown that the same principles can be extended to
the control of mechanical properties of objects, exemplary enabling the
micro-management of vibrations and heat, with unprecedented circular economy
potential.
","['\nChristos Liaskos\n', '\nAgeliki Tsioliaridou\n', '\nSotiris Ioannidis\n']","Accepted for publication at IEEE CAMAD 2018 Special Session on
  Circular Economy. Partially funded by the European Union via the Horizon
  2020: Future Emerging Topics call (FETOPEN), grant EU736876, project
  VISORSURF (http://www.visorsurf.eu)",,http://arxiv.org/abs/1807.06006v1,cs.OH,['cs.OH'],,,[]
"Data Likelihood of Active Fires Satellite Detection and Applications to
  Ignition Estimation and Data Assimilation",http://arxiv.org/abs/1808.03318v1,2018-07-09T04:41:24Z,2018-07-09T04:41:24Z,"  Data likelihood of fire detection is the probability of the observed
detection outcome given the state of the fire spread model. We derive fire
detection likelihood of satellite data as a function of the fire arrival time
on the model grid. The data likelihood is constructed by a combination of the
burn model, the logistic regression of the active fires detections, and the
Gaussian distribution of the geolocation error. The use of the data likelihood
is then demonstrated by an estimation of the ignition point of a wildland fire
by the maximization of the likelihood of MODIS and VIIRS data over multiple
possible ignition points.
","['\nJames Haley\n', '\nAngel Farguell Caus\n', '\nAdam K. Kochanski\n', '\nSher Schranz\n', '\nJan Mandel\n']","12 pages, 6 figures; VIII International Conference on Forest Fire
  Research, Coimbra, Portugal, November 2018","Advances in Forest Fire Research 2018, edited by D.X. Viegas,
  Imprensa da Universidade de Coimbra, 1918, pp. 959-968",http://dx.doi.org/10.14195/978-989-26-16-506_105,stat.AP,"['stat.AP', 'cs.OH', 'eess.IV', '62F15, 65D19']",10.14195/978-989-26-16-506_105,,[]
"Automatic streetlights that glow on detecting night and object using
  Arduino",http://arxiv.org/abs/1806.10968v1,2018-06-28T13:47:37Z,2018-06-28T13:47:37Z,"  Our manuscript aims to develop a system which will lead to energy
conservation and by doing so, we would be able to lighten few more homes. The
proposed work is accomplished by using Arduino microcontroller and sensors that
will control the electricity based on night and object's detection. Meanwhile,
a counter is set that will count the number of objects passed through the road.
The beauty of the proposed work is that the wastage of unused electricity can
be reduced, lifetime of the streetlights gets enhance because the lights do not
stay ON during the whole night, and helps to increase safety measurements. We
are confident that the proposed idea will be beneficial in the future
applications of microcontrollers and sensors etc.
","['\nZain Mumtaz\n', '\nSaleem Ullah\n', '\nZeeshan Ilyas\n', '\nShuo Liu\n', '\nNaila Aslam\n', '\nJehangir Arshad Meo\n', '\nHamza Ahmad Madni\n']",,,http://arxiv.org/abs/1806.10968v1,cs.OH,['cs.OH'],,,[]
"FluidDyn: a Python open-source framework for research and teaching in
  fluid dynamics",http://arxiv.org/abs/1807.09224v1,2018-07-03T10:04:39Z,2018-07-03T10:04:39Z,"  FluidDyn is a project to foster open-science and open-source in the fluid
dynamics community. It is thought of as a research project to channel
open-source dynamics, methods and tools to do science. We propose a set of
Python packages forming a framework to study fluid dynamics with different
methods, in particular laboratory experiments (package fluidlab), simulations
(packages fluidfft, fluidsim and fluidfoam) and data processing (package
fluidimage). In the present article, we give an overview of the specialized
packages of the project and then focus on the base package called fluiddyn,
which contains common code used in the specialized packages. Packages fluidfft
and fluidsim are described with greater detail in two companion papers, Mohanan
et al. (2018a,b). With the project FluidDyn, we demonstrate that specialized
scientific code can be written with methods and good practices of the
open-source community. The Mercurial repositories are available in Bitbucket
(https://bitbucket.org/fluiddyn/). All codes are documented using Sphinx and
Read the Docs, and tested with continuous integration run on Bitbucket,
Pipelines and Travis. To improve the reuse potential, the codes are as modular
as possible, leveraging the simple object-oriented programming model of Python.
All codes are also written to be highly efficient, using C++, Cython and
Pythran to speedup the performance of critical functions.
","['\nPierre Augier\n', '\nAshwin Vishnu Mohanan\n', '\nCyrille Bonamy\n']",,,http://dx.doi.org/10.5334/jors.237,cs.OH,['cs.OH'],10.5334/jors.237,,[]
Design and Application of Data Aquistion Interface Circuit,http://arxiv.org/abs/1806.08721v1,2018-06-21T16:53:01Z,2018-06-21T16:53:01Z,"  A commitment to condition monitoring involves the operators of plant in the
conduct of a range of activities. These activities may be compli-cated in
nature and indeed may often be performed automatically under computer control.
They can, however, always be down into a rela-tively small number of easily
identifiable functional tasks. This makes it much easier to identify the common
elements of machine condition monitoring schemes. A proposed interface circuit
design and application will be further explain in this paper, the implemented
monitoring unit circuit also illustrated, see appendix A. Two scenarios
presented in this paper, first ten turns assume to be shorted, and in the
second thirty turns shorted to show the difference in the amplitude of
frequencies at each case. This paper present. An improvement in three-phase
squirrel-cage induction motor stator inter-turn fault detection and diagnosis
based on a neural network approach is presented.
","['\nHayder O. Alwan\n', '\nNoor M. Farhan\n', '\nQais S- Al-Sabbagh\n']",,,http://arxiv.org/abs/1806.08721v1,cs.OH,['cs.OH'],,,[]
Gamorithm,http://arxiv.org/abs/1806.02717v2,2018-06-07T14:59:11Z,2018-08-27T17:26:42Z,"  Examining games from a fresh perspective we present the idea of game-inspired
and game-based algorithms, dubbed ""gamorithms"".
","['\nMoshe Sipper\n', '\nJason H. Moore\n']","IEEE Transactions on Games, 2018","IEEE Transactions on Games, Volume: 12 , Issue: 1 , March 2020,
  pp. 115 - 118",http://dx.doi.org/10.1109/TG.2018.2867743,cs.OH,['cs.OH'],10.1109/TG.2018.2867743,,[]
"A Framework for Detecting and Translating User Behavior from Smart Meter
  Data",http://arxiv.org/abs/1807.03111v1,2018-06-13T10:03:44Z,2018-06-13T10:03:44Z,"  The European adoption of smart electricity meters triggers the developments
of new value-added service for smart energy and optimal consumption. Recently,
several algorithms and tools have been built to analyze smart meter's data.
This paper introduces an open framework and prototypes for detecting and
presenting user behavior from its smart meter power consumption data. The
framework aims at presenting the detected user behavior in natural language
reports. In order to validate the proposed framework, an experiment has been
performed and the results have been presented.
","['\nEgon Kidmose\n', '\nEmad Ebeid\n', '\nRune Hylsberg Jacobsen\n']","3 pages, 4 figures, Presented at SMART 2015: The Fourth International
  Conference on Smart Systems, Devices and Technologies, ISBN:
  978-1-61208-414-5. June 21-26, 2015, Brussels, Belgium",,http://arxiv.org/abs/1807.03111v1,cs.OH,['cs.OH'],,,[]
"Research on Artificial Intelligence Ethics Based on the Evolution of
  Population Knowledge Base",http://arxiv.org/abs/1806.10095v3,2018-06-09T17:01:27Z,2018-11-18T04:03:39Z,"  The unclear development direction of human society is a deep reason for that
it is difficult to form a uniform ethical standard for human society and
artificial intelligence. Since the 21st century, the latest advances in the
Internet, brain science and artificial intelligence have brought new
inspiration to the research on the development direction of human society.
Through the study of the Internet brain model, AI IQ evaluation, and the
evolution of the brain, this paper proposes that the evolution of population
knowledge base is the key for judging the development direction of human
society, thereby discussing the standards and norms for the construction of
artificial intelligence ethics.
","['\nFeng Liu\n', '\nYong Shi\n']","12 pages, 6 figures,1 table","Intelligence Science II. ICIS 2018.IFIP Advances in Information
  and Communication Technology, vol 539. Springer, Cham",http://dx.doi.org/10.1007/978-3-030-01313-4_48,cs.OH,"['cs.OH', 'cs.AI', 'cs.CY']",10.1007/978-3-030-01313-4_48,,[]
"Constraining the Synopsys Pin Access Checker Utility for Improved
  Standard Cells Library Verification Flow",http://arxiv.org/abs/1805.10012v1,2018-05-25T07:18:08Z,2018-05-25T07:18:08Z,"  While standard cell layouts are drawn with minimum design rules for maximum
benefit of design area shrinkage, the complicated design rules begin to cause
difficulties with signal routes accessing the pins in standard cell layouts.
Multiple design iterations are required to resolve routing issues, thus
increasing the runtime and the overall chip area. To optimize the chip
performance, power and area (PPA) and improve the routability, it is necessary
to consider the pin accessibility during standard cell development phase so
that each cell is designed to maximize the number of feasible pin-access
solutions available to the router. As part of the Synopsys IC Compiler Library
Preparation Reference Methodology, the Synopsys Pin Access Checker (PAC)
reports DRC violations associated with the standard cell. Based on Synopsys
PAC's methodology, we demonstrate several methods to improve the probability of
detecting pin accessibility issues, such as reducing the number of cells
required for each Synopsys 'testcell', increasing the complexity of the pin
connectivity assignment and recommending the router constraints.
","['\nYongfu Li\n', '\nChin Hui Lee\n', '\nWan Chia Ang\n', '\nKok Peng Chua\n', '\nYoong Seang Jonathan Ong\n', '\nChiu Wing Colin Hui\n']",,Synopsys User Conference (SNUG) Silicon Valley 2017,http://arxiv.org/abs/1805.10012v1,cs.OH,['cs.OH'],,,[]
In Design DFM Rule Scoring and Fixing Method using ICV,http://arxiv.org/abs/1805.10016v1,2018-05-25T07:45:40Z,2018-05-25T07:45:40Z,"  As compared to DRC rules, DFM rules are a list of selected recommended rules
which aim to improve the design margins for better manufacturability. In
GLOBALFOUNDRIES, we use DFM scoring methodology as an effective technique to
analyze design quality in terms of manufacturability. Physical design engineers
can perform our Manufacturability Check Deck (MCD) to asset their design
quality during the sign-off stage. In the past, Synopsys users have to convert
their design though milkyway database to GDSII format and execute the
verification through the third party EDA tools. This method is costly and
time-consuming for our Synopsys users. Today, we propose a new and easy-to-use
integrated flow which leverages on the ICV engine to provide DFM scoring and
in-design fixing techniques. The new methodology address DFM violations early
in the design flow and achieve DFM compliance design during sign-off phase.
","['\nVikas Tripathi\n', '\nYongfu Li\n', '\nZhao Chuan Lee\n', '\nI-Lun Tseng\n', '\nJason Khaw\n', '\nJonathan Ong\n']",,Synopsys User Group Penang (SNUG) 2017,http://arxiv.org/abs/1805.10016v1,cs.OH,['cs.OH'],,,[]
"Advanced In-Design Auto-Fixing Flow for Cell Abutment Pattern Matching
  Weakpoints",http://arxiv.org/abs/1805.10283v1,2018-05-25T09:42:03Z,2018-05-25T09:42:03Z,"  Pattern matching design verification has gained noticeable attention in
semiconductor technologies as it can precisely identify more localized
problematic areas (weakpoints) in the layout. To address these weakpoints,
engineers adopt 'Rip-up and Reroute' methodology to reroute the nets and avoid
these weakpoints. However, the technique is unable to address weakpoints due to
the cell placement. The only present approach is to manually shift or flip the
standard cells to eradicate the weakpoint. To overcome the challenge in going
from a manual and laborious process to a fully automated fixing, we have
proposed an in-design auto-fixing feature, tested with the commercial design
tool, Synopsys IC Compiler. Our experimental result has demonstrated close to
one hundred percent lithography weakpoints fixing on all of our 14nm designs.
","['\nYongfu Li\n', '\nValerio Perez\n', '\nI-Lun Tseng\n', '\nZhao Chuan Lee\n', '\nVikas Tripathi\n', '\nJason Khaw\n', '\nYoong Seang Jonathan Ong\n']",,Synopsys User Group Singapore (SNUG) 2017,http://arxiv.org/abs/1805.10283v1,cs.OH,['cs.OH'],,,[]
IoT for Green Building Management,http://arxiv.org/abs/1805.10635v1,2018-05-27T14:57:14Z,2018-05-27T14:57:14Z,"  Buildings consume 60% of global electricity. However, current building
management systems (BMSs) are highly expensive and difficult to justify for
small to medium-sized buildings. As such, the Internet of Things (IoT), which
can monitor and collect a large amount of data on different contexts of a
building and feed the data to the processor of the BMS, provides a new
opportunity to integrate intelligence into the BMS to monitor and manage the
energy consumption of the building in a cost-effective manner. Although an
extensive literature is available on IoT based BMS and applications of signal
processing techniques for some aspects of building energy management
separately, detailed study on their integration to address the overall BMS is
quite limited. As such, the proposed paper will address this gap by providing
an overview of an IoT based BMS leveraging signal processing and machine
learning techniques. It is demonstrated how to extract high-level building
occupancy information through simple and low-cost IoT sensors and studied the
impact of human activities on energy usage of a building, which can be
exploited to design energy conservation measures to reduce the building's
energy consumption.
","['\nWayes Tushar\n', '\nNipun Wijerathne\n', '\nWen-Tai Li\n', '\nChau Yuen\n', '\nH. Vincent Poor\n', '\nTapan Kumar Saha\n', '\nKristin L. Wood\n']","20 pages, 7 figures, 1 table, accepted journal paper",,http://arxiv.org/abs/1805.10635v1,cs.OH,['cs.OH'],,,[]
"Multiple-Lithography-Compliant Verification for Standard Cell Library
  Development Flow",http://arxiv.org/abs/1805.10745v1,2018-05-28T02:48:22Z,2018-05-28T02:48:22Z,"  Starting from 22-nm, a standard cell must be designed to be full
lithography-compliant, which includes Design Rule Check,
Design-for-Manufacturability and Double-Patterning compliant. It has become a
great challenge for physical layout designers to provide a full
lithography-compliant standard cell layout that is optimized for area, power,
timing, signal integrity, and yield. This challenge is further exacerbated with
abutted single- and multiple-height standard cells. At present, different
foundries and library vendors have different approaches for full
lithography-compliant library preparation and validation. To the best of our
knowledge, there is no single tool integrates all types of
lithography-compliant check in standard cell libraries validation flow. In this
work, we will demonstrate multiple lithography-compliant verification for
standard cell library development flow. Validation flow and detailed algorithm
implementation will be explained to assist engineers to achieve full
lithography-compliant standard cell libraries. An area-efficient standard cell
placement methodology will also be discussed to validate the issues arises from
standard cell abutment.
","['\nYongfu Li\n', '\nWan Chia Ang\n', '\nChin Hui Lee\n', '\nKok Peng Chua\n', '\nYoong Seang Jonathan Ong\n', '\nChiu Wing Colin Hui\n']",Synopsys User Group Silicon Valley (SNUG) 2017,,http://arxiv.org/abs/1805.10745v1,cs.OH,['cs.OH'],,,[]
"Standard Cell Library Evaluation with Multiple lithography-compliant
  verification and Improved Synopsys Pin Access Checking Utility",http://arxiv.org/abs/1805.11426v1,2018-05-28T02:51:04Z,2018-05-28T02:51:04Z,"  While standard cell layouts are drawn with minimum design rules to maximize
the benefit of design area shrinkage, the complicated design rules have caused
difficulties with signal routes accessing the pins in standard cell layouts. As
a result, it has become a great challenge for physical layout designers to
design a standard cell layout that is optimized for area, power, timing, signal
integrity, and printability. Multiple design iterations are required to
consider pin accessibility during standard cells layout to increase the number
of feasible solutions available to the router. In this work, we will
demonstrate several improvements with the Synopsys PAC methodology, such as
reducing the number of cells required for each Synopsys 'testcell' with the
same cell abutment condition, increasing the complexity of the pin connection
for better pin accessibility evaluation. We also recommend additional
constraints to improve the probability of detecting pin accessibility issues.
We also integrate other physical verification methods to access the design rule
compliance and the printability of standard cells. We hope that the easy to use
utility enables layout engineers to perform the verification, simplifying the
verification methodology.
","['\nYongfu Li\n', '\nWan Chia Ang\n', '\nChin Hui Lee\n', '\nKok Peng Chua\n', '\nYoong Seang Jonathan Ong\n', '\nChiu Wing Colin Hui\n']","Synopsys User Group Singapore (SNUG) 2017. arXiv admin note:
  substantial text overlap with arXiv:1805.10012, arXiv:1805.10745",,http://arxiv.org/abs/1805.11426v1,cs.OH,['cs.OH'],,,[]
DATA:SEARCH'18 -- Searching Data on the Web,http://arxiv.org/abs/1805.11883v1,2018-05-30T09:53:05Z,2018-05-30T09:53:05Z,"  This half day workshop explores challenges in data search, with a particular
focus on data on the web. We want to stimulate an interdisciplinary discussion
around how to improve the description, discovery, ranking and presentation of
structured and semi-structured data, across data formats and domain
applications. We welcome contributions describing algorithms and systems, as
well as frameworks and studies in human data interaction. The workshop aims to
bring together communities interested in making the web of data more
discoverable, easier to search and more user friendly.
","['\nPaul Groth\nElsevier Labs\n', '\nLaura Koesten\nThe Open Data Institute + University of Southampton\n', '\nPhilipp Mayr\nGESIS - Leibniz Institute for the Social Sciences\n', '\nMaarten de Rijke\nUniversity of Amsterdam\n', '\nElena Simperl\nUniversity of Southampton\n']",,,http://arxiv.org/abs/1805.11883v1,cs.OH,['cs.OH'],,,"['Elsevier Labs', 'The Open Data Institute + University of Southampton', 'GESIS - Leibniz Institute for the Social Sciences', 'University of Amsterdam', 'University of Southampton']"
A Guide to the SPHERE 100 Homes Study Dataset,http://arxiv.org/abs/1805.11907v2,2018-05-30T11:34:48Z,2018-10-30T14:33:45Z,"  The SPHERE project has developed a multi-modal sensor platform for health and
behavior monitoring in residential environments. So far, the SPHERE platform
has been deployed for data collection in approximately 50 homes for duration up
to one year. This technical document describes the format and the expected
content of the SPHERE dataset(s) under preparation. It includes a list of some
data quality problems (both known to exist in the dataset(s) and potential
ones), their workarounds, and other information important to people working
with the SPHERE data, software, and hardware. This document does not aim to be
an exhaustive descriptor of the SPHERE dataset(s); it also does not aim to
discuss or validate the potential scientific uses of the SPHERE data.
","['\nAtis Elsts\n', '\nTilo Burghardt\n', '\nDallan Byrne\n', '\nMassimo Camplani\n', '\nDima Damen\n', '\nXenofon Fafoutis\n', '\nSion Hannuna\n', '\nWilliam Harwin\n', '\nMichael Holmes\n', '\nBalazs Janko\n', '\nVictor Ponce Lopez\n', '\nAlessandro Masullo\n', '\nMajid Mirmehdi\n', '\nGeorge Oikonomou\n', '\nRobert Piechocki\n', '\nR. Simon Sherratt\n', '\nEmma Tonkin\n', '\nNiall Twomey\n', '\nAntonis Vafeas\n', '\nPrzemyslaw Woznowski\n', '\nIan Craddock\n']",,,http://arxiv.org/abs/1805.11907v2,cs.OH,['cs.OH'],,,[]
An Integrated View on the Future of Logistics and Information Technology,http://arxiv.org/abs/1805.12485v1,2018-05-31T14:15:29Z,2018-05-31T14:15:29Z,"  In this position paper, we present our vision on the future of the logistics
business domain and the use of information technology (IT) in this domain. The
vision is based on extensive experience with Dutch and European logistics in
various contexts and from various perspectives. We expect that the vision also
holds for logistics outside Europe. We build our vision in a number of steps.
First, we make an inventory of the most important trends in the logistics
domain - we call these mega-trends. Next, we do the same for the information
technology domain, restricted to technologies that have relevance for
logistics. Then, we introduce logistics meta-concepts that we use to describe
our vision and relate them to business engineering. We use these three
ingredients to analyze leading concepts that we currently observe in the
logistics domain. Next, we consolidate all elements into a model that
represents our vision of the integrated future of logistics and IT. We
elaborate on the role of data platforms and open standards in this integrated
vision.
","['\nPaul Grefen\n', '\nWout Hofman\n', '\nRemco Dijkman\n', '\nAlbert Veenstra\n', '\nSander Peters\n']","22 pages, 7 figures, 3 tables",,http://arxiv.org/abs/1805.12485v1,cs.OH,['cs.OH'],,,[]
"Distributed Optimization Strategy for Multi Area Economic Dispatch Based
  on Electro Search Optimization Algorithm",http://arxiv.org/abs/1806.06062v1,2018-05-25T06:25:17Z,2018-05-25T06:25:17Z,"  A new adopted evolutionary algorithm is presented in this paper to solve the
non-smooth, non-convex and non-linear multi-area economic dispatch (MAED). MAED
includes some areas which contains its own power generation and loads. By
transmitting the power from the area with lower cost to the area with higher
cost, the total cost function can be minimized greatly. The tie line capacity,
multi-fuel generator and the prohibited operating zones are satisfied in this
study. In addition, a new algorithm based on electro search optimization
algorithm (ESOA) is proposed to solve the MAED optimization problem with
considering all the constraints. In ESOA algorithm all probable moving states
for individuals to get away from or move towards the worst or best solution
needs to be considered. To evaluate the performance of the ESOA algorithm, the
algorithm is applied to both the original economic dispatch with 40 generator
systems and the multi-area economic dispatch with 3 different systems such as:
6 generators in 2 areas; and 40 generators in 4 areas. It can be concluded
that, ESOA algorithm is more accurate and robust in comparison with other
methods.
","['\nMina Yazdandoost\n', '\nPeyman Khazaei\n', '\nSalar Saadatian\n', '\nRahim Kamali\n']",This paper is accepted for WAC 2018 conference,,http://arxiv.org/abs/1806.06062v1,cs.OH,"['cs.OH', 'cs.NE', 'eess.SP']",,,[]
"Characterizing the Temporal Dynamics of Information in Visually Guided
  Predictive Control Using LSTM Recurrent Neural Networks",http://arxiv.org/abs/1805.05946v1,2018-05-15T00:43:23Z,2018-05-15T00:43:23Z,"  Theories for visually guided action account for online control in the
presence of reliable sources of visual information, and predictive control to
compensate for visuomotor delay and temporary occlusion. In this study, we
characterize the temporal relationship between information integration window
and prediction distance using computational models. Subjects were immersed in a
simulated environment and attempted to catch virtual balls that were
transiently ""blanked"" during flight. Recurrent neural networks were trained to
reproduce subject's gaze and hand movements during blank. The models
successfully predict gaze behavior within 3 degrees, and hand movements within
8.5 cm as far as 500 ms in time, with integration window as short as 27 ms.
Furthermore, we quantified the contribution of each input source of information
to motor output through an ablation study. The model is a proof of concept for
prediction as a discrete mapping between information integrated over time and a
temporally distant motor output.
","['\nKamran Binaee\n', '\nAnna Starynska\n', '\nJeff B Pelz\n', '\nChristopher Kanan\n', '\nGabriel Jacob Diaz\n']","6 pages, 6 figures, Cognitive Science Conference 2018",,http://arxiv.org/abs/1805.05946v1,cs.OH,['cs.OH'],,,[]
"Autonomous Vehicle Scheduling At Intersections Based On Production Line
  Technique",http://arxiv.org/abs/1805.06033v1,2018-05-15T20:57:16Z,2018-05-15T20:57:16Z,"  This thesis considers the problem of scheduling autonomous vehicles at
intersections. A new system is proposed which is more efficient and could
replace the recently introduced Autonomous Intersection Management (AIM) model.
The proposed system is based on the production line technique. The environment
of the intersection, vehicles position, speeds, and turning are specified and
determined in advance. The goal of the proposed system is to eliminate vehicle
collision and reduce the waiting time to cross the intersection. Three
different patterns of traffic flow towards the intersection have been tested.
The system requires less waiting time, compared to the other models, including
the random case where the flow is unpredictable. The K-Nearest Neighbors (KNN)
algorithm has been used to predict vehicles making a right turn at the
intersection. The experimental results show there is no chance of collision
inside the intersection using the proposed model; however, the system might
require more space in the traffic lane for some specific traffic patterns.
",['\nNasser Aloufi\n'],,,http://arxiv.org/abs/1805.06033v1,cs.OH,['cs.OH'],,,[]
"Cost-Benefit Analysis of Data Intelligence -- Its Broader
  Interpretations",http://arxiv.org/abs/1805.08575v2,2018-05-20T15:03:09Z,2018-12-03T10:35:09Z,"  The core of data science is our fundamental understanding about data
intelligence processes for transforming data to decisions. One aspect of this
understanding is how to analyze the cost-benefit of data intelligence
workflows. This work is built on the information-theoretic metric proposed by
Chen and Golan for this purpose and several recent studies and applications of
the metric. We present a set of extended interpretations of the metric by
relating the metric to encryption, compression, model development, perception,
cognition, languages, and news media.
",['\nMin Chen\n'],"The first version was archived in May 2018. It was updated in
  December 2018 following a minor revision according to the reviewers' comments
  and suggestions",,http://arxiv.org/abs/1805.08575v2,cs.OH,['cs.OH'],,,[]
Quantum Adiabatic Evolution for Global Optimization in Big Data,http://arxiv.org/abs/1805.11479v1,2018-05-16T23:49:52Z,2018-05-16T23:49:52Z,"  Big Data is characterized by Volume, Velocity, Veracity and Complexity. The
interaction between this huge data is complex with an associated free will
having dynamic and non linear nature. We reduced big data based on its
characteristics, conceptually driven by quantum field theory and utilizing the
physics of condensed matter theory in a complex nonlinear dynamic system:
Quantum Topological Field Theory of Data. The model is formulated from the
dynamics and evolution of single datum, eventually defining the global
properties and evolution of collective data space via action, partition
function, green propagators in almost polynomially solvable O(nlogn)
complexity. The simulated results show that the time complexity of our
algorithm for global optimization via quantum adiabatic evolution is almost in
O(logn) Our algorithm first mines the space via greedy approach and makes a
list of all ground state Hamiltonians, then utilizing the tunnelling property
of quantum mechanics optimizes the algorithm unlike up hill and iterative
techniques and doesnot let algorithm to get localized in local minima or sharp
valley due to adiabatic evolution of the system. The loss in quantumness, non
realizable, no clone, noise, decoherence, splitting of energy states due to
electric and magnetic fields, variant to perturbations and less lifetime makes
it inefficient for practical implementation. The inefficiencies of qubit can be
overcome via property that remains invariant to perturbation and Cartesian
independent having well defined mathematical structure. It can be well
addressed via topological field theory of data.
",['\nSahil Imtiyaz\n'],":118 Pages: 2 figures:5 graphs:Conferences 2:Journal Papers 3 under
  review. arXiv admin note: text overlap with arXiv:1506.08978,
  arXiv:1511.03010, arXiv:0811.2519 by other authors",,http://arxiv.org/abs/1805.11479v1,cs.OH,"['cs.OH', '00Bxx']",,,[]
"Energy Efficiency and Emission Testing for Connected and Automated
  Vehicles Using Real-World Driving Data",http://arxiv.org/abs/1805.07643v2,2018-05-19T19:20:31Z,2018-09-07T14:29:51Z,"  By using the onboard sensing and external connectivity technology, connected
and automated vehicles (CAV) could lead to improved energy efficiency, better
routing, and lower traffic congestion. With the rapid development of the
technology and adaptation of CAV, it is more critical to develop the universal
evaluation method and the testing standard which could evaluate the impacts on
energy consumption and environmental pollution of CAV fairly, especially under
the various traffic conditions. In this paper, we proposed a new method and
framework to evaluate the energy efficiency and emission of the vehicle based
on the unsupervised learning methods. Both the real-world driving data of the
evaluated vehicle and the large naturalistic driving dataset are used to
perform the driving primitive analysis and coupling. Then the linear weighted
estimation method could be used to calculate the testing result of the
evaluated vehicle. The results show that this method can successfully identify
the typical driving primitives. The couples of the driving primitives from the
evaluated vehicle and the typical driving primitives from the large real-world
driving dataset coincide with each other very well. This new method could
enhance the standard development of the energy efficiency and emission testing
of CAV and other off-cycle credits.
","['\nYan Chang\n', '\nWeiqing Yang\n', '\nDing Zhao\n']",,,http://arxiv.org/abs/1805.07643v2,cs.OH,"['cs.OH', 'eess.SP', 'stat.AP']",,,[]
"Data-Driven Exploration of Factors Affecting Federal Student Loan
  Repayment",http://arxiv.org/abs/1805.01586v1,2018-05-03T15:02:35Z,2018-05-03T15:02:35Z,"  Student loans occupy a significant portion of the federal budget, as well as,
the largest financial burden in terms of debt for graduates. This paper
explores data-driven approaches towards understanding the repayment of such
loans. Using statistical and machine learning models on the College Scorecard
Data, this research focuses on extracting and identifying key factors affecting
the repayment of a student loan. The specific factors can be used to develop
models which provide predictive capability towards repayment rate, detect
irregularities/non-repayment, and help understand the intricacies of student
loans.
","['\nBin Luo\n', '\nQi Zhang\n', '\nSomya D. Mohanty\n']",7 Pages,,http://arxiv.org/abs/1805.01586v1,stat.AP,"['stat.AP', 'cs.OH']",,,[]
A Cyberinfrastructure for BigData Transportation Engineering,http://arxiv.org/abs/1805.00105v1,2018-04-30T21:22:02Z,2018-04-30T21:22:02Z,"  Big Data-driven transportation engineering has the potential to improve
utilization of road infrastructure, decrease traffic fatalities, improve fuel
consumption, decrease construction worker injuries, among others. Despite these
benefits, research on Big Data-driven transportation engineering is difficult
today due to the computational expertise required to get started. This work
proposes BoaT, a transportation-specific programming language, and it's Big
Data infrastructure that is aimed at decreasing this barrier to entry. Our
evaluation that uses over two dozen research questions from six categories show
that research is easier to realize as a BoaT computer program, an order of
magnitude faster when this program is run, and exhibits 12-14x decrease in
storage requirements.
","['\nMd Johirul Islam\n', '\nAnuj Sharma\n', '\nHridesh Rajan\n']",,,http://arxiv.org/abs/1805.00105v1,cs.OH,['cs.OH'],,,[]
Investigating Power Outage Effects on Reliability of Solid-State Drives,http://arxiv.org/abs/1805.00140v1,2018-04-29T07:20:10Z,2018-04-29T07:20:10Z,"  Solid-State Drives (SSDs) are recently employed in enterprise servers and
high-end storage systems in order to enhance performance of storage subsystem.
Although employing high speed SSDs in the storage subsystems can significantly
improve system performance, it comes with significant reliability threat for
write operations upon power failures. In this paper, we present a comprehensive
analysis investigating the impact of workload dependent parameters on the
reliability of SSDs under power failure for variety of SSDs (from top
manufacturers). To this end, we first develop a platform to perform two
important features required for study: a) a realistic fault injection into the
SSD in the computing systems and b) data loss detection mechanism on the SSD
upon power failure. In the proposed physical fault injection platform, SSDs
experience a real discharge phase of Power Supply Unit (PSU) that occurs during
power failure in data centers which was neglected in previous studies. The
impact of workload dependent parameters such as workload Working Set Size
(WSS), request size, request type, access pattern, and sequence of accesses on
the failure of SSDs is carefully studied in the presence of realistic power
failures. Experimental results over thousands number of fault injections show
that data loss occurs even after completion of the request (up to 700ms) where
the failure rate is influenced by the type, size, access pattern, and sequence
of IO accesses while other parameters such as workload WSS has no impact on the
failure of SSDs.
","['\nSaba Ahmadian\n', '\nFarhad Taheri\n', '\nMehrshad Lotfi\n', '\nMaryam Karimi\n', '\nHossein Asad\n']","Design, Automation & Test in Europe Conference & Exhibition (DATE),
  2018. IEEE, 2018",,http://dx.doi.org/10.23919/DATE.2018.8342004,cs.OH,['cs.OH'],10.23919/DATE.2018.8342004,,[]
"Designing a cost-time-quality-efficient grinding process using MODM
  methods",http://arxiv.org/abs/1804.10710v3,2018-04-27T23:03:50Z,2022-03-13T19:44:13Z,"  In this paper a multi-objective mathematical model has been used to optimize
grinding parameters include workpiece speed, depth of cut and wheel speed which
highly affect the final surface quality. The mathematical model of the
optimization problem consists of three conflict objective functions subject to
wheel wear and production rate constraints. Exact methods can solve the NLP
model in few seconds, therefore using Meta-heuristic algorithms which provide
near optimal solutions in not suitable. Considering this, five Multi-Objective
Decision Making methods have been used to solve the multi-objective
mathematical model using GAMS software to achieve the optimal parameters of the
grinding process. The Multi-Objective Decision Making methods provide different
effective solutions where the decision maker can choose each solution in
different situations. Different criteria have been considered to evaluate the
performance of the five Multi-Objective Decision Making methods. Also,
Technique for Order of Preference by Similarity to Ideal Solution method has
been used to obtain the priority of each method and determine which
Multi-Objective Decision Making method performs better considering all criteria
simultaneously. The results indicated that Weighted Sum Method and Goal
programming method are the best Multi-Objective Decision Making methods. The
Weighted Sum Method and Goal programming provided solutions which are
competitive to each other. In addition, these methods obtained solutions which
have minimum grinding time, cost and surface roughness among other
Multi-Objective Decision Making methods.
",['\nMeysam Mahjoob\n'],17 pages,,http://dx.doi.org/10.24018/ejeng.2022.7.2.2719,cs.OH,['cs.OH'],10.24018/ejeng.2022.7.2.2719,,[]
Aesthetical Attributes for Segmenting Arabic Word,http://arxiv.org/abs/1804.04690v1,2018-04-11T10:26:06Z,2018-04-11T10:26:06Z,"  The connected allograph representing calligraphic Arabic word does not appear
individually in any calligraphic resource but in association with other letters
all adapted to each other. The graphic segmentation of the word by respecting
aesthetical attributes indicating the grapheme of every letter is far from
being an obvious task. The question consists in discovering every letter
constituting the word, points of cutting which separate its grapheme from other
constituents of word's shape. The obtained segment must be a complete drawing
of the represented letter. This segmentation according to contextual graphic
and qualitative criteria connecting the attached allograph will have to satisfy
typographic constraints varying in conformity with the possibilities offered by
the wanted technology. In this paper, we develop an approach for segmenting
Arabic word from which the purpose is to extract graphemes respecting the
design of Arabic letters such as it is in the calligraphic literature. The
procedure bases itself on the principle that the Arabic connected letters have
a common part included in the cursive area, which must not be lost during the
process of cutting.
","['\nMohamed Hssini\n', '\nAzzeddine Lazrek\n']","15 pages, 20 figures","Communications in Information Science and Management Engineering
  Oct. 2013, Vol. 3 Iss. 10, PP. 477-491",http://arxiv.org/abs/1804.04690v1,cs.OH,['cs.OH'],,,[]
Problem of Multiple Diacritics Design for Arabic Script,http://arxiv.org/abs/1804.04691v1,2018-04-11T10:23:20Z,2018-04-11T10:23:20Z,"  This study focuses on the design of multiple Arabic diacritical marks and to
developing a model that generates the stacking of multiples Arabic diacritics
in order to integrate it into a system of Arabic composition. The problem
concerns the presence of multiple diacritics on a single basic letter. This
model is based on the layering composition. The combination of diacritics with
letters requires a basic layering to combine any diacritics in the word with
their base letter, without having to deal individually and separately each pair
of base letter and diacritics.
","['\nMohamed Hssini\n', '\nAzzeddine Lazrek\n']","6 pages, 10 figures","IOSR Journal of Engineering e-ISSN: 2250-3021, p-ISSN: 2278-8719,
  Vol. 2, Issue 12 (Dec. 2012), V3 PP 48-53",http://dx.doi.org/10.9790/3021-021234853,cs.OH,['cs.OH'],10.9790/3021-021234853,,[]
SMT Solving for Vesicle Traffic Systems in Cells,http://arxiv.org/abs/1804.05414v1,2018-04-15T19:37:49Z,2018-04-15T19:37:49Z,"  In biology, there are several questions that translate to combinatorial
search. For example, vesicle traffic systems that move cargo within eukaryotic
cells have been proposed to exhibit several graph properties such as three
connectivity. These properties are consequences of underlying biophysical
constraints. A natural question for biologists is: what are the possible
networks for various combinations of those properties? In this paper, we
present novel SMT based encodings of the properties over vesicle traffic
systems and a tool that searches for the networks that satisfies the properties
using SMT solvers. In our experiments, we show that our tool can search for
networks of sizes that are considered to be relevant by biologists.
","['\nAshutosh Gupta\n', '\nAnkit Shukla\n', '\nMandyam Srivas\n', '\nMukund Thattai\n']","13 pages, 1 figure, Workshop SASB-2017",,http://arxiv.org/abs/1804.05414v1,cs.OH,['cs.OH'],,,[]
"Automatic Detection of Indoor and Outdoor Scenarios using NMEA Message
  Data from GPS Receivers",http://arxiv.org/abs/1804.05907v1,2018-04-16T19:34:48Z,2018-04-16T19:34:48Z,"  Detection of indoor and outdoor scenarios is an important resource for many
types of activities such as multisensor navigation and location-based services.
This research presents the use of NMEA data provided by GPS receivers to
characterize different types of scenarios automatically. A set of static tests
was performed to evaluate metrics such as number of satellites, positioning
solution geometry and carrier-to-receiver noise-density ratio values to detect
possible patterns to determine indoor and outdoor scenarios. Subsequently,
validation tests are applied to verify that parameters obtained are adequate.
","['\nR. S. Pissardini\n', '\nE. S. Fonseca Junior\n']",in Portuguese,"Revista Brasileira de Geom\'atica, v.6, n.4, 2018",http://dx.doi.org/10.3895/rbgeo.v6n4.8269,cs.OH,['cs.OH'],10.3895/rbgeo.v6n4.8269,,[]
Random Tilings with the GPU,http://arxiv.org/abs/1804.07250v1,2018-04-18T00:36:58Z,2018-04-18T00:36:58Z,"  We present GPU accelerated implementations of Markov chain algorithms to
sample random tilings, dimers, and the six-vertex model.
","['\nDavid Keating\n', '\nAnanth Sridhar\n']",,,http://dx.doi.org/10.1063/1.5038732,cs.OH,"['cs.OH', 'cond-mat.stat-mech']",10.1063/1.5038732,,[]
PURE: Scalable Phase Unwrapping with Spatial Redundant Arcs,http://arxiv.org/abs/1805.00321v2,2018-04-19T06:05:07Z,2018-05-03T04:13:51Z,"  Phase unwrapping is a key problem in many coherent imaging systems, such as
synthetic aperture radar (SAR) interferometry. A general formulation for
redundant integration of finite differences for phase unwrapping (Costantini et
al., 2010) was shown to produce a more reliable solution by exploiting
redundant differential estimates. However, this technique requires a commercial
linear programming solver for large-scale problems. For a linear cost function,
we propose a method based on Dual Decomposition that breaks the given problem
defined over a non-planar graph into tractable sub-problems over planar
subgraphs. We also propose a decomposition technique that exploits the
underlying graph structure for solving the sub-problems efficiently and
guarantees asymptotic convergence to the globally optimal solution. The
experimental results demonstrate that the proposed approach is comparable to
the existing state-of-the-art methods in terms of the estimate with a better
runtime and memory footprint.
",['\nRavi Lanka\n'],,,http://arxiv.org/abs/1805.00321v2,cs.OH,"['cs.OH', 'cs.CV', 'cs.DS']",,,[]
"On the Energy Consumption Forecasting of Data Centers Based on Weather
  Conditions: Remote Sensing and Machine Learning Approach",http://arxiv.org/abs/1804.01754v2,2018-04-05T10:00:44Z,2018-05-30T13:51:04Z,"  The energy consumption of Data Centers (DCs) is a very important figure for
the telecommunications operators, not only in terms of cost, but also in terms
of operational reliability. A relation between the energy consumption and the
weather conditions would indicate that weather forecast models could be used
for predicting energy consumption of DCs. A reliable forecast would result in a
more efficient management of the available energy and would make it easier to
take advantage of the modern types of power-grid based on renewable energy
resources. In this ,paper, we exploit the capabilities provided by the
FIESTA-IoT platform in order to investigate the correlation between the weather
conditions and the energy consumption in DCs. Then, by using multi-variable
linear regression process, we model this correlation between the energy
consumption and the dominant weather conditions parameters in order to
effectively forecast the energy consumption based on the weather forecast. We
have validated our results through live measurements from the RealDC testbed.
Results from our proposed approach indicate that forecasting of energy
consumption based on weather conditions could help not only DC operators in
managing their cooling systems and power usage, but also electricity companies
in optimizing their power distribution systems.
","['\nGeorgios Smpokos\n', '\nMohamed A. Elshatshat\n', '\nAthanasios Lioumpas\n', '\nIlias Iliopoulos\n']","6 pages, energy efficiency of data center, DC-IoT, FIESTA- IoT",,http://arxiv.org/abs/1804.01754v2,cs.OH,['cs.OH'],,,[]
"Increased Prediction Accuracy in the Game of Cricket using Machine
  Learning",http://arxiv.org/abs/1804.04226v1,2018-04-09T13:56:41Z,2018-04-09T13:56:41Z,"  Player selection is one the most important tasks for any sport and cricket is
no exception. The performance of the players depends on various factors such as
the opposition team, the venue, his current form etc. The team management, the
coach and the captain select 11 players for each match from a squad of 15 to 20
players. They analyze different characteristics and the statistics of the
players to select the best playing 11 for each match. Each batsman contributes
by scoring maximum runs possible and each bowler contributes by taking maximum
wickets and conceding minimum runs. This paper attempts to predict the
performance of players as how many runs will each batsman score and how many
wickets will each bowler take for both the teams. Both the problems are
targeted as classification problems where number of runs and number of wickets
are classified in different ranges. We used na\""ive bayes, random forest,
multiclass SVM and decision tree classifiers to generate the prediction models
for both the problems. Random Forest classifier was found to be the most
accurate for both the problems.
","['\nKalpdrum Passi\n', '\nNiravkumar Pandey\n']",,"International Journal of Data Mining & Knowledge Management
  Process (IJDKP) Vol.8, No.2, March 2018",http://arxiv.org/abs/1804.04226v1,cs.OH,['cs.OH'],,,[]
"Angular and Temporal Correlation of V2X Channels Across Sub-6 GHz and
  mmWave Bands",http://arxiv.org/abs/1804.03505v1,2018-04-06T21:36:35Z,2018-04-06T21:36:35Z,"  5G millimeter wave (mmWave) technology is envisioned to be an integral part
of next-generation vehicle-to-everything (V2X) networks and autonomous vehicles
due to its broad bandwidth, wide field of view sensing, and precise
localization capabilities. The reliability of mmWave links may be compromised
due to difficulties in beam alignment for mobile channels and due to blocking
effects between a mmWave transmitter and a receiver. To address such
challenges, out-of-band information from sub-6 GHz channels can be utilized for
predicting the temporal and angular channel characteristics in mmWave bands,
which necessitates a good understanding of how propagation characteristics are
coupled across different bands. In this paper, we use ray tracing simulations
to characterize the angular and temporal correlation across a wide range of
propagation frequencies for V2X channels ranging from 900 MHz up to 73 GHz, for
a vehicle maintaining line-of-sight (LOS) and non-LOS (NLOS) beams with a
transmitter in an urban environment. Our results shed light on increasing
sparsity behavior of propagation channels with increasing frequency and
highlight the strong temporal/angular correlation among 5.9 GHz and 28 GHz
bands especially for LOS channels.
","['\nChethan Kumar Anjinappa\n', '\nIsmail Guvenc\n']",,,http://arxiv.org/abs/1804.03505v1,eess.SP,"['eess.SP', 'cs.OH']",,,[]
"Prediction-Based Fast Thermoelectric Generator Reconfiguration for
  Energy Harvesting from Vehicle Radiators",http://arxiv.org/abs/1804.01574v1,2018-03-28T14:34:10Z,2018-03-28T14:34:10Z,"  Thermoelectric generation (TEG) has increasingly drawn attention for being
environmentally friendly. A few researches have focused on improving TEG
efficiency at the system level on vehicle radiators. The most recent
reconfiguration algorithm shows improvement in performance but suffers from
major drawback on computational time and energy overhead, and non-scalability
in terms of array size and processing frequency. In this paper, we propose a
novel TEG array reconfiguration algorithm that determines near-optimal
configuration with an acceptable computational time. More precisely, with
$O(N)$ time complexity, our prediction-based fast TEG reconfiguration algorithm
enables all modules to work at or near their maximum power points (MPP).
Additionally, we incorporate prediction methods to further reduce the runtime
and switching overhead during the reconfiguration process. Experimental results
present $30\%$ performance improvement, almost $100\times$ reduction on
switching overhead and $13\times$ enhancement on computational speed compared
to the baseline and prior work. The scalability of our algorithm makes it
applicable to larger scale systems such as industrial boilers and heat
exchangers.
","['\nHanchen Yang\n', '\nFeiyang Kang\n', '\nCaiwen Ding\n', '\nJi Li\n', '\nJaemin Kim\n', '\nDonkyu Baek\n', '\nShahin Nazarian\n', '\nXue Lin\n', '\nPaul Bogdan\n', '\nNaehyuck Chang\n']","4 pages, 7figurs; Accepted at Design Automation and Test in Europe
  (DATE) 2018",,http://arxiv.org/abs/1804.01574v1,cs.OH,['cs.OH'],,,[]
"A System for the Generation of Synthetic Wide Area Aerial Surveillance
  Imagery",http://arxiv.org/abs/1803.04856v1,2018-03-13T14:51:21Z,2018-03-13T14:51:21Z,"  The development, benchmarking and validation of aerial Persistent
Surveillance (PS) algorithms requires access to specialist Wide Area Aerial
Surveillance (WAAS) datasets. Such datasets are difficult to obtain and are
often extremely large both in spatial resolution and temporal duration. This
paper outlines an approach to the simulation of complex urban environments and
demonstrates the viability of using this approach for the generation of
simulated sensor data, corresponding to the use of wide area imaging systems
for surveillance and reconnaissance applications. This provides a
cost-effective method to generate datasets for vehicle tracking algorithms and
anomaly detection methods. The system fuses the Simulation of Urban Mobility
(SUMO) traffic simulator with a MATLAB controller and an image generator to
create scenes containing uninterrupted door-to-door journeys across large areas
of the urban environment. This `pattern-of-life' approach provides
three-dimensional visual information with natural movement and traffic flows.
This can then be used to provide simulated sensor measurements (e.g. visual
band and infrared video imagery) and automatic access to ground-truth data for
the evaluation of multi-target tracking systems.
","['\nElias J Griffith\n', '\nChinmaya Mishra\n', '\nJason F. Ralph\n', '\nSimon Maskell\n']","v1 (Accepted for publication in Simulation Modelling Practice and
  Theory)",,http://arxiv.org/abs/1803.04856v1,cs.OH,"['cs.OH', 'cs.SY', 'eess.IV']",,,[]
"CANA: A python package for quantifying control and canalization in
  Boolean Networks",http://arxiv.org/abs/1803.04774v2,2018-03-09T20:07:52Z,2018-05-09T18:15:06Z,"  Logical models offer a simple but powerful means to understand the complex
dynamics of biochemical regulation, without the need to estimate kinetic
parameters. However, even simple automata components can lead to collective
dynamics that are computationally intractable when aggregated into networks. In
previous work we demonstrated that automata network models of biochemical
regulation are highly canalizing, whereby many variable states and their
groupings are redundant (Marques-Pita and Rocha, 2013). The precise charting
and measurement of such canalization simplifies these models, making even very
large networks amenable to analysis. Moreover, canalization plays an important
role in the control, robustness, modularity and criticality of Boolean network
dynamics, especially those used to model biochemical regulation (Gates and
Rocha, 2016; Gates et al., 2016; Manicka, 2017). Here we describe a new
publicly-available Python package that provides the necessary tools to extract,
measure, and visualize canalizing redundancy present in Boolean network models.
It extracts the pathways most effective in controlling dynamics in these
models, including their effective graph and dynamics canalizing map, as well as
other tools to uncover minimum sets of control variables.
","['\nRion Brattig Correia\n', '\nAlexander J. Gates\n', '\nXuan Wang\n', '\nLuis M. Rocha\n']",Submitted to the Systems Biology section of Frontiers in Physiology,"Frontiers in Physiology, 9:1046, 2018",http://dx.doi.org/10.3389/fphys.2018.01046,cs.OH,"['cs.OH', 'cs.CE', 'cs.DM', 'cs.SY', 'q-bio.MN', 'q-bio.QM', '94C (Primary) 93, 92C42 (Secondary)', 'G.4; I.1; J.3']",10.3389/fphys.2018.01046,,[]
"Predicting Transportation Modes of GPS Trajectories using Feature
  Engineering and Noise Removal",http://arxiv.org/abs/1802.10164v1,2018-02-27T21:07:21Z,2018-02-27T21:07:21Z,"  Understanding transportation mode from GPS (Global Positioning System) traces
is an essential topic in the data mobility domain. In this paper, a framework
is proposed to predict transportation modes. This framework follows a sequence
of five steps: (i) data preparation, where GPS points are grouped in trajectory
samples; (ii) point features generation; (iii) trajectory features extraction;
(iv) noise removal; (v) normalization. We show that the extraction of the new
point features: bearing rate, the rate of rate of change of the bearing rate
and the global and local trajectory features, like medians and percentiles
enables many classifiers to achieve high accuracy (96.5%) and f1 (96.3%)
scores. We also show that the noise removal task affects the performance of all
the models tested. Finally, the empirical tests where we compare this work
against state-of-art transportation mode prediction strategies show that our
framework is competitive and outperforms most of them.
","['\nMohammad Etemad\n', '\nAmilcar Soares Junior\n', '\nStan Matwin\n']",6 pages,,http://dx.doi.org/10.1007/978-3-319-89656-4_24,cs.OH,['cs.OH'],10.1007/978-3-319-89656-4_24,,[]
"A Generative Model for Non-Intrusive Load Monitoring in Commercial
  Buildings",http://arxiv.org/abs/1803.00515v1,2018-02-26T15:40:42Z,2018-02-26T15:40:42Z,"  In the recent years, there has been an increasing academic and industrial
interest for analyzing the electrical consumption of commercial buildings.
Whilst having similarities with the Non Intrusive Load Monitoring (NILM) tasks
for residential buildings, the nature of the signals that are collected from
large commercial buildings introduces additional difficulties to the NILM
research causing existing NILM approaches to fail. On the other hand, the
amount of publicly available datasets collected from commercial buildings is
very limited, which makes the NILM research even more challenging for this type
of large buildings. In this study, we aim at addressing these issues. We first
present an extensive statistical analysis of both commercial and residential
measurements from public and private datasets and show important differences.
Secondly, we develop an algorithm for generating synthetic current waveforms.
We then demonstrate using real measurement and quantitative metrics that both
our device model and our simulations are realistic and can be used to evaluate
NILM algorithms. Finally, to encourage research on commercial buildings we
release a synthesized dataset.
","['\nSimon Henriet\n', '\nUmut Simsekli\n', '\nBenoit Fuentes\n', '\nGaël Richard\n']","Submitted to Energy and Buildings, Elsevier",,http://arxiv.org/abs/1803.00515v1,cs.OH,['cs.OH'],,,[]
Analysing the Potential of BLE to Support Dynamic Broadcasting Scenarios,http://arxiv.org/abs/1803.02309v1,2018-03-06T17:20:38Z,2018-03-06T17:20:38Z,"  In this paper, we present a novel approach for broadcasting information based
on a Bluetooth Low Energy (BLE) ibeacon technology. We propose a dynamic method
that uses a combination of Wi-Fi and BLE technology where every technology
plays a part in a user discovery and broadcasting process. In such system, a
specific ibeacon device broadcasts the information when a user is in proximity.
Using experiments, we conduct a scenario where the system discovers users,
disseminates information, and later we use collected data to examine the system
performance and capability. The results show that our proposed approach has a
promising potential to become a powerful tool in the discovery and broadcasting
concept that can be easily implemented and used in business environments.
","['\nMiran Borić\n', '\nAna Fernández Vilas\n', '\nRebeca P. Díaz Redondo\n']",,,http://arxiv.org/abs/1803.02309v1,cs.OH,['cs.OH'],,,[]
Automation of Processor Verification Using Recurrent Neural Networks,http://arxiv.org/abs/1803.09810v1,2018-03-06T10:49:34Z,2018-03-06T10:49:34Z,"  When considering simulation-based verification of processors, the current
trend is to generate stimuli using pseudorandom generators (PRGs), apply them
to the processor inputs and monitor the achieved coverage of its functionality
in order to determine verification completeness. Stimuli can have different
forms, for example, they can be represented by bit vectors applied to the input
ports of the processor or by programs that are loaded directly into the program
memory. In this paper, we propose a new technique dynamically altering
constraints for PRG via recurrent neural network, which receives a coverage
feedback from the simulation of design under verification. For the
demonstration purposes we used processors provided by Codasip as their coverage
state space is reasonably big and differs for various kinds of processors.
Nevertheless, techniques presented in this paper are widely applicable. The
results of experiments show that not only the coverage closure is achieved much
sooner, but we are able to isolate a small set of stimuli with high coverage
that can be used for running regression tests.
","['\nMartin Fajcik\n', '\nMarcela Zachariasova\n', '\nPavel Smrz\n']","Paper contains 6 pages, 6 figures. Presented on MTVCon 2017. Soon to
  be released by IEEE",,http://arxiv.org/abs/1803.09810v1,cs.OH,['cs.OH'],,,[]
"A Novel Approach for Fast and Accurate Mean Error Distance Computation
  in Approximate Adders",http://arxiv.org/abs/1803.08005v1,2018-03-06T10:04:29Z,2018-03-06T10:04:29Z,"  In error-tolerant applications, approximate adders have been exploited
extensively to achieve energy efficient system designs. Mean error distance is
one of the important error metrics used as a performance measure of approximate
adders. In this work, a fast and efficient methodology is proposed to determine
the exact mean error distance in approximate lower significant bit adders. A
detailed description of the proposed algorithm along with an example has been
demonstrated in this paper. Experimental analysis shows that the proposed
method performs better than existing Monte Carlo simulation approach both in
terms of accuracy and execution time.
","['\nAvishek Sinha Roy\n', '\nAnindya Sundar Dhar\n']",Paper accepted at ISCAS 2018,,http://arxiv.org/abs/1803.08005v1,cs.OH,"['cs.OH', 'cs.ET']",,,[]
"On the economics of electrical storage for variable renewable energy
  sources",http://arxiv.org/abs/1802.07885v2,2018-02-19T15:52:14Z,2018-06-05T00:10:56Z,"  The use of renewable energy sources is a major strategy to mitigate climate
change. Yet Sinn (2017) argues that excessive electrical storage requirements
limit the further expansion of variable wind and solar energy. We question, and
alter, strong implicit assumptions of Sinn's approach and find that storage
needs are considerably lower, up to two orders of magnitude. First, we move
away from corner solutions by allowing for combinations of storage and
renewable curtailment. Second, we specify a parsimonious optimization model
that explicitly considers an economic efficiency perspective. We conclude that
electrical storage is unlikely to limit the transition to renewable energy.
","['\nAlexander Zerrahn\n', '\nWolf-Peter Schill\n', '\nClaudia Kemfert\n']",,European Economic Review 2018,http://dx.doi.org/10.1016/j.euroecorev.2018.07.004,physics.soc-ph,"['physics.soc-ph', 'cs.OH']",10.1016/j.euroecorev.2018.07.004,,[]
On the Constituent Attributes of Software and Organisational Resilience,http://arxiv.org/abs/1803.05992v1,2018-02-21T18:18:10Z,2018-02-21T18:18:10Z,"  Our societies are increasingly dependent on services supplied by computers &
their software. New technology only exacerbates this dependence by increasing
the number, performance, and degree of autonomy and inter-connectivity of
software-empowered computers and cyber-physical ""things"", which translates into
unprecedented scenarios of interdependence. As a consequence, guaranteeing the
persistence-of-identity of individual & collective software systems and
software-backed organisations becomes an important prerequisite toward
sustaining the safety, security, & quality of the computer services supporting
human societies. Resilience is the term used to refer to the ability of a
system to retain its functional and non-functional identity. In this article we
conjecture that a better understanding of resilience may be reached by
decomposing it into ancillary constituent properties, the same way as a better
insight in system dependability was obtained by breaking it down into
sub-properties. 3 of the main sub-properties of resilience proposed here refer
respectively to the ability to perceive environmental changes; understand the
implications introduced by those changes; and plan & enact adjustments intended
to improve the system-environment fit. A fourth property characterises the way
the above abilities manifest themselves in computer systems. The 4 properties
are then analyzed in 3 families of case studies, each consisting of 3 software
systems that embed different resilience methods. Our major conclusion is that
reasoning in terms of resilience sub-properties may help revealing the
characteristics and limitations of classic methods and tools meant to achieve
system and organisational resilience. We conclude by suggesting that our method
may prelude to meta-resilient systems -- systems, that is, able to adjust
optimally their own resilience with respect to changing environmental
conditions.
",['\nDe Florio Vincenzo\n'],"Pre-camera-ready of a paper published in ""Interdisciplinary Science
  Reviews"", Volume 38, 2013 - Issue 2. arXiv admin note: text overlap with
  arXiv:1401.3621 by other authors",,http://dx.doi.org/10.1179/0308018813Z.00000000040,cs.CY,"['cs.CY', 'cs.OH']",10.1179/0308018813Z.00000000040,,[]
Road Network Fusion for Incremental Map Updates,http://arxiv.org/abs/1802.02351v1,2018-02-07T08:41:39Z,2018-02-07T08:41:39Z,"  In the recent years a number of novel, automatic map-inference techniques
have been proposed, which derive road-network from a cohort of GPS traces
collected by a fleet of vehicles. In spite of considerable attention, these
maps are imperfect in many ways: they create an abundance of spurious
connections, have poor coverage, and are visually confusing. Hence, commercial
and crowd-sourced mapping services heavily use human annotation to minimize the
mapping errors. Consequently, their response to changes in the road network is
inevitably slow. In this paper we describe \mapfuse, a system which fuses a
human-annotated map (e.g., OpenStreetMap) with any automatically inferred map,
thus effectively enabling quick map updates. In addition to new road creation,
we study in depth road closure, which have not been examined in the past. By
leveraging solid, human-annotated maps with minor corrections, we derive maps
which minimize the trajectory matching errors due to both road network change
and imperfect map inference of fully-automatic approaches.
","['\nRade Stanojevic\n', '\nSofiane Abbar\n', '\nSaravanan Thirumuruganathan\n', '\nGianmarco De Francisci Morales\n', '\nSanjay Chawla\n', '\nFethi Filali\n', '\nAhid Aleimat\n']",,"In the special volume of Springer's Lecture Notes in Cartography
  and Geoinformation (LBS 2018.)",http://arxiv.org/abs/1802.02351v1,cs.OH,['cs.OH'],,,[]
"A veracity preserving model for synthesizing scalable electricity load
  profiles",http://arxiv.org/abs/1802.03500v1,2018-02-10T01:56:06Z,2018-02-10T01:56:06Z,"  Electricity users are the major players of the electric systems, and
electricity consumption is growing at an extraordinary rate. The research on
electricity consumption behaviors is becoming increasingly important to design
and deployment of the electric systems. Unfortunately, electricity load
profiles are difficult to acquire. Data synthesis is one of the best approaches
to solving the lack of data, and the key is the model that preserves the real
electricity consumption behaviors. In this paper, we propose a hierarchical
multi-matrices Markov Chain (HMMC) model to synthesize scalable electricity
load profiles that preserve the real consumption behavior on three time scales:
per day, per week, and per year. To promote the research on the electricity
consumption behavior, we use the HMMC approach to model two distinctive raw
electricity load profiles. One is collected from the resident sector, and the
other is collected from the non-resident sectors, including different
industries such as education, finance, and manufacturing. The experiments show
our model performs much better than the classical Markov Chain model. We
publish two trained models online, and researchers can directly use these
trained models to synthesize scalable electricity load profiles for further
researches.
","['\nYunyou Huang\n', '\nJianfeng Zhan\n', '\nChunjie Luo\n', '\nLei Wang\n', '\nNana Wang\n', '\nDaoyi Zheng\n', '\nFanda Fan\n', '\nRui Ren\n']",,,http://arxiv.org/abs/1802.03500v1,cs.OH,['cs.OH'],,,[]
"A cost effective and reliable environment monitoring system for HPC
  applications",http://arxiv.org/abs/1802.00724v1,2018-01-29T10:29:21Z,2018-01-29T10:29:21Z,"  We present a slow control system to gather all relevant environment
information necessary to effectively and reliably run an HPC (High Performance
Computing) system at a high value over price ratio. The scalable and reliable
overall concept is presented as well as a newly developed hardware device for
sensor read out. This device incorporates a Raspberry Pi, an Arduino and PoE
(Power over Ethernet) functionality in a compact form factor. The system is in
use at the 2 PFLOPS cluster of the Johannes Gutenberg-University and
Helmholtz-Institute in Mainz.
","['\nPeter Bernd Otte\n', '\nDalibor Djukanovic\n']",,,http://arxiv.org/abs/1802.00724v1,cs.OH,['cs.OH'],,,[]
"Hardware implementation of auto-mutual information function for
  condition monitoring",http://arxiv.org/abs/1801.08444v1,2018-01-25T15:15:02Z,2018-01-25T15:15:02Z,"  This study is aimed at showing applicability of mutual information, namely
auto-mutual information function for condition monitoring in electrical motors,
through age detection in accelerated motor aging. Vibration data collected in
artificial induction motor experiment is used for verification of both the
original auto-mutual information function algorithm and its hardware
implementation in Verilog, produced from an initial version made with Matlab
HDL (Hardware Description Language) Coder. A conceptual model for industry and
education based on a field programmable logic array development board is
developed and demonstrated on the auto-mutual information function example,
while suggesting other applications as well. It has also been shown that
attractor reconstruction for the vibration data cannot be straightforward.
","['\nHarun Siljak\n', '\nAbdulhamit Subasi\n', '\nBelle R. Upadhyaya\n']",,,http://dx.doi.org/10.1016/j.compeleceng.2018.01.038,cs.OH,"['cs.OH', 'eess.SP']",10.1016/j.compeleceng.2018.01.038,,[]
TikZ-FeynHand: Basic User Guide,http://arxiv.org/abs/1802.00689v1,2018-01-31T19:19:03Z,2018-01-31T19:19:03Z,"  This is a userguide for the LaTex package Tikz-FeynHand at
https://ctan.org/pkg/tikz-feynhand which let's you draw Feynman diagrams using
TikZ. It contains many examples and a 5-minute introduction to TikZ.
  The package is a low-end modification of the package TikZ-Feynman at
https://ctan.org/pkg/tikz-feynman, one of whose principal advantages is the
automatic generation of diagrams, for which it needs LuaTex. FeynHand only
provides the manual mode and hence runs in LaTex without any reference to
LuaTex.
  In addition it provides some NEW STYLES for vertices and propagators,
alternative SHORTER KEYWORDS in addition to TikZ-Feynman's longer ones, some
shortcut commands for QUICKLY CUSTOMIZING the diagrams' look, and the new
feature to put one propagator ""ON TOP"" of another.
",['\nMax Dohse\n'],"12 pages, many figures",,http://arxiv.org/abs/1802.00689v1,cs.OH,"['cs.OH', 'hep-ph', 'hep-th']",,,[]
The Socket Store: An App Model for the Application-Network Interaction,http://arxiv.org/abs/1801.05611v1,2018-01-17T10:11:39Z,2018-01-17T10:11:39Z,"  A developer of mobile or desktop applications is responsible for implementing
the network logic of his software. Nonetheless: i) Developers are not network
specialists, while pressure for emphasis on the visible application parts
places the network logic out of the coding focus. Moreover, computer networks
undergo evolution at paces that developers may not follow. ii) From the network
resource provider point of view, marketing novel services and involving a broad
audience is also challenge for the same reason. Moreover, the objectives of
end-user networking logic are neither clear nor uniform. This constitutes the
central optimization of network resources an additional challenge. As a
solution to these problems, we propose the Socket Store. The Store is a
marketplace containing end-user network logic in modular form. The Store
modules act as intelligent mediators between the end-user and the network
resources. Each module has a clear, specialized objective, such as connecting
two clients over the Internet while avoiding transit networks suspicious for
eavesdropping. The Store is populated and peer-reviewed by network specialists,
whose motive is the visibility, practical applicability and monetization
potential of their work. A developer first purchases access to a given socket
module. Subsequently, he incorporates it to his applications under development,
obtaining state-of-the-art performance with trivial coding burden. A full Store
prototype is implemented and a critical data streaming module is evaluated as a
driving case.
","['\nChristos Liaskos\n', '\nAgeliki Tsioliaridou\n', '\nSotiris Ioannidis\n']",,,http://dx.doi.org/10.1109/ISCC.2017.8024557,cs.OH,['cs.OH'],10.1109/ISCC.2017.8024557,,[]
"StreetGen : In base city scale procedural generation of streets: road
  network, road surface and street objects",http://arxiv.org/abs/1801.05741v1,2018-01-17T16:33:28Z,2018-01-17T16:33:28Z,"  Streets are large, diverse, and used for several (and possibly conflicting)
transport modalities as well as social and cultural activities. Proper planning
is essential and requires data. Manually fabricating data that represent
streets (street reconstruction) is error-prone and time consuming. Automatising
street reconstruction is a challenge because of the diversity, size, and scale
of the details (few centimetres for cornerstone) required. The state-of-the-art
focuses on roads (no context, no urban features) and is strongly determined by
each application (simulation, visualisation, planning). We propose a unified
framework that works on real Geographic Information System (GIS) data and uses
a strong, yet simple modelling hypothesis when possible to robustly model
streets at the city level or street level. Our method produces a coherent
street-network model containing topological traffic information, road surface
and street objects. We demonstrate the robustness and genericity of our method
by reconstructing the entire city of Paris streets and exploring other similar
reconstruction (airport driveway).
","['\nRémi Cura\n', '\nJulien Perret\n', '\nNicolas Paparoditis\n']","Paper extracted from thesis manuscript, is also an extension of
  doi:10.5194/isprsannals-II-3-W5-409-201",,http://arxiv.org/abs/1801.05741v1,cs.OH,['cs.OH'],,,[]
"Mobility Based Routing Protocol with MAC Collision Improvement in
  Vehicular Ad Hoc Networks",http://arxiv.org/abs/1801.06502v1,2018-01-19T17:39:27Z,2018-01-19T17:39:27Z,"  Intelligent transportation system attracts a great deal of research attention
because it helps enhance traffic safety, improve driving experiences, and
transportation efficiency. Vehicular Ad Hoc Network (VANET) supports wireless
connections among vehicles and offers information exchange, thus significantly
facilitating intelligent transportation systems. Since the vehicles move fast
and often change lanes unpredictably, the network topology evolves rapidly in a
random fashion, which imposes diverse challenges in routing protocol design
over VANET. When it comes to the 5G era, the fulfilment of ultra low end-to-end
delay and ultra high reliability becomes more crucial than ever. In this paper,
we propose a novel routing protocol that incorporates mobility status and MAC
layer channel contention information. The proposed routing protocol determines
next hop by applying mobility information and MAC contention information which
differs from existing greedy perimeter stateless routing (GPSR) protocol.
Simulation results of the proposed routing protocol show its performance
superiority over the existing approach.
","['\nZhihao Ding\n', '\nPinyi Ren\n', '\nQinghe Du\n']",,,http://arxiv.org/abs/1801.06502v1,cs.OH,['cs.OH'],,,[]
Approximability in the GPAC,http://arxiv.org/abs/1801.07661v3,2018-01-15T10:49:03Z,2019-08-28T12:33:20Z,"  Most of the physical processes arising in nature are modeled by either
ordinary or partial differential equations. From the point of view of analog
computability, the existence of an effective way to obtain solutions of these
systems is essential. A pioneering model of analog computation is the General
Purpose Analog Computer (GPAC), introduced by Shannon as a model of the
Differential Analyzer and improved by Pour-El, Lipshitz and Rubel, Costa and
Gra\c{c}a and others. Its power is known to be characterized by the class of
differentially algebraic functions, which includes the solutions of initial
value problems for ordinary differential equations. We address one of the
limitations of this model, concerning the notion of approximability, a
desirable property in computation over continuous spaces that is however absent
in the GPAC. In particular, the Shannon GPAC cannot be used to generate
non-differentially algebraic functions which can be approximately computed in
other models of computation. We extend the class of data types using networks
with channels which carry information on a general complete metric space $X$;
for example $X=C(R,R)$, the class of continuous functions of one real (spatial)
variable. We consider the original modules in Shannon's construction
(constants, adders, multipliers, integrators) and we add \emph{(continuous or
discrete) limit} modules which have one input and one output. We then define an
L-GPAC to be a network built with $X$-stream channels and the above-mentioned
modules. This leads us to a framework in which the specifications of such
analog systems are given by fixed points of certain operators on continuous
data streams. We study these analog systems and their associated operators, and
show how some classically non-generable functions, such as the gamma function
and the zeta function, can be captured with the L-GPAC.
","['\nDiogo Poças\n', '\nJeffery Zucker\n']",,"Logical Methods in Computer Science, Volume 15, Issue 3 (August
  29, 2019) lmcs:4235",http://dx.doi.org/10.23638/LMCS-15(3:24)2019,cs.OH,['cs.OH'],10.23638/LMCS-15(3:24)2019,,[]
"A state of the art of urban reconstruction: street, street network,
  vegetation, urban feature",http://arxiv.org/abs/1803.04332v1,2018-01-18T02:11:18Z,2018-01-18T02:11:18Z,"  World population is raising, especially the part of people living in cities.
With increased population and complex roles regarding their inhabitants and
their surroundings, cities concentrate difficulties for design, planning and
analysis. These tasks require a way to reconstruct/model a city. Traditionally,
much attention has been given to buildings reconstruction, yet an essential
part of city were neglected: streets. Streets reconstruction has been seldom
researched. Streets are also complex compositions of urban features, and have a
unique role for transportation (as they comprise roads). We aim at completing
the recent state of the art for building reconstruction (Musialski2012) by
considering all other aspect of urban reconstruction. We introduce the need for
city models. Because reconstruction always necessitates data, we first analyse
which data are available. We then expose a state of the art of street
reconstruction, street network reconstruction, urban features
reconstruction/modelling, vegetation , and urban objects
reconstruction/modelling.
  Although reconstruction strategies vary widely, we can order them by the role
the model plays, from data driven approach, to model-based approach, to inverse
procedural modelling and model catalogue matching. The main challenges seems to
come from the complex nature of urban environment and from the limitations of
the available data. Urban features have strong relationships, between them, and
to their surrounding, as well as in hierarchical relations. Procedural
modelling has the power to express these relations, and could be applied to the
reconstruction of urban features via the Inverse Procedural Modelling paradigm.
","['\nRemi Cura\n', '\nJulien Perret\n', '\nNicolas Paparoditis\n']",Extracted from PhD (chap1),,http://arxiv.org/abs/1803.04332v1,cs.OH,"['cs.OH', 'cs.CV']",,,[]
"Step Detection Algorithm For Accurate Distance Estimation Using Dynamic
  Step Length",http://arxiv.org/abs/1801.02336v1,2018-01-08T08:26:12Z,2018-01-08T08:26:12Z,"  In this paper, a new Smartphone sensor based algorithm is proposed to detect
accurate distance estimation. The algorithm consists of two phases, the first
phase is for detecting the peaks from the Smartphone accelerometer sensor. The
other one is for detecting the step length which varies from step to step. The
proposed algorithm is tested and implemented in real environment and it showed
promising results. Unlike the conventional approaches, the error of the
proposed algorithm is fixed and is not affected by the long distance.
  Keywords distance estimation, peaks, step length, accelerometer.
","['\nAhmad Abadleh\n', '\nEshraq Al-Hawari\n', ""\nEsra'a Alkafaween\n"", '\nHamad Al-Sawalqah\n']",this paper contains of 5 pages and 6 figures,"In Mobile Data Management (MDM), 2017 18th IEEE International
  Conference on, pp. 324-327. IEEE, 2017",http://arxiv.org/abs/1801.02336v1,cs.OH,['cs.OH'],,,[]
Analytical Inverter Delay Modeling Using Matlab's Curve Fitting Toolbox,http://arxiv.org/abs/1801.00005v1,2017-12-29T14:33:16Z,2017-12-29T14:33:16Z,"  This paper presents a new analytical propagation delay model for deep
submicron CMOS inverters. The model is inspired by the key observation that the
inverter delay is a complicated function of several process parameters as well
as load capacitance. These relationships are considered by fitting functions
for each parameter derived from the Curve Fitting Toolbox in Matlab. Compared
to SPICE simulations based on the BSIM4 transistor model, the analytical delay
model shows very good accuracy with an average error less than 2% over a wide
range of process parameters and output loads. Hence, the proposed model can be
efficiently used for different technology nodes as well as statistical gate
delay characterisation.
",['\nWalter Schneider\n'],,,http://arxiv.org/abs/1801.00005v1,cs.OH,"['cs.OH', 'cs.AR']",,,[]
OpenSEA: Semi-Formal Methods for Soft Error Analysis,http://arxiv.org/abs/1712.04291v1,2017-12-12T13:51:57Z,2017-12-12T13:51:57Z,"  Alpha-particles and cosmic rays cause bit flips in chips. Protection circuits
ease the problem, but cost chip area and power, and so designers try hard to
optimize them. This leads to bugs: an undetected fault can bring
miscalculations, the checker that alarms about harmless faults incurs
performance penalty. Such bugs are hard to find: circuit simulation with tests
is inefficient since it enumerates the huge fault time-location space, and
formal methods do not scale since they explore the whole inputs. In this paper,
we use formal methods on designer's input tests, while keeping time-location
open. This idea is at the core of the tool OpenSEA. OpenSEA can (i) find
latches vulnerable to and protected against faults, (ii) find tests that
exhibit checker false alarms, (iii) use fixed and open inputs, and (iv) use
environment assumptions. Evaluation on a number of industrial designs shows
that OpenSEA produces valuable results.
","['\nPatrick Klampfl\n', '\nRobert Koenighofer\n', '\nRoderick Bloem\n', '\nAyrat Khalimov\n', '\nAiman Abu-Yonis\n', '\nShiri Moran\n']",,,http://arxiv.org/abs/1712.04291v1,cs.OH,['cs.OH'],,,[]
"The View from the Other Side: The Border Between Controversial Speech
  and Harassment on Kotaku in Action",http://arxiv.org/abs/1712.05851v2,2017-12-11T20:56:27Z,2018-02-08T06:57:34Z,"  In this paper, we use mixed methods to study a controversial Internet site:
The Kotaku in Action (KiA) subreddit. Members of KiA are part of GamerGate, a
distributed social movement. We present an emic account of what takes place on
KiA who are they, what are their goals and beliefs, and what rules do they
follow. Members of GamerGate in general and KiA in particular have often been
accused of harassment. However, KiA site policies explicitly prohibit such
behavior, and members insist that they have been falsely accused. Underlying
the controversy over whether KiA supports harassment is a complex disagreement
about what ""harassment"" is, and where to draw the line between freedom of
expression and censorship. We propose a model that characterizes perceptions of
controversial speech, dividing it into four categories: criticism, insult,
public shaming, and harassment. We also discuss design solutions that address
the challenges of moderating harassment without impinging on free speech, and
communicating across different ideologies.
","['\nShagun Jhaver\n', '\nLarry Chan\n', '\nAmy Bruckman\n']","41 pages, 3 figures, under review at First Monday Journal","Jhaver, S., Chan, L., & Bruckman, A. (2018). The view from the
  other side: The border between controversial speech and harassment on Kotaku
  in Action. First Monday, 23(2)",http://dx.doi.org/10.5210/fm.v23i2.8232,cs.OH,['cs.OH'],10.5210/fm.v23i2.8232,,[]
"""Oh Tanenbaum, oh Tanenbaum..."": Technical Foundations of Xmas 4.0
  Research",http://arxiv.org/abs/1712.06259v1,2017-12-18T06:09:29Z,2017-12-18T06:09:29Z,"  Andrew Tanenbaum and his textbooks -- e.g. on Operating Systems, Computer
Networks, Structured Computer Organization and Distributed Systems, to name but
a few -- have had a tremendous impact on generations of computer science
students (and teachers at the same time). Given this, it is striking to observe
that this comprehensive body of work apparently does not provide a single line
on a research topic that seems to be intimately related with his name (at least
in German), i.e. Xmas Research (XR). Hence, the goal of this paper is to fill
this gap and provide insight into a number of paradigmatic XR research
questions, for instance: Can we today still count on Santa Claus? Or at least
on Xmas trees? And does this depend on basic tree structures, or can we rather
find solutions on the level of programming languages? By addressing such basic
open issues, we aim at providing a solid technical foundation for future steps
towards the imminent evolution of Xmas 4.0.
","['\nP. Reichl\n', '\nS. Claus\n']","5 pages, 12 figures",,http://arxiv.org/abs/1712.06259v1,cs.OH,['cs.OH'],,,[]
"Effect of NBTI/PBTI Aging and Process Variations on Write Failures in
  MOSFET and FinFET Flip-Flops",http://arxiv.org/abs/1712.06934v1,2017-12-15T15:10:35Z,2017-12-15T15:10:35Z,"  The assessment of noise margins and the related probability of failure in
digital cells has growingly become essential, as nano-scale CMOS and FinFET
technologies are confronting reliability issues caused by aging mechanisms,
such as NBTI, and variability in process parameters. The influence of such
phenomena is particularly associated to the Write Noise Margins (WNM) in memory
elements, since a wrong stored logic value can result in an upset of the system
state. In this work, we calculated and compared the effect of process
variations and NBTI aging over the years on the actual WNM of various CMOS and
FinFET based flip-flop cells. The massive transistor-level Monte Carlo
simulations produced both nominal (i.e. mean) values and associated standard
deviations of the WNM of the chosen flip-flops. This allowed calculating the
consequent write failure probability as a function of an input voltage shift on
the flip-flop cells, and assessing a comparison for robustness among different
circuit topologies and technologies.
","['\nUsman Khalid\n', '\nAntonio Mastrandrea\n', '\nMauro Olivieri\n']",14 pages,"Microelectronics Reliability 55(12), August 2015, Elsevier",http://dx.doi.org/10.1016/j.microrel.2015.07.050,cs.OH,['cs.OH'],10.1016/j.microrel.2015.07.050,,[]
"automan: a simple, Python-based, automation framework for numerical
  computing",http://arxiv.org/abs/1712.04786v2,2017-12-11T21:58:40Z,2018-02-04T11:52:35Z,"  We present an easy-to-use, Python-based framework that allows a researcher to
automate their computational simulations. In particular the framework
facilitates assembling several long-running computations and producing various
plots from the data produced by these computations. The framework makes it
possible to reproduce every figure made for a publication with a single
command. It also allows one to distribute the computations across a network of
computers. The framework has been used to write research papers in numerical
computing. This paper discusses the design of the framework, and the benefits
of using it. The ideas presented are general and should help researchers
organize their computations for better reproducibility.
",['\nPrabhu Ramachandran\n'],,"CiSE, vol. 20, no. 5, pp. 81-97, 2018",http://dx.doi.org/10.1109/MCSE.2018.05329818,cs.OH,"['cs.OH', 'cs.DC', '68U20']",10.1109/MCSE.2018.05329818,,[]
Understanding Career Progression in Baseball Through Machine Learning,http://arxiv.org/abs/1712.05754v1,2017-12-15T17:02:03Z,2017-12-15T17:02:03Z,"  Professional baseball players are increasingly guaranteed expensive long-term
contracts, with over 70 deals signed in excess of \$90 million, mostly in the
last decade. These are substantial sums compared to a typical franchise
valuation of \$1-2 billion. Hence, the players to whom a team chooses to give
such a contract can have an enormous impact on both competitiveness and profit.
Despite this, most published approaches examining career progression in
baseball are fairly simplistic. We applied four machine learning algorithms to
the problem and soundly improved upon existing approaches, particularly for
batting data.
","['\nBrian Bierig\n', '\nJonathan Hollenbeck\n', '\nAlexander Stroud\n']","5 pages, class project for CS229 Fall 2017 at Stanford",,http://arxiv.org/abs/1712.05754v1,stat.ML,"['stat.ML', 'cs.OH']",,,[]
Limits for Rumor Spreading in stochastic populations,http://arxiv.org/abs/1712.08507v1,2017-12-20T19:02:33Z,2017-12-20T19:02:33Z,"  Biological systems can share and collectively process information to yield
emergent effects, despite inherent noise in communication. While man-made
systems often employ intricate structural solutions to overcome noise, the
structure of many biological systems is more amorphous. It is not well
understood how communication noise may affect the computational repertoire of
such groups. To approach this question we consider the basic collective task of
rumor spreading, in which information from few knowledgeable sources must
reliably flow into the rest of the population.
  In order to study the effect of communication noise on the ability of groups
that lack stable structures to efficiently solve this task, we consider a noisy
version of the uniform PULL model. We prove a lower bound which implies that,
in the presence of even moderate levels of noise that affect all facets of the
communication, no scheme can significantly outperform the trivial one in which
agents have to wait until directly interacting with the sources. Our results
thus show an exponential separation between the uniform PUSH and PULL
communication models in the presence of noise. Such separation may be
interpreted as suggesting that, in order to achieve efficient rumor spreading,
a system must exhibit either some degree of structural stability or,
alternatively, some facet of the communication which is immune to noise.
  We corroborate our theoretical findings with a new analysis of experimental
data regarding recruitment in Cataglyphis niger desert ants.
","['\nLucas Boczkowski\n', '\nOfer Feinerman\n', '\nAmos Korman\n', '\nEmanuele Natale\n']",,,http://arxiv.org/abs/1712.08507v1,cs.MA,"['cs.MA', 'cs.OH', 'F.2.2']",,,[]
DeepPicar: A Low-cost Deep Neural Network-based Autonomous Car,http://arxiv.org/abs/1712.08644v4,2017-12-19T22:24:08Z,2018-07-30T02:29:01Z,"  We present DeepPicar, a low-cost deep neural network based autonomous car
platform. DeepPicar is a small scale replication of a real self-driving car
called DAVE-2 by NVIDIA. DAVE-2 uses a deep convolutional neural network (CNN),
which takes images from a front-facing camera as input and produces car
steering angles as output. DeepPicar uses the same network architecture---9
layers, 27 million connections and 250K parameters---and can drive itself in
real-time using a web camera and a Raspberry Pi 3 quad-core platform. Using
DeepPicar, we analyze the Pi 3's computing capabilities to support end-to-end
deep learning based real-time control of autonomous vehicles. We also
systematically compare other contemporary embedded computing platforms using
the DeepPicar's CNN-based real-time control workload. We find that all tested
platforms, including the Pi 3, are capable of supporting the CNN-based
real-time control, from 20 Hz up to 100 Hz, depending on hardware platform.
However, we find that shared resource contention remains an important issue
that must be considered in applying CNN models on shared memory based embedded
computing platforms; we observe up to 11.6X execution time increase in the CNN
based control loop due to shared resource contention. To protect the CNN
workload, we also evaluate state-of-the-art cache partitioning and memory
bandwidth throttling techniques on the Pi 3. We find that cache partitioning is
ineffective, while memory bandwidth throttling is an effective solution.
","['\nMichael G. Bechtel\n', '\nElise McEllhiney\n', '\nMinje Kim\n', '\nHeechul Yun\n']",To be published as a conference paper at RTCSA 2018,,http://arxiv.org/abs/1712.08644v4,cs.OH,"['cs.OH', 'cs.DC', 'cs.PF']",,,[]
An optical solution for the set splitting problem,http://arxiv.org/abs/1712.00651v1,2017-12-02T18:13:07Z,2017-12-02T18:13:07Z,"  We describe here an optical device, based on time-delays, for solving the set
splitting problem which is well-known NP-complete problem. The device has a
graph-like structure and the light is traversing it from a start node to a
destination node. All possible (potential) paths in the graph are generated and
at the destination we will check which one satisfies completely the problem's
constrains.
",['\nMihai Oltean\n'],"10 pages, 2 figures","Acta Univ. Sapientiae, Informatica 9, 2 (2017) 134-143",http://dx.doi.org/10.1515/ausi-2017-0009,cs.OH,"['cs.OH', '68Q05, 68W10', 'F.1.1']",10.1515/ausi-2017-0009,,[]
The process of 3D-printed skull models for the anatomy education,http://arxiv.org/abs/1711.07106v1,2017-11-19T23:58:25Z,2017-11-19T23:58:25Z,"  Objective The 3D printed medical models can come from virtual digital
resources, like CT scanning. Nevertheless, the accuracy of CT scanning
technology is limited, which is 1mm. In this situation, the collected data is
not exactly the same as the real structure and there might be some errors
causing the print to fail. This study presents a common and practical way to
process the skull data to make the structures correctly. And then we make a
skull model through 3D printing technology, which is useful for medical
students to understand the complex structure of skull. Materials and Methods
The skull data is collected by the CT scan. To get a corrected medical model,
the computer-assisted image processing goes with the combination of five 3D
manipulation tools: Mimics, 3ds Max, Geomagic, Mudbox and Meshmixer, to
reconstruct the digital model and repair it. Subsequently, we utilize a
low-cost desktop 3D printer, Ultimaker2, with polylactide filament (PLA)
material to print the model and paint it based on the atlas. Result After the
restoration and repairing, we eliminate the errors and repair the model by
adding the missing parts of the uploaded data within 6 hours. Then we print it
and compare the model with the cadaveric skull from frontal, left, right and
anterior views respectively. The printed model can show the same structures and
also the details of the skull clearly and is a good alternative of the
cadaveric skull.
","['\nZhen Shen\n', '\nYong Yao\n', '\nYi Xie\n', '\nChao Guo\n', '\nXiuqin Shang\n', '\nXisong Dong\n', '\nYuqing Li\n', '\nZhouxian Pan\n', '\nShi Chen\n', '\nHui Pan\n', '\nGang Xiong\n']",,,http://arxiv.org/abs/1711.07106v1,cs.OH,['cs.OH'],,,[]
"Obtaining the coefficients of a Vector Autoregression Model through
  minimization of parameter criteria",http://arxiv.org/abs/1711.09369v1,2017-11-26T11:22:53Z,2017-11-26T11:22:53Z,"  VAR models are a type of multi-equation model that have been widely applied
in econometrics. With the arrival of Big Data, huge amounts of data are being
collected in numerous fields, making feasible the application of these kind of
statistical models. Tools exist to tackle this problem, but the large amount of
data, along with the availability of computational techniques and high
performance systems, advise an in-depth analysis of the computational aspects
of VAR, so large models can be solved efficiently with today's computational
systems.
  This work aims to solve a VAR model by obtaining the coefficients through
heuristic and metaheuristic algorithms, minimizing one parameter criterion, and
also to compare with those coefficients obtained by OLS. Furthermore, we
consider different approaches to reduce the time required to find the model
like using matrix decompositions (QR or LQ), exploiting matrix structure, using
high performance linear algebra subroutines (BLAS and LAPACK) or parallel
metaheuristics.
","['\nAlfonso L. Castaño\n', '\nJavier Cuenca\n', '\nDomingo Giménez\n', '\nJose J. López-Espín\n', '\nAlberto Pérez-Bernabeu\n']","International Workshop on Optimization and Learning: Challenges and
  Applications, Alicante (Spain)",,http://arxiv.org/abs/1711.09369v1,cs.OH,['cs.OH'],,,[]
Treatment of Unicode canoncal decomposition among operating systems,http://arxiv.org/abs/1711.10481v1,2017-11-28T12:59:22Z,2017-11-28T12:59:22Z,"  This article shows how the text characters that have multiple representations
under the Unicode standard are treated by popular operating systems. Whilst
most characters have a unique representation in Unicode, some characters such
as the accented European letters, can have multiple representations due to a
feature of Unicode called normalization. These characters are treated
differently by popular operating systems, leading to additional challenges
during interoperability of computer programs.
",['\nEfstratios Rappos\n'],7 pages,,http://arxiv.org/abs/1711.10481v1,cs.OH,['cs.OH'],,,[]
"Explanation of an Invisible Common Constraint of Mind, Mathematics and
  Computational Complexity",http://arxiv.org/abs/1711.10874v4,2017-11-23T18:23:07Z,2022-12-21T14:51:07Z,"  There is a cognitive limit in Human Mind. This cognitive limit has played a
decisive role in almost all fields including computer sciences. The cognitive
limit replicated in computer sciences is responsible for inherent Computational
Complexity. The complexity starts decreasing if certain conditions are met,
even sometime it does not appears at all. Very simple Mechanical computing
systems are designed and implemented to demonstrate this idea and it is further
supported by Electrical systems. These verifiable and consistent systems
demonstrate the idea of computational complexity reduction. This work explains
a very important but invisible connection from Mind to Mathematical axioms
(Peano Axioms etc.) and Mathematical axioms to computational complexity. This
study gives a completely new perspective that goes well beyond Cognitive
Science, Mathematics, Physics, Computer Sciences and Philosophy. Based on this
new insight some important predictions are made.
",['\nAsad Malik\n'],"This Paper has been withdrawn. This paper is too vague to be
  understood. It was a rough sketch of something that was not clear, and it
  contain many errors (improper use of terms, notation, etc.). Proof of some
  assertions made in this paper will be presented in future, at appropriate
  time",,http://arxiv.org/abs/1711.10874v4,cs.OH,['cs.OH'],,,[]
"Solution of network localization problem with noisy distances and its
  convergence",http://arxiv.org/abs/1711.07304v1,2017-11-20T13:34:10Z,2017-11-20T13:34:10Z,"  The network localization problem with convex and non-convex distance
constraints may be modeled as a nonlinear optimization problem. The existing
localization techniques are mainly based on convex optimization. In those
techniques, the non-convex distance constraints are either ignored or relaxed
into convex constraints for using the convex optimization methods like SDP,
least square approximation, etc.. We propose a method to solve the nonlinear
non-convex network localization problem with noisy distance measurements
without any modification of constraints in the general model. We use the
nonlinear Lagrangian technique for non-convex optimization to convert the
problem to a root finding problem of a single variable continuous function.
This problem is then solved using an iterative method. However, in each step of
the iteration the computation of the functional value involves a finite
mini-max problem (FMX). We use smoothing gradient method to fix the FMX
problem. We also prove that the solution obtained from the proposed iterative
method converges to the actual solution of the general localization problem.
The proposed method obtains the solutions with a desired label of accuracy in
real time.
","['\nAnanya Saha\n', '\nBuddhadeb Sau\n']",,,http://arxiv.org/abs/1711.07304v1,math.OC,"['math.OC', 'cs.OH']",,,[]
"Design Automation for Binarized Neural Networks: A Quantum Leap
  Opportunity?",http://arxiv.org/abs/1712.01743v1,2017-11-21T09:54:37Z,2017-11-21T09:54:37Z,"  Design automation in general, and in particular logic synthesis, can play a
key role in enabling the design of application-specific Binarized Neural
Networks (BNN). This paper presents the hardware design and synthesis of a
purely combinational BNN for ultra-low power near-sensor processing. We
leverage the major opportunities raised by BNN models, which consist mostly of
logical bit-wise operations and integer counting and comparisons, for pushing
ultra-low power deep learning circuits close to the sensor and coupling it with
binarized mixed-signal image sensor data. We analyze area, power and energy
metrics of BNNs synthesized as combinational networks. Our synthesis results in
GlobalFoundries 22nm SOI technology shows a silicon area of 2.61mm2 for
implementing a combinational BNN with 32x32 binary input sensor receptive field
and weight parameters fixed at design time. This is 2.2x smaller than a
synthesized network with re-configurable parameters. With respect to other
comparable techniques for deep learning near-sensor processing, our approach
features a 10x higher energy efficiency.
","['\nManuele Rusci\n', '\nLukas Cavigelli\n', '\nLuca Benini\n']",,,http://arxiv.org/abs/1712.01743v1,cs.OH,"['cs.OH', 'cs.AR', 'cs.CV', 'cs.NE', 'eess.SP']",,,[]
"(geo)graphs - Complex Networks as a shapefile of nodes and a shapefile
  of edges for different applications",http://arxiv.org/abs/1711.05879v1,2017-11-16T01:19:14Z,2017-11-16T01:19:14Z,"  Spatial dependency and spatial embedding are basic physical properties of
many phenomena modeled by networks. The most indicated computational
environment to deal with spatial information is to use Georeferenced
Information System (GIS) and Geographical Database Management Systems (GDBMS).
Several models have been proposed in this direction, however there is a gap in
the literature in generic frameworks for working with Complex Networks in
GIS/GDBMS environments. Here we introduce the concept of (geo)graphs: graphs in
which the nodes have a known geographical location and the edges have spatial
dependence. We present case studies and two open source softwares (GIS4GRAPH
and GeoCNet) that indicate how to retrieve networks from GIS data and how to
represent networks over GIS data by using (geo)graphs.
","['\nLeonardo B L Santos\n', '\nAurelienne A S Jorge\n', '\nMarcio Rossato\n', '\nJessica D Santos\n', '\nOnofre A Candido\n', '\nWilson Seron\n', '\nCharles N de Santana\n']",,,http://arxiv.org/abs/1711.05879v1,cs.OH,['cs.OH'],,,[]
Machine Learning Based Fast Power Integrity Classifier,http://arxiv.org/abs/1711.03406v1,2017-11-08T03:07:05Z,2017-11-08T03:07:05Z,"  In this paper, we proposed a new machine learning based fast power integrity
classifier that quickly flags the EM/IR hotspots. We discussed the features to
extract to describe the power grid, cell power density, routing impact and
controlled collapse chip connection (C4) bumps, etc. The continuous and
discontinuous cases are identified and treated using different machine learning
models. Nearest neighbors, random forest and neural network models are compared
to select the best performance candidates. Experiments are run on open source
benchmark, and result is showing promising prediction accuracy.
","['\nHuaChun Zhang\n', '\nLynden Kagan\n', '\nChen Zheng\n']","6 pages, 4 figures, 1 table",,http://arxiv.org/abs/1711.03406v1,cs.OH,"['cs.OH', 'cs.LG']",,,[]
How Long Will My Phone Battery Last?,http://arxiv.org/abs/1711.03651v1,2017-11-09T23:52:41Z,2017-11-09T23:52:41Z,"  Mobile devices are only as useful as their battery lasts. Unfortunately, the
operation and life of a mobile device's battery degrade over time and usage.
The state-of-health (SoH) of batteries quantifies their degradation, but mobile
devices are unable to support its accurate estimation -- despite its importance
-- due mainly to their limited hardware and dynamic usage patterns, causing
various problems such as unexpected device shutoffs or even fire/explosion. To
remedy this lack of support, we design, implement and evaluate V-Health, a
low-cost user-level SoH estimation service for mobile devices based only on
their battery voltage, which is commonly available on all commodity mobile
devices. V-Health also enables four novel use-cases that improve mobile users'
experience from different perspectives. The design of V-Health is inspired by
our empirical finding that the relaxing voltages of a device battery
fingerprint its SoH, and is steered by extensive measurements with 15 batteries
used for various commodity mobile devices, such as Nexus 6P, Galaxy S3, iPhone
6 Plus, etc. These measurements consist of 13,377 battery
discharging/charging/resting cycles and have been conducted over 72 months
cumulatively. V-Health has been evaluated via both laboratory experiments and
field tests over 4-6 months, showing <5% error in SoH estimation.
","['\nLiang He\n', '\nKang G. Shin\n']",,,http://arxiv.org/abs/1711.03651v1,eess.SP,"['eess.SP', 'cs.OH']",,,[]
"An Experimental Analysis of the Power Consumption of Convolutional
  Neural Networks for Keyword Spotting",http://arxiv.org/abs/1711.00333v2,2017-10-30T18:24:35Z,2018-09-21T11:00:40Z,"  Nearly all previous work on small-footprint keyword spotting with neural
networks quantify model footprint in terms of the number of parameters and
multiply operations for a feedforward inference pass. These values are,
however, proxy measures since empirical performance in actual deployments is
determined by many factors. In this paper, we study the power consumption of a
family of convolutional neural networks for keyword spotting on a Raspberry Pi.
We find that both proxies are good predictors of energy usage, although the
number of multiplies is more predictive than the number of model parameters. We
also confirm that models with the highest accuracies are, unsurprisingly, the
most power hungry.
","['\nRaphael Tang\n', '\nWeijie Wang\n', '\nZhucheng Tu\n', '\nJimmy Lin\n']",Published in ICASSP 2018,,http://arxiv.org/abs/1711.00333v2,cs.OH,['cs.OH'],,,[]
Detecting Disguised Plagiarism,http://arxiv.org/abs/1711.02149v1,2017-11-01T08:58:52Z,2017-11-01T08:58:52Z,"  Source code plagiarism detection is a problem that has been addressed several
times before; and several tools have been developed for that purpose. In this
research project we investigated a set of possible disguises that can be
mechanically applied to plagiarized source code to defeat plagiarism detection
tools. We propose a preprocessor to be used with existing plagiarism detection
tools to ""normalize"" source code before checking it, thus making such disguises
ineffective.
",['\nHatem A. Mahmoud\n'],,,http://arxiv.org/abs/1711.02149v1,cs.OH,['cs.OH'],,,[]
Customized Routing Optimization Based on Gradient Boost Regressor Model,http://arxiv.org/abs/1710.11118v1,2017-10-28T06:24:42Z,2017-10-28T06:24:42Z,"  In this paper, we discussed limitation of current
electronic-design-automoation (EDA) tool and proposed a machine learning
framework to overcome the limitations and achieve better design quality. We
explored how to efficiently extract relevant features and leverage gradient
boost regressor (GBR) model to predict underestimated risky net (URN).
Customized routing optimizations are applied to the URNs and results show clear
timing improvement and trend to converge toward timing closure.
","['\nChen Zheng\n', '\nClara Grzegorz Kasprowicz\n', '\nCarol Saunders\n']","6 pages, 7 tables, 3 figures",,http://arxiv.org/abs/1710.11118v1,cs.OH,"['cs.OH', 'cs.LG']",,,[]
Adapting Engineering Education to Industrie 4.0 Vision,http://arxiv.org/abs/1710.08806v1,2017-10-24T14:39:08Z,2017-10-24T14:39:08Z,"  Industrie 4.0 is originally a future vision described in the high-tech
strategy of the German government that is conceived upon the information and
communication technologies like Cyber-Physical Systems, Internet of Things,
Physical Internet and Internet of Services to achieve a high degree of
flexibility in production, higher productivity rates through real-time
monitoring and diagnosis, and a lower wastage rate of material in production.
An important part of the tasks in the preparation for Industrie 4.0 is the
adaption of the higher education to the requirements of this vision, in
particular the engineering education. In this work, we introduce a road map
consisting of three pillars describing the changes/enhancements to be conducted
in the areas of curriculum development, lab concept, and student club
activities. We also report our current application of this road map at the
Turkish-German University, Istanbul.
","['\nSelim Coskun\n', '\nYasanur Kayikci\n', '\nEray Gencay\n']","Presented at the 16th Production Research Symposium, Istanbul",,http://arxiv.org/abs/1710.08806v1,cs.OH,['cs.OH'],,,[]
"Do two parties represent the US? Clustering analysis of US public
  ideology survey",http://arxiv.org/abs/1710.09347v2,2017-10-25T17:14:47Z,2018-06-30T22:57:35Z,"  Recent surveys have shown that an increasing portion of the US public
believes the two major US parties adequately represent the US public opinion
and think additional parties are needed. However, there are high barriers for
third parties in political elections. In this paper, we aim to address two
questions: ""How well do the two major US parties represent the public's
ideology?"" and ""Does a more-than-two-party system better represent the ideology
of the public?"". To address these questions, we utilize the American National
Election Studies Time series dataset. We perform unsupervised clustering with
Gaussian Mixture Model method on this dataset. When clustered into two
clusters, we find a large centrist cluster and a small right-wing cluster. The
Democratic Party's position (estimated using the mean position of the
individuals self-identified with the parties) is similar to that of the
centrist cluster, and the Republican Party's position is between the two
clusters. We investigate if more than two parties represent the population
better by comparing the Akaike Information Criteria for clustering results of
the various number of clusters. We find that additional clusters give a better
representation of the data, even after penalizing for the additional
parameters. This suggests a multiparty system represents of the ideology of the
public better.
","['\nLouisa Lee\n', '\nSiyu Zhang\n', '\nVicky Chuqiao Yang\n']","11 pages, 6 figures. This is the outcome of an undergraduate summer
  research project",,http://dx.doi.org/10.1137/17S016518,cs.OH,['cs.OH'],10.1137/17S016518,,[]
"A software framework for pipelined arithmetic algorithms in field
  programmable gate arrays",http://arxiv.org/abs/1710.09235v3,2017-10-20T05:00:48Z,2017-12-14T02:39:55Z,"  Pipelined algorithms implemented in field programmable gate arrays are being
extensively used for hardware triggers in the modern experimental high energy
physics field and the complexity of such algorithms are increases rapidly. For
development of such hardware triggers, algorithms are developed in
$\texttt{C++}$, ported to hardware description language for synthesizing
firmware, and then ported back to $\texttt{C++}$ for simulating the firmware
response down to the single bit level. We present a $\texttt{C++}$ software
framework which automatically simulates and generates hardware description
language code for pipelined arithmetic algorithms.
","['\nJ. B. Kim\n', '\nE. Won\n']","8 pages, 8 figures","Nucl. Instr. Meth. Phys. Res. A, Volume 883, 1 March 2018, Pages
  83-89",http://dx.doi.org/10.1016/j.nima.2017.11.064,cs.OH,"['cs.OH', 'cs.AR', 'hep-ex', 'physics.ins-det']",10.1016/j.nima.2017.11.064,,[]
Hotspot-aware DSA Grouping and Mask Assignment,http://arxiv.org/abs/1710.02921v1,2017-10-09T02:55:33Z,2017-10-09T02:55:33Z,"  In Directed Self Assembly (DSA), poor printing of guiding templates can cause
misassembly resulting in high defect probability. Therefore, hotspots should be
avoided in the choice of the DSA groups. Accordingly, Directed Self-Assembly
(DSA) technologies which use Multiple Patterning (MP) to print the guiding
templates need to be aware of hotspots during the DSA grouping and MP
Decomposition. In this paper, we present a hotspot-aware heuristic for DSA
grouping and MP decomposition. Results show that that the proposed heuristic
eliminates 78% of the hotspots and conflicts that result from using a
hotspot-unaware grouping and decomposition algorithm. In comparison to the
optimal solution using Integer Linear Programming, the proposed heuristic
results in ~24% more violations.
","['\nYasmine Badr\n', '\nPuneet Gupta\n']",5 pages,,http://arxiv.org/abs/1710.02921v1,cs.OH,['cs.OH'],,,[]
"Stackable vs Autonomous Cars for Shared Mobility Systems: a Preliminary
  Performance Evaluation",http://arxiv.org/abs/1709.09553v1,2017-09-27T14:34:43Z,2017-09-27T14:34:43Z,"  Car sharing is one of the key elements of a Mobility-on-Demand system, but it
still suffers from several shortcomings, the most significant of which is the
fleet unbalance during the day. What is typically observed in car sharing
systems, in fact, is a vehicle shortage in so-called hot spots (i.e., areas
with high demand) and vehicle accumulation in cold spots, due to the patterns
in people flows during the day. In this work, we overview the main approaches
to vehicle redistribution based on the type of vehicles the car sharing fleet
is composed of, and we evaluate their performance using a realistic car sharing
demand derived for a suburban area around Lyon, France. The main result of this
paper is that stackable vehicles can achieve a relocation performance close to
that of autonomous vehicles, significantly improving over the no-relocation
approach and over traditional relocation with standard cars.
","['\nChiara Boldrini\n', '\nRaffaele Bruno\n']",Accepted at the MoD@ITSC2017 workshop,,http://arxiv.org/abs/1709.09553v1,cs.OH,['cs.OH'],,,[]
Simple Signal Extension Method for Discrete Wavelet Transform,http://arxiv.org/abs/1709.08460v1,2017-09-25T12:52:44Z,2017-09-25T12:52:44Z,"  Discrete wavelet transform of finite-length signals must necessarily handle
the signal boundaries. The state-of-the-art approaches treat such boundaries in
a complicated and inflexible way, using special prolog or epilog phases. This
holds true in particular for images decomposed into a number of scales,
exemplary in JPEG 2000 coding system. In this paper, the state-of-the-art
approaches are extended to perform the treatment using a compact streaming
core, possibly in multi-scale fashion. We present the core focused on CDF 5/3
wavelet and the symmetric border extension method, both employed in the JPEG
2000. As a result of our work, every input sample is visited only once, while
the results are produced immediately, i.e. without buffering.
","['\nDavid Barina\n', '\nPavel Zemcik\n', '\nMichal Kula\n']",preprint; presented on ICSIP 2016,,http://dx.doi.org/10.1109/SIPROCESS.2016.7888319,eess.SP,"['eess.SP', 'cs.OH']",10.1109/SIPROCESS.2016.7888319,,[]
"Technical Note: Towards Virtual Monitors for Image Guided Interventions
  - Real-time Streaming to Optical See-Through Head-Mounted Displays",http://arxiv.org/abs/1710.00808v1,2017-10-02T17:39:07Z,2017-10-02T17:39:07Z,"  Purpose: Image guidance is crucial for the success of many interventions.
Images are displayed on designated monitors that cannot be positioned optimally
due to sterility and spatial constraints. This indirect visualization causes
potential occlusion, hinders hand-eye coordination, leads to increased
procedure duration and surgeon load. Methods: We propose a virtual monitor
system that displays medical images in a mixed reality visualization using
optical see-through head-mounted displays. The system streams high-resolution
medical images from any modality to the head-mounted display in real-time that
are blended with the surgical site. It allows for mixed reality visualization
of images in head-, world-, or body-anchored mode and can thus be adapted to
specific procedural needs. Results: For typical image sizes, the proposed
system exhibits an average end-to-end delay and refresh rate of 214 +- 30 ms
and 41:4 +- 32:0 Hz, respectively. Conclusions: The proposed virtual monitor
system is capable of real-time mixed reality visualization of medical images.
In future, we seek to conduct first pre-clinical studies to quantitatively
assess the impact of the system on standard image guided procedures.
","['\nLong Qian\n', '\nMathias Unberath\n', '\nKevin Yu\n', '\nBernhard Fuerst\n', '\nAlex Johnson\n', '\nNassir Navab\n', '\nGreg Osgood\n']","6 pages, 2 Figures. Under review at Medical Physics",,http://arxiv.org/abs/1710.00808v1,cs.OH,"['cs.OH', 'cs.HC']",,,[]
"Improving Compression Based Dissimilarity Measure for Music Score
  Analysis",http://arxiv.org/abs/1710.01446v1,2017-10-04T03:11:31Z,2017-10-04T03:11:31Z,"  In this paper, we propose a way to improve the compression based
dissimilarity measure, CDM. We propose to use a modified value of the file
size, where the original CDM uses an unmodified file size. Our application is a
music score analysis. We have chosen piano pieces from five different
composers. We have selected 75 famous pieces (15 pieces for each composer). We
computed the distances among all pieces by using the modified CDM. We use the
K-nearest neighbor method when we estimate the composer of each piece of music.
The modified CDM shows improved accuracy. The difference is statistically
significant.
","['\nAyaka Takamoto\n', '\nMayu Umemura\n', '\nMitsuo Yoshida\n', '\nKyoji Umemura\n']","The 2016 International Conference On Advanced Informatics: Concepts,
  Theory And Application (ICAICTA2016)",,http://arxiv.org/abs/1710.01446v1,cs.SD,"['cs.SD', 'cs.OH', 'eess.AS']",,,[]
From Logic to Biology via Physics: a survey,http://arxiv.org/abs/1709.06001v2,2017-09-14T13:24:58Z,2017-11-28T20:14:06Z,"  This short text summarizes the work in biology proposed in our book,
Perspectives on Organisms, where we analyse the unity proper to organisms by
looking at it from different viewpoints. We discuss the theoretical roles of
biological time, complexity, theoretical symmetries, singularities and critical
transitions. We explicitly borrow from the conclusions in some key chapters and
introduce them by a reflection on ""incompleteness"", also proposed in the book.
We consider that incompleteness is a fundamental notion to understand the way
in which we construct knowledge. Then we will introduce an approach to
biological dynamics where randomness is central to the theoretical
determination: randomness does not oppose biological stability but contributes
to it by variability, adaptation, and diversity. Then, evolutionary and
ontogenetic trajectories are continual changes of coherence structures
involving symmetry changes within an ever-changing global stability.
","['\nGiuseppe Longo\n', '\nMaël Montévil\n']",accepted for publication in Logical Methods in Computer Science,"Logical Methods in Computer Science, Volume 13, Issue 4 (November
  30, 2017) lmcs:4112",http://dx.doi.org/10.23638/LMCS-13(4:21)2017,cs.OH,"['cs.OH', 'physics.soc-ph']",10.23638/LMCS-13(4:21)2017,,[]
TikZ-network manual,http://arxiv.org/abs/1709.06005v2,2017-09-18T15:22:25Z,2018-07-31T05:58:00Z,"  TikZ-network is an open source software project for visualizing graphs and
networks in LaTeX. It aims to provide a simple and easy tool to create,
visualize and modify complex networks. The packaged is based on the PGF/TikZ
languages for producing vector graphics from a geometric/algebraic description.
Particular focus is made on the software usability and interoperability with
other tools. Simple networks can be directly created within LaTeX, while more
complex networks can be imported from external sources (e.g. igraph, networkx,
QGIS, ...). Additionally, tikz-network supports visualization of multilayer
networks in two and three dimensions. The software is available at:
https://github.com/hackl/tikz-network.
",['\nJürgen Hackl\n'],Version 1.0,,http://arxiv.org/abs/1709.06005v2,cs.OH,"['cs.OH', 'physics.soc-ph']",,,[]
Sensor Fusion for Public Space Utilization Monitoring in a Smart City,http://arxiv.org/abs/1710.01581v2,2017-09-15T01:35:16Z,2017-10-05T15:35:08Z,"  Public space utilization is crucial for urban developers to understand how
efficient a place is being occupied in order to improve existing or future
infrastructures. In a smart cities approach, implementing public space
monitoring with Internet-of-Things (IoT) sensors appear to be a viable
solution. However, choice of sensors often is a challenging problem and often
linked with scalability, coverage, energy consumption, accuracy, and privacy.
To get the most from low cost sensor with aforementioned design in mind, we
proposed data processing modules for capturing public space utilization with
Renewable Wireless Sensor Network (RWSN) platform using pyroelectric infrared
(PIR) and analog sound sensor. We first proposed a calibration process to
remove false alarm of PIR sensor due to the impact of weather and environment.
We then demonstrate how the sounds sensor can be processed to provide various
insight of a public space. Lastly, we fused both sensors and study a particular
public space utilization based on one month data to unveil its usage.
","['\nBilly Pik Lik Lau\n', '\nNipun Wijerathne\n', '\nBenny Kai Kiat Ng\n', '\nand Chau Yuen\n']",,,http://dx.doi.org/10.1109/JIOT.2017.2748987,cs.OH,"['cs.OH', 'stat.AP']",10.1109/JIOT.2017.2748987,,[]
A HelloWord \textsc{Bib}\negthinspace\TeX~stile file .\textbf{bst},http://arxiv.org/abs/1709.03643v1,2017-09-12T01:32:46Z,2017-09-12T01:32:46Z,"  A HelloWord \textsc{Bib}\negthinspace\TeX~stile file .\textbf{bst} is
described
",['\nMakar Plakhotnyk\n'],,,http://arxiv.org/abs/1709.03643v1,cs.OH,['cs.OH'],,,[]
"Computational prediction and analysis of protein-protein interaction
  networks",http://arxiv.org/abs/1709.01923v2,2017-09-06T15:44:01Z,2017-09-13T03:17:06Z,"  Biological networks provide insight into the complex organization of
biological processes in a cell at the system level. They are an effective tool
for understanding the comprehensive map of functional interactions, finding the
functional modules and pathways. Reconstruction and comparative analysis of
these networks provide useful information to identify functional modules,
prioritization of disease causing genes and also identification of drug
targets. The talk will consist of two parts. I will discuss several methods for
protein-protein interaction network alignment and investigate their preferences
to other existing methods. Further, I briefly talk about reconstruction of
protein-protein interaction networks by using deep learning.
",['\nSomaye Hashemifar\n'],"PhD thesis, Toyota Technological Institute at Chicago (2017)",,http://arxiv.org/abs/1709.01923v2,q-bio.MN,"['q-bio.MN', 'cs.OH']",,,[]
Applications of Biological Cell Models in Robotics,http://arxiv.org/abs/1712.02303v1,2017-08-31T08:34:22Z,2017-08-31T08:34:22Z,"  In this paper I present some of the most representative biological models
applied to robotics. In particular, this work represents a survey of some
models inspired, or making use of concepts, by gene regulatory networks (GRNs):
these networks describe the complex interactions that affect gene expression
and, consequently, cell behaviour.
",['\nMichele Braccini\n'],,,http://arxiv.org/abs/1712.02303v1,cs.OH,"['cs.OH', 'cs.ET', 'cs.RO']",,,[]
Neville's algorithm revisited,http://arxiv.org/abs/1708.06293v1,2017-08-17T01:14:32Z,2017-08-17T01:14:32Z,"  Neville's algorithm is known to provide an efficient and numerically stable
solution for polynomial interpolations. In this paper, an extension of this
algorithm is presented which includes the derivatives of the interpolating
polynomial.
",['\nM. de Jong\n'],3 pages,,http://arxiv.org/abs/1708.06293v1,cs.OH,['cs.OH'],,,[]
"SigViewer: Visualizing Multimodal Signals Stored in XDF (Extensible Data
  Format) Files",http://arxiv.org/abs/1708.06333v1,2017-08-12T02:58:50Z,2017-08-12T02:58:50Z,"  Multimodal biosignal acquisition is facilitated by recently introduced
software solutions such as LabStreaming Layer (LSL) and its associated data
format XDF (Extensible Data Format). However, there are no stand-alone
applications that can visualize multimodal time series stored in XDF files. We
extended SigViewer, an open source cross-platform Qt C++ application with the
capability of loading, resampling, annotating, and visualizing signals stored
in XDF files and successfully applied the tool for post-hoc visual verification
of the accuracy of a system that aims to predict the phase of alpha
oscillations within the electroencephalogram in real-time.
","['\nYida Lin\n', '\nClemens Brunner\n', '\nPaul Sajda\n', '\nJosef Faller\n']","39th Annual International Conference of the IEEE Engineering in
  Medicine and Biology Society",,http://arxiv.org/abs/1708.06333v1,cs.OH,['cs.OH'],,,[]
"A Method with Feedback for Aggregation of Group Incomplete Pair-Wise
  Comparisons",http://arxiv.org/abs/1708.06419v1,2017-08-21T21:02:08Z,2017-08-21T21:02:08Z,"  A method for aggregation of expert estimates in small groups is proposed. The
method is based on combinatorial approach to decomposition of pair-wise
comparison matrices and to processing of expert data. It also uses the basic
principles of Analytic Hierarchy/Network Process approaches, such as building
of criteria hierarchy to decompose and describe the problem, and evaluation of
objects by means of pair-wise comparisons. It allows to derive priorities based
on group incomplete pair-wise comparisons and to organize feedback with experts
in order to achieve sufficient agreement of their estimates. Double entropy
inter-rater index is suggested for usage as agreement measure. Every expert is
given an opportunity to use the scale, in which the degree of detail (number of
points/grades) most adequately reflects this expert's competence in the issue
under consideration, for every single pair comparison. The method takes all
conceptual levels of individual expert competence (subject domain, specific
problem, individual pair-wise comparison matrix, separate pair-wise comparison)
into consideration. The method is intended to be used in the process of
strategic planning in weakly-structured subject domains.
","['\nVitaliy Tsyganok\n', '\nSergii Kadenko\n', '\nOleh Andriichuk\n', '\nPavlo Roik\n']","13 pages, 6 figures",,http://arxiv.org/abs/1708.06419v1,math.OC,"['math.OC', 'cs.OH']",,,[]
"Relatório Técnico: Controle Distribuído de Tráfego Baseado em
  Veículos Conectados e Comunicações Veiculares Centradas em Interesses",http://arxiv.org/abs/1708.00741v1,2017-08-01T17:30:28Z,2017-08-01T17:30:28Z,"  Although advanced traffic management systems can deal with the heterogeneous
traffic flows approaching of intersections, their performances are compromised,
when the traffic volume is not distributed uniformly. To evenly distribute the
traffic flow, an advanced driver information system should be aware of the
traffic control operations. However, such requirement can not ultimately be
satisfied due to the gaps in state of the art in advanced traffic management
systems. Therefore, this study proposes a distributed traffic control system,
in which agents embedded in connected vehicles, traffic signals, urban elements
and a traffic control center interact with each other to provide a greater
traffic fluidity. Therefore, the agents depend strongly on a heterogeneous
vehicular network. In this sense, this study also proposes a heterogeneous
vehicular network whose communication protocol can satisfy the communication
requirements of intelligent transportation systems service applications.
According to the results obtained from simulations, the distributed traffic
control system was able to maximize the flow of vehicles and the mean speed of
the vehicles, and minimize the wait time, travel time, fuel consume and
emissions (CO, CO$_2$, HC, NOx and PMx).
",['\nFabrício Barros Gonçalves\n'],in Portuguese,,http://arxiv.org/abs/1708.00741v1,cs.OH,['cs.OH'],,,[]
"Enhanced power grid evaluation through efficient stochastic model-based
  analysis",http://arxiv.org/abs/1708.04576v1,2017-08-07T15:43:18Z,2017-08-07T15:43:18Z,"  Electrical infrastructures provide services at the basis of a number of
application sectors, several of which are critical from the perspective of
human life, environment or financials. Following the increasing trend in
electricity generation from renewable sources, pushed by the need to meet
sustainable energy goals in many countries, more sophisticated control
strategies are being adopted to regulate the operation of the electric power
system, driving electrical infrastructures towards the so called Smart Grid
scenario. It is therefore paramount to be assisted by technologies able to
analyze the Smart Grid behavior in critical scenarios, e.g. where cyber
malfunctions or grid disruptions occur. In this context, stochastic model-based
analysis are well suited to assess dependability and quality of service related
indicators, and continuous improvements in modeling strategies and system
models design are required. Thus, my PhD work addresses this topic by
contributing to study new Smart Grid scenarios, concerning the advanced
interplay between ICT and electrical infrastructures in presence of cyber
faults/attacks, define a new modeling approach, based on modularity and
composition, and start to study how to improve the electrical grid dynamics
representation. In this article these studies are briefly presented and
discussed.
",['\nGiulio Masetti\n'],,,http://arxiv.org/abs/1708.04576v1,cs.OH,['cs.OH'],,,[]
"Optimizing Google Shopping Campaigns Structures With Query-Level
  Matching",http://arxiv.org/abs/1708.04586v1,2017-08-03T15:25:19Z,2017-08-03T15:25:19Z,"  How to bid on a Google shopping account (set of shopping campaigns) with
query-level matching like in Google Adwords.
","['\nMathieu Raffinot\n', '\nRomain Rivière\n']",,,http://arxiv.org/abs/1708.04586v1,cs.OH,['cs.OH'],,,[]
Modular AWG-based Optical Shuffle Network,http://arxiv.org/abs/1707.09280v1,2017-07-27T10:31:09Z,2017-07-27T10:31:09Z,"  This paper proposes an arrayed-waveguide grating (AWG) based
wavelength-division-multiplexing (WDM) shuffle network. Compared with previous
optical shuffle networks, our proposal is compact, easy to implement, highly
scalable, and cost effective.
","['\nJingjie Ding\n', '\nTong Ye\n', '\nTony T. Lee\n', '\nWeisheng Hu\n']",,,http://arxiv.org/abs/1707.09280v1,cs.OH,['cs.OH'],,,[]
Ethics of autonomous information systems towards an artificial thinking,http://arxiv.org/abs/1707.05259v1,2017-07-12T11:44:09Z,2017-07-12T11:44:09Z,"  Many projects relies on cognitives sciences, neurosciences, computer sciences
and robotics. They concerned today the building of autonomous artificial beings
able to think. This paper shows a model to compare the human thinking with an
hypothetic numerical way of thinking based on four hierarchies : the
information system classification, the cognitive pyramid, the linguistic
pyramid and the digital information hierarchy. After a state of art on the
nature of human thinking, feasibility of autonomous multi-agent systems
provided with artificial consciousness which are able to think is discussed.
The ethical aspects and consequences for humanity of such systems is evaluated.
These systems lead the scientific community to react.
",['\nJoël Colloc\nIDEES\n'],in French,"Les Cahiers du num\'erique, Lavoisier, Enjeux du big data et
  identifications des donn\'ees m\'edicales, 12 (1-2), pp.187-211 (2016)",http://dx.doi.org/10.3166/LCN.12.1-2.187-212,cs.OH,['cs.OH'],10.3166/LCN.12.1-2.187-212,,['IDEES']
The Limits to Machine Consciousness,http://arxiv.org/abs/1707.06257v1,2017-07-17T21:35:35Z,2017-07-17T21:35:35Z,"  It is generally accepted that machines can replicate cognitive tasks
performed by conscious agents as long as they are not based on the capacity of
awareness. We consider several views on the nature of subjective awareness,
which is fundamental for self-reflection and review, and present reasons why
this property is not computable. We argue that consciousness is more than an
epiphenomenon and assuming it to be a separate category is consistent with both
quantum mechanics and cognitive science. We speak of two kinds of
consciousness, little-C and big-C, and discuss the significance of this
classification in analyzing the current academic debates in the field. The
interaction between the system and the measuring apparatus of the experimenter
is examined both from the perspectives of decoherence and the quantum Zeno
effect. These ideas are used as context to address the question of limits to
machine consciousness.
",['\nSubhash Kak\n'],"14 pages, 8 figures","Journal of Artificial Intelligence and Consciousness, August 2021",http://dx.doi.org/10.1142/s2705078521500193,cs.OH,['cs.OH'],10.1142/s2705078521500193,,[]
"Meaningless comparisons lead to false optimism in medical machine
  learning",http://arxiv.org/abs/1707.06289v1,2017-07-19T20:39:51Z,2017-07-19T20:39:51Z,"  A new trend in medicine is the use of algorithms to analyze big datasets,
e.g. using everything your phone measures about you for diagnostics or
monitoring. However, these algorithms are commonly compared against weak
baselines, which may contribute to excessive optimism. To assess how well an
algorithm works, scientists typically ask how well its output correlates with
medically assigned scores. Here we perform a meta-analysis to quantify how the
literature evaluates their algorithms for monitoring mental wellbeing. We find
that the bulk of the literature ($\sim$77%) uses meaningless comparisons that
ignore patient baseline state. For example, having an algorithm that uses phone
data to diagnose mood disorders would be useful. However, it is possible to
over 80% of the variance of some mood measures in the population by simply
guessing that each patient has their own average mood - the patient-specific
baseline. Thus, an algorithm that just predicts that our mood is like it
usually is can explain the majority of variance, but is, obviously, entirely
useless. Comparing to the wrong (population) baseline has a massive effect on
the perceived quality of algorithms and produces baseless optimism in the
field. To solve this problem we propose ""user lift"" that reduces these
systematic errors in the evaluation of personalized medical monitoring.
","['\nOrianna DeMasi\n', '\nKonrad Kording\n', '\nBenjamin Recht\n']",,,http://dx.doi.org/10.1371/journal.pone.0184604,cs.OH,['cs.OH'],10.1371/journal.pone.0184604,,[]
Control Flow Information Analysis in Process Model Matching Techniques,http://arxiv.org/abs/1707.01089v1,2017-07-04T04:09:21Z,2017-07-04T04:09:21Z,"  Online Appendix to: ""Analyzing Control Flow Information to Improve the
Effectiveness of Process Model Matching Techniques"" by the same authors.
","['\nChristopher Klinkmüler\n', '\nIngo Weber\n']",,,http://arxiv.org/abs/1707.01089v1,cs.OH,['cs.OH'],,,[]
"Project Makespan Estimation: Computational Load of Interval and Point
  Estimates",http://arxiv.org/abs/1707.01880v1,2017-07-06T17:36:33Z,2017-07-06T17:36:33Z,"  The estimation of project completion time is to be repeated several times in
the project planning phase to reach the optimal tradeoff between time, cost,
and quality. Estimation procedures provide either an interval or a point
estimate. The computational load of several estimation procedures is reviewed.
A multiple polynomial regression model is provided for major interval
estimation procedures and shows that the accuracy in the probability model for
activities is the most influential factor. The computational time does not
appear to be an impeding factor, though it is larger for MonteCarlo simulation,
so that the computational time can be traded off in search of a simpler
estimation procedure.
","['\nMaurizio Naldi\n', '\nMarta Flamini\n']",,,http://arxiv.org/abs/1707.01880v1,cs.OH,['cs.OH'],,,[]
Skin Temperature Measurement,http://arxiv.org/abs/1707.02296v1,2017-07-07T15:03:43Z,2017-07-07T15:03:43Z,"  This report represents the design and implementation of a skin temperature
measurement system. The system aims to measure the skin temperature from a
sensor and send it to the PC using a USB cable to display on screen. The data
needs to be updated every second. The PIC18F4550 microcontroller has been used
in this project to obtain data from the sensor and send it to the PC using USB
2.0 that has been built into the microcontroller. The microcontroller has a
10-bit Analog Digital Converting accuracy that is one of the important criteria
for this design as it is going to be used for medical purposes. As the project
concentrates more on designing software than hardware, the EasyPIC4 development
board was used which comes with all hardware required for this project. The
Jackson diagram method was used to design and implement the coding program for
the microcontroller software part of the system. The MikroC IDE has been used
to compile and load the program into PIC18F4550 microcontroller. The program
for the microcontroller uses C language that aims to keep the USB link alive by
using interrupt function. A sensor collects data from sensor as 4 bits and send
it to the PC every second using a USB cable. The data received from sensor,
will be sent by microcontroller to the PC. The Visual Basic software was used
in the PC side of device to catch and output the data on the screen. A template
file for the Visual Basic program was generated by Easy HID wizard to make
software programming part easier for designer. The USBTrace analyzer has been
used in the scenario any problems occur during or after the design and
construction of the software. The software enables a user to monitor the data
on the USB bus that sends the data to the PC from microcontroller.
",['\nSiamak Sarjoghian\n'],"MSc Dissertation, London South Bank University",,http://arxiv.org/abs/1707.02296v1,cs.OH,['cs.OH'],,,[]
Duty to Delete on Non-Volatile Memory,http://arxiv.org/abs/1707.02842v1,2017-07-07T15:44:11Z,2017-07-07T15:44:11Z,"  We firstly suggest new cache policy applying the duty to delete invalid cache
data on Non-volatile Memory (NVM). This cache policy includes generating random
data and overwriting the random data into invalid cache data. Proposed cache
policy is more economical and effective regarding perfect deletion of data. It
is ensure that the invalid cache data in NVM is secure against malicious
hackers.
","['\nNa-Young Ahn\n', '\nDong Hoon Lee\n']","3 pages, 8 figures",,http://dx.doi.org/10.8080/1020190046820,cs.OH,['cs.OH'],10.8080/1020190046820,,[]
Smart Asset Management for Electric Utilities: Big Data and Future,http://arxiv.org/abs/1706.09711v2,2017-06-18T14:59:06Z,2018-02-17T13:33:27Z,"  This paper discusses about future challenges in terms of big data and new
technologies. Utilities have been collecting data in large amounts but they are
hardly utilized because they are huge in amount and also there is uncertainty
associated with it. Condition monitoring of assets collects large amounts of
data during daily operations. The question arises ""How to extract information
from large chunk of data?"" The concept of ""rich data and poor information"" is
being challenged by big data analytics with advent of machine learning
techniques. Along with technological advancements like Internet of Things
(IoT), big data analytics will play an important role for electric utilities.
In this paper, challenges are answered by pathways and guidelines to make the
current asset management practices smarter for the future.
","['\nSwasti R. Khuntia\n', '\nJose L. Rueda\n', '\nMart A. M. M. van der Meijden\n']","13 pages, 3 figures, Proceedings of 12th World Congress on
  Engineering Asset Management (WCEAM) 2017",,http://arxiv.org/abs/1706.09711v2,cs.OH,"['cs.OH', 'cs.CY']",,,[]
An Online Development Environment for Answer Set Programming,http://arxiv.org/abs/1707.01865v1,2017-06-20T10:01:24Z,2017-06-20T10:01:24Z,"  Recent progress in logic programming (e.g., the development of the Answer Set
Programming paradigm) has made it possible to teach it to general undergraduate
and even high school students. Given the limited exposure of these students to
computer science, the complexity of downloading, installing and using tools for
writing logic programs could be a major barrier for logic programming to reach
a much wider audience. We developed an online answer set programming
environment with a self contained file system and a simple interface, allowing
users to write logic programs and perform several tasks over the programs.
","['\nElias Marcopoulos\n', '\nChristian Reotutar\n', '\nYuanlin Zhang\n']","Proceedings of the 2nd International Workshop on User-Oriented Logic
  Paradigms(IULP 2017), Editors: Claudia Schulz and Stefan Ellmauthaler",,http://arxiv.org/abs/1707.01865v1,cs.OH,"['cs.OH', 'cs.AI', 'cs.HC']",,,[]
Computational Anatomy in Theano,http://arxiv.org/abs/1706.07690v1,2017-06-15T13:10:51Z,2017-06-15T13:10:51Z,"  To model deformation of anatomical shapes, non-linear statistics are required
to take into account the non-linear structure of the data space. Computer
implementations of non-linear statistics and differential geometry algorithms
often lead to long and complex code sequences. The aim of the paper is to show
how the Theano framework can be used for simple and concise implementation of
complex differential geometry algorithms while being able to handle complex and
high-dimensional data structures. We show how the Theano framework meets both
of these requirements. The framework provides a symbolic language that allows
mathematical equations to be directly translated into Theano code, and it is
able to perform both fast CPU and GPU computations on high-dimensional data. We
show how different concepts from non-linear statistics and differential
geometry can be implemented in Theano, and give examples of the implemented
theory visualized on landmark representations of Corpus Callosum shapes.
","['\nLine Kühnel\n', '\nStefan Sommer\n']",,,http://arxiv.org/abs/1706.07690v1,cs.OH,"['cs.OH', '53A35']",,,[]
"Z-checker: A Framework for Assessing Lossy Compression of Scientific
  Data",http://arxiv.org/abs/1707.09320v2,2017-06-12T18:09:33Z,2017-11-10T20:16:10Z,"  Because of vast volume of data being produced by today's scientific
simulations and experiments, lossy data compressor allowing user-controlled
loss of accuracy during the compression is a relevant solution for
significantly reducing the data size. However, lossy compressor developers and
users are missing a tool to explore the features of scientific datasets and
understand the data alteration after compression in a systematic and reliable
way. To address this gap, we have designed and implemented a generic framework
called Z-checker. On the one hand, Z-checker combines a battery of data
analysis components for data compression. On the other hand, Z-checker is
implemented as an open-source community tool to which users and developers can
contribute and add new analysis components based on their additional analysis
demands. In this paper, we present a survey of existing lossy compressors. Then
we describe the design framework of Z-checker, in which we integrated
evaluation metrics proposed in prior work as well as other analysis tools.
Specifically, for lossy compressor developers, Z-checker can be used to
characterize critical properties of any dataset to improve compression
strategies. For lossy compression users, Z-checker can detect the compression
quality, provide various global distortion analysis comparing the original data
with the decompressed data and statistical analysis of the compression error.
Z-checker can perform the analysis with either coarse granularity or fine
granularity, such that the users and developers can select the best-fit,
adaptive compressors for different parts of the dataset. Z-checker features a
visualization interface displaying all analysis results in addition to some
basic views of the datasets such as time series. To the best of our knowledge,
Z-checker is the first tool designed to assess lossy compression
comprehensively for scientific datasets.
","['\nDingwen Tao\n', '\nSheng Di\n', '\nHanqi Guo\n', '\nZizhong Chen\n', '\nFranck Cappello\n']","Accepted by The International Journal of High Performance Computing
  Application",,http://dx.doi.org/10.1177/1094342017737147,cs.OH,"['cs.OH', 'astro-ph.IM', 'cs.CE']",10.1177/1094342017737147,,[]
"The Role of Data Analysis in the Development of Intelligent Energy
  Networks",http://arxiv.org/abs/1705.11132v1,2017-05-30T08:53:29Z,2017-05-30T08:53:29Z,"  Data analysis plays an important role in the development of intelligent
energy networks (IENs). This article reviews and discusses the application of
data analysis methods for energy big data. The installation of smart energy
meters has provided a huge volume of data at different time resolutions,
suggesting data analysis is required for clustering, demand forecasting, energy
generation optimization, energy pricing, monitoring and diagnostics. The
currently adopted data analysis technologies for IENs include pattern
recognition, machine learning, data mining, statistics methods, etc. However,
existing methods for data analysis cannot fully meet the requirements for
processing the big data produced by the IENs and, therefore, more comprehensive
data analysis methods are needed to handle the increasing amount of data and to
mine more valuable information.
","['\nZhanyu Ma\n', '\nJiyang Xie\n', '\nHailong Li\n', '\nQie Sun\n', '\nZhongwei Si\n', '\nJianhua Zhang\n', '\nJun Guo\n']",,,http://arxiv.org/abs/1705.11132v1,cs.OH,['cs.OH'],,,[]
"Reservoir Computing for Detection of Steady State in Performance Tests
  of Compressors",http://arxiv.org/abs/1706.00782v1,2017-06-02T15:21:48Z,2017-06-02T15:21:48Z,"  Fabrication of devices in industrial plants often includes undergoing quality
assurance tests or tests that seek to determine some attributes or capacities
of the device. For instance, in testing refrigeration compressors, we want to
find the true refrigeration capacity of the compressor being tested. Such test
(also called an episode) may take up to four hours, being an actual hindrance
to applying it to the total number of compressors produced. This work seeks to
reduce the time spent on such industrial trials by employing Recurrent Neural
Networks (RNNs) as dynamical models for detecting when a test is entering the
so-called steady-state region. Specifically, we use Reservoir Computing (RC)
networks which simplify the learning of RNNs by speeding up training time and
showing convergence to a global optimum. Also, this work proposes a
self-organized subspace projection method for RC networks which uses
information from the beginning of the episode to define a cluster to which the
episode belongs to. This assigned cluster defines a particular binary input
that shifts the operating point of the reservoir to a subspace of trajectories
for the duration of the episode. This new method is shown to turn the RC model
robust in performance with respect to varying combination of reservoir
parameters, such as spectral radius and leak rate, when compared to a standard
RC network.
","['\nEric Aislan Antonelo\n', '\nCarlos Alberto Flesch\n', '\nFilipe Schmitz\n']",,,http://dx.doi.org/10.1016/j.neucom.2017.09.005,cs.OH,['cs.OH'],10.1016/j.neucom.2017.09.005,,[]
Optimal placement of mix zones in road networks,http://arxiv.org/abs/1705.11104v1,2017-05-17T01:20:07Z,2017-05-17T01:20:07Z,"  The road networks, vehicle users could enjoy numerous kind of services such
as location based service in vehicle users can connected to Internet and
communication of different users. Therefore, in order to acquire adequate
privacy level and quality of service, one must have to wisely place mix zones
to connect vehicle users to internet or some other internetwork. According to
this research, we have analyzed the problem of optimal placement mix zones over
road network. To enhance the coverage capacity of vehicles, in order to reduce
the cost and communication delay. Further, it has also been discovered to
minimize the cost of mix zone placement. Moreover, it has also been shown that,
as the best deployment mix zones get minimized cost while at the same time the
average capacity of mix zone can be maximized also privacy level increased
because of optimal placement and high traffic environment.
","['\nImran Memon\n', '\nQasim Ali Arain\n']","10 pages,9 figures",,http://arxiv.org/abs/1705.11104v1,cs.OH,['cs.OH'],,,[]
"Increasing the Discovery Power and Confidence Levels of Disease
  Association Studies: A Survey",http://arxiv.org/abs/1705.03391v1,2017-05-09T15:37:47Z,2017-05-09T15:37:47Z,"  The majority of common diseases are influenced by multiple genetic and
environmental factors such as Cancer. Even though uncovering the main causes of
disease is deemed difficult due to the complexity of gene-gene and
gene-environment interactions, major research efforts aim at identifying
disease risk factors, especially genetic ones. Over the past decade, disease
association studies have been used to uncover the susceptibility, aetiology and
mechanisms of action pertaining to common diseases. In disease association
studies, genetic data is analyzed in order to reveal the relationship between
different types of variants, and a disease of interest. The ultimate goal of
association studies is to facilitate susceptibility testing for disease
prediction, early diagnosis and enhanced prognosis . Susceptibility testing and
disease prediction are particularly important for diseases that can be
prevented by diet, drugs or change in lifestyle. The discovered associations
assist in understanding the molecular mechanisms influenced by the reported
variants, and in identifying important risk factors. Current association
studies suffer from several shortcomings. This report surveys the literature
that addresses the shortcomings of current methods the identify genetic disease
associations. In addition, it reviews the suggested solutions that either
enhance some aspect of the methodologies, or complement them.
",['\nLayan Nahlawi\n'],,,http://arxiv.org/abs/1705.03391v1,cs.OH,['cs.OH'],,,[]
Cloud-based Fault Detection and Classification for Oil & Gas Industry,http://arxiv.org/abs/1705.04583v1,2017-05-11T14:46:32Z,2017-05-11T14:46:32Z,"  Oil & Gas industry relies on automated, mission-critical equipment and
complex systems built upon their interaction and cooperation. To assure
continuous operation and avoid any supervision, architects embed Distributed
Control Systems (DCS), a.k.a. Supervisory Control and Data Acquisition (SCADA)
systems, on top of their equipment to generate data, monitor state and make
critical online & offline decisions.
  In this paper, we propose a new Lambda architecture for oil & gas industry
for unified data and analytical processing on data received from DCS, discuss
cloud integration issues and share our experiences with the implementation of
sensor fault-detection and classification modules inside the proposed
architecture.
","['\nAthar Khodabakhsh\n', '\nIsmail Ari\n', '\nMustafa Bakir\n']",Part of DM4OG 2017 proceedings (arXiv:1705.03451),,http://arxiv.org/abs/1705.04583v1,cs.OH,['cs.OH'],,,[]
"Zampa's systems theory: a comprehensive theory of measurement in dynamic
  systems",http://arxiv.org/abs/1705.04832v2,2017-05-13T14:10:47Z,2018-06-12T12:28:38Z,"  The article outlines in memoriam Prof. Pavel Zampa's concepts of system
theory which enable to devise a measurement in dynamic systems independently of
the particular system behaviour. From the point of view of Zampa's theory,
terms like system time, system attributes, system link, system element, input,
output, subsystems, and state variables are defined. In Conclusions, Zampa's
theory is discussed together with another mathematical approaches of
qualitative dynamics known since the 19th century. In Appendices, we present
applications of Zampa's technical approach to measurement of complex dynamical
(chemical and biological) systems at the Institute of Complex Systems,
University of South Bohemia in Ceske Budejovice.
","['\nRenata Rychtarikova\n', '\nJan Urban\n', '\nDalibor Stys\n']","16 pages, 9 figures","Acta Polytechnica 58(2), 128-143, 2018",http://dx.doi.org/10.14311/AP.2018.58.0128,cs.OH,['cs.OH'],10.14311/AP.2018.58.0128,,[]
A Proposed Architecture for Big Data Driven Supply Chain Analytics,http://arxiv.org/abs/1705.04958v1,2017-05-14T12:39:46Z,2017-05-14T12:39:46Z,"  Advancement in information and communication technology (ICT) has given rise
to explosion of data in every field of operations. Working with the enormous
volume of data (or Big Data, as it is popularly known as) for extraction of
useful information to support decision making is one of the sources of
competitive advantage for organizations today. Enterprises are leveraging the
power of analytics in formulating business strategy in every facet of their
operations to mitigate business risk. Volatile global market scenario has
compelled the organizations to redefine their supply chain management (SCM). In
this paper, we have delineated the relevance of Big Data and its importance in
managing end to end supply chains for achieving business excellence. A Big
Data-centric architecture for SCM has been proposed that exploits the current
state of the art technology of data management, analytics and visualization.
The security and privacy requirements of a Big Data system have also been
highlighted and several mechanisms have been discussed to implement these
features in a real world Big Data system deployment in the context of SCM. Some
future scope of work has also been pointed out. Keyword: Big Data, Analytics,
Cloud, Architecture, Protocols, Supply Chain Management, Security, Privacy.
","['\nSanjib Biswas\n', '\nJaydip Sen\n']","24 pages, 4 figures, 3 tables","ICFAI University Press (IUP) Journal of Supply Chain Management,
  Vol XIII, No 3 (2016), pp. 7 - 34",http://dx.doi.org/10.2139/ssrn.2795906,cs.OH,['cs.OH'],10.2139/ssrn.2795906,,[]
"Statistical Timing Analysis for Latch-Controlled Circuits with Reduced
  Iterations and Graph Transformations",http://arxiv.org/abs/1705.04980v1,2017-05-14T15:44:38Z,2017-05-14T15:44:38Z,"  Level-sensitive latches are widely used in high- performance designs. For
such circuits efficient statistical timing analysis algorithms are needed to
take increasing process vari- ations into account. But existing methods solving
this problem are still computationally expensive and can only provide the yield
at a given clock period. In this paper we propose a method combining reduced
iterations and graph transformations. The reduced iterations extract setup time
constraints and identify a subgraph for the following graph transformations
handling the constraints from nonpositive loops. The combined algorithms are
very efficient, more than 10 times faster than other existing methods, and
result in a parametric minimum clock period, which together with the hold time
constraints can be used to compute the yield at any given clock period very
easily.
","['\nBing Li\n', '\nNing Chen\n', '\nUlf Schlichtmann\n']",,"IEEE Transactions on Computer-Aided Design of Integrated Circuits
  and Systems 31(11), 1670-1683, November 2012",http://dx.doi.org/10.1109/TCAD.2012.2202393,cs.OH,['cs.OH'],10.1109/TCAD.2012.2202393,,[]
"RAE: The Rainforest Automation Energy Dataset for Smart Grid Meter Data
  Analysis",http://arxiv.org/abs/1705.05767v4,2017-05-14T04:57:27Z,2018-02-12T10:09:36Z,"  Datasets are important for researchers to build models and test how well
their machine learning algorithms perform. This paper presents the Rainforest
Automation Energy (RAE) dataset to help smart grid researchers test their
algorithms which make use of smart meter data. This initial release of RAE
contains 1Hz data (mains and sub-meters) from two a residential house. In
addition to power data, environmental and sensor data from the house's
thermostat is included. Sub-meter data from one of the houses includes heat
pump and rental suite captures which is of interest to power utilities. We also
show and energy breakdown of each house and show (by example) how RAE can be
used to test non-intrusive load monitoring (NILM) algorithms.
","['\nStephen Makonin\n', '\nZ. Jane Wang\n', '\nChris Tumpach\n']",,,http://dx.doi.org/10.3390/data3010008,cs.OH,['cs.OH'],10.3390/data3010008,,[]
"An Overview of Data Mining Applications in Oil and Gas Exploration:
  Structural Geology and Reservoir Property-Issues",http://arxiv.org/abs/1705.06345v1,2017-05-12T16:22:06Z,2017-05-12T16:22:06Z,"  Low oil prices have motivated energy executives to look into cost reduction
in their supply chains more seriously. To this end, a new technology that is
experimentally considered in hydrocarbon exploration is data mining. There are
two major categories of geoscientific problems in which data mining is applied:
structural geology and reservoir property-issues. This research overviews these
categories by considering a variety of interesting works in each of them. The
result is an understanding of the specific geoscientific problems studied in
the literature, along with the relative data mining methods. This way, this
work tries to lay the ground for a mutual understanding on oil and gas
exploration between the data miners and the geoscientists.
","['\nHamed Nikhalat Jahromi\n', '\nAlpio M. Jorge\n']",Part of DM4OG 2017 proceedings (arXiv:1705.03451),,http://arxiv.org/abs/1705.06345v1,cs.OH,['cs.OH'],,,[]
Discrete Event Simulation of Personal Rapid Transit (PRT) Systems,http://arxiv.org/abs/1705.05237v1,2017-05-12T09:44:25Z,2017-05-12T09:44:25Z,"  The article discusses issues related to the construction of the PRT network
simulator and the simulation process: the elements of PRT network structure,
their representation in the simulator, the simulation process itself,
animation, and automation of the experiments. An example of a simulation
environment Feniks is described, elaborated within the framework of the
Eco-Mobility project.
",['\nWiktor B. Daszczuk\n'],"12 pages, 3 figures","Autobusy-TEST, vol. 17(2016), No.3, pp.1302-1310",http://arxiv.org/abs/1705.05237v1,cs.OH,"['cs.OH', 'cs.MA', '68U20', 'I.6.8']",,,[]
"Visual-Based Analysis of Classification Measures with Applications to
  Imbalanced Data",http://arxiv.org/abs/1704.07122v2,2017-04-24T10:06:36Z,2017-07-21T18:49:45Z,"  With a plethora of available classification performance measures, choosing
the right metric for the right task requires careful thought. To make this
decision in an informed manner, one should study and compare general properties
of candidate measures. However, analysing measures with respect to complete
ranges of their domain values is a difficult and challenging task. In this
study, we attempt to support such analyses with a specialized visualization
technique, which operates in a barycentric coordinate system using a 3D
tetrahedron. Additionally, we adapt this technique to the context of imbalanced
data and put forward a set of properties which should be taken into account
when selecting a classification performance measure. As a result, we compare 22
popular measures and show important differences in their behaviour. Moreover,
for parametric measures such as the F$_{\beta}$ and IBA$_\alpha$(G-mean), we
analytically derive parameter thresholds that change measure properties.
Finally, we provide an online visualization tool that can aid the analysis of
complete domain ranges of performance measures.
","['\nDariusz Brzezinski\n', '\nJerzy Stefanowski\n', '\nRobert Susmaga\n', '\nIzabela Szczęch\n']",,,http://dx.doi.org/10.1016/j.ins.2018.06.020,cs.OH,['cs.OH'],10.1016/j.ins.2018.06.020,,[]
"Participating in a Computer Science Linked-courses Learning Community
  Reduces Isolation",http://arxiv.org/abs/1704.07898v1,2017-04-24T15:33:09Z,2017-04-24T15:33:09Z,"  In our previous work we reported on a linked-courses learning community for
underrepresented groups in computer science, finding differences in attitudes
and resource utilization between students in the community and other
programming students. Here we present the first statistically significant
differences in pre- to post-quarter student attitudes between those in the
learning community and others taking equivalent programming classes. We find
that students in the learning community are less likely to feel isolated
post-quarter than other programming students. We also present results showing
differences in resource utilization by learning-community participants.
","['\nAmber Settle\n', '\nJames Doyle\n', '\nTheresa Steinbach\n']",,,http://arxiv.org/abs/1704.07898v1,cs.OH,['cs.OH'],,,[]
"A sub-mW IoT-endnode for always-on visual monitoring and smart
  triggering",http://arxiv.org/abs/1705.00221v1,2017-04-29T18:23:43Z,2017-04-29T18:23:43Z,"  This work presents a fully-programmable Internet of Things (IoT) visual
sensing node that targets sub-mW power consumption in always-on monitoring
scenarios. The system features a spatial-contrast $128\mathrm{x}64$ binary
pixel imager with focal-plane processing. The sensor, when working at its
lowest power mode ($10\mu W$ at 10 fps), provides as output the number of
changed pixels. Based on this information, a dedicated camera interface,
implemented on a low-power FPGA, wakes up an ultra-low-power parallel
processing unit to extract context-aware visual information. We evaluate the
smart sensor on three always-on visual triggering application scenarios.
Triggering accuracy comparable to RGB image sensors is achieved at nominal
lighting conditions, while consuming an average power between $193\mu W$ and
$277\mu W$, depending on context activity. The digital sub-system is extremely
flexible, thanks to a fully-programmable digital signal processing engine, but
still achieves 19x lower power consumption compared to MCU-based cameras with
significantly lower on-board computing capabilities.
","['\nManuele Rusci\n', '\nDavide Rossi\n', '\nElisabetta Farella\n', '\nLuca Benini\n']","11 pages, 9 figures, submitteted to IEEE IoT Journal",,http://arxiv.org/abs/1705.00221v1,cs.OH,['cs.OH'],,,[]
Reconstruction of Missing Big Sensor Data,http://arxiv.org/abs/1705.01402v1,2017-05-03T13:17:49Z,2017-05-03T13:17:49Z,"  With ubiquitous sensors continuously monitoring and collecting large amounts
of information, there is no doubt that this is an era of big data. One of the
important sources for scientific big data is the datasets collected by Internet
of things (IoT). It's considered that these datesets contain highly useful and
valuable information. For an IoT application to analyze big sensor data, it is
necessary that the data are clean and lossless. However, due to unreliable
wireless link or hardware failure in the nodes, data loss in IoT is very
common. To reconstruct the missing big sensor data, firstly, we propose an
algorithm based on matrix rank-minimization method. Then, we consider IoT with
multiple types of sensor in each node. Accounting for possible correlations
among multiple-attribute sensor data, we propose tensor-based methods to
estimate missing values. Moreover, effective solutions are proposed using the
alternating direction method of multipliers. Finally, we evaluate the
approaches using two real sensor datasets with two missing data-patterns, i.e.,
random missing pattern and consecutive missing pattern. The experiments with
real-world sensor data show the effectiveness of the proposed methods.
","['\nYongshuai Shao\n', '\nZhe Chen\n']",,,http://arxiv.org/abs/1705.01402v1,cs.OH,['cs.OH'],,,[]
Quantum Mechanical Approach to Modelling Reliability of Sensor Reports,http://arxiv.org/abs/1705.01013v1,2017-04-17T01:22:15Z,2017-04-17T01:22:15Z,"  Dempster-Shafer evidence theory is wildly applied in multi-sensor data
fusion. However, lots of uncertainty and interference exist in practical
situation, especially in the battle field. It is still an open issue to model
the reliability of sensor reports. Many methods are proposed based on the
relationship among collected data. In this letter, we proposed a quantum
mechanical approach to evaluate the reliability of sensor reports, which is
based on the properties of a sensor itself. The proposed method is used to
modify the combining of evidences.
","['\nZichang He\n', '\nWen Jiang\n']","13 pages, 4 figures",,http://arxiv.org/abs/1705.01013v1,cs.OH,"['cs.OH', 'cs.AI', 'cs.CV']",,,[]
Wireless Health Monitoring using Passive WiFi Sensing,http://arxiv.org/abs/1704.00620v1,2017-04-03T14:40:35Z,2017-04-03T14:40:35Z,"  This paper presents a two-dimensional phase extraction system using passive
WiFi sensing to monitor three basic elderly care activities including breathing
rate, essential tremor and falls. Specifically, a WiFi signal is acquired
through two channels where the first channel is the reference one, whereas the
other signal is acquired by a passive receiver after reflection from the human
target. Using signal processing of cross-ambiguity function, various features
in the signal are extracted. The entire implementations are performed using
software defined radios having directional antennas. We report the accuracy of
our system in different conditions and environments and show that breathing
rate can be measured with an accuracy of 87% when there are no obstacles. We
also show a 98% accuracy in detecting falls and 93% accuracy in classifying
tremor. The results indicate that passive WiFi systems show great promise in
replacing typical invasive health devices as standard tools for health care.
","['\nU. M. Khan\n', '\nZ. Kabir\n', '\nS. A. Hassan\n']","6 pages, 8 figures, conference paper",,http://arxiv.org/abs/1704.00620v1,cs.OH,['cs.OH'],,,[]
RootJS: Node.js Bindings for ROOT 6,http://arxiv.org/abs/1704.07887v1,2017-03-28T15:09:19Z,2017-03-28T15:09:19Z,"  We present rootJS, an interface making it possible to seamlessly integrate
ROOT 6 into applications written for Node.js, the JavaScript runtime platform
increasingly commonly used to create high-performance Web applications. ROOT
features can be called both directly from Node.js code and by JIT-compiling C++
macros. All rootJS methods are invoked asynchronously and support callback
functions, allowing non-blocking operation of Node.js applications using them.
Last but not least, our bindings have been designed to platform-independent and
should therefore work on all systems supporting both ROOT 6 and Node.js.
  Thanks to rootJS it is now possible to create ROOT-aware Web applications
taking full advantage of the high performance and extensive capabilities of
Node.js. Examples include platforms for the quality assurance of acquired,
reconstructed or simulated data, book-keeping and e-log systems, and even Web
browser-based data visualisation and analysis.
","['\nTheo Beffart\n', '\nMaximilian Früh\n', '\nChristoph Haas\n', '\nSachin Rajgopal\n', '\nJonas Schwabe\n', '\nChristoph Wolff\n', '\nMarek Szuba\n']","7 pages, 1 figure. To appear in the Proceedings of the 22nd
  International Conference on Computing in High Energy and Nuclear Physics
  (CHEP 2016)",,http://dx.doi.org/10.1088/1742-6596/898/7/072028,cs.OH,['cs.OH'],10.1088/1742-6596/898/7/072028,,[]
"JetsonLEAP: a Framework to Measure Power on a Heterogeneous
  System-on-a-Chip Device",http://arxiv.org/abs/1706.03042v1,2017-03-29T16:45:45Z,2017-03-29T16:45:45Z,"  Computer science marches towards energy-aware practices. This trend impacts
not only the design of computer architectures, but also the design of programs.
However, developers still lack affordable and accurate technology to measure
energy consumption in computing systems. The goal of this paper is to mitigate
such problem. To this end, we introduce JetsonLEAP, a framework that supports
the implementation of energy-aware programs. JetsonLEAP consists of an embedded
hardware, in our case, the Nvidia Tegra TK1 System-on-a-chip device, a circuit
to control the flow of energy, of our own design, plus a library to instrument
program parts. We discuss two different circuit setups. The most precise setup
lets us reliably measure the energy spent by 225,000 instructions, the least
precise, although more affordable setup, gives us a window of 975,000
instructions. To probe the precision of our system, we use it in tandem with a
high-precision, high-cost acquisition system, and show that results do not
differ in any significant way from those that we get using our simpler
apparatus. Our entire infrastructure - board, power meter and both circuits -
can be reproduced with about $500.00. To demonstrate the efficacy of our
framework, we have used it to measure the energy consumed by programs running
on ARM cores, on the GPU, and on a remote server. Furthermore, we have studied
the impact of OpenACC directives on the energy efficiency of high-performance
applications.
","['\nTarsila Bessa\n', '\nChristopher Gull\n', '\nPedro Quintão\n', '\nMichael Frank\n', '\nJosé Nacif\n', '\nFernando Magno Quintão Pereira\n']","31 pages, 19 figures",,http://arxiv.org/abs/1706.03042v1,cs.OH,['cs.OH'],,,[]
"Algorithm/Architecture Co-design of Proportionate-type LMS Adaptive
  Filters for Sparse System Identification",http://arxiv.org/abs/1703.10658v1,2017-03-17T09:12:11Z,2017-03-17T09:12:11Z,"  This paper investigates the problem of implementing proportionate-type LMS
family of algorithms in hardware for sparse adaptive filtering applications
especially the network echo cancelation. We derive a re-formulated
proportionate type algorithm through algorithm-architecture co-design
methodology that can be pipelined and has an efficient architecture for
hardware implementation. We study the convergence, steady-state and tracking
performances of these re-formulated algorithms for white, color and speech
inputs before implementing them in hardware. To the best of our knowledge this
is the first attempt to implement proportionate-type algorithms in hardware. We
show that Delayed $\mu$-law Proportionate LMS (DMPLMS) algorithm for white
input and Delayed Wavelet MPLMS (DWMPLMS) for colored input are the robust VLSI
solutions for network echo cancellation where the sparsity of the echo paths
can vary with time. We implemented all the designs considering $16$-bit fixed
point representation in hardware, synthesized the designs and synthesis results
show that DMPLMS algorithm with $\approx25\%$ increase in hardware over
conventional DLMS architecture, achieves $3X$ improvement in convergence rate
for white input and DWMPLMS algorithm with $\approx58\%$ increase in hardware
achieves $15X$ improvement in convergence rate for correlated input conditions.
","['\nSubrahmanyam Mula\n', '\nVinay Chakravarthi Gogineni\n', '\nAnindya Sundar Dhar\n']",Under communication,,http://arxiv.org/abs/1703.10658v1,cs.OH,['cs.OH'],,,[]
Stochastic Development Regression on Non-Linear Manifolds,http://arxiv.org/abs/1703.00291v1,2017-03-01T13:32:27Z,2017-03-01T13:32:27Z,"  We introduce a regression model for data on non-linear manifolds. The model
describes the relation between a set of manifold valued observations, such as
shapes of anatomical objects, and Euclidean explanatory variables. The approach
is based on stochastic development of Euclidean diffusion processes to the
manifold. Defining the data distribution as the transition distribution of the
mapped stochastic process, parameters of the model, the non-linear analogue of
design matrix and intercept, are found via maximum likelihood. The model is
intrinsically related to the geometry encoded in the connection of the
manifold. We propose an estimation procedure which applies the Laplace
approximation of the likelihood function. A simulation study of the performance
of the model is performed and the model is applied to a real dataset of Corpus
Callosum shapes.
","['\nLine Kühnel\n', '\nStefan Sommer\n']",,,http://arxiv.org/abs/1703.00291v1,cs.OH,['cs.OH'],,,[]
"A Survey on Non-Intrusive Load Monitoring Methodies and Techniques for
  Energy Disaggregation Problem",http://arxiv.org/abs/1703.00785v3,2017-03-02T13:52:30Z,2017-03-10T17:13:52Z,"  The rapid urbanization of developing countries coupled with explosion in
construction of high rising buildings and the high power usage in them calls
for conservation and efficient energy program. Such a program require
monitoring of end-use appliances energy consumption in real-time. The worldwide
recent adoption of smart-meter in smart-grid, has led to the rise of
Non-Intrusive Load Monitoring (NILM); which enables estimation of
appliance-specific power consumption from building's aggregate power
consumption reading. NILM provides households with cost-effective real-time
monitoring of end-use appliances to help them understand their consumption
pattern and become part and parcel of energy conservation strategy. This paper
presents an up to date overview of NILM system and its associated methods and
techniques for energy disaggregation problem. This is followed by the review of
the state-of-the art NILM algorithms. Furthermore, we review several
performance metrics used by NILM researcher to evaluate NILM algorithms and
discuss existing benchmarking framework for direct comparison of the state of
the art NILM algorithms. Finally, the paper discuss potential NILM use-cases,
presents an overview of the public available dataset and highlight challenges
and future research directions.
","['\nAnthony Faustine\n', '\nNerey Henry Mvungi\n', '\nShubi Kaijage\n', '\nKisangiri Michael\n']",,,http://arxiv.org/abs/1703.00785v3,cs.OH,['cs.OH'],,,[]
Bayesian Gates for Reliable Logical Operations under Noisy Condition,http://arxiv.org/abs/1703.00444v3,2017-03-01T05:52:39Z,2019-12-16T04:05:01Z,"  The reliability of logical operations is indispensable for the reliable
operation of computational systems. Since the down-sizing of micro-fabrication
generates non-negligible noise in these systems, a new approach for designing
noise-immune gates is required. In this paper, we demonstrate that noise-immune
gates can be designed by combining Bayesian inference theory with the idea of
computation over a noisy signal. To reveal their practical advantages, the
performance of these gates is evaluated in comparison with a stochastic
resonance-based gate proposed previously. This approach for computation is also
demonstrated to be better than a conventional one that conducts information
transmission and computation separately.
",['\nTetsuya J. Kobayashi\n'],3figures + 3 supplementary figures,"Phys. Rev. E 101, 042205 (2020)",http://dx.doi.org/10.1103/PhysRevE.101.042205,cs.OH,"['cs.OH', 'quant-ph']",10.1103/PhysRevE.101.042205,,[]
Kharita: Robust Map Inference using Graph Spanners,http://arxiv.org/abs/1702.06025v1,2017-02-20T15:51:07Z,2017-02-20T15:51:07Z,"  The widespread availability of GPS information in everyday devices such as
cars, smartphones and smart watches make it possible to collect large amount of
geospatial trajectory information. A particularly important, yet technically
challenging, application of this data is to identify the underlying road
network and keep it updated under various changes. In this paper, we propose
efficient algorithms that can generate accurate maps in both batch and online
settings. Our algorithms utilize techniques from graph spanners so that they
produce maps can effectively handle a wide variety of road and intersection
shapes. We conduct a rigorous evaluation of our algorithms over two real-world
datasets and under a wide variety of performance metrics. Our experiments show
a significant improvement over prior work. In particular, we observe an
increase in Biagioni f-score of up to 20% when compared to the state of the art
while reducing the execution time by an order of magnitude. We also make our
source code open source for reproducibility and enable other researchers to
build on our work.
","['\nRade Stanojevic\n', '\nSofiane Abbar\n', '\nSaravanan Thirumuruganathan\n', '\nSanjay Chawla\n', '\nFethi Filali\n', '\nAhid Aleimat\n']",,,http://arxiv.org/abs/1702.06025v1,cs.OH,['cs.OH'],,,[]
"Characterizing Classes of Potential Outliers through Traffic Data Set
  Data Signature 2D nMDS Projection",http://arxiv.org/abs/1702.07501v1,2017-02-24T09:03:00Z,2017-02-24T09:03:00Z,"  This paper presents a formal method for characterizing the potential outliers
from the data signature projection of traffic data set using Non-Metric
Multidimensional Scaling (nMDS) visualization. Previous work had only relied on
visual inspection and the subjective nature of this technique may derive false
and invalid potential outliers. The identification of correct potential
outliers had already been an open problem proposed in literature. This is due
to the fact that they pinpoint areas and time frames where traffic
incidents/accidents occur along the North Luzon Expressway (NLEX) in Luzon. In
this paper, potential outliers are classified into (1) absolute potential
outliers; (2) valid potential outliers; and (3) ambiguous potential outliers
through the use of confidence bands and confidence ellipse. A method is also
described to validate cluster membership of identified ambiguous potential
outliers. Using the 2006 NLEX Balintawak Northbound (BLK-NB) data set, we were
able to identify two absolute potential outliers, nine valid potential
outliers, and five ambiguous potential outliers. In a literature where Vector
Fusion was used, 10 potential outliers were identified. Given the results for
the nMDS visualization using the confidence bands and confidence ellipses, all
of these 10 potential outliers were also found and 8 new potential outliers
were also found.
","['\nErlo Robert F. Oquendo\n', '\nJhoirene B. Clemente\n', '\nJasmine A. Malinao\n', '\nHenry N. Adorna\n']",,,http://arxiv.org/abs/1702.07501v1,cs.OH,"['cs.OH', '97K80', 'G.3; H.2.8']",,,[]
Modulation and Multiple Access for 5G Networks,http://arxiv.org/abs/1702.07673v1,2017-02-21T14:23:14Z,2017-02-21T14:23:14Z,"  Fifth generation (5G) wireless networks face various challenges in order to
support large-scale heterogeneous traffic and users, therefore new modulation
and multiple access (MA) schemes are being developed to meet the changing
demands. As this research space is ever increasing, it becomes more important
to analyze the various approaches, therefore in this article we present a
comprehensive overview of the most promising modulation and MA schemes for 5G
networks. We first introduce the different types of modulation that indicate
their potential for orthogonal multiple access (OMA) schemes and compare their
performance in terms of spectral efficiency, out-of-band leakage, and bit-error
rate. We then pay close attention to various types of non-orthogonal multiple
access (NOMA) candidates, including power-domain NOMA, code-domain NOMA, and
NOMA multiplexing in multiple domains. From this exploration we can identify
the opportunities and challenges that will have significant impact on the
design of modulation and MA for 5G networks.
","['\nYunlong Cai\n', '\nZhijin Qin\n', '\nFangyu Cui\n', '\nGeoffrey Ye Li\n', '\nJulie A. McCann\n']",,,http://arxiv.org/abs/1702.07673v1,cs.OH,['cs.OH'],,,[]
"Building up user confidence for the spaceborne derived global and
  continental land cover products for the Mediterranean region: the case of
  Thessaly",http://arxiv.org/abs/1702.07890v1,2017-02-25T13:19:01Z,2017-02-25T13:19:01Z,"  Across globe and space agencies nations recognize the importance of
homogenized land cover information, prone to regular updates, both in the
context of thematic and spatial resolutions. Recent sensor advances and the
free distribution policy promote the utilization of spaceborne products in an
unprecedented pace into an increasingly wider range of applications. Ensuring
credibility to the users is a major enabler in this process. To this end this
study contributes with a systematic accuracy performance measurement and
continental/global land cover layers' inter-comparison moving towards
confidence built up. Confidence levels during validation and a weighted overall
accuracy assessment were applied. Google Earth imagery was employed to assess
the accuracy of three land cover products, i.e., Globeland30, HRLs and CLC
2012, for the years 2010 and 2012. Reported rates indicate a minimum weighted
overall accuracy of 84%. Specific classes' performance deviations from the
general trend were noted and discussed on the basis of an unbiased sampling
approach. By integrating confidence levels during the ground truth annotation,
stratified sampling on the several Corine Level 3 subclasses and the weighted
overall accuracy assessment, the different aspects of the considered land cover
products can be highlighted more objectively.
","['\nIoannis Manakos\n', '\nChristina Karakizi\n', '\nGiannis Gkinis\n', '\nKonstantinos Karantzalos\n']",,,http://arxiv.org/abs/1702.07890v1,cs.OH,['cs.OH'],,,[]
"Preventing Hospital Acquired Infections Through a Workflow-Based
  Cyber-Physical System",http://arxiv.org/abs/1702.08010v1,2017-02-26T09:40:36Z,2017-02-26T09:40:36Z,"  Hospital acquired infections (HAI) are infections acquired within the
hospital from healthcare workers, patients or from the environment, but which
have no connection to the initial reason for the patient's hospital admission.
HAI are a serious world-wide problem, leading to an increase in mortality
rates, duration of hospitalisation as well as significant economic burden on
hospitals. Although clear preventive guidelines exist, studies show that
compliance to them is frequently poor. This paper details the software
perspective for an innovative, business process software based cyber-physical
system that will be implemented as part of a European Union-funded research
project. The system is composed of a network of sensors mounted in different
sites around the hospital, a series of wearables used by the healthcare workers
and a server side workflow engine. For better understanding, we describe the
system through the lens of a single, simple clinical workflow that is
responsible for a significant portion of all hospital infections. The goal is
that when completed, the system will be configurable in the sense of
facilitating the creation and automated monitoring of those clinical workflows
that when combined, account for over 90\% of hospital infections.
","['\nMaria Iuliana Bocicor\n', '\nArthur-Jozsef Molnar\n', '\nCristian Taslitchi\n']","Proceedings of ENASE 2016, ISBN: 978-989-758-189-2",,http://dx.doi.org/10.5220/0005916900630068,cs.OH,"['cs.OH', 'D.2.2, J.3']",10.5220/0005916900630068,,[]
On absolutely normal numbers and their discrepancy estimate,http://arxiv.org/abs/1702.04072v2,2017-02-14T04:10:27Z,2017-07-11T02:19:02Z,"  We construct the base $2$ expansion of an absolutely normal real number $x$
so that, for every integer $b$ greater than or equal to $2$, the discrepancy
modulo $1$ of the sequence $(b^0 x, b^1 x, b^2 x , \ldots)$ is essentially the
same as that realized by almost all real numbers.
","['\nVerónica Becher\n', '\nAdrian-Maria Scheerer\n', '\nTheodore Slaman\n']","This paper has been superseded by: ""On the construction of absolutely
  normal numbers"" by Christoph Aistleitner, Ver\'onica Becher, Adrian-Maria
  Scheerer and Theodore Slaman, July 2017, arXiv:1707.02628,
  http://arxiv.org/abs/1707.02628",,http://arxiv.org/abs/1702.04072v2,math.NT,"['math.NT', 'cs.OH', '11K16 (primary), 11Y16, 68-04(secondary)']",,,[]
An Introduction to Classic DEVS,http://arxiv.org/abs/1701.07697v5,2017-01-26T13:38:19Z,2018-05-02T06:49:51Z,"  DEVS is a popular formalism for modelling complex dynamic systems using a
discrete-event abstraction. At this abstraction level, a timed sequence
ofpertinent ""events"" input to a system (or internal, in the case of timeouts)
cause instantaneous changes to the state of the system. Between events, the
state does not change, resulting in a a piecewise constant state trajectory.
Main advantages of DEVS are its rigorous formal definition, and its support for
modular composition.
  This chapter introduces the Classic DEVS formalism in a bottom-up fashion,
using a simple traffic light example. The syntax and operational semantics of
Atomic (i.e., non-hierarchical) models are intruced first. The semantics of
Coupled (hierarchical) models is then given by translation into Atomic DEVS
models. As this formal ""flattening"" is not efficient, a modular abstract
simulator which operates directly on the coupled model is also presented. This
is the common basis for subsequent efficient implementations. We continue to
actual applications of DEVS modelling and simulation, as seen in performance
analysis for queueing systems. Finally, we present some of the shortcomings in
the Classic DEVS formalism, and show solutions to them in the form of variants
of the original formalism.
","['\nYentl Van Tendeloo\n', '\nHans Vangheluwe\n']",,,http://arxiv.org/abs/1701.07697v5,cs.OH,"['cs.OH', 'I.6.2']",,,[]
"Quantitative Characterization of Components of Computer Assisted
  Interventions",http://arxiv.org/abs/1702.00582v1,2017-02-02T08:44:54Z,2017-02-02T08:44:54Z,"  Purpose: We propose a mathematical framework for quantitative analysis
weighting the impact of heterogeneous components of a surgery. While
multi-level approaches, surgical process modeling and other workflow analysis
methods exist, this is to our knowledge the first quantitative approach.
Methods: Inspired by the group decision making problem from the field of
operational research, we define event impact factors, which combine independent
and very diverse low-level functions. This allows us to rate surgical events by
their importance. Results: We conducted surveys with 4 surgeons to determine
the importance of roles, phases and their combinations within a laparoscopic
cholecystectomy. Applying this data on a recorded surgery, we showed that it is
possible to define a quantitative measure for deciding on acception or
rejection of calls to different roles and at different phases of surgery.
Conclusions: This methodology allows us to use components such as expertise and
role of the surgical staff and other aspects of a given surgery in order to
quantitatively analyze and evaluate events, actions, user interfaces or
procedures.
","['\nAsli Okur\n', '\nRalf Stauder\n', '\nHubertus Feussner\n', '\nNassir Navab\n']","13 pages, 3 figures",,http://arxiv.org/abs/1702.00582v1,cs.OH,['cs.OH'],,,[]
Sensitivity Analysis of Expensive Black-Box Systems Using Metamodeling,http://arxiv.org/abs/1702.00650v1,2017-02-02T12:47:48Z,2017-02-02T12:47:48Z,"  Simulations are becoming ever more common as a tool for designing complex
products. Sensitivity analysis techniques can be applied to these simulations
to gain insight, or to reduce the complexity of the problem at hand. However,
these simulators are often expensive to evaluate and sensitivity analysis
typically requires a large amount of evaluations. Metamodeling has been
successfully applied in the past to reduce the amount of required evaluations
for design tasks such as optimization and design space exploration. In this
paper, we propose a novel sensitivity analysis algorithm for variance and
derivative based indices using sequential sampling and metamodeling. Several
stopping criteria are proposed and investigated to keep the total number of
evaluations minimal. The results show that both variance and derivative based
techniques can be accurately computed with a minimal amount of evaluations
using fast metamodels and FLOLA-Voronoi or density sequential sampling
algorithms.
","['\nTom Van Steenkiste\n', '\nJoachim van der Herten\n', '\nIvo Couckuyt\n', '\nTom Dhaene\n']",proceedings of winter simulation conference 2016,,http://dx.doi.org/10.1109/WSC.2016.7822123,cs.OH,"['cs.OH', 'I.6; I.6.4; I.6.5; I.6.6']",10.1109/WSC.2016.7822123,,[]
Taxi-based Emergency Medical System,http://arxiv.org/abs/1701.04126v1,2017-01-15T23:02:34Z,2017-01-15T23:02:34Z,"  In case of a severe accident, the key to saving lives is the time between the
incident and when the victim receives treatment from the first-responders. In
areas with well designed emergency medical systems, the time for an ambulance
to arrive at the accident location is often not too long. However, in many low
and middle income countries, it usually takes much longer for an ambulance to
arrive at the accident location due to lack of proper services. On the other
hand, with ubiquitous wireless connectivity, and emergence of radio based
taxis, it seems feasible to build a low-cost emergency response system based on
taxi service. In this report, we explore one such solution for deployment of a
taxi-based emergency response systems using reinforcement learning.
",['\nLi-Yi Lin\n'],,,http://arxiv.org/abs/1701.04126v1,cs.OH,['cs.OH'],,,[]
Stay-point Identification as Curve Extrema,http://arxiv.org/abs/1701.06276v1,2017-01-23T06:45:01Z,2017-01-23T06:45:01Z,"  In a nutshell, stay-points are locations that a person has stopped for some
amount of time. Previous work depends mainly on stay-point identification
methods using experimentally fine tuned threshold values. These behave well on
their experimental datasets but may exhibit reduced performance on other
datasets.
  In this work, we demonstrate the potential of a geometry-based method for
stay-point extraction. This is accomplished by transforming the user's
trajectory path to a two-dimensional discrete time series curve that in turn
transforms the stay-points to the local minima of the first derivative of this
curve.
  To demonstrate the soundness of the proposed method, we evaluated it on raw,
noisy trajectory data acquired over the period of 28 different days using four
different techniques. The results demonstrate, among others, that given a good
trajectory tracking technique, we can identify correctly 86% to 98% of the
stay-points.
",['\nGeorgios Stylianou\n'],,,http://arxiv.org/abs/1701.06276v1,cs.OH,['cs.OH'],,,[]
Validation of Internal Meters of Mobile Android Devices,http://arxiv.org/abs/1701.07095v1,2017-01-24T22:25:18Z,2017-01-24T22:25:18Z,"  In this paper we outline our results for validating the precision of the
internal power meters of smart-phones under different workloads. We compare its
results with an external power meter. This is the first step towards creating
customized energy models on the fly and towards optimizing battery efficiency
using genetic program improvements. Our experimental results indicate that the
internal meters are sufficiently precise when large enough time windows are
considered.
  This is part of our work on the ""dreaming smart-phone"". For a technical
demonstration please watch our videos
https://www.youtube.com/watch?v=xeeFz2GLFdU and
https://www.youtube.com/watch?v=C7WHoLW1KYw.
","['\nMahmoud A. Bokhari\n', '\nYuanzhong Xia\n', '\nBo Zhou\n', '\nBrad Alexander\n', '\nMarkus Wagner\n']","Used watt meters: Yoctopuce YoctoWatt, Maxim MAX17050. Used
  smart-phones: Nexus 6, Nexus 9",,http://arxiv.org/abs/1701.07095v1,cs.OH,['cs.OH'],,,[]
"Persistent Entropy for Separating Topological Features from Noise in
  Vietoris-Rips Complexes",http://arxiv.org/abs/1701.07857v1,2017-01-18T12:27:20Z,2017-01-18T12:27:20Z,"  Persistent homology studies the evolution of k-dimensional holes along a
nested sequence of simplicial complexes (called a filtration). The set of bars
(i.e. intervals) representing birth and death times of k-dimensional holes
along such sequence is called the persistence barcode. k-Dimensional holes with
short lifetimes are informally considered to be ""topological noise"", and those
with long lifetimes are considered to be ""topological features"" associated to
the filtration. Persistent entropy is defined as the Shannon entropy of the
persistence barcode of a given filtration. In this paper we present new
important properties of persistent entropy of Cech and Vietoris-Rips
filtrations. Among the properties, we put a focus on the stability theorem that
allows to use persistent entropy for comparing persistence barcodes. Later, we
derive a simple method for separating topological noise from features in
Vietoris-Rips filtrations.
","['\nNieves Atienza\n', '\nRocio Gonzalez-Diaz\n', '\nMatteo Rucco\n']",arXiv admin note: text overlap with arXiv:1605.02885,,http://arxiv.org/abs/1701.07857v1,cs.OH,['cs.OH'],,,[]
"Correct Convergence of Min-Sum Loopy Belief Propagation in a Block
  Interpolation Problem",http://arxiv.org/abs/1702.06391v1,2017-01-22T21:42:45Z,2017-01-22T21:42:45Z,"  This work proves a new result on the correct convergence of Min-Sum Loopy
Belief Propagation (LBP) in an interpolation problem on a square grid graph.
The focus is on the notion of local solutions, a numerical quantity attached to
each site of the graph that can be used for obtaining MAP estimates. The main
result is that over an $N\times N$ grid graph with a one-run boundary
configuration, the local solutions at each $i \in B$ can be calculated using
Min-Sum LBP by passing difference messages in $2N$ iterations, which parallels
the well-known convergence time in trees.
","['\nYutong Wang\n', '\nMatthew G. Reyes\n', '\nDavid L. Neuhoff\n']","16 pages, 6 figures. An abbreviated version of this paper has been
  submitted to ISIT 2017",,http://arxiv.org/abs/1702.06391v1,cs.OH,['cs.OH'],,,[]
SOI RF Switch for Wireless Sensor Network,http://arxiv.org/abs/1701.01763v1,2017-01-06T21:50:56Z,2017-01-06T21:50:56Z,"  The objective of this research was to design a 0-5 GHz RF SOI switch, with
0.18um power Jazz SOI technology by using Cadence software, for health care
applications. This paper introduces the design of a RF switch implemented in
shunt-series topology. An insertion loss of 0.906 dB and an isolation of 30.95
dB were obtained at 5 GHz. The switch also achieved a third order distortion of
53.05 dBm and 1 dB compression point reached 50.06dBm. The RF switch
performance meets the desired specification requirements.
","['\nWei Cai\n', '\nCheng Li\n', '\nShiWei Luan\n']",,,http://arxiv.org/abs/1701.01763v1,cs.OH,['cs.OH'],,,[]
Low Power SI Class E Power Amplifier and RF Switch For Health Care,http://arxiv.org/abs/1701.01771v1,2017-01-06T22:33:19Z,2017-01-06T22:33:19Z,"  This research was to design a 2.4 GHz class E Power Amplifier (PA) for health
care, with 0.18um Semiconductor Manufacturing International Corporation CMOS
technology by using Cadence software. And also RF switch was designed at
cadence software with power Jazz 180nm SOI process. The ultimate goal for such
application is to reach high performance and low cost, and between high
performance and low power consumption design. This paper introduces the design
of a 2.4GHz class E power amplifier and RF switch design. PA consists of
cascade stage with negative capacitance. This power amplifier can transmit
16dBm output power to a 50{\Omega} load. The performance of the power amplifier
and switch meet the specification requirements of the desired.
","['\nWei Cai\n', '\nJian Xu\n', '\nLiang Huang\n']",,,http://arxiv.org/abs/1701.01771v1,cs.OH,['cs.OH'],,,[]
"Online characterization of planetary surfaces: PlanetServer, an
  open-source analysis and visualization tool",http://arxiv.org/abs/1701.01726v2,2017-01-06T11:40:27Z,2017-04-29T13:24:42Z,"  The lack of open-source tools for hyperspectral data visualization and
analysiscreates a demand for new tools. In this paper we present the new
PlanetServer,a set of tools comprising a web Geographic Information System
(GIS) and arecently developed Python Application Programming Interface (API)
capableof visualizing and analyzing a wide variety of hyperspectral data from
differentplanetary bodies. Current WebGIS open-source tools are evaluated in
orderto give an overview and contextualize how PlanetServer can help in this
mat-ters. The web client is thoroughly described as well as the datasets
availablein PlanetServer. Also, the Python API is described and exposed the
reason ofits development. Two different examples of mineral characterization of
differenthydrosilicates such as chlorites, prehnites and kaolinites in the Nili
Fossae areaon Mars are presented. As the obtained results show positive outcome
in hyper-spectral analysis and visualization compared to previous literature,
we suggestusing the PlanetServer approach for such investigations.
","['\nR. Marco Figuera\n', '\nB. Pham Huu\n', '\nA. P. Rossi\n', '\nM. Minin\n', '\nJ. Flahaut\n', '\nA. Halder\n']",,,http://dx.doi.org/10.1016/j.pss.2017.09.007,astro-ph.EP,"['astro-ph.EP', 'cs.OH']",10.1016/j.pss.2017.09.007,,[]
"Computational Intelligence: are you crazy? Since when has intelligence
  become computational?",http://arxiv.org/abs/1612.05087v1,2016-12-14T04:40:16Z,2016-12-14T04:40:16Z,"  Computational Intelligence is a dead-end attempt to recreate human-like
intelligence in a computing machine. The goal is unattainable because the means
chosen for its accomplishment are mutually inconsistent and contradictory:
""Computational"" implies data processing ability while ""Intelligence"" implies
the ability to process information. In the research community, there is a lack
of interest in data versus information divergence. The cause of this
indifference is the Shannon's Information theory, which has dominated the
scientific community since the early 1950s. However, today it is clear that
Shannon's theory is applicable only to a specific case of data communication
and is inapplicable to the majority of other occasions, where information about
semantic properties of a message must be taken into account. The paper will try
to explain the devastating results of overlooking some of these very important
issues - what is intelligence, what is semantic information, how they are
interrelated and what happens when the relationship is disregarded.
",['\nEmanuel Diamant\n'],"Presented at the 2016 IEEE Symposium Series on Computational
  Intelligence (IEEE SSCI2016), December 6-9, 2016, Athens, Greece. arXiv admin
  note: text overlap with arXiv:1607.05810, arXiv:1505.05186",,http://arxiv.org/abs/1612.05087v1,cs.OH,['cs.OH'],,,[]
"Improving the Quality of Random Number Generators by Applying a Simple
  Ratio Transformation",http://arxiv.org/abs/1612.07318v1,2016-12-21T03:48:24Z,2016-12-21T03:48:24Z,"  It is well-known that the quality of random number generators can often be
improved by combining several generators, e.g. by summing or subtracting their
results. In this paper we investigate the ratio of two random number generators
as an alternative approach: the smaller of two input random numbers is divided
by the larger, resulting in a rational number from $[0,1]$.
  We investigate theoretical properties of this approach and show that it
yields a good approximation to the ideal uniform distribution. To evaluate the
empirical properties we use the well-known test suite \textsc{TestU01}. We
apply the ratio transformation to moderately bad generators, i.e. those that
failed up to 40\% of the tests from the test battery \textsc{Crush} of
\textsc{TestU01}. We show that more than half of them turn into very good
generators that pass all tests of \textsc{Crush} and \textsc{BigCrush} from
\textsc{TestU01} when the ratio transformation is applied. In particular,
generators based on linear operations seem to benefit from the ratio, as this
breaks up some of the unwanted regularities in the input sequences. Thus the
additional effort to produce a second random number and to calculate the ratio
allows to increase the quality of available random number generators.
","['\nMichael Kolonko\n', '\nZijun Wu\n', '\nFeng Gu\n']",18 pages,,http://arxiv.org/abs/1612.07318v1,cs.OH,['cs.OH'],,,[]
DEMoS Manifesto,http://arxiv.org/abs/1612.04191v1,2016-12-11T12:01:59Z,2016-12-11T12:01:59Z,"  This is a manifesto for DEMoS, which is a Distributed Embedded Modular
System, but also a manifesto addressing the need for more
inter-/cross-disciplinary mastery of working knowledge related to installing
this class of systems in the real world. There is somehow room for yet another
class of systems - complementary to existing embedded systems - complementing
distributed operating systems - which takes on an interdisciplinary
cyber-physical-materiality approach, a dedicated holistic perspective that
recognizes the true value of interdisciplinary mastery vs. the implicit and
overlooked expense of narrow intra-disciplinary focus dominating much of
systems development (e.g. EE, CE, CS, SE, and IS). Interdisciplinary mastery
yields its accumulated value across the development, deployment, use, re-use,
and decommission phases for this class of systems: DEMoS is a system
architected to be locally distributed, embedded, and modular as outlined herein
and with the additional goals of human interdisciplinary mastery in this
context: A potential set of goals for developing and applying DEMoS can be
found in UN Resolution 70/1.
",['\nRasmus Ulslev Pedersen\n'],"Invited talk UiO, IfI",,http://arxiv.org/abs/1612.04191v1,cs.OH,"['cs.OH', 'C.1.0']",,,[]
"City traffic forecasting using taxi GPS data: A coarse-grained cellular
  automata model",http://arxiv.org/abs/1612.02540v1,2016-12-08T06:17:52Z,2016-12-08T06:17:52Z,"  City traffic is a dynamic system of enormous complexity. Modeling and
predicting city traffic flow remains to be a challenge task and the main
difficulties are how to specify the supply and demands and how to parameterize
the model. In this paper we attempt to solve these problems with the help of
large amount of floating car data. We propose a coarse-grained cellular
automata model that simulates vehicles moving on uniform grids whose size are
much larger compared with the microscopic cellular automata model. The car-car
interaction in the microscopic model is replaced by the coupling between
vehicles and coarse-grained state variables in our model. To parameterize the
model, flux-occupancy relations are fitted from the historical data at every
grids, which serve as the coarse-grained fundamental diagrams coupling the
occupancy and speed. To evaluate the model, we feed it with the historical
travel demands and trajectories obtained from the floating car data and use the
model to predict road speed one hour into the future. Numerical results show
that our model can capture the traffic flow pattern of the entire city and make
reasonable predictions. The current work can be considered a prototype for a
model-based forecasting system for city traffic.
","['\nYucheng Hu\n', '\nMinwei Li\n', '\nHao Liu\n', '\nXiaolu Guo\n', '\nXiaowei Wang\n', '\nTiejun Li\n']",,,http://arxiv.org/abs/1612.02540v1,cs.OH,"['cs.OH', 'physics.soc-ph']",,,[]
Detecting Plagiarism based on the Creation Process,http://arxiv.org/abs/1612.09183v2,2016-12-10T22:31:07Z,2017-07-19T22:52:32Z,"  All methodologies for detecting plagiarism to date have focused on the final
digital ""outcome"", such as a document or source code. Our novel approach takes
the creation process into account using logged events collected by special
software or by the macro recorders found in most office applications. We look
at an author's interaction logs with the software used to create the work.
Detection relies on comparing the histograms of multiple logs' command use. A
work is classified as plagiarism if its log deviates too much from logs of
""honestly created"" works or if its log is too similar to another log. The
technique supports the detection of plagiarism for digital outcomes that stem
from \emph{unique} tasks, such as theses and \emph{equal} tasks such as
assignments for which the same problem sets are solved by multiple students.
Focusing on the latter case, we evaluate this approach using logs collected by
an interactive development environment (IDE) from more than sixty students who
completed three programming assignments.
","['\nJohannes Schneider\n', '\nAvi Bernstein\n', '\nJan Vom Brocke\n', '\nKostadin Damevski\n', '\nDavid C. Shepherd\n']",,,http://arxiv.org/abs/1612.09183v2,cs.OH,"['cs.OH', 'cs.SE']",,,[]
On Vague Computers,http://arxiv.org/abs/1709.10373v1,2016-11-21T13:47:21Z,2016-11-21T13:47:21Z,"  Vagueness is something everyone is familiar with. In fact, most people think
that vagueness is closely related to language and exists only there. However,
vagueness is a property of the physical world. Quantum computers harness
superposition and entanglement to perform their computational tasks. Both
superposition and entanglement are vague processes. Thus quantum computers,
which process exact data without ""exploiting"" vagueness, are actually vague
computers.
",['\nApostolos Syropoulos\n'],,,http://dx.doi.org/10.1007/978-3-319-46376-6_17,cs.OH,['cs.OH'],10.1007/978-3-319-46376-6_17,,[]
Length Matters: Clustering System Log Messages using Length of Words,http://arxiv.org/abs/1611.03213v1,2016-11-10T08:06:37Z,2016-11-10T08:06:37Z,"  The analysis techniques of system log messages (syslog messages) have a long
history from when the syslog mechanism was invented. Typically, the analysis
consists of two parts, one is a message template generation, and the other is
finding something interesting using the messages classified by the inferred
templates. It is important to generate better templates to achieve better,
precise, or convincible analysis results. In this paper, we propose a
classification methodology using the length of words of each message. Our
method is suitable for online template generation because it does not require
two-pass analysis to generate template messages, that is an important factor
considering increasing amount of log messages produced by a large number of
system components such as cloud infrastructure.
",['\nKeiichi Shima\n'],,,http://arxiv.org/abs/1611.03213v1,cs.OH,"['cs.OH', 'H.3.3; C.2.0']",,,[]
"A Novel Approach for Learning How to Automatically Match Job Offers and
  Candidate Profiles",http://arxiv.org/abs/1611.04931v2,2016-11-15T16:53:00Z,2017-09-07T17:26:02Z,"  Automatic matching of job offers and job candidates is a major problem for a
number of organizations and job applicants that if it were successfully
addressed could have a positive impact in many countries around the world. In
this context, it is widely accepted that semi-automatic matching algorithms
between job and candidate profiles would provide a vital technology for making
the recruitment processes faster, more accurate and transparent. In this work,
we present our research towards achieving a realistic matching approach for
satisfactorily addressing this challenge. This novel approach relies on a
matching learning solution aiming to learn from past solved cases in order to
accurately predict the results in new situations. An empirical study shows us
that our approach is able to beat solutions with no learning capabilities by a
wide margin.
","['\nJorge Martinez-Gil\n', '\nAlejandra Lorena Paoletti\n', '\nMario Pichler\n']","15 pages, 6 figures",,http://dx.doi.org/10.1007/s10796-019-09929-7,cs.OH,['cs.OH'],10.1007/s10796-019-09929-7,,[]
"COOLL: Controlled On/Off Loads Library, a Public Dataset of High-Sampled
  Electrical Signals for Appliance Identification",http://arxiv.org/abs/1611.05803v1,2016-11-17T18:03:05Z,2016-11-17T18:03:05Z,"  This paper gives a brief description of the Controlled On/Off Loads Library
(COOLL) dataset. This latter is a dataset of high-sampled electrical current
and voltage measurements representing individual appliances consumption. The
measurements were taken in June 2016 in the PRISME laboratory of the University
of Orl\'eans, France. The appliances are mainly controllable appliances (i.e.
we can precisely control their turn-on/off time instants). 42 appliances of 12
types were measured at a 100 kHz sampling frequency.
","['\nThomas Picon\n', '\nMohamed Nait Meziane\n', '\nPhilippe Ravier\n', '\nGuy Lamarque\n', '\nClarisse Novello\n', '\nJean-Charles Le Bunetel\n', '\nYves Raingeaud\n']","5 pages, 2 figures, 3 tables",,http://arxiv.org/abs/1611.05803v1,cs.OH,['cs.OH'],,,[]
"Enerji İzleme Yazılımları için Merkezi ve Genel bir
  Mimari (A Centralized and Generic Architecture for Energy Monitoring
  Software)",http://arxiv.org/abs/1611.00739v1,2016-11-02T19:33:45Z,2016-11-02T19:33:45Z,"  There is need for several software systems within the energy domain and
corresponding systems are being developed to satisfy these needs. These systems
include energy monitoring, information, wide area monitoring and control
systems, and SCADA systems. Energy monitoring systems are one of the most
important and common systems among them. In this study, after briefly reviewing
several of the software systems within the energy domain, a centralized and
generic software architecture for energy monitoring systems is presented. Next,
sample projects are described in which energy monitoring systems based on this
architecture have been implemented. We envisage that this study will be an
important resource for software projects in the energy domain.
","['\nDilek Küçük\n', '\nTuran Demirci\n']","in Turkish. 10. Ulusal Yaz{\i}l{\i}m M\""uhendisli\u{g}i Sempozyumu'na
  (UYMS 2016) kabul edildi",,http://arxiv.org/abs/1611.00739v1,cs.OH,['cs.OH'],,,[]
Activity Recognition Based on Micro-Doppler Signature with In-Home Wi-Fi,http://arxiv.org/abs/1611.01801v1,2016-11-06T16:06:30Z,2016-11-06T16:06:30Z,"  Device free activity recognition and monitoring has become a promising
research area with increasing public interest in pattern of life monitoring and
chronic health conditions. This paper proposes a novel framework for in-home
Wi-Fi signal-based activity recognition in e-healthcare applications using
passive micro-Doppler (m-D) signature classification. The framework includes
signal modeling, Doppler extraction and m-D classification. A data collection
campaign was designed to verify the framework where six m-D signatures
corresponding to typical daily activities are sucessfully detected and
classified using our software defined radio (SDR) demo system. Analysis of the
data focussed on potential discriminative characteristics, such as maximum
Doppler frequency and time duration of activity. Finally, a sparsity induced
classifier is applied for adaptting the method in healthcare application
scenarios and the results are compared with those from the well-known Support
Vector Machine (SVM) method.
","['\nQingchao Chen\n', '\nBo Tan\n', '\nKevin Chetty\n', '\nKarl Woodbridge\n']",,,http://arxiv.org/abs/1611.01801v1,cs.OH,['cs.OH'],,,[]
GSM based CommSense system to measure and estimate environmental changes,http://arxiv.org/abs/1611.02659v2,2016-11-08T19:18:38Z,2017-05-08T13:59:01Z,"  Facilitating the coexistence of radar systems with communication systems has
been a major area of research in radar engineering. The current work presents a
new way to sense the environment using the channel equalization block of
existing communication systems. We have named this system CommSense. In the
current paper we demonstrate the feasibility of the system using Global System
for Mobile Communications (GSM) signals. The implementation has been done using
open-source Software Defined Radio (SDR) environment. In the preliminary
results obtained in our work we show that it is possible to distinguish
environmental changes using the proposed system. The major advantage of the
system is that it is inexpensive as channel estimation is an inherent block in
any communication system and hence the added cost to make it work as an
environment sensor is minimal. The major challenge, on which we are continuing
our work, is how to characterize the features in the environmental changes.
This is an acute challenge given the fact that the bandwidth available is
narrow and the system is inherently a forward looking radar. However the
initial results, as shown in this paper, are encouraging and we intend to use
an application specific instrumentation (ASIN) scheme to distinguish the
environmental changes.
","['\nAbhishek Bhatta\n', '\nAmit Kumar Mishra\n']",,"IEEE Aerospace and Electronic Systems Magazine Volume 32 Issue 2
  February 2017",http://dx.doi.org/10.1109/MAES.2017.150272,cs.OH,['cs.OH'],10.1109/MAES.2017.150272,,[]
"Application Specific Instrumentation (ASIN): A Bio-inspired Paradigm to
  Instrumentation using recognition before detection",http://arxiv.org/abs/1611.00228v1,2016-10-31T10:58:12Z,2016-10-31T10:58:12Z,"  In this paper we present a new scheme for instrumentation, which has been
inspired by the way small mammals sense their environment. We call this scheme
Application Specific Instrumentation (ASIN). A conventional instrumentation
system focuses on gathering as much information about the scene as possible.
This, usually, is a generic system whose data can be used by another system to
take a specific action. ASIN fuses these two steps into one. The major merit of
the proposed scheme is that it uses low resolution sensors and much less
computational overhead to give good performance for a highly specialised
application
",['\nAmit Kumar Mishra\n'],,,http://arxiv.org/abs/1611.00228v1,cs.OH,"['cs.OH', 'cs.LG']",,,[]
Perspectives and Networks,http://arxiv.org/abs/1610.08765v1,2016-10-26T13:06:42Z,2016-10-26T13:06:42Z,"  The perspective we take on a system determines the features and properties of
this system that we are focusing on. It determines where we search for causes
to explain the effects on the system that we observe. It determines the terms
in which we expect the information about the system to be expressed. And it can
also influence the choice of formalism that will be used to convey the
information. using Boolean Automata Networks as prototypes of interaction
systems, this paper means to start making these considerations concrete in
order to draw a practical benefit out of them.
",['\nMathilde Noual\n'],,,http://arxiv.org/abs/1610.08765v1,cs.OH,['cs.OH'],,,[]
Causality and Networks,http://arxiv.org/abs/1610.08766v1,2016-10-26T13:09:12Z,2016-10-26T13:09:12Z,"  Causality is omnipresent in scientists' verbalisations of their
understanding, even though we have no formal consensual scientific definition
for it. In Automata Networks, it suffices to say that automata ""influence"" one
another to introduce a notion of causality. One might argue that this merely is
an incidental side effect of preferring statements expressed in natural
languages to mathematical formulae. The discussion of this paper shows that if
this is the case, then it is worth considering the effects of those preferences
on the contents of the statements we make and the formulae we derive. And if it
is not the case, that causality is a mere incidental side effect of our
preferences of formulation, then causality must be worth some scientific
attention per se. In any case, the paper illustrates how the innate sense of
causality we have may be made deliberate and formal use of without having to
pin down the elusive notion of causality to anything fixed and formal that
wouldn't do justice to the wide range of ways it is involved in science-making.
",['\nMathilde Noual\n'],,,http://arxiv.org/abs/1610.08766v1,cs.OH,['cs.OH'],,,[]
Novel Grid Topology Estimation Technique Exploiting PLC Modems,http://arxiv.org/abs/1610.09267v2,2016-10-27T07:01:30Z,2016-11-10T14:03:08Z,"  A fundamental requirement to develop routing strategies in power line
networks is the knowledge of the network topology, which might not be complete.
In this work, we present a novel method to derive the topology of a
distribution network that exploits the capability of Power Line Communication
modems to measure the network admittance, and we report the most significant
results.
","['\nFederico Passerini\n', '\nAndrea M. Tonello\n']","A version of this paper has been presented at the 10th Workshop on
  Power Line Communications, Paris 2016",,http://arxiv.org/abs/1610.09267v2,cs.OH,['cs.OH'],,,[]
"On the optimality of ternary arithmetic for compactness and hardware
  design",http://arxiv.org/abs/1611.03715v1,2016-10-25T13:57:13Z,2016-10-25T13:57:13Z,"  In this paper, the optimality of ternary arithmetic is investigated under
strict mathematical formulation. The arithmetic systems are presented in
generic form, as the means to encode numeric values, and the choice of radix is
asserted as the main parameter to assess the efficiency of the representation,
in terms of information compactness and estimated implementation cost in
hardware. Using proper formulations for the optimization task, the universal
constant 'e' (base of natural logarithms) is proven as the most efficient radix
and ternary is asserted as the closest integer choice.
",['\nHarris V. Georgiou\n'],"10 pages, 3 figures",,http://arxiv.org/abs/1611.03715v1,cs.OH,['cs.OH'],,,[]
Finite Computational Structures and Implementations,http://arxiv.org/abs/1610.05849v1,2016-10-19T02:25:50Z,2016-10-19T02:25:50Z,"  What is computable with limited resources? How can we verify the correctness
of computations? How to measure computational power with precision? Despite the
immense scientific and engineering progress in computing, we still have only
partial answers to these questions. In order to make these problems more
precise, we describe an abstract algebraic definition of classical computation,
generalizing traditional models to semigroups. The mathematical abstraction
also allows the investigation of different computing paradigms (e.g. cellular
automata, reversible computing) in the same framework. Here we summarize the
main questions and recent results of the research of finite computation.
",['\nAttila Egri-Nagy\n'],"12 pages, 3 figures, will be presented at CANDAR'16 and final version
  published by IEEE Computer Society",,http://arxiv.org/abs/1610.05849v1,cs.OH,"['cs.OH', 'math.GR', '20M20, 20M35, 68Q70, 68Q05', 'F.1.1; F.4.0']",,,[]
"Doing Moore with Less -- Leapfrogging Moore's Law with Inexactness for
  Supercomputing",http://arxiv.org/abs/1610.02606v2,2016-10-09T01:00:48Z,2016-10-12T15:50:35Z,"  Energy and power consumption are major limitations to continued scaling of
computing systems. Inexactness, where the quality of the solution can be traded
for energy savings, has been proposed as an approach to overcoming those
limitations. In the past, however, inexactness necessitated the need for highly
customized or specialized hardware. The current evolution of commercial
off-the-shelf(COTS) processors facilitates the use of lower-precision
arithmetic in ways that reduce energy consumption. We study these new
opportunities in this paper, using the example of an inexact Newton algorithm
for solving nonlinear equations. Moreover, we have begun developing a set of
techniques we call reinvestment that, paradoxically, use reduced precision to
improve the quality of the computed result: They do so by reinvesting the
energy saved by reduced precision.
","['\nSven Leyffer\nMathematics and Computer Science Division/Argonne National Laboratory\n', '\nStefan M. Wild\nMathematics and Computer Science Division/Argonne National Laboratory\n', '\nMike Fagan\nDepartment of Computer Science/Rice University\n', '\nMarc Snir\nDepartment of Computer Science/University of Illinois at Urbana-Champaign\n', '\nKrishna Palem\nDepartment of Computer Science/Rice University\n', '\nKazutomo Yoshii\nMathematics and Computer Science Division/Argonne National Laboratory\n', '\nHal Finkel\nArgonne Leadership Computing Facility\n']","9 pages, 12 figures, PDFLaTeX. 12 Oct 2016: Corrected author Hal
  Finkel's affiliation to show ALCF/Argonne",,http://arxiv.org/abs/1610.02606v2,cs.OH,"['cs.OH', 'F.2.1; G.1.5']",,,"['Mathematics and Computer Science Division/Argonne National Laboratory', 'Mathematics and Computer Science Division/Argonne National Laboratory', 'Department of Computer Science/Rice University', 'Department of Computer Science/University of Illinois at Urbana-Champaign', 'Department of Computer Science/Rice University', 'Mathematics and Computer Science Division/Argonne National Laboratory', 'Argonne Leadership Computing Facility']"
"Highly Robust Clustering of GPS Driver Data for Energy Efficient Driving
  Style Modelling",http://arxiv.org/abs/1610.02815v1,2016-10-10T09:16:02Z,2016-10-10T09:16:02Z,"  This paper presents a novel approach to distinguish driving styles with
respect to their energy efficiency. A distinct property of our method is that
it relies exclusively on Global Positioning System (GPS) logs of drivers. This
setting is highly relevant in practice as these data can easily be acquired.
  Relying on positional data alone means that all derived features will be
correlated, so we strive to find a single quantity that allows us to perform
the driving style analysis. To this end we consider a robust variation of the
so called jerk of a movement. We show that our feature choice outperforms other
more commonly used jerk-based formulations and we discuss the handling of
noisy, inconsistent, and incomplete data as this is a notorious problem when
dealing with real-world GPS logs.
  Our solving strategy relies on an agglomerative hierarchical clustering
combined with an L-term heuristic to determine the relevant number of clusters.
It can easily be implemented and performs fast, even on very large, real-world
data sets. Experiments show that our approach is robust against noise and able
to discern different driving styles.
","['\nMichael Breuß\n', '\nLaurent Hoeltgen\n', '\nAli Sharifi Boroujerdi\n', '\nAshkan Mansouri Yarahmadi\n']",,,http://arxiv.org/abs/1610.02815v1,cs.OH,"['cs.OH', '62H30, 62P30, 68U99']",,,[]
"SWoTSuite: A Development Framework for Prototyping Cross-domain Semantic
  Web of Things Applications",http://arxiv.org/abs/1609.09014v1,2016-09-28T17:32:06Z,2016-09-28T17:32:06Z,"  Semantic Web of Things (SWoT) applications focus on providing a wide-scale
interoperability that allows the sharing of IoT devices across domains and the
reusing of available knowledge on the web. However, the application development
is difficult because developers have to do various tasks such as designing an
application, annotating IoT data, interpreting data, and combining application
domains.
  To address the above challenges, this paper demonstrates SWoTSuite, a toolkit
for prototyping SWoT applications. It hides the use of semantic web
technologies as much as possible to avoid the burden of designing SWoT
applications that involves designing ontologies, annotating sensor data, and
using reasoning mechanisms to enrich data. Taking inspiration from sharing and
reuse approaches, SWoTSuite reuses data and vocabularies. It leverages existing
technologies to build applications. We take a hello world naturopathy
application as an example and demonstrate an application development process
using SWoTSuite. The demo video is available at URL:
http://tinyurl.com/zs9flrt.
","['\nPankesh Patel\n', '\nAmelie Gyrard\n', '\nDhavalkumar Thakker\n', '\nAmit Sheth\n', '\nMartin Serrano\n']",8 pages,,http://arxiv.org/abs/1609.09014v1,cs.OH,['cs.OH'],,,[]
Non-Intrusive Load Monitoring: A Review and Outlook,http://arxiv.org/abs/1610.01191v1,2016-10-04T20:37:04Z,2016-10-04T20:37:04Z,"  With the roll-out of smart meters the importance of effective non-intrusive
load monitoring (NILM) techniques has risen rapidly. NILM estimates the power
consumption of individual devices given their aggregate consumption. In this
way, the combined consumption must only be monitored at a single, central point
in the household, providing various advantages such as reduced cost for
metering equipment. In this paper we discuss the fundamental building-blocks of
NILM, first giving a taxonomy of appliance models and device signatures and
then explaining common supervised and unsupervised learning methods.
Furthermore, we outline a fundamental algorithm that tackles the task of NILM.
Subsequently, this paper reviews recent research that has brought novel insight
to the field and more effective techniques. Finally, we formulate future
challenges in the domain of NILM and smart meters.
","['\nChristoph Klemenjak\n', '\nPeter Goldsborough\n']","A shorter version of this paper was presented at the SKILL Students
  Conference 2016, part of the INFORMATIK2016 congress",,http://arxiv.org/abs/1610.01191v1,cs.OH,['cs.OH'],,,[]
"CONE: Zero-Calibration Accurate Confidence Estimation for Indoor
  Localization Systems",http://arxiv.org/abs/1610.02274v1,2016-10-06T06:02:05Z,2016-10-06T06:02:05Z,"  Accurate estimation of the confidence of an indoor localization system is
crucial for a number of applications including crowd-sensing applications,
map-matching services, and probabilistic location fusion techniques; all of
which lead to an enhanced user experience. Current approaches for quantifying
the output accuracy of a localization system in real-time either do not provide
a distance metric, require an extensive training process, and/or are tailored
to a specific localization system. In this paper, we present the design,
implementation, and evaluation of CONE: a novel calibration-free accurate
confidence estimation system that can work in real-time with any location
determination system. CONE builds on a sound theoretical model that allows it
to trade the required user confidence with tight bound on the estimated
confidence radius. We also introduce a new metric for evaluating confidence
estimation systems that can capture new aspects of their performance.
Evaluation of CONE on Android phones in a typical testbed using the iBeacons
BLE technology with a side-by-side comparison with traditional confidence
estimation techniques shows that CONE can achieve a consistent median absolute
error difference accuracy of less than 2.7m while estimating the user position
more than 80% of the time within the confidence circle. This is significantly
better than the state-of-the-art confidence estimation systems that are
tailored to the specific localization system in use. Moreover, CONE does not
require any calibration and therefore provides a scalable and ubiquitous
confidence estimation system for pervasive applications.
","['\nRizanne Elbakly\n', '\nMoustafa Youssef\n']","Accepted for publication in IPIN 2016 - International Conference on
  Indoor Positioning and Indoor Navigation 2016 6 pages, 9 figures",,http://arxiv.org/abs/1610.02274v1,cs.OH,['cs.OH'],,,[]
"An Efficient Framework for Floor-plan Prediction of Dynamic Runtime
  Reconfigurable Systems",http://arxiv.org/abs/1611.05438v1,2016-09-29T21:48:29Z,2016-09-29T21:48:29Z,"  Several embedded application domains for reconfigurable systems tend to
combine frequent changes with high performance demands of their workloads such
as image processing, wearable computing and network processors. Time
multiplexing of reconfigurable hardware resources raises a number of new
issues, ranging from run-time systems to complex programming models that
usually form a Reconfigurable hardware Operating System (ROS). The Operating
System performs online task scheduling and handles resource management. There
are many challenges in adaptive computing and dynamic reconfigurable systems.
One of the major understudied challenges is estimating the required resources
in terms of soft cores, Programmable Reconfigurable Regions (PRRs), the
appropriate communication infrastructure, and to predict a near optimal layout
and floorplan of the reconfigurable logic fabric. Some of these issues are
specific to the application being designed, while others are more general and
relate to the underlying run-time environment. Static resource allocation for
Run- Time Reconfiguration (RTR) often leads to inferior and unacceptable
results. In this paper, we present a novel adaptive and dynamic methodology,
based on a Machine Learning approach, for predicting and estimating the
necessary resources for an application based on past historical information. An
important feature of the proposed methodology is that the system is able to
learn and generalize and, therefore, is expected to improve its accuracy over
time. The goal of the entire process is to extract useful hidden knowledge from
the data. This knowledge is the prediction and estimation of the necessary
resources for an unknown or not previously seen application.
","['\nA. Al-Wattar\n', '\nS. Areibi\n', '\nG. Grewal\n']","23 pages, 12 figures","International Journal of Reconfigurable and Embedded Systems
  (IJRES) Vol. 4, No. 2, July 2015, pp. 99~121",http://arxiv.org/abs/1611.05438v1,cs.OH,"['cs.OH', 'cs.AR']",,,[]
Organized Complexity: is Big History a Big Computation?,http://arxiv.org/abs/1609.07111v2,2016-09-16T09:30:40Z,2018-05-15T12:54:26Z,"  The concept of ""logical depth"" introduced by Charles H. Bennett (1988) seems
to capture, at least partially, the notion of organized complexity, so central
in big history. More precisely, the increase in organized complexity refers
here to the wealth, variety and intricacy of structures, and should not be
confused with the increase of random complexity, formalized by Kolmogorov
(1965). If Bennett is right in proposing to assimilate organized complexity
with ""computational content"", then the fundamental cause of the increase of
complexity in the universe is the existence of computing mechanisms with
memory, and able to cumulatively create and preserve computational contents. In
this view, the universe computes, remembers its calculations, and reuses them
to conduct further computations. Evolutionary mechanisms are such forms of
cumulative computation with memory and we owe them the organized complexity of
life. Language, writing, culture, science and technology can also be analyzed
as computation mechanisms generating, preserving and accelerating the increase
in organized complexity. The main unifying theme for big history is the energy
rate density, a metric based on thermodynamics. However useful, this metric
does not provide much insight into the role that information and computation
play in our universe. The concept of ""logical depth"" provides a new lens to
examine the increase of organized complexity. We argue in this paper that
organized complexity is a valid and useful way to make sense of big history.
Additionally, logical depth has a rigorous formal definition in theoretical
computer science that hints at a broader research program to quantify
complexity in the universe.
  Keywords: organized complexity, Kolmogorov complexity, logical depth, big
history, cosmic evolution, evolution, complexity, complexification,
computation, artificial life, philosophy of information
","['\nJean-Paul Delahaye\n', '\nClement Vidal\n']","10 pages, Published in American Philosophical Association Newsletter
  on Philosophy and Computers 17, (2) pages 49 to 54",,http://arxiv.org/abs/1609.07111v2,cs.OH,['cs.OH'],,,[]
"The curse of variety in computing, and what can be done about it",http://arxiv.org/abs/1609.08517v1,2016-09-25T16:57:46Z,2016-09-25T16:57:46Z,"  Excess freedom in how computers are used creates problems that include: bit
rot, problems with big data, problems in the creation and debugging of
software, and problems with cyber security. To tame excess freedom, ""tough
love"" is needed in the form of a {\em universal framework for the
representation and processing of diverse kinds of knowledge} (UFK). The ""SP
machine"", based on the ""SP theory of intelligence"", has the potential to
provide that framework and to help solve the problems above. There is potential
to reduce the near-4000 different kinds of computer file to one, and to reduce
the hundreds of different computer languages to one.
",['\nJ Gerard Wolff\n'],,,http://arxiv.org/abs/1609.08517v1,cs.OH,['cs.OH'],,,[]
DELTA: Data Extraction and Logging Tool for Android,http://arxiv.org/abs/1609.02769v1,2016-09-09T12:47:02Z,2016-09-09T12:47:02Z,"  In the past few years, the use of smartphones has increased exponentially,
and so have the capabilities of such devices. Together with an increase in raw
processing power, modern smartphones are equipped with a wide variety of
sensors and expose an extensive set of API (Accessible Programming Interface).
These capabilities allow us to extract a wide spectrum of data that ranges from
information about the environment (e.g., position, orientation) to user habits
(e.g., which apps she uses and when), as well as about the status of the
operating system itself (e.g., memory, network adapters). This data can be
extremely valuable in many research fields such as user authentication,
intrusion detection and detection of information leaks. For these reasons,
researchers need to use a solid and reliable logging tool to collect data from
mobile devices.
  In this paper, we first survey the existing logging tools available on the
Android platform, comparing the features offered by different tools and their
impact on the system, and highlighting some of their shortcomings. Then, we
present DELTA - Data Extraction and Logging Tool for Android, which improves
the existing Android logging solutions in terms of flexibility, fine-grained
tuning capabilities, extensibility, and available set of logging features. We
performed a full implementation of DELTA and we run a thorough evaluation on
its performance. The results show that our tool has low impact on the
performance of the system, on battery consumption, and on user experience.
Finally, we make the DELTA source code and toolset available to the research
community.
","['\nMauro Conti\n', '\nElia Dal Santo\n', '\nRiccardo Spolaor\n']","11 pages, 7 figures",,http://arxiv.org/abs/1609.02769v1,cs.OH,['cs.OH'],,,[]
Knowledge management and measurement in Public Sector Organizations,http://arxiv.org/abs/1609.02995v1,2016-09-10T02:30:25Z,2016-09-10T02:30:25Z,"  Knowledge Management (KM) is a strategic component that enables development,
growth and continuous improvement of Public Sector Organizations (PSO). This
thesis is bounded to this specific context. Indeed, we critically and
comprehensively study the factors that characterize KM strategies and those
that foster its development and success in PSO, then finally we propose metrics
to measure and evaluate, in order to continuous and systematically improve KM
practices in PSO. Main problems are related to the lack of academic literature
that explains the elements that KM address in the given context. In addition,
it is identified as a problem the lack of criteria for measuring and evaluating
KM in PSO, from a different viewpoint than business perspective. The
contribution of this thesis is that it provides valuable elements for an
academic debate on the previous factors, strategies and metrics to promote KM
initiatives in PSO. To achieve the research objective of this thesis we
performed a comprehensive systematic literature review in order to discover
what the KM critical success factors are, and also we integrated and proposed
some metrics to evaluate and assess the performance of KM in PSO. We conducted
an in-depth study among different Public Sector Organizations in order to learn
what are the critical success factors that must be first considered before
implement KM initiatives. based on this. Finally, based on this methodological
proposal and as a result of this research, we were able to analyze and explain
the critical success factors, we identified some strategies to encourage the
success of KM and integrated a proposal of metrics, from different approaches,
for KM in PSO.
",['\nHector Perez Lopez-Portillo\n'],"Master Thesis, 124 pages, Spanish",,http://arxiv.org/abs/1609.02995v1,cs.OH,['cs.OH'],,,[]
Toward an Algebraic Theory of Systems,http://arxiv.org/abs/1609.04293v2,2016-09-13T19:43:37Z,2018-09-22T05:46:31Z,"  We propose the concept of a system algebra with a parallel composition
operation and an interface connection operation, and formalize
composition-order invariance, which postulates that the order of composing and
connecting systems is irrelevant, a generalized form of associativity.
Composition-order invariance explicitly captures a common property that is
implicit in any context where one can draw a figure (hiding the drawing order)
of several connected systems, which appears in many scientific contexts. This
abstract algebra captures settings where one is interested in the behavior of a
composed system in an environment and wants to abstract away anything internal
not relevant for the behavior. This may include physical systems, electronic
circuits, or interacting distributed systems.
  One specific such setting, of special interest in computer science, are
functional system algebras, which capture, in the most general sense, any type
of system that takes inputs and produces outputs depending on the inputs, and
where the output of a system can be the input to another system. The behavior
of such a system is uniquely determined by the function mapping inputs to
outputs. We consider several instantiations of this very general concept. In
particular, we show that Kahn networks form a functional system algebra and
prove their composition-order invariance.
  Moreover, we define a functional system algebra of causal systems,
characterized by the property that inputs can only influence future outputs,
where an abstract partial order relation captures the notion of ""later"". This
system algebra is also shown to be composition-order invariant and appropriate
instantiations thereof allow to model and analyze systems that depend on time.
","['\nChristian Matt\n', '\nUeli Maurer\n', '\nChristopher Portmann\n', '\nRenato Renner\n', '\nBjörn Tackmann\n']",,"Theoretical Computer Science, vol. 747, pp. 1-25, Nov. 2018",http://dx.doi.org/10.1016/j.tcs.2018.06.001,cs.OH,['cs.OH'],10.1016/j.tcs.2018.06.001,,[]
Experimental Characterization of In Vivo Wireless Communication Channels,http://arxiv.org/abs/1610.01516v1,2016-09-09T22:58:22Z,2016-09-09T22:58:22Z,"  In vivo wireless medical devices have a critical role in healthcare
technologies due to their continuous health monitoring and noninvasive surgery
capabilities. In order to fully exploit the potential of such devices, it is
necessary to characterize the in vivo wireless communication channel which will
help to build reliable and high-performance communication systems. This paper
presents preliminary results of experimental characterization for this
fascinating communications medium on a human cadaver and compares the results
with numerical studies.
","['\nA. Fatih Demir\n', '\nQammer H. Abbasi\n', '\nZ. Esat Ankarali\n', '\nMarwa Qaraqe\n', '\nErchin Serpedin\n', '\nHuseyin Arslan\n']",2015 IEEE 82nd Vehicular Technology Conference: VTC2015-Fall,"VTC Fall, 2015 IEEE 82nd, Boston, MA, 2015, pp. 1-2",http://dx.doi.org/10.1109/VTCFall.2015.7390942,cs.OH,['cs.OH'],10.1109/VTCFall.2015.7390942,,[]
Energy Transparency for Deeply Embedded Programs,http://arxiv.org/abs/1609.02193v3,2016-08-25T20:55:51Z,2017-05-25T12:08:40Z,"  Energy transparency is a concept that makes a program's energy consumption
visible, from hardware up to software, through the different system layers.
Such transparency can enable energy optimizations at each layer and between
layers, and help both programmers and operating systems make energy-aware
decisions. In this paper, we focus on deeply embedded devices, typically used
for Internet of Things (IoT) applications, and demonstrate how to enable energy
transparency through existing Static Resource Analysis (SRA) techniques and a
new target-agnostic profiling technique, without hardware energy measurements.
Our novel mapping technique enables software energy consumption estimations at
a higher level than the Instruction Set Architecture (ISA), namely the LLVM
Intermediate Representation (IR) level, and therefore introduces energy
transparency directly to the LLVM optimizer. We apply our energy estimation
techniques to a comprehensive set of benchmarks, including single- and also
multi-threaded embedded programs from two commonly used concurrency patterns,
task farms and pipelines. Using SRA, our LLVM IR results demonstrate a high
accuracy with a deviation in the range of 1% from the ISA SRA. Our profiling
technique captures the actual energy consumption at the LLVM IR level with an
average error of 3%.
","['\nKyriakos Georgiou\n', '\nSteve Kerrison\n', '\nZbigniew Chamski\n', '\nKerstin Eder\n']","33 pages, 7 figures. arXiv admin note: substantial text overlap with
  arXiv:1510.07095","ACM Trans. Archit. Code Optim. 14, 1, Article 8 (March 2017), 26
  pages",http://dx.doi.org/10.1145/3046679,cs.OH,"['cs.OH', 'D.2.8']",10.1145/3046679,,[]
A Generalization of the Directed Graph Layering Problem,http://arxiv.org/abs/1608.07809v1,2016-08-28T13:38:55Z,2016-08-28T13:38:55Z,"  The Directed Layering Problem (DLP) solves a step of the widely used
layer-based approach to automatically draw directed acyclic graphs. To cater
for cyclic graphs, usually a preprocessing step is used that solves the
Feedback Arc Set Problem (FASP) to make the graph acyclic before a layering is
determined. Here we present the Generalized Layering Problem (GLP), which
solves the combination of DLP and FASP simultaneously, allowing general graphs
as input. We present an integer programming model and a heuristic to solve the
NP-complete GLP and perform thorough evaluations on different sets of graphs
and with different implementations for the steps of the layer-based approach.
We observe that GLP reduces the number of dummy nodes significantly, can
produce more compact drawings, and improves on graphs where DLP yields poor
aspect ratios.
","['\nUlf Rüegg\n', '\nThorsten Ehlers\n', '\nMiro Spönemann\n', '\nReinhard von Hanxleden\n']","Appears in the Proceedings of the 24th International Symposium on
  Graph Drawing and Network Visualization (GD 2016)",,http://arxiv.org/abs/1608.07809v1,cs.OH,['cs.OH'],,,[]
"Overview of Spintronic Sensors, Internet of Things, and Smart Living",http://arxiv.org/abs/1611.00317v1,2016-08-29T09:16:54Z,2016-08-29T09:16:54Z,"  Smart living is a trending lifestyle that envisions lower energy consumption,
sound public services, and better quality of life for human being. The Internet
of Things (IoT) is a compelling platform connecting various sensors around us
to the Internet, providing great opportunities for the realization of smart
living. Spintronic sensors with superb measuring ability and multiple unique
advantages can be an important piece of cornerstone for IoT. In this review, we
discuss successful applications of spintronic sensors in electrical current
sensing, transmission and distribution lines monitoring, vehicle detection, and
biodetection. Traditional monitoring systems with limited sensors and wired
communication can merely collect fragmented data in the application domains. In
this paper, the wireless spintronic sensor networks (WSSNs) will be proposed
and illustrated to provide pervasive monitoring systems, which facilitate the
intelligent surveillance and management over building, power grid, transport,
and healthcare. The database of collected information will be of great use to
the policy making in public services and city planning. This work provides
insights for realizing smart living through the integration of IoT with
spintronic sensor technology.
","['\nX. Liu\n', '\nK. H. Lam\n', '\nK. Zhu\n', '\nC. Zheng\n', '\nX. Li\n', '\nY. Du\n', '\nChunhua Liu\n', '\nP. W. T. Pong\n']",,,http://arxiv.org/abs/1611.00317v1,cs.OH,['cs.OH'],,,[]
"Online Charging Scheduling Algorithms of Electric Vehicles in Smart
  Grid: An Overview",http://arxiv.org/abs/1609.02791v1,2016-08-26T04:12:23Z,2016-08-26T04:12:23Z,"  As an environment-friendly substitute for conventional fuel-powered vehicles,
electric vehicles (EVs) and their components have been widely developed and
deployed worldwide. The large-scale integration of EVs into power grid brings
both challenges and opportunities to the system performance. On one hand, the
load demand from EV charging imposes large impact on the stability and
efficiency of power grid. On the other hand, EVs could potentially act as
mobile energy storage systems to improve the power network performance, such as
load flattening, fast frequency control, and facilitating renewable energy
integration. Evidently, uncontrolled EV charging could lead to inefficient
power network operation or even security issues. This spurs enormous research
interests in designing charging coordination mechanisms. A key design challenge
here lies in the lack of complete knowledge of events that occur in the future.
Indeed, the amount of knowledge of future events significantly impacts the
design of efficient charging control algorithms. This article focuses on
introducing online EV charging scheduling techniques that deal with different
degrees of uncertainty and randomness of future knowledge. Besides, we
highlight the promising future research directions for EV charging control.
","['\nWanrong Tang\nAngela\n', '\nSuzhi Bi\nAngela\n', '\nYing Jun\nAngela\n', '\n Zhang\n']","18 pages, 5 figures, 1 table; This article has been accepted for
  publication by IEEE Communication Magazine, 2016",,http://arxiv.org/abs/1609.02791v1,cs.OH,"['cs.OH', 'cs.SY']",,,"['Angela', 'Angela', 'Angela']"
"On the Origin of Samples: Attribution of Output to a Particular
  Algorithm",http://arxiv.org/abs/1608.06172v1,2016-08-18T19:05:25Z,2016-08-18T19:05:25Z,"  With unprecedented advances in genetic engineering we are starting to see
progressively more original examples of synthetic life. As such organisms
become more common it is desirable to be able to distinguish between natural
and artificial life forms. In this paper, we present this challenge as a
generalized version of Darwin's original problem, which he so brilliantly
addressed in On the Origin of Species. After formalizing the problem of
determining origin of samples we demonstrate that the problem is in fact
unsolvable, in the general case, if computational resources of considered
originator algorithms have not been limited and priors for such algorithms are
known to be equal. Our results should be of interest to astrobiologists and
scientists interested in producing a more complete theory of life, as well as
to AI-Safety researchers.
",['\nRoman V. Yampolskiy\n'],,Phys. Scr. 92 (2017),http://dx.doi.org/10.1088/0031-8949/92/1/013002,cs.OH,['cs.OH'],10.1088/0031-8949/92/1/013002,,[]
"Virtual Micromagnetics: A Framework for Accessible and Reproducible
  Micromagnetic Simulation",http://arxiv.org/abs/1609.05135v2,2016-08-11T10:59:40Z,2016-11-25T10:15:16Z,"  Computational micromagnetics requires numerical solution of partial
differential equations to resolve complex interactions in magnetic
nanomaterials. The Virtual Micromagnetics project described here provides
virtual machine simulation environments to run open-source micromagnetic
simulation packages. These environments allow easy access to simulation
packages that are often difficult to compile and install, and enable
simulations and their data to be shared and stored in a single virtual hard
disk file, which encourages reproducible research. Virtual Micromagnetics can
be extended to automate the installation of micromagnetic simulation packages
on non-virtual machines, and to support closed-source and new open-source
simulation packages, including packages from disciplines other than
micromagnetics, encouraging reuse. Virtual Micromagnetics is stored in a public
GitHub repository under a three-clause Berkeley Software Distribution (BSD)
license.
","['\nMark Vousden\n', '\nMarc-Antonio Bisotti\n', '\nMaximilian Albert\n', '\nHans Fangohr\n']","12 pages, 1 figure","Journal of Open Research Software, 4(1), p.e41 (2016)",http://dx.doi.org/10.5334/jors.141,cs.OH,['cs.OH'],10.5334/jors.141,,[]
RFID-Cloud Smart Cart System,http://arxiv.org/abs/1608.03724v1,2016-08-12T08:57:41Z,2016-08-12T08:57:41Z,"  The main purpose of this work is in reducing the queuing delays in major
supermarkets or other shopping centers by means of an Electronic Smart Cart
System which will introduce an intellectual approach to billing process through
RFID technology. Smart Cart System is a cooperative performance of three
separate systems: a website developed for the shopping market, electronic smart
cart device and anti-theft RFID gates. This project focuses on developing the
electronic smart cart device itself. It involves an embedded electronic
hardware that consists of an OLED display, Arduino Mega 2560 board, a
specifically designed PCB, a Wi-Fi module, 13.56 MHz HF RFID reader, a power
supply and a shopping cart.
","['\nYerlan Berdaliyev\n', '\nAlex Pappachen James\n']","to appear as a poster paper in International Conference on Advances
  in Computing, Communications and Informatics, 2016",,http://arxiv.org/abs/1608.03724v1,cs.OH,['cs.OH'],,,[]
Example Data Sets and Collections for BeSpaceD Explained,http://arxiv.org/abs/1608.00433v1,2016-08-01T14:02:26Z,2016-08-01T14:02:26Z,"  In this report, we present example data sets and collections for the BeSpaceD
platform. BeSpaceD is a spatio-temporal modelling and reasoning software
framework. We describe the content of a number of the data sets and how the
data was obtained. We also present the programming API in BeSpaceD used to
store and access these data sets so that future BeSpaceD users can utilise the
data collections in their own experiments with minimal effort and expand the
library of data collections for BeSpaceD.
","['\nKeith Foster\n', '\nJan Olaf Blech\n']",,,http://arxiv.org/abs/1608.00433v1,cs.OH,['cs.OH'],,,[]
"A Step towards Advanced Metering for the Smart Grid: A Survey of Energy
  Monitors",http://arxiv.org/abs/1607.07780v1,2016-07-26T16:23:30Z,2016-07-26T16:23:30Z,"  The smart grid initiative has encouraged utility companies worldwide to
rollout new and smarter versions of energy meters. Before an extensive rollout,
which is both labor-intensive and incurs high capital costs, consumers need to
be incentivized to reap the long-term benefits of smart grid. Off-the-shelf
energy monitors can provide consumers with an insight of such potential
benefits. Since energy monitors are owned by the consumer, the consumer has
greater control over data which significantly reduces privacy and data
confidentiality concerns. We evaluate several existing energy monitors using an
online technical survey and online product literature. For consumers, the use
of different off-the-shelf energy monitors can help demonstrate the potential
gains of smart grid. Our survey indicates a trend towards incorporation of
state-of-the-art capabilities, like appliance level monitoring through load
disaggregation in energy monitors, which can encourage effective consumer
participation. Multiple sensor types and ratings allow some monitors to operate
in various configurations and environments.
","['\nAnwar Ul Haq\n', '\nHans-Arno Jacobsen\n']",Technical Report,"Energies 2018, 11(1), 189",http://dx.doi.org/10.3390/en11010189,cs.OH,['cs.OH'],10.3390/en11010189,,[]
Generating Cycloidal Gears for 3D Printing,http://arxiv.org/abs/1607.03739v1,2016-07-12T08:56:46Z,2016-07-12T08:56:46Z,"  (Shortened version of abstract in article itself)
  This article describes an algorithm for producing, for any desired resolution
and any desired numbers of wheel and pinion teeth, polygonal approximations to
the shapes of a pair of cycloidal gears that mesh correctly. An Octave
implementation of the algorithm, mostly written in 2014, is included. The
Octave implementation contains a (crude, but evidently adequate, at least for
reasonable numbers of wheel and pinion teeth) solution of the problem of
iteratively finding the generating wheel angle corresponding to the tips of the
tooth addenda.
  However, this Octave implementation does not contain a good solution to the
problem of automatically determining the generating wheel angles required to
produce a polygon which approximates the curved addenda to a resolution
specified by the user. A proposed better solution to this problem, involving a
priority queue, is discussed.
",['\nSunny Daniels\n'],,,http://arxiv.org/abs/1607.03739v1,cs.OH,['cs.OH'],,,[]
8th European Conference on Python in Science (EuroSciPy 2015),http://arxiv.org/abs/1607.03971v1,2016-07-14T00:53:02Z,2016-07-14T00:53:02Z,"  The 8th edition of the European Conference on Python in Science, EuroSciPy
was held for the second time in the beautiful city of Cambridge, UK from
August, 26th to 29th, 2014. More than 200 participants, both from academia and
industry, attended the conference.
  As usual, the conference kicked off with two days of tutorials, divided into
an introductory and an advanced track. The introductory track, presented by
Joris Vankerschaver, Valerio Maggio Joris Van den Bossche, Stijn Van Hoey and
Nicolas Rougier, gave a quick but thorough overview of the SciPy stack, while
the experience track focused on different advanced topics. This second track
began with an introduction to Bokeh, by Bryan Van den Ven, followed by an image
processing tutorial with scikit-image by Emmanuelle Gouillart and Juan
Nunez-Iglesias. The afternoon continued with two tutorials on data analysis:
the first, intitulated ""How 'good' is your model, and how can you make it
better?"" (by Chih-Chun Chen, Dimitry Foures, Elena Chatzimichali, Giuseppe
Vettigli) focused on the challenges face while attempting model selections, and
the first day concluded with a statistics in python tutorial by Gael Varoquaux.
During the second day, the attendees tackled an in depth 4 hour tutorial on
Cython, presented by Stefan Behnel, and a crash course on ""Evidence-Based
Teaching: What We Know and How to Use It"", by Greg Wilson.
",['\nNelle Varoquaux\n'],euroscipy-proceedings2015-01,,http://arxiv.org/abs/1607.03971v1,cs.OH,['cs.OH'],,,[]
Characterizing Smartphone Power Management in the Wild,http://arxiv.org/abs/1607.06402v1,2016-07-21T17:32:33Z,2016-07-21T17:32:33Z,"  For better reliability and prolonged battery life, it is important that users
and vendors understand the quality of charging and the performance of
smartphone batteries. Considering the diverse set of devices and user behavior
it is a challenge. In this work, we analyze a large collection of battery
analytics dataset collected from 30K devices of 1.5K unique smartphone models.
We analyze their battery properties and state of charge while charging, and
reveal the characteristics of different components of their power management
systems: charging mechanisms, state of charge estimation techniques, and their
battery properties. We explore diverse charging behavior of devices and their
users.
","['\nMohammad A. Hoque\n', '\nSasu Tarkoma\n']","Proceedings of 7th International Workshop on Hot Topics in
  Planet-Scale Measurement, HotPlanet'16",,http://dx.doi.org/10.1145/2968219.2968295,cs.OH,['cs.OH'],10.1145/2968219.2968295,,[]
"Phase Noise Influence in Optical OFDM Systems employing RF Pilot Tone
  for Phase Noise Cancellation",http://arxiv.org/abs/1607.08791v1,2016-07-19T12:01:05Z,2016-07-19T12:01:05Z,"  For coherent and direct-detection Orthogonal Frequency Division Multiplexed
(OFDM) systems employing radio frequency (RF) pilot tone phase noise
cancellation the influence of laser phase noise is evaluated. Novel analytical
results for the common phase error and for the (modulation dependent) inter
carrier interference are evaluated based upon Gaussian statistics for the laser
phase noise. In the evaluation it is accounted for that the laser phase noise
is filtered in the correlation signal detection. Numerical results are
presented for OFDM systems with 4 and 16 PSK modulation, 200 OFDM bins and baud
rate of 1 GS/s. It is found that about 225 km transmission is feasible for the
coherent 4PSK-OFDM system over normal (G.652) fiber.
","['\nGunnar Jacobsen\n', '\nLeonid G. Kazovsky\n', '\nTianhua Xu\n', '\nSergei Popov\n', '\nJie Li\n', '\nYimo Zhang\n', '\nAri T. Friberg\n']",5 pages. arXiv admin note: text overlap with arXiv:1607.04899,"Journal of Optical Communications, Vol.32(2), 141-145, 2011",http://arxiv.org/abs/1607.08791v1,cs.OH,"['cs.OH', 'cs.IT', 'math.IT', 'physics.optics', '94A12', 'C.2.5']",,,[]
Want Drugs? Use Python,http://arxiv.org/abs/1607.00378v1,2016-07-01T19:02:36Z,2016-07-01T19:02:36Z,"  We describe how Python can be leveraged to streamline the curation, modelling
and dissemination of drug discovery data as well as the development of
innovative, freely available tools for the related scientific community. We
look at various examples, such as chemistry toolkits, machine-learning
applications and web frameworks and show how Python can glue it all together to
create efficient data science pipelines.
","['\nMichał Nowotka\n', '\nGeorge Papadatos\n', '\nMark Davies\n', '\nNathan Dedman\n', '\nAnne Hersey\n']",,,http://arxiv.org/abs/1607.00378v1,cs.OH,['cs.OH'],,,[]
Probabilistic Programming and PyMC3,http://arxiv.org/abs/1607.00379v1,2016-07-01T19:07:37Z,2016-07-01T19:07:37Z,"  In recent years sports analytics has gotten more and more popular. We propose
a model for Rugby data - in particular to model the 2014 Six Nations
tournament. We propose a Bayesian hierarchical model to estimate the
characteristics that bring a team to lose or win a game, and predict the score
of particular matches. This is intended to be a brief introduction to
Probabilistic Programming in Python and in particular the powerful library
called PyMC3.
",['\nPeadar Coyle\n'],,,http://arxiv.org/abs/1607.00379v1,cs.OH,['cs.OH'],,,[]
PyCells for an Open Semiconductor Industry,http://arxiv.org/abs/1607.00859v1,2016-07-01T19:13:10Z,2016-07-01T19:13:10Z,"  In the modern semiconductor industry, automatic generation of parameterized
and recurring layout structures plays an important role and should be present
as a feature in Electronic Design Automation (EDA)-tools. Currently these
layout generators are developed with a proprietary programming language and can
be used with a specific EDA-tool. Therefore, the semiconductor companies find
the development of the layout generators that can be used in all state of the
art EDA-tools which support OpenAccess database appealing. The goal of this
project is to develop computationally efficient layout generators with Python
(PyCells), for ams AG technologies, that possess all the features of
comprehensive layout generators.
","['\nSepideh Alassi\n', '\nBertram Winter\n']",,,http://arxiv.org/abs/1607.00859v1,cs.OH,['cs.OH'],,,[]
Accelerated Evaluation of Automated Vehicles in Car-Following Maneuvers,http://arxiv.org/abs/1607.02687v2,2016-07-10T03:29:35Z,2017-02-19T14:46:29Z,"  The safety of Automated Vehicles (AVs) must be assured before their release
and deployment. The current approach to evaluation relies primarily on (i)
testing AVs on public roads or (ii) track testing with scenarios defined in a
test matrix. These two methods have completely opposing drawbacks: the former,
while offering realistic scenarios, takes too much time to execute; the latter,
though it can be completed in a short amount of time, has no clear correlation
to safety benefits in the real world. To avoid the aforementioned problems, we
propose Accelerated Evaluation, focusing on the car-following scenario. The
stochastic human-controlled vehicle (HV) motions are modeled based on 1.3
million miles of naturalistic driving data collected by the University of
Michigan Safety Pilot Model Deployment Program. The statistics of the HV
behaviors are then modified to generate more intense interactions between HVs
and AVs to accelerate the evaluation procedure. The Importance Sampling theory
was used to ensure that the safety benefits of AVs are accurately assessed
under accelerated tests. Crash, injury and conflict rates for a simulated AV
are simulated to demonstrate the proposed approach. Results show that test
duration is reduced by a factor of 300 to 100,000 compared with the
non-accelerated (naturalistic) evaluation. In other words, the proposed
techniques have great potential for accelerating the AV evaluation process.
","['\nDing Zhao\n', '\nXianan Huang\n', '\nHuei Peng\n', '\nHenry Lam\n', '\nDavid J. LeBlanc\n']","11 pages, 11 figures",,http://arxiv.org/abs/1607.02687v2,cs.OH,['cs.OH'],,,[]
"Spectral Clustering for Optical Confirmation and Redshift Estimation of
  X-ray Selected Galaxy Cluster Candidates in the SDSS Stripe 82",http://arxiv.org/abs/1607.04193v1,2016-07-10T09:18:58Z,2016-07-10T09:18:58Z,"  We develop a galaxy cluster finding algorithm based on spectral clustering
technique to identify optical counterparts and estimate optical redshifts for
X-ray selected cluster candidates. As an application, we run our algorithm on a
sample of X-ray cluster candidates selected from the third XMM-Newton
serendipitous source catalog (3XMM-DR5) that are located in the Stripe 82 of
the Sloan Digital Sky Survey (SDSS). Our method works on galaxies described in
the color-magnitude feature space. We begin by examining 45 galaxy clusters
with published spectroscopic redshifts in the range of 0.1 to 0.8 with a median
of 0.36. As a result, we are able to identify their optical counterparts and
estimate their photometric redshifts, which have a typical accuracy of 0.025
and agree with the published ones. Then, we investigate another 40 X-ray
cluster candidates (from the same cluster survey) with no redshift information
in the literature and found that 12 candidates are considered as galaxy
clusters in the redshift range from 0.29 to 0.76 with a median of 0.57. These
systems are newly discovered clusters in X-rays and optical data. Among them 7
clusters have spectroscopic redshifts for at least one member galaxy.
","['\nEman Mahmoud\n', '\nAli Takey\n', '\nAmin Shoukry\n']","15 pages, 7 figures, 3 tables, 1 appendix, Accepted by Journal of
  ""Astronomy and Computing""",,http://dx.doi.org/10.1016/j.ascom.2016.07.001,astro-ph.GA,"['astro-ph.GA', 'cs.OH']",10.1016/j.ascom.2016.07.001,,[]
Some comments on the reliability of NOAA's Storm Events Database,http://arxiv.org/abs/1606.06973v2,2016-06-22T15:00:37Z,2016-06-23T13:01:53Z,"  Storms and other severe weather events can result in fatalities, injuries,
and property damage. Therefore, preventing such outcomes to the extent possible
is a key concern, and the scientific community faces an increasing demand for
regularly updated appraisals of evolving climate conditions and extreme
weather. NOAA's Storm Events Database is undoubtedly an invaluable resource to
the general public, to the professional, and to the researcher. Due to such
importance, the primary objective of this study was to explore this database
and get clues about its reliability. A complete investigation of the damage
estimates, injuries or fatalities figures is unfeasible due to the extension of
the database. However, an exploratory data analysis with the resources of the R
statistical data analysis language found that damage reports are missing in
more than half of the records, that part of the damage values are incorrect,
and that, despite all efforts of standardizations, non-standard event type
names are still finding their way into the database. These few results are
enough to demonstrate that the database suffers from incompleteness and
inconsistencies and should not be used without taking reservations and
appropriate precautions before advancing any inferences from the data.
",['\nRenato P. dos Santos\n'],"22 pages, 7 figures",,http://arxiv.org/abs/1606.06973v2,cs.OH,"['cs.OH', '86A10']",,,[]
Artificial Fun: Mapping Minds to the Space of Fun,http://arxiv.org/abs/1606.07092v1,2016-06-22T20:28:53Z,2016-06-22T20:28:53Z,"  Yampolskiy and others have shown that the space of possible minds is vast,
actually infinite (Yampolskiy, 2015). A question of interest is 'Which
activities can minds perform during their lifetime?' This question is very
broad, thus in this article restricted to 'Which non-boring activities can
minds perform?' The space of potential non-boring activities has been called by
Yudkowsky 'fun space' (Yudkowsky, 2009). This paper aims to discuss the
relation between various types of minds and the part of the fun space, which is
accessible for them.
","['\nSoenke Ziesche\n', '\nRoman V. Yampolskiy\n']",,,http://arxiv.org/abs/1606.07092v1,cs.OH,['cs.OH'],,,[]
Project Based Learning of Embedded Systems,http://arxiv.org/abs/1606.07498v2,2016-06-23T22:19:01Z,2017-01-19T22:23:21Z,"  Traditional teaching, usually based on lectures and tutorials fosters the
idea of instruction-driven learning model where students are passive listeners.
Besides this approach, Project Based Learning (PBL) as a different learning
paradigm is standing behind constructivism learning theory, where learning from
real-world situations is put on the first place. The purpose of this paper is
to present our approach in learning embedded systems at our University. It is
based on combination of traditional (face-to-face) learning and PBL. Our PBL
represents an interdisciplinary project based on wireless sensor monitoring of
real-world environment (greenhouse). The students use UML that was shown as an
excellent tool for developing such a projects. From the student perspective, we
found that this high level of interdisciplinary is very valuable from the point
of view of facing the students with real-life problems.
","['\nDanco Davcev\n', '\nBiljana Stojkoska\n', '\nSlobodan Kalajdziski\n', '\nKire Trivodaliev\n']",,"Proceedings of the 2nd WSEAS International Conference on CIRCUITS,
  SYSTEMS, SIGNAL and TELECOMMUNICATIONS (CISST'08) Acapulco, Mexico, January
  25-27, 2008. ISSN: 1790-5117, ISBN: 978-960-6766-34-3, pp.120-125",http://arxiv.org/abs/1606.07498v2,cs.OH,['cs.OH'],,,[]
On the optimality of grid cells,http://arxiv.org/abs/1606.04876v1,2016-06-15T17:36:44Z,2016-06-15T17:36:44Z,"  Grid cells, discovered more than a decade ago [5], are neurons in the brain
of mammals that fire when the animal is located near certain specific points in
its familiar terrain. Intriguingly, these points form, for a single cell, a
two-dimensional triangular grid, not unlike our Figure 3. Grid cells are widely
believed to be involved in path integration, that is, the maintenance of a
location state through the summation of small displacements. We provide
theoretical evidence for this assertion by showing that cells with grid-like
tuning curves are indeed well adapted for the path integration task. In
particular we prove that, in one dimension under Gaussian noise, the
sensitivity of measuring small displacements is maximized by a population of
neurons whose tuning curves are near-sinusoids -- that is to say, with peaks
forming a one-dimensional grid. We also show that effective computation of the
displacement is possible through a second population of cells whose sinusoid
tuning curves are in phase difference from the first. In two dimensions, under
additional assumptions it can be shown that measurement sensitivity is
optimized by the product of two sinusoids, again yielding a grid-like pattern.
We discuss the connection of our results to the triangular grid pattern
observed in animals.
",['\nChristos H. Papadimitriou\n'],,,http://arxiv.org/abs/1606.04876v1,q-bio.NC,"['q-bio.NC', 'cs.OH']",,,[]
"Mobile phone data for public health: towards data-sharing solutions that
  protect individual privacy and national security",http://arxiv.org/abs/1606.00864v1,2016-06-02T20:38:32Z,2016-06-02T20:38:32Z,"  We outline the constraints faced by operators when deciding to share
de-identified data with researchers or policy makers. We describe a
conservative approach that we have taken to harness the value of CDRs for
infectious disease epidemiology while ensuring that identification of
individuals is impossible. We believe this approach serves as a useful and
highly conservative model for productive partnerships between mobile operators,
researchers, and public health practitioners.
","['\nCaroline O. Buckee\n', '\nKenth Engø-Monsen\n']",,,http://arxiv.org/abs/1606.00864v1,cs.OH,['cs.OH'],,,[]
"Exploiting AIS Data for Intelligent Maritime Navigation: A Comprehensive
  Survey",http://arxiv.org/abs/1606.00981v1,2016-06-03T06:46:31Z,2016-06-03T06:46:31Z,"  The Automatic Identification System (AIS) tracks vessel movement by means of
electronic exchange of navigation data between vessels, with onboard
transceiver, terrestrial and/or satellite base stations. The gathered data
contains a wealth of information useful for maritime safety, security and
efficiency. This paper surveys AIS data sources and relevant aspects of
navigation in which such data is or could be exploited for safety of seafaring,
namely traffic anomaly detection, route estimation, collision prediction and
path planning.
","['\nEnmei Tu\n', '\nGuanghao Zhang\n', '\nLily Rachmawati\n', '\nEshan Rajabally\n', '\nGuang-Bin Huang\n']","24 pages, 7 figures, 3 tables",,http://arxiv.org/abs/1606.00981v1,cs.OH,['cs.OH'],,,[]
One-dimensional Cutting Stock Problem with Divisible Items,http://arxiv.org/abs/1606.01419v1,2016-06-04T22:01:25Z,2016-06-04T22:01:25Z,"  This paper considers the one-dimensional cutting stock problem with divisible
items, which is a new problem in the cutting stock literature. The problem
exists in steel industries. In the new problem, each item can be divided into
smaller pieces, then they can be recombined again by welding. The objective is
to minimize both the trim loss and the number of the welds. We present a
mathematical model and a dynamic programming based heuristic for the problem.
Furthermore, a software, which is based on the proposed heuristic algorithm, is
developed to use in MKA company, and its performance is analyzed by solving
real-life problems in the steel industry. The computational experiments show
the efficiency of the proposed algorithm.
","['\nDeniz Tanir\n', '\nOnur Ugurlu\n', '\nAsli Guler\n', '\nUrfat Nuriyev\n']","12 pages, 2 figures","TWMS Journal of Applied and Engineering Mathematics, 9(3), (2019).
  473-484",http://arxiv.org/abs/1606.01419v1,cs.OH,"['cs.OH', '90B99']",,,[]
"Leveraging ERP Implementation to Create Intellectual Capital: the Role
  of Organizational Learning Capability",http://arxiv.org/abs/1606.01431v1,2016-06-04T23:12:17Z,2016-06-04T23:12:17Z,"  The extent to which enterprise resource planning (ERP) systems deliver value
for organizations has been debated. In this study, we argue that the presence
of appropriate organizational resources is essential for capturing the
potential of ERP implementation. We investigate the relationship between ERP
implementation and two organizational resources, specifically, Intellectual
Capital (IC) and Organizational Learning Capability (OLC) to enrich the
understanding of the way the value of ERP implementations can be realized. A
sample of 226 manufacturing firms in Vietnam was surveyed to test the
theoretical model. Structural equation modelling with partial least square
method and two approaches for moderation analysis were used to analyze the
data. The results indicate that ERP implementation scope has a positive impact
on intellectual capital (IC). However, firms need to build a certain level of
OLC to utilize ERP implementation for the enhancement of IC.
","['\nQuang V Nguyen\n', '\nMary Tate\n', '\nPhilip Calvert\n', '\nBenoit Aubert\n']","ISBN# 978-0-646-95337-3 Presented at the Australasian Conference on
  Information Systems 2015 (arXiv:1605.01032)",,http://arxiv.org/abs/1606.01431v1,cs.OH,['cs.OH'],,,[]
"Undecidability and Irreducibility Conditions for Open-Ended Evolution
  and Emergence",http://arxiv.org/abs/1606.01810v4,2016-06-06T16:19:13Z,2016-12-27T20:01:37Z,"  Is undecidability a requirement for open-ended evolution (OEE)? Using methods
derived from algorithmic complexity theory, we propose robust computational
definitions of open-ended evolution and the adaptability of computable
dynamical systems. Within this framework, we show that decidability imposes
absolute limits to the stable growth of complexity in computable dynamical
systems. Conversely, systems that exhibit (strong) open-ended evolution must be
undecidable, establishing undecidability as a requirement for such systems.
Complexity is assessed in terms of three measures: sophistication, coarse
sophistication and busy beaver logical depth. These three complexity measures
assign low complexity values to random (incompressible) objects. As time grows,
the stated complexity measures allow for the existence of complex states during
the evolution of a computable dynamical system. We show, however, that finding
these states involves undecidable computations. We conjecture that for similar
complexity measures that assign low complexity values, decidability imposes
comparable limits to the stable growth of complexity, and that such behaviour
is necessary for non-trivial evolutionary systems. We show that the
undecidability of adapted states imposes novel and unpredictable behaviour on
the individuals or populations being modelled. Such behaviour is irreducible.
Finally, we offer an example of a system, first proposed by Chaitin, that
exhibits strong OEE.
","['\nSantiago Hernández-Orozco\n', '\nFrancisco Hernández-Quiroz\n', '\nHector Zenil\n']","Reduced version of this article was submitted and accepted for oral
  presentation at ALife XV (July 4-8, 2016, Cancun, Mexico)",,http://arxiv.org/abs/1606.01810v4,cs.OH,"['cs.OH', '92B20']",,,[]
Big Data Refinement,http://arxiv.org/abs/1606.02017v1,2016-06-07T04:09:00Z,2016-06-07T04:09:00Z,"  ""Big data"" has become a major area of research and associated funding, as
well as a focus of utopian thinking. In the still growing research community,
one of the favourite optimistic analogies for data processing is that of the
oil refinery, extracting the essence out of the raw data. Pessimists look for
their imagery to the other end of the petrol cycle, and talk about the ""data
exhausts"" of our society.
  Obviously, the refinement community knows how to do ""refining"". This paper
explores the extent to which notions of refinement and data in the formal
methods community relate to the core concepts in ""big data"". In particular, can
the data refinement paradigm can be used to explain aspects of big data
processing?
",['\nEerke A. Boiten\nUniversity of Kent\n'],"In Proceedings Refine'15, arXiv:1606.01344","EPTCS 209, 2016, pp. 17-23",http://dx.doi.org/10.4204/EPTCS.209.2,cs.OH,['cs.OH'],10.4204/EPTCS.209.2,,['University of Kent']
"Adaptive Quantization Matrices for HD and UHD Display Resolutions in
  Scalable HEVC",http://arxiv.org/abs/1606.02042v2,2016-06-07T07:13:59Z,2016-06-12T10:45:56Z,"  HEVC contains an option to enable custom quantization matrices, which are
designed based on the Human Visual System and a 2D Contrast Sensitivity
Function. Visual Display Units, capable of displaying video data at High
Definition and Ultra HD display resolutions, are frequently utilized on a
global scale. Video compression artifacts that are present due to high levels
of quantization, which are typically inconspicuous in low display resolution
environments, are clearly visible on HD and UHD video data and VDUs. The
default QM technique in HEVC does not take into account the video data
resolution, nor does it take into consideration the associated display
resolution of a VDU to determine the appropriate levels of quantization
required to reduce unwanted video compression artifacts. Based on this fact, we
propose a novel, adaptive quantization matrix technique for the HEVC standard,
including Scalable HEVC. Our technique, which is based on a refinement of the
current HVS-CSF QM approach in HEVC, takes into consideration the display
resolution of the target VDU for the purpose of minimizing video compression
artifacts. In SHVC SHM 9.0, and compared with anchors, the proposed technique
yields important quality and coding improvements for the Random Access
configuration, with a maximum of 56.5% luma BD-Rate reductions in the
enhancement layer. Furthermore, compared with the default QMs and the Sony QMs,
our method yields encoding time reductions of 0.75% and 1.19%, respectively.
","['\nLee Prangnell\n', '\nVictor Sanchez\n']",Data Compression Conference 2016,,http://arxiv.org/abs/1606.02042v2,cs.OH,['cs.OH'],,,[]
The bitwise operations in relation to obtaining Latin squares,http://arxiv.org/abs/1605.07171v2,2016-05-23T13:33:09Z,2016-07-08T05:54:39Z,"  The main thrust of the article is to provide interesting example, useful for
students of using bitwise operations in the programming languages C ++ and
Java. As an example, we describe an algorithm for obtaining a Latin square of
arbitrary order. We will outline some techniques for the use of bitwise
operations.
",['\nKrasimir Yordzhev\n'],,"British Journal of Mathematics & Computer Science, 17(5): 1-7,
  2016, Article no. BJMCS.26471",http://dx.doi.org/10.9734/BJMCS/2016/26471,cs.OH,"['cs.OH', '68N15']",10.9734/BJMCS/2016/26471,,[]
Requirements for storing electrophysiology data,http://arxiv.org/abs/1605.07673v2,2016-05-24T22:18:27Z,2016-06-03T19:50:11Z,"  The purpose of this document is to specify the basic data types required for
storing electrophysiology and optical imaging data to facilitate computer-based
neuroscience studies and data sharing. These requirements are being developed
within a working group of the Electrophysiology Task Force in the International
Neuroinformatics Coordinating Facility (INCF) Program on Standards for Data
Sharing. While this document describes the requirements of the standard
independent of the actual storage technology, the Task Force has recommended
basing a standard on HDF5. This is in line with a number of groups who are
already using HDF5 to store electrophysiology data, although currently without
being based on a standard.
","['\nJeff Teeters\n', '\nJan Benda\n', '\nAndrew Davison\n', '\nStephen Eglen\n', '\nRichard C. Gerkin\n', '\nJeffrey Grethe\n', '\nJan Grewe\n', '\nKenneth Harris\n', '\nChristian Kellner\n', '\nYann Le Franc\n', '\nRoman Mouček\n', '\nDimiter Prodanov\n', '\nRobert Pröpper\n', '\nHyrum L. Sessions\n', '\nLeslie Smith\n', '\nAndrey Sobolev\n', '\nFriedrich Sommer\n', '\nAdrian Stoewer\n', '\nThomas Wachtler\n', '\nBarry Wark\n']","5 pages, 1 table",,http://arxiv.org/abs/1605.07673v2,cs.OH,"['cs.OH', 'E.1; E.2; E.5']",,,[]
Axodraw Version 2,http://arxiv.org/abs/1606.01177v1,2016-05-27T14:28:38Z,2016-05-27T14:28:38Z,"  We present version two of the Latex graphical style file Axodraw. It has a
number of new drawing primitives and many extra options, and it can now work
with \program{pdflatex} to directly produce output in PDF file format (but with
the aid of an auxiliary program).
","['\nJohn C. Collins\n', '\nJ. A. M. Vermaseren\n']","Files can be found at
  www.nikhef.nl/~form/maindir/others/axodraw2/axodraw2.html",,http://arxiv.org/abs/1606.01177v1,cs.OH,"['cs.OH', 'hep-ph', 'hep-th']",,,[]
"Design and Implementation of a Novel Compatible Encoding Scheme in the
  Time Domain for Image Sensor Communication",http://arxiv.org/abs/1606.05666v1,2016-05-25T12:02:32Z,2016-05-25T12:02:32Z,"  This paper presents a modulation scheme in the time domain based on
On-Off-Keying and proposes various compatible supports for different types of
image sensors. The content of this article is a sub-proposal to the IEEE
802.15.7r1 Task Group (TG7r1) aimed at Optical Wireless Communication (OWC)
using an image sensor as the receiver. The compatibility support is
indispensable for Image Sensor Communications (ISC) because the rolling shutter
image sensors currently available have different frame rates, shutter speeds,
sampling rates, and resolutions. However, focusing on unidirectional
communications (i.e., data broadcasting, beacons), an asynchronous
communication prototype is also discussed in the paper. Due to the physical
limitations associated with typical image sensors (including low and varying
frame rates, long exposures, and low shutter speeds), the link speed
performance is critically considered. Based on the practical measurement of
camera response to modulated light, an operating frequency range is suggested
along with the similar system architecture, decoding procedure, and algorithms.
A significant feature of our novel data frame structure is that it can support
both typical frame rate cameras (in the oversampling mode) as well as very low
frame rate cameras (in the error detection mode for a camera whose frame rate
is lower than the transmission packet rate). A high frame rate camera, i.e., no
less than 20 fps, is supported in an oversampling mode in which a majority
voting scheme for decoding data is applied. A low frame rate camera, i.e., when
the frame rate drops to less than 20 fps at some certain time, is supported by
an error detection mode in which any missing data sub-packet is detected in
decoding and later corrected by external code. Numerical results and valuable
analysis are also included to indicate the capability of the proposed schemes.
","['\nTrang Nguyen\n', '\nMohammad Arif Hossain\n', '\nYeong Min Jang\n']","24 pages, 15 figures, 5 tables, Sensors Journal",,http://dx.doi.org/10.3390/s16050736,cs.OH,"['cs.OH', 'cs.IT', 'math.IT', '68P30']",10.3390/s16050736,,[]
A Survey: Embedded Systems Supporting By Different Operating Systems,http://arxiv.org/abs/1610.07899v1,2016-05-11T07:17:06Z,2016-05-11T07:17:06Z,"  In these days embedded system have an important role in different Fields and
applications like Network embedded system , Real-time embedded systems which
supports the mission-critical domains, mostly having the time constraints,
Stand-alone systems which includes the network router etc. A great deployment
in the processors made for completing the demanding needs of the users. There
is also a large-scale deployment occurs in sensor networks for providing the
advance facilities, for handled such type of embedded systems a specific
operating system must provide. This paper presents some software
infrastructures that have the ability of supporting such types of embedded
systems.
","['\nQamar Jabeen\n', '\nFazlullah Khan\n', '\nMuhammad Nouman Hayat\n', '\nHaroon Khan\n', '\nSyed Roohullah Jan\n', '\nFarman Ullah\n']","15 pages, International Journal of Scientific Research in Science,
  Engineering and Technology(IJSRSET), Print ISSN : 2395-1990, Online ISSN :
  2394-4099, Volume 2 Issue 2, pp.664-673, March-April 2016",,http://arxiv.org/abs/1610.07899v1,cs.OH,['cs.OH'],,,[]
2.4GHZ Class AB power Amplifier For Healthcare Application,http://arxiv.org/abs/1605.02455v1,2016-05-09T07:44:42Z,2016-05-09T07:44:42Z,"  The objective of this research was to design a 2.4 GHz class AB Power
Amplifier, with 0.18 um SMIC CMOS technology by using Cadence software, for
health care applications. The ultimate goal for such application is to minimize
the trade-offs between performance and cost, and between performance and low
power consumption design. The performance of the power amplifier meets the
specification requirements of the desired.
","['\nWei Cai\n', '\nLiang Huang\n', '\nWuJie Wen\n']",6 pages,,http://arxiv.org/abs/1605.02455v1,cs.OH,['cs.OH'],,,[]
Process Information Model for Sheet Metal Operations,http://arxiv.org/abs/1605.02514v1,2016-05-09T10:48:17Z,2016-05-09T10:48:17Z,"  The paper extracts the process parameters from a sheet metal part model
(B-Rep). These process parameters can be used in sheet metal manufacturing to
control the manufacturing operations. By extracting these process parameters
required for manufacturing, CAM program can be generated automatically using
the part model and resource information. A Product model is generated in
modeling software and converted into STEP file which is used for extracting
B-Rep which interned is used to classify and extract feature by using sheet
metal feature recognition module. The feature edges are classified as CEEs,
IEEs, CIEs and IIEs based on topological properties. Database is created for
material properties of the sheet metal and machine tools required to
manufacture features in a part model. The extracted feature, feature's edge
information and resource information are then used to compute process
parameters and values required to control manufacturing operations. The
extracted feature, feature's edge information, resource information and process
parameters are the integral components of the proposed process information
model for sheet metal operations.
","['\nRavi Kumar Gupta\n', '\nPothala Sreenu\n', '\nAlain Bernard\n', '\nFlorent Laroche\n']","The IFIP Working Group WG 5.1 11th International Conference on
  Product Lifecycle Management, Jul 2014, Yokohama, Japan",,http://arxiv.org/abs/1605.02514v1,cs.OH,['cs.OH'],,,[]
"An Alternative Framework for Time Series Decomposition and Forecasting
  and its Relevance for Portfolio Choice: A Comparative Study of the Indian
  Consumer Durable and Small Cap Sectors",http://arxiv.org/abs/1605.03930v1,2016-05-11T06:33:41Z,2016-05-11T06:33:41Z,"  One of the challenging research problems in the domain of time series
analysis and forecasting is making efficient and robust prediction of stock
market prices. With rapid development and evolution of sophisticated algorithms
and with the availability of extremely fast computing platforms, it has now
become possible to effectively extract, store, process and analyze high volume
stock market time series data. Complex algorithms for forecasting are now
available for speedy execution over parallel architecture leading to fairly
accurate results. In this paper, we have used time series data of the two
sectors of the Indian economy: Consumer Durables sector and the Small Cap
sector for the period January 2010 to December 2015 and proposed a
decomposition approach for better understanding of the behavior of each of the
time series. Our contention is that various sectors reveal different time
series patterns and understanding them is essential for portfolio formation.
Further, based on this structural analysis, we have also proposed several
robust forecasting techniques and analyzed their accuracy in prediction using
suitably chosen training and test data sets. Extensive results are presented to
demonstrate the effectiveness of our propositions.
","['\nJaydip Sen\n', '\nTamal Datta Chaudhuri\n']","30 pages, 6 figures, 16 tables. The paper is accepted for publication
  in the ""Journal of Economics"" Vol 3, No 2, June 2016. arXiv admin note: text
  overlap with arXiv:1604.04044","Journal of Economics Library, Vol 3, No 2, June 2016",http://arxiv.org/abs/1605.03930v1,cs.OH,['cs.OH'],,,[]
Scientific notations for the digital era,http://arxiv.org/abs/1605.02960v1,2016-05-10T11:56:49Z,2016-05-10T11:56:49Z,"  Computers have profoundly changed the way scientific research is done.
Whereas the importance of computers as research tools is evident to everyone,
the impact of the digital revolution on the representation of scientific
knowledge is not yet widely recognized. An ever increasing part of today's
scientific knowledge is expressed, published, and archived exclusively in the
form of software and electronic datasets. In this essay, I compare these
digital scientific notations to the the traditional scientific notations that
have been used for centuries, showing how the digital notations optimized for
computerized processing are often an obstacle to scientific communication and
to creative work by human scientists. I analyze the causes and propose
guidelines for the design of more human-friendly digital scientific notations.
",['\nKonrad Hinsen\n'],,,http://arxiv.org/abs/1605.02960v1,physics.soc-ph,"['physics.soc-ph', 'cs.OH', 'physics.comp-ph', 'physics.hist-ph']",,,[]
"EDF-VD Scheduling of Mixed-Criticality Systems with Degraded Quality
  Guarantees",http://arxiv.org/abs/1605.01302v1,2016-05-04T14:50:27Z,2016-05-04T14:50:27Z,"  This paper studies real-time scheduling of mixed-criticality systems where
low-criticality tasks are still guaranteed some service in the high-criticality
mode, with reduced execution budgets. First, we present a utilization-based
schedulability test for such systems under EDF-VD scheduling. Second, we
quantify the suboptimality of EDF-VD (with our test condition) in terms of
speedup factors. In general, the speedup factor is a function with respect to
the ratio between the amount of resource required by different types of tasks
in different criticality modes, and reaches 4/3 in the worst case. Furthermore,
we show that the proposed utilization-based schedulability test and speedup
factor results apply to the elastic mixed-criticality model as well.
Experiments show effectiveness of our proposed method and confirm the
theoretical suboptimality results.
","['\nDi Liu\n', '\nJelena Spasic\n', '\nGang Chen\n', '\nNan Guan\n', '\nSongran Liu\n', '\nTodor Stefanov\n', '\nWang Yi\n']",,,http://dx.doi.org/10.1109/RTSS.2016.013,cs.OH,['cs.OH'],10.1109/RTSS.2016.013,,[]
Towards a characterization of the uncertainty curve for graphs,http://arxiv.org/abs/1605.00451v1,2016-04-27T07:25:20Z,2016-04-27T07:25:20Z,"  Signal processing on graphs is a recent research domain that aims at
generalizing classical tools in signal processing, in order to analyze signals
evolving on complex domains. Such domains are represented by graphs, for which
one can compute a particular matrix, called the normalized Laplacian. It was
shown that the eigenvalues of this Laplacian correspond to the frequencies of
the Fourier domain in classical signal processing. Therefore, the frequency
domain is not the same for every support graph. A consequence of this is that
there is no non-trivial generalization of Heisenberg's uncertainty principle,
that states that a signal cannot be fully localized both in the time domain and
in the frequency domain. A way to generalize this principle, introduced by
Agaskar and Lu, consists in determining a curve that represents a lower bound
on the compromise between precision in the graph domain and precision in the
spectral domain. The aim of this paper is to propose a characterization of the
signals achieving this curve, for a larger class of graphs than the one studied
by Agaskar and Lu.
","['\nBastien Pasdeloup\n', '\nVincent Gripon\n', '\nGrégoire Mercier\n', '\nDominique Pastor\n']","ICASSP 2016 : 41st IEEE International Conference on Acoustics, Speech
  and Signal Processing, 20-25 march 2016, Shanghai, China, 2016",,http://dx.doi.org/10.1109/ICASSP.2016.7472540,cs.OH,"['cs.OH', 'cs.SY']",10.1109/ICASSP.2016.7472540,,[]
"Real-Time Contingency Analysis with Corrective Transmission Switching -
  Part II: Results and Discussion",http://arxiv.org/abs/1604.05571v1,2016-04-15T19:43:56Z,2016-04-15T19:43:56Z,"  This paper presents the performance of an AC transmission switching (TS)
based real-time contingency analysis (RTCA) tool that is introduced in Part I
of this paper. The approach quickly proposes high quality corrective switching
actions for relief of potential post-contingency network violations. The
approach is confirmed by testing it on actual EMS snapshots of two large-scale
systems, the Electric Reliability Council of Texas (ERCOT) and the Pennsylvania
New Jersey Maryland (PJM) Interconnection; the approach is also tested on data
provided by the Tennessee Valley Authority (TVA). The results show that the
tool effectively reduces post-contingency violations. Fast heuristics are used
along with parallel computing to reduce the computational difficulty of the
problem. The tool is able to handle the PJM system in about five minutes with a
standard desktop computer. Time-domain simulations are performed to check
system stability with corrective transmission switching (CTS). In conclusion,
the paper shows that corrective switching is ripe for industry adoption. CTS
can provide significant reliability benefits that can be translated into
significant cost savings.
","['\nXingpeng Li\n', '\nMostafa Sahraei-Ardakani\n', '\nPranavamoorthy Balasubramanian\n', '\nMojdeh Abdi-Khorsand\n', '\nKory W. Hedman\n', '\nRobin Podmore\n']","8 pages, 8 figures",,http://arxiv.org/abs/1604.05571v1,cs.OH,['cs.OH'],,,[]
"Real-Time Contingency Analysis with Corrective Transmission Switching -
  Part I: Methodology",http://arxiv.org/abs/1604.05570v1,2016-04-15T19:30:20Z,2016-04-15T19:30:20Z,"  Transmission switching (TS) has gained significant attention recently.
However, barriers still remain and must be overcome before the technology can
be adopted by the industry. The state of the art challenges include AC
feasibility and performance, computational complexity, the ability to handle
large-scale real power systems, and dynamic stability. This two-part paper
investigates these challenges by developing an AC TS-based real-time
contingency analysis (RTCA) tool that can handle large-scale systems within a
reasonable time. The tool proposes multiple corrective switching actions, after
detection of a contingency with potential violations. To reduce the
computational complexity, three heuristic algorithms are proposed to generate a
small set of candidates for switching. Parallel computing is implemented to
further speed up the solution time. Furthermore, stability analysis is
performed to check for dynamic stability of proposed TS solutions. Part I of
the paper presents a comprehensive literature review and the methodology. The
promising results, tested on the Tennessee Valley Authority (TVA) system and
actual energy management system (EMS) snapshots from Pennsylvania New Jersey
Maryland (PJM) and the Electric Reliability Council of Texas (ERCOT), are
presented in Part II. It is concluded that RTCA with corrective TS
significantly reduces potential post-contingency violations and is ripe for
industry adoption.
","['\nXingpeng Li\n', '\nPranavamoorthy Balasubramanian\n', '\nMostafa Sahraei-Ardakani\n', '\nMojdeh Abdi-Khorsand\n', '\nKory W. Hedman\n', '\nRobin Podmore\n']","8 pages, 3 figures",,http://arxiv.org/abs/1604.05570v1,cs.OH,"['cs.OH', 'cs.SY']",,,[]
"Non-convex Global Minimization and False Discovery Rate Control for the
  TREX",http://arxiv.org/abs/1604.06815v2,2016-04-22T20:28:55Z,2016-09-20T20:07:35Z,"  The TREX is a recently introduced method for performing sparse
high-dimensional regression. Despite its statistical promise as an alternative
to the lasso, square-root lasso, and scaled lasso, the TREX is computationally
challenging in that it requires solving a non-convex optimization problem. This
paper shows a remarkable result: despite the non-convexity of the TREX problem,
there exists a polynomial-time algorithm that is guaranteed to find the global
minimum. This result adds the TREX to a very short list of non-convex
optimization problems that can be globally optimized (principal components
analysis being a famous example). After deriving and developing this new
approach, we demonstrate that (i) the ability of the preexisting TREX heuristic
to reach the global minimum is strongly dependent on the difficulty of the
underlying statistical problem, (ii) the new polynomial-time algorithm for TREX
permits a novel variable ranking and selection scheme, (iii) this scheme can be
incorporated into a rule that controls the false discovery rate (FDR) of
included features in the model. To achieve this last aim, we provide an
extension of the results of Barber & Candes (2015) to establish that the
knockoff filter framework can be applied to the TREX. This investigation thus
provides both a rare case study of a heuristic for non-convex optimization and
a novel way of exploiting non-convexity for statistical inference.
","['\nJacob Bien\n', '\nIrina Gaynanova\n', '\nJohannes Lederer\n', '\nChristian Müller\n']",,"Journal of Computational and Graphical Statistics 2017, Vol. 27,
  No. 1, 23-33",http://dx.doi.org/10.1080/10618600.2017.1341414,stat.ML,"['stat.ML', 'cs.OH', 'stat.CO', 'stat.ME']",10.1080/10618600.2017.1341414,,[]
"Cooperative communications for sleep monitoring in wireless body area
  networks",http://arxiv.org/abs/1604.00818v1,2016-04-04T11:34:33Z,2016-04-04T11:34:33Z,"  This paper investigates the performance of cooperative receive diversity, for
the wireless body area network (WBAN) radio channel, compliant with the IEEE
802.15.6 Standard, in the case of monitoring a sleeping person. Extensive WBAN
measurements near the 2.4 GHz ISM band were used. Up to 7 dB and 20%
improvement for two-hop communications with the use of relays are empirically
demonstrated with respect to outage probability and outage duration, with
3-branch cooperative selection combining and 3-branch cooperative
switch-and-examine combining.
","['\nSamiya Shimly\n', '\nSamaneh Movassaghi\n', '\nDavid Smith\n']","2 pages, 3 figures, Electronics Letters","Electronics Letters, 52, (8), 2016, 2 pp",http://dx.doi.org/10.1049/el.2015.3008,cs.OH,['cs.OH'],10.1049/el.2015.3008,,[]
Designing robust watermark barcodes for multiplex long-read sequencing,http://arxiv.org/abs/1604.01344v1,2016-04-05T17:39:03Z,2016-04-05T17:39:03Z,"  A method for designing sequencing barcodes that can withstand a large number
of insertion, deletion and substitution errors and are suitable for use in
multiplex single-molecule real-time sequencing is presented. The manuscript
focuses on the design of barcodes for full-length single-pass reads, impaired
by challenging error rates in the order of 11%. To the authors' knowledge, this
is the first method to specifically address this problem without requiring
upstream quality improvement. The proposed barcodes can multiplex hundreds or
thousands of samples while achieving sample misassignment probabilities as low
as $10^{-7}$, and are designed to be compatible with chemical constraints
imposed by the sequencing process. Software for constructing watermark barcode
sets and demultiplexing barcoded reads, together with example sets of barcodes
and synthetic barcoded reads, are freely available at
www.cifasis-conicet.gov.ar/ezpeleta/NS-watermark.
","['\nJoaquín Ezpeleta\n', '\nFlavia J. Krsticevic\n', '\nPilar Bulacio\n', '\nElizabeth Tapia\n']",Paper accepted at the RECOMB-Seq 2016,,http://arxiv.org/abs/1604.01344v1,cs.OH,"['cs.OH', 'H.1.1; J.3']",,,[]
Niépce-Bell or Turing: How to Test Odor Reproduction?,http://arxiv.org/abs/1603.08666v5,2016-03-29T07:35:15Z,2016-11-10T10:19:09Z,"  In a 1950 article in Mind, decades before the existence of anything
resembling an artificial intelligence system, Alan Turing addressed the
question of how to test whether machines can think, or in modern terminology,
whether a computer claimed to exhibit intelligence indeed does so. The current
paper raises the analogous issue for olfaction: how to test the validity of a
system claimed to reproduce arbitrary odors artificially, in a way recognizable
to humans, in face of the unavailability of a general naming method for odors.
Although odor reproduction systems are still far from being viable, the
question of how to test candidates thereof is claimed to be interesting and
nontrivial, and a novel method is proposed. To some extent, the method is
inspired by Turing`s test for AI, in that it involves a human challenger and
the real and artificial entities, yet it is very different: our test is
conditional, requiring from the artificial no more than is required from the
original, and it employs a novel method of immersion that takes advantage of
the availability of near-perfect reproduction methods for sight and sound.
",['\nDavid Harel\n'],"12 pages, 4 figures",,http://arxiv.org/abs/1603.08666v5,cs.OH,['cs.OH'],,,[]
"A General World Model with Poiesis: Poppers Three Worlds updated with
  Software",http://arxiv.org/abs/1604.00360v1,2016-04-01T19:07:40Z,2016-04-01T19:07:40Z,"  With the famous Three Worlds of Karl Popper as template, the paper rigorously
introduces the concept of software to define the counterpart of the physical
subworld. Digesting the scientific-technical view of biology and neurology on a
high level, results in an updated Three Worlds scheme consistent with an
information technical view. Chance and mathematics complete the world model.
Some simple examples illustrate the move from Poppers view of the world with
physics, psyche and World 3, to a new extended model with physics, extended
software (which we call Poiesis), and Geist (the notion which embodies spirit,
mind and soul).
",['\nWalter Hehl\n'],"9 pages. 1 Figure of a world model with physics, software and Geist,
  Book published 2016 (in German) by Springer, Heidelberg, with the general
  fundamentals of Software for philosophy",,http://arxiv.org/abs/1604.00360v1,cs.OH,['cs.OH'],,,[]
A (Basis for a) Philosophy of a Theory of Fuzzy Computation,http://arxiv.org/abs/1603.05162v1,2016-03-16T16:10:21Z,2016-03-16T16:10:21Z,"  Vagueness is a linguistic phenomenon as well as a property of physical
objects. Fuzzy set theory is a mathematical model of vagueness that has been
used to define vague models of computation. The prominent model of vague
computation is the fuzzy Turing machine. This conceptual computing device gives
an idea of what computing under vagueness means, nevertheless, it is not the
most natural model. Based on the properties of this and other models of vague
computing, it is aimed to formulate a basis for a philosophy of a theory of
fuzzy computation.
",['\nApostolos Syropoulos\n'],,"Kairos. Journal of Philosophy & Science, Vol 20, No 1, pp.
  181-201, 2018",http://dx.doi.org/10.2478/kjps-2018-0009,cs.OH,['cs.OH'],10.2478/kjps-2018-0009,,[]
"Norm-1 Regularized Consensus-based ADMM for Imaging with a Compressive
  Antenna",http://arxiv.org/abs/1603.05581v1,2016-03-16T15:44:34Z,2016-03-16T15:44:34Z,"  This paper presents a novel norm-one-regularized, consensus-based imaging
algorithm, based on the Alternating Direction Method of Multipliers (ADMM).
This algorithm is capable of imaging composite dielectric and metallic targets
by using limited amount of data. The distributed capabilities of the ADMM
accelerates the convergence of the imaging. Recently, a Compressive Reflector
Antenna (CRA) has been proposed as a way to provide high-sensing-capacity with
a minimum cost and complexity in the hardware architecture. The ADMM algorithm
applied to the imaging capabilities of the Compressive Antenna (CA) outperforms
current state of the art iterative reconstruction algorithms, such as
Nesterov-based methods, in terms of computational cost; and it ultimately
enables the use of a CA in quasi-real-time, compressive sensing imaging
applications.
","['\nJuan Heredia Juesas\n', '\nAli Molaei\n', '\nLuis Tirado\n', '\nWilliam Blackwell\n', '\nJose A Martinez Lorenzo\n']","4 pages, 4 figures",,http://dx.doi.org/10.1109/LAWP.2017.2718242,cs.OH,['cs.OH'],10.1109/LAWP.2017.2718242,,[]
"Multiprocessor Scheduling of a Multi-mode Dataflow Graph Considering
  Mode Transition Delay",http://arxiv.org/abs/1603.05775v1,2016-03-18T06:16:17Z,2016-03-18T06:16:17Z,"  Synchronous Data Flow (SDF) model is widely used for specifying signal
processing or streaming applications. Since modern embedded applications become
more complex with dynamic behavior changes at run-time, several extensions of
the SDF model have been proposed to specify the dynamic behavior changes while
preserving static analyzability of the SDF model. They assume that an
application has a finite number of behaviors (or modes) and each behavior
(mode) is represented by an SDF graph. They are classified as multi-mode
dataflow models in this paper. While there exist several scheduling techniques
for multi-mode dataflow models, no one allows task migration between modes. By
observing that the resource requirement can be additionally reduced if task
migration is allowed, we propose a multiprocessor scheduling technique of a
multi-mode dataflow graph considering task migration between modes. Based on a
genetic algorithm, the proposed technique schedules all SDF graphs in all modes
simultaneously to minimize the resource requirement. To satisfy the throughput
constraint, the proposed technique calculates the actual throughput requirement
of each mode and the output buffer size for tolerating throughput jitter. We
compare the proposed technique with a method which analyzes SDF graphs in each
execution mode separately and a method that does not allow task migration for
synthetic examples and three real applications: H.264 decoder, vocoder, and LTE
receiver algorithms.
","['\nHanwoong Jung\n', '\nHyunok Oh\n', '\nSoonhoi Ha\n']",,,http://dx.doi.org/10.1145/2997645,cs.OH,['cs.OH'],10.1145/2997645,,[]
"Beyond Binary Computers: How To Implement Multi-Switch Computer Hardware
  and Software and; The Advantage of a Multi-Switched Computer",http://arxiv.org/abs/1603.06223v1,2016-03-20T14:48:37Z,2016-03-20T14:48:37Z,"  This paper explores the possibilities of using a computing methodology
--hardware and software-- that employs technology other than binary. I refer to
this as ""supra - binary"" computing. Software constructs that use more than
binary techniques are discussed. The gains in supra - binary software are
demonstrated, which includes supra - binary code being RISC. Possible hardware
implementations of a computer with other than binary based architecture are
demonstrated and considered. The advantages and possible disadvantages of these
hardware implementations are discussed. The gain in computing speed is
evaluated and demonstrated. Supra - binary processing would streamline parallel
processing and make its implementation a built - in feature of the software and
hardware. Also, supra - binary would bring an advancement to neural networking.
This is discussed and demonstrated. In addition, possible applications of supra
- binary computing to database and neural networking are discussed. Also, the
possible implementations could be applied to telecommunications with dramatic
results.
",['\nGivon Zirkind\n'],"14 Pages, 11 Figures, 2 Truth Tables, Figures 9-11 are Flowcharts",,http://arxiv.org/abs/1603.06223v1,cs.OH,"['cs.OH', 'H.3.4']",,,[]
Continuous-Flow Graph Transportation Distances,http://arxiv.org/abs/1603.06927v1,2016-03-22T19:39:18Z,2016-03-22T19:39:18Z,"  Optimal transportation distances are valuable for comparing and analyzing
probability distributions, but larger-scale computational techniques for the
theoretically favorable quadratic case are limited to smooth domains or
regularized approximations. Motivated by fluid flow-based transportation on
$\mathbb{R}^n$, however, this paper introduces an alternative definition of
optimal transportation between distributions over graph vertices. This new
distance still satisfies the triangle inequality but has better scaling and a
connection to continuous theories of transportation. It is constructed by
adapting a Riemannian structure over probability distributions to the graph
case, providing transportation distances as shortest-paths in probability
space. After defining and analyzing theoretical properties of our new distance,
we provide a time discretization as well as experiments verifying its
effectiveness.
","['\nJustin Solomon\n', '\nRaif Rustamov\n', '\nLeonidas Guibas\n', '\nAdrian Butscher\n']",,,http://arxiv.org/abs/1603.06927v1,cs.OH,['cs.OH'],,,[]
"Using Newton's method to model a spatial light distribution of a LED
  with attached secondary optics",http://arxiv.org/abs/1603.01090v1,2016-03-03T13:23:24Z,2016-03-03T13:23:24Z,"  In design of optical systems based on LED (Light emitting diode) technology,
a crucial task is to handle the unstructured data describing properties of
optical elements in standard formats. This leads to the problem of data fitting
within an appropriate model. Newton's method is used as an upgrade of
previously developed most promising discrete optimization heuristics showing
improvement of both performance and quality of solutions. Experiment also
indicates that a combination of an algorithm that finds promising initial
solutions as a preprocessor to Newton's method may be a winning idea, at least
on some datasets of instances.
","['\nDavid Kaljun\n', '\nJoze Petrišič\n', '\nJanez Žerovnik\n']","submitted to Journal of Mecanical enginering (Strojni\v{s}ki vestnik,
  Ljubljana)","Journal of Mechanical Engineering 62(2016)5, 307-317",http://dx.doi.org/10.5545/sv-jme.2015.3234,cs.OH,"['cs.OH', '90C59']",10.5545/sv-jme.2015.3234,,[]
In-Vehicle PLC: In-Car and In-Ship Channel Characterization,http://arxiv.org/abs/1603.02260v1,2016-03-05T18:59:01Z,2016-03-05T18:59:01Z,"  This paper deals with power line communication (PLC) in the context of
in-vehicle data networks. This technology can provide high-speed data
connectivity via the exploitation of the existing power network, with clear
potential benefits in terms of cost and weight reduction. The focus is on two
scenarios: an electric car and a cruise ship. An overview of the wiring
infrastructure and the network topology in these two scenarios is provided. The
main findings reported in the literature related to the channel characteristics
are reported. Noise is also assessed with emphasis to the electric car context.
Then, new results from the statistical analysis of measurements made in a
compact electric car and in a large cruise ship are shown. The channel
characteristics are analysed in terms of average channel gain, delay spread,
coherence bandwidth and achievable transmission rate. Finally, an overall
comparison is made, highlighting similarities and differences taking into
account also the conventional (combustion engine) car and the largely
investigated in-home scenario.
","['\nAlberto Pittolo\n', '\nMarco De Piante\n', '\nFabio Versolatto\n', '\nAndrea M. Tonello\n']","7 pages (2-column), 10 figures, 1 table, accepted for publication on
  IEEE Vehicular Technology Magazine",,http://arxiv.org/abs/1603.02260v1,cs.OH,['cs.OH'],,,[]
Microprocessor Optimizations for the Internet of Things: A Survey,http://arxiv.org/abs/1603.02393v2,2016-03-08T06:45:27Z,2018-02-20T19:42:53Z,"  The Internet of Things (IoT) refers to a pervasive presence of interconnected
and uniquely identifiable physical devices. These devices' goal is to gather
data and drive actions in order to improve productivity, and ultimately reduce
or eliminate reliance on human intervention for data acquisition,
interpretation, and use. The proliferation of these connected low-power devices
will result in a data explosion that will significantly increase data
transmission costs with respect to energy consumption and latency. Edge
computing reduces these costs by performing computations at the edge nodes,
prior to data transmission, to interpret and/or utilize the data. While much
research has focused on the IoT's connected nature and communication
challenges, the challenges of IoT embedded computing with respect to device
microprocessors has received much less attention. This paper explores IoT
applications' execution characteristics from a microarchitectural perspective
and the microarchitectural characteristics that will enable efficient and
effective edge computing. To tractably represent a wide variety of
next-generation IoT applications, we present a broad IoT application
classification methodology based on application functions, to enable quicker
workload characterizations for IoT microprocessors. We then survey and discuss
potential microarchitectural optimizations and computing paradigms that will
enable the design of right-provisioned microprocessors that are efficient,
configurable, extensible, and scalable. This paper provides a foundation for
the analysis and design of a diverse set of microprocessor architectures for
next-generation IoT devices.
","['\nTosiron Adegbija\n', '\nAnita Rogacs\n', '\nChandrakant Patel\n', '\nAnn Gordon-Ross\n']","Published at IEEE Transactions on Computer-Aided Design of Integrated
  Circuits and Systems (TCAD); Special Issue on Circuit and System Design for
  Internet of Things",,http://dx.doi.org/10.1109/TCAD.2017.2717782,cs.OH,['cs.OH'],10.1109/TCAD.2017.2717782,,[]
The Design Principles of Konrad Zuse's Mechanical Computers,http://arxiv.org/abs/1603.02396v1,2016-03-08T06:50:43Z,2016-03-08T06:50:43Z,"  Konrad Zuse built the Z1, a mechanical programmable computing machine,
between 1935/36 and 1937/38. The Z1 was a binary floating-point computing
device. The individual logical gates were constructed using metallic plates and
interconnection rods. This paper describes the design principles Zuse followed
in order to complete a complex calculating machine, as the Z1 was. Zuse called
his basic switching elements ""mechanical relays"" in analogy to the electrical
relays used in telephony.
",['\nRaul Rojas\n'],"9 pages, 11 figures",,http://arxiv.org/abs/1603.02396v1,cs.OH,"['cs.OH', 'B.0; K.2']",,,[]
Mapping EU fishing activities using ship tracking data,http://arxiv.org/abs/1603.03826v3,2016-03-11T23:51:20Z,2016-06-05T20:37:10Z,"  Information and understanding of fishing activities at sea are fundamental
components of marine knowledge and maritime situational awareness. Such
information is important to fisheries science, public authorities and policy
makers. In this paper we introduce a first map at European scale of EU fishing
activities extracted using Automatic Identification System (AIS) ship tracking
data. The resulting map is a density of points that identify fishing
activities. A measure of the reliability of such information is also presented
as a map of coverage reception capabilities.
","['\nMichele Vespe\n', '\nMaurizio Gibin\n', '\nAlfredo Alessandrini\n', '\nFabrizio Natale\n', '\nFabio Mazzarella\n', '\nGiacomo C. Osio\n']",Paper accepted for publication,,http://dx.doi.org/10.1080/17445647.2016.1195299,cs.OH,"['cs.OH', '68U01']",10.1080/17445647.2016.1195299,,[]
Analyzing In-Game Movements of Soccer Players at Scale,http://arxiv.org/abs/1603.05583v1,2016-03-11T23:54:55Z,2016-03-11T23:54:55Z,"  It is challenging to get access to datasets related to the physical
performance of soccer players. The teams consider such information highly
confidential, especially if it covers in-game performance.Hence, most of the
analysis and evaluation of the players' performance do not contain much
information on the physical aspect of the game, creating a blindspot in
performance analysis. We propose a novel method to solve this issue by deriving
movement characteristics of soccer players. We use event-based datasets from
data provider companies covering 50+ soccer leagues allowing us to analyze the
movement profiles of potentially tens of thousands of players without any major
investment. Our methodology does not require expensive, dedicated player
tracking system deployed in the stadium. We also compute the similarity of the
players based on their movement characteristics and as such identify potential
candidates who may be able to replace a given player. Finally, we quantify the
uniqueness and consistency of players in terms of their in-game movements. Our
study is the first of its kind that focuses on the movements of soccer players
at scale, while it derives novel, actionable insights for the soccer industry
from event-based datasets.
","['\nLaszlo Gyarmati\n', '\nMohamed Hefeeda\n']",MIT Sloan Sports Analytics Conference 2016,,http://arxiv.org/abs/1603.05583v1,cs.OH,"['cs.OH', 'stat.AP']",,,[]
Spatio-Temporal Analysis of Team Sports -- A Survey,http://arxiv.org/abs/1602.06994v1,2016-02-22T23:17:37Z,2016-02-22T23:17:37Z,"  Team-based invasion sports such as football, basketball and hockey are
similar in the sense that the players are able to move freely around the
playing area; and that player and team performance cannot be fully analysed
without considering the movements and interactions of all players as a group.
State of the art object tracking systems now produce spatio-temporal traces of
player trajectories with high definition and high frequency, and this, in turn,
has facilitated a variety of research efforts, across many disciplines, to
extract insight from the trajectories. We survey recent research efforts that
use spatio-temporal data from team sports as input, and involve non-trivial
computation. This article categorises the research efforts in a coherent
framework and identifies a number of open research questions.
","['\nJoachim Gudmundsson\n', '\nMichael Horton\n']","42 pages, 11 figures",,http://dx.doi.org/10.1145/3054132,cs.OH,"['cs.OH', 'A.1; H.2.8']",10.1145/3054132,,[]
"δ-MAPS: From spatio-temporal data to a weighted and lagged
  network between functional domains",http://arxiv.org/abs/1602.07249v3,2016-02-23T17:57:22Z,2016-09-23T22:50:37Z,"  We propose {\delta}-MAPS, a method that analyzes spatio-temporal data to
first identify the distinct spatial components of the underlying system,
referred to as ""domains"", and second to infer the connections between them. A
domain is a spatially contiguous region of highly correlated temporal activity.
The core of a domain is a point or subregion at which a metric of local
homogeneity is maximum across the entire domain. We compute a domain as the
maximum-sized set of spatially contiguous cells that include the detected core
and satisfy a homogeneity constraint, expressed in terms of the average
pairwise cross-correlation across all cells in the domain. Domains may be
spatially overlapping. Different domains may have correlated activity,
potentially at a lag, because of direct or indirect interactions. The proposed
edge inference method examines the statistical significance of each lagged
cross-correlation between two domains, infers a range of lag values for each
edge, and assigns a weight to each edge based on the covariance of the two
domains. We illustrate the application of {\delta}-MAPS on data from two
domains: climate science and neuroscience.
","['\nIlias Fountalis\n', '\nAnnalisa Bracco\n', '\nBistra Dilkina\n', '\nConstantine Dovrolis\n', '\nShella Keilholz\n']","12 pages, 6 figures",,http://arxiv.org/abs/1602.07249v3,cs.OH,['cs.OH'],,,[]
Philosophical Fictionalism and Problem of Artificial Intelligence,http://arxiv.org/abs/1602.07259v1,2016-02-23T18:32:41Z,2016-02-23T18:32:41Z,"  The artificial intelligence received broad interpretation as a literary
image. This approach did not have unambiguous refering to the scopes of logical
studies and mathematical investigations. An author applied methods peculiar to
the semiotic approach, offered by Boris Uspensky and Yury Lotman. In addition,
the article presented the criticism of modern versions of educational
technologies, which led to the unconditional expectations for possibilities of
information and telecommunication technologies. Methodological culture's
growth, which was described on the base of semiotics and functional approach to
word formation of new meanings for the description of the studied subjects,
provided the development of pupils' thought. As a result, the research opened
new prospects on understanding of artificial intelligence within educational
practice.
",['\nSergey B. Kulikov\n'],"15 pages, in Russian","Philosophical Problems of Information Technologies and Cyberspace.
  2015. 10 (2): 42-57",http://dx.doi.org/10.17726/philIT.2015.10.2.811.93,cs.OH,"['cs.OH', '00A30']",10.17726/philIT.2015.10.2.811.93,,[]
Investigating Drivers' Head and Glance Correspondence,http://arxiv.org/abs/1602.07324v1,2016-02-23T21:35:44Z,2016-02-23T21:35:44Z,"  The relationship between a driver's glance pattern and corresponding head
rotation is highly complex due to its nonlinear dependence on the individual,
task, and driving context. This study explores the ability of head pose to
serve as an estimator for driver gaze by connecting head rotation data with
manually coded gaze region data using both a statistical analysis approach and
a predictive (i.e., machine learning) approach. For the latter, classification
accuracy increased as visual angles between two glance locations increased. In
other words, the greater the shift in gaze, the higher the accuracy of
classification. This is an intuitive but important concept that we make
explicit through our analysis. The highest accuracy achieved was 83% using the
method of Hidden Markov Models (HMM) for the binary gaze classification problem
of (1) the forward roadway versus (2) the center stack. Results suggest that
although there are individual differences in head-glance correspondence while
driving, classifier models based on head-rotation data may be robust to these
differences and therefore can serve as reasonable estimators for glance
location. The results suggest that driver head pose can be used as a surrogate
for eye gaze in several key conditions including the identification of
high-eccentricity glances. Inexpensive driver head pose tracking may be a key
element in detection systems developed to mitigate driver distraction and
inattention.
","['\nJoonbum Lee\n', '\nMauricio Muñoz\n', '\nLex Fridman\n', '\nTrent Victor\n', '\nBryan Reimer\n', '\nBruce Mehler\n']","27 pages, 7 figures, 2 tables","PeerJ Computer Science 4:e146 (2018)
  https://doi.org/10.7717/peerj-cs.146",http://dx.doi.org/10.7717/peerj-cs.146,cs.OH,['cs.OH'],10.7717/peerj-cs.146,,[]
Loongson IoT Gateway: A Technical Review,http://arxiv.org/abs/1602.07891v5,2016-02-25T11:41:45Z,2017-10-10T21:13:13Z,"  A prototype of Loongson IoT (Internet of Things) ZigBee gateway is already
designed and implemented. However, this prototype is not perfect enough because
of the lack of a number of functions. And a lot of things should be done to
improve this prototype, such as adding widely used IEEE 802.11 function, using
a fully open source ZigBee protocol stack to get rid of proprietary implement
or using a fully open source embedded operating system to support 6LoWPAN, and
implementing multiple interfaces.
","['\nZhibang Xie\n', '\nQingjin Deng\n']","4 pages, 4 figures",,http://arxiv.org/abs/1602.07891v5,cs.OH,"['cs.OH', '97P60', 'C.3']",,,[]
"Exponential capacity of associative memories under quantum annealing
  recall",http://arxiv.org/abs/1602.08149v1,2016-02-25T23:42:49Z,2016-02-25T23:42:49Z,"  Associative memory models, in theoretical neuro- and computer sciences, can
generally store a sublinear number of memories. We show that using quantum
annealing for recall tasks endows associative memory models with exponential
storage capacities. Theoretically, we obtain the radius of attractor basins,
$R(N)$, and the capacity, $C(N)$, of such a scheme and their tradeoffs. Our
calculations establish that for randomly chosen memories the capacity of a
model using the Hebbian learning rule with recall via quantum annealing is
exponential in the size of the problem, $C(N)=\mathcal{O}(e^{C_1N}),~C_1\geq0$,
and succeeds on randomly chosen memory sets with a probability of
$(1-e^{-C_2N}),~C_2\geq0$ with $C_1+C_2=(.5-f)^2/(1-f)$, where,
$f=R(N)/N,~0\leq f\leq .5$ is the radius of attraction in terms of Hamming
distance of an input probe from a stored memory as a fraction of the problem
size. We demonstrate the application of this scheme on a programmable quantum
annealing device - the Dwave processor.
","['\nSiddhartha Santra\n', '\nOmar Shehab\n', '\nRadhakrishnan Balu\n']","9 pages, 4 figures. Comments welcome","Phys. Rev. A 96, 062330 (2017)",http://dx.doi.org/10.1103/PhysRevA.96.062330,quant-ph,"['quant-ph', 'cs.OH']",10.1103/PhysRevA.96.062330,,[]
On short-term traffic flow forecasting and its reliability,http://arxiv.org/abs/1602.08355v1,2016-02-25T18:24:56Z,2016-02-25T18:24:56Z,"  Recent advances in time series, where deterministic and stochastic modelings
as well as the storage and analysis of big data are useless, permit a new
approach to short-term traffic flow forecasting and to its reliability, i.e.,
to the traffic volatility. Several convincing computer simulations, which
utilize concrete data, are presented and discussed.
","['\nHassane Abouaïssa\n', '\nMichel Fliess\n', '\nCédric Join\n']","8th IFAC Conference on Manufacturing Modeling, Management & Control
  (Troyes, France, June 2016)",,http://arxiv.org/abs/1602.08355v1,stat.AP,"['stat.AP', 'cs.OH']",,,[]
Improving Data Quality in Intelligent Transportation Systems,http://arxiv.org/abs/1602.03100v1,2016-02-09T18:01:24Z,2016-02-09T18:01:24Z,"  Intelligent Transportation Systems (ITS) use data and information technology
to improve the operation of our transportation network. ITS contributes to
sustainable development by using technology to make the transportation system
more efficient; improving our environment by reducing emissions, reducing the
need for new construction and improving our daily lives through reduced
congestion. A key component of ITS is traveler information. The Oregon
Department of Transportation (ODOT) recently implemented a new traveler
information system on selected freeways to provide drivers with travel time
estimates that allow them to make more informed decisions about routing to
their destinations. The ODOT project aims to improve traffic flow and promote
efficient traffic movement, which can reduce emissions rates and improve air
quality. The new ODOT system is based on travel data collected from a
recently-increased set of sensors installed on its freeways. Our current
project investigates novel data cleaning methodologies and the integration of
those methodologies into the prediction of travel times. We use machine
learning techniques on our archive to identify suspect data, and calculate
revised travel times excluding this suspect data. We compare the resulting
travel time predictions to ground-truth data, and to predictions based on
simple, rule-based data cleaning. We report on the results of our study using
qualitative and quantitative methods.
","['\nV. M. Megler\n', '\nKristin Tufte\n', '\nDavid Maier\n']",,,http://arxiv.org/abs/1602.03100v1,cs.OH,['cs.OH'],,,[]
"Customizable Precision of Floating-Point Arithmetic with Bitslice Vector
  Types",http://arxiv.org/abs/1602.04716v1,2016-02-15T15:59:38Z,2016-02-15T15:59:38Z,"  Customizing the precision of data can provide attractive trade-offs between
accuracy and hardware resources. We propose a novel form of vector computing
aimed at arrays of custom-precision floating point data. We represent these
vectors in bitslice format. Bitwise instructions are used to implement
arithmetic circuits in software that operate on customized bit-precision.
Experiments show that this approach can be efficient for vectors of
low-precision custom floating point types, while providing arbitrary bit
precision.
","['\nShixiong Xu\n', '\nDavid Gregg\n']",,,http://arxiv.org/abs/1602.04716v1,cs.OH,['cs.OH'],,,[]
"Encoding Distortion Modeling For DWT-Based Wireless EEG Monitoring
  System",http://arxiv.org/abs/1602.04974v1,2016-02-16T10:32:30Z,2016-02-16T10:32:30Z,"  Recent advances in wireless body area sensor net- works leverage wireless and
mobile communication technologies to facilitate development of innovative
medical applications that can significantly enhance healthcare services and
improve quality of life. Specifically, Electroencephalography (EEG)-based
applications lie at the heart of these promising technologies. However, the
design and operation of such applications is challenging. Power consumption
requirements of the sensor nodes may turn some of these applications
impractical. Hence, implementing efficient encoding schemes are essential to
reduce power consumption in such applications. In this paper, we propose an
analytical distortion model for the EEG-based encoding systems. Using this
model, the encoder can effectively reconfigure its complexity by adjusting its
control parameters to satisfy application constraints while maintaining
reconstruction accuracy at the receiver side. The simulation results illustrate
that the main parameters that affect the distortion are compression ratio and
filter length of the considered DWT-based encoder. Furthermore, it is found
that the wireless channel variations have a significant influence on the
estimated distortion at the receiver side.
","['\nAlaa Awad\n', '\nMedhat H. M. Elsayed\n', '\nAmr Mohamed\n']",,,http://arxiv.org/abs/1602.04974v1,cs.OH,['cs.OH'],,,[]
An Estimation Method Using Periodic Inspection of Indicators,http://arxiv.org/abs/1602.05656v1,2016-02-18T02:19:16Z,2016-02-18T02:19:16Z,"  This paper proposes a new approach for estimating the failure time
distribution using the indicator data. The indicators, which are checked by
periodic inspection of a standby redundant system, only convey whether at least
one failure occurs per interval. The estimation procedure first obtains the
estimation of the forward recurrence time using the indicator data. Then the
mean is estimated based on its relationship with the forward recurrence time.
And the estimation of the sampled Cdf is thus derived based on its relationship
with the forward recurrence time and the mean. Finally, the Cdf function is
estimated using interpolation method. The simulation results showed that the
estimation method performed well for the four Weibull distributions.
",['\nZheng Wang\n'],,,http://arxiv.org/abs/1602.05656v1,cs.OH,['cs.OH'],,,[]
"Reliability of Checking an Answer Given by a Mathematical Expression in
  Interactive Learning Systems",http://arxiv.org/abs/1602.00243v1,2016-01-31T13:26:26Z,2016-01-31T13:26:26Z,"  In this article we address the problem of automatic answer checking in
interactive learning systems that support mathematical notation. This problem
consists of the problem of establishing identities in formal mathematical
systems and hence is formally unsolvable. However, there is a way to cope with
the issue. We suggest to reinforce the standard algorithm for function
comparison with an additional pointwise checking procedure. An error might
appear in this case. The article provides a detailed analysis of the
probability of this error. It appears that the error probability is extremely
low in most common cases. Generally speaking, this means that such an
additional checking procedure can be quite successfully used in order to
support standard algorithms for functions comparison. The results, obtained in
this article, help avoiding some sudden effects of the identity problem, and
provide a way to estimate the reliability of answer checking procedure in
interactive learning systems.
","['\nVladimir G. Danilov\n', '\nIlya S. Turuntaev\n']","11 pages, 7 figures",,http://arxiv.org/abs/1602.00243v1,cs.OH,['cs.OH'],,,[]
"A Formal Approach to Power Optimization in CPSs with Delay-Workload
  Dependence Awareness",http://arxiv.org/abs/1601.08046v1,2016-01-29T10:38:27Z,2016-01-29T10:38:27Z,"  The design of cyber-physical systems (CPSs) faces various new challenges that
are unheard of in the design of classical real-time systems. Power optimization
is one of the major design goals that is witnessing such new challenges. The
presence of interaction between the cyber and physical components of a CPS
leads to dependence between the time delay of a computational task and the
amount of workload in the next iteration. We demonstrate that it is essential
to take this delay-workload dependence into consideration in order to achieve
low power consumption.
  In this paper, we identify this new challenge, and present the first formal
and comprehensive model to enable rigorous investigations on this topic. We
propose a simple power management policy, and show that this policy achieves a
best possible notion of optimality. In fact, we show that the optimal power
consumption is attained in a ""steady-state"" operation and a simple policy of
finding and entering this steady state suffices, which can be quite surprising
considering the added complexity of this problem. Finally, we validated the
efficiency of our policy with experiments.
","['\nHyung-Chan An\n', '\nHoeseok Yang\n', '\nSoonhoi Ha\n']","27 pages, 8 figures, 3 tables",,http://arxiv.org/abs/1601.08046v1,cs.OH,"['cs.OH', 'C.3']",,,[]
Fast inference of ill-posed problems within a convex space,http://arxiv.org/abs/1602.08412v1,2016-02-01T13:45:50Z,2016-02-01T13:45:50Z,"  In multiple scientific and technological applications we face the problem of
having low dimensional data to be justified by a linear model defined in a high
dimensional parameter space. The difference in dimensionality makes the problem
ill-defined: the model is consistent with the data for many values of its
parameters. The objective is to find the probability distribution of parameter
values consistent with the data, a problem that can be cast as the exploration
of a high dimensional convex polytope. In this work we introduce a novel
algorithm to solve this problem efficiently. It provides results that are
statistically indistinguishable from currently used numerical techniques while
its running time scales linearly with the system size. We show that the
algorithm performs robustly in many abstract and practical applications. As
working examples we simulate the effects of restricting reaction fluxes on the
space of feasible phenotypes of a {\em genome} scale E. Coli metabolic network
and infer the traffic flow between origin and destination nodes in a real
communication network.
","['\nJorge Fernandez-de-Cossio-Diaz\n', '\nRoberto Mulet\n']","25 pages, 11 figures",J. Stat. Mech. (2016) 073207,http://dx.doi.org/10.1088/1742-5468/2016/07/073207,cs.OH,"['cs.OH', 'cond-mat.dis-nn', 'cond-mat.stat-mech', 'q-bio.MN', '62F30, 52A20, 91D30', 'G.3; F.2.1; C.2.1; J.3']",10.1088/1742-5468/2016/07/073207,,[]
Superposition principle in linear networks with controlled sources,http://arxiv.org/abs/1601.04563v1,2016-01-18T15:14:28Z,2016-01-18T15:14:28Z,"  The manuscript discusses a well-known issue that, despite its fundamental
role in basic electric circuit theory, seems to be tackled without the needful
attention. The question if the Principle of Superposition (POS) can be applied
to linear networks containing linear dependent sources still appears as an
addressed point unworthy to be further discussed. Conversely, the analysis of
this point has been recently re-proposed [5,6] and an alternative conclusion
has been drawn. From this result, the manuscript provides an alternative
approach to such issue from a more general point of view. It is oriented to
clarify the issue from the didactic viewpoint, rather than provide a more
efficient general technique for circuit analysis. By starting from a linear
system of equations, representing a general linear circuit containing
controlled sources, the correct interpretation of turning off the controlled
elements in terms of circuit equivalent is provided, so allowing a statement of
the POS for linear circuits in a wider context. Further, this approach is
sufficiently intuitive and straightforward to fit the needs of a Basic Electric
Circuit Theory class.
",['\nCiro Visone\n'],,,http://arxiv.org/abs/1601.04563v1,cs.OH,['cs.OH'],,,[]
PGR: A Graph Repository of Protein 3D-Structures,http://arxiv.org/abs/1604.00045v1,2016-01-25T02:37:08Z,2016-01-25T02:37:08Z,"  Graph theory and graph mining constitute rich fields of computational
techniques to study the structures, topologies and properties of graphs. These
techniques constitute a good asset in bioinformatics if there exist efficient
methods for transforming biological data into graphs. In this paper, we present
Protein Graph Repository (PGR), a novel database of protein 3D-structures
transformed into graphs allowing the use of the large repertoire of graph
theory techniques in protein mining. This repository contains graph
representations of all currently known protein 3D-structures described in the
Protein Data Bank (PDB). PGR also provides an efficient online converter of
protein 3D-structures into graphs, biological and graph-based description,
pre-computed protein graph attributes and statistics, visualization of each
protein graph, as well as graph-based protein similarity search tool. Such
repository presents an enrichment of existing online databases that will help
bridging the gap between graph mining and protein structure analysis. PGR data
and features are unique and not included in any other protein database. The
repository is available at http://wjdi.bioinfo.uqam.ca/.
","['\nWajdi Dhifli\n', '\nAbdoulaye Baniré Diallo\n']",,,http://arxiv.org/abs/1604.00045v1,q-bio.BM,"['q-bio.BM', 'cs.OH']",,,[]
Pulse processing routines for neutron time-of-flight data,http://arxiv.org/abs/1601.04512v1,2016-01-18T13:46:08Z,2016-01-18T13:46:08Z,"  A pulse shape analysis framework is described, which was developed for
n_TOF-Phase3, the third phase in the operation of the n_TOF facility at CERN.
The most notable feature of this new framework is the adoption of generic pulse
shape analysis routines, characterized by a minimal number of explicit
assumptions about the nature of pulses. The aim of these routines is to be
applicable to a wide variety of detectors, thus facilitating the introduction
of the new detectors or types of detectors into the analysis framework. The
operational details of the routines are suited to the specific requirements of
particular detectors by adjusting the set of external input parameters. Pulse
recognition, baseline calculation and the pulse shape fitting procedure are
described. Special emphasis is put on their computational efficiency, since the
most basic implementations of these conceptually simple methods are often
computationally inefficient.
","['\nP. Žugec\n', '\nC. Weiß\n', '\nC. Guerrero\n', '\nF. Gunsing\n', '\nV. Vlachoudis\n', '\nM. Sabate-Gilarte\n', '\nA. Stamatopoulos\n', '\nT. Wright\n', '\nJ. Lerendegui-Marco\n', '\nF. Mingrone\n', '\nJ. A. Ryan\n', '\nS. G. Warren\n', '\nA. Tsinganis\n', '\nM. Barbagallo\n']","13 pages, 10 figures, 5 tables","Nuclear Instruments and Methods in Physics Research A 812 (2016)
  134-144",http://dx.doi.org/10.1016/j.nima.2015.12.054,physics.ins-det,"['physics.ins-det', 'cs.OH', 'nucl-ex']",10.1016/j.nima.2015.12.054,,[]
"Novel velocity model to improve indoor localization using inertial
  navigation with sensors on a smartphone",http://arxiv.org/abs/1601.03004v1,2016-01-12T19:20:21Z,2016-01-12T19:20:21Z,"  We present a generalized velocity model to improve localization when using an
Inertial Navigation System (INS). This algorithm was applied to correct the
velocity of a smart phone based indoor INS system to increase the accuracy by
counteracting the accumulation of large drift caused by sensor reading errors.
We investigated the accuracy of the algorithm with three different velocity
models which were derived from the actual velocity measured at the hip of
walking person. Our results show that the proposed method with Gaussian
velocity model achieves competitive accuracy with a 50\% less variance over
Step and Heading approach proving the accuracy and robustness of proposed
method. We also investigated the frequency of applying corrections and found
that a minimum of 5\% corrections per step is sufficient for improved accuracy.
The proposed method is applicable in indoor localization and tracking
applications based on smart phone where traditional approaches such as GNSS
suffers from many issues.
","['\nRasika Lakmal Hettiarachchige Don\n', '\nJagath Samarabandu\n']",,,http://arxiv.org/abs/1601.03004v1,cs.OH,['cs.OH'],,,[]
Transit directions at global scale,http://arxiv.org/abs/1601.03633v1,2016-01-13T00:49:49Z,2016-01-13T00:49:49Z,"  A novel approach to integrated ground and air public transport journey
planning, operating at continent scale. Flexible date search, prerequisite for
long distance trips given their typical low and irregular service frequencies,
is core functionality. The algorithm is especially suited for irregular and
poorly structured networks. Almost all of the described functionality is
implemented in a working prototype. Using ground transport only, the system is
on par with Google Transit on random country-wide trips in the US.
",['\nJoris van der Geer\n'],"13 pages, 3 tables",,http://arxiv.org/abs/1601.03633v1,cs.OH,"['cs.OH', '90-04 Operations research', 'I.2.8']",,,[]
"Decomposition of Time Series Data of Stock Markets and its Implications
  for Prediction: An Application for the Indian Auto Sector",http://arxiv.org/abs/1601.02407v1,2016-01-11T11:32:50Z,2016-01-11T11:32:50Z,"  With the rapid development and evolution of sophisticated algorithms for
statistical analysis of time series data, the research community has started
spending considerable effort in technical analysis of such data. Forecasting is
also an area which has witnessed a paradigm shift in its approach. In this
work, we have used the time series of the index values of the Auto sector in
India during January 2010 to December 2015 for a deeper understanding of the
behavior of its three constituent components, e.g., the Trend, the Seasonal
component, and the Random component. Based on this structural analysis, we have
also designed three approaches for forecasting and also computed their accuracy
in prediction using suitably chosen training and test data sets. The results
clearly demonstrate the accuracy of our decomposition results and efficiency of
our forecasting techniques, even in presence of a dominant Random component in
the time series.
","['\nJaydip Sen\n', '\nTamal Datta Chaudhuri\n']","14 pages, 2 figures, 4 tables. The paper is published in the
  Proceedings of the 2nd National Conference on Advances in Business Research
  and Management Practices (ABRMP'2016), January 8-9, 2016, Kolkata, INDIA",,http://dx.doi.org/10.13140/RG.2.1.3232.0241,q-fin.ST,"['q-fin.ST', 'cs.OH']",10.13140/RG.2.1.3232.0241,,[]
"Design of portable power consumption measuring system for green
  computing needs",http://arxiv.org/abs/1512.08201v1,2015-12-27T10:47:29Z,2015-12-27T10:47:29Z,"  The article presents the design of a digital power measurement device
intended for the green IT. Article comprises: use case analysis, accuracy and
precision measurements and real life test of apache web server as exemplary
application.
","['\nKamil Rzepka\n', '\nPrzemysław Skurowski\n', '\nBłażej Adamczyk\n', '\nAdam Pilśniak\n']","submitted to Studia Informatica ( http://studiainformatica.polsl.pl/
  )",,http://arxiv.org/abs/1512.08201v1,cs.OH,"['cs.OH', 'C.4']",,,[]
ePlace-3D: Electrostatics based Placement for 3D-ICs,http://arxiv.org/abs/1512.08291v5,2015-12-27T23:55:38Z,2016-02-28T06:22:02Z,"  We propose a flat, analytic, mixed-size placement algorithm ePlace-3D for
three-dimension integrated circuits (3D-ICs) using nonlinear optimization. Our
contributions are (1) electrostatics based 3D density function with globally
uniform smoothness (2) 3D numerical solution with improved spectral formulation
(3) 3D nonlinear pre-conditioner for convergence acceleration (4) interleaved
2D-3D placement for efficiency enhancement. Our placer outperforms the leading
work mPL6-3D and NTUplace3-3D with 6.44% and 37.15% shorter wirelength, 9.11%
and 10.27% fewer 3D vertical interconnects (VI) on average of IBM-PLACE
circuits. Validation on the large-scale modern mixed-size (MMS) 3D circuits
shows high performance and scalability.
","['\nJingwei Lu\n', '\nHao Zhuang\n', '\nIlgweon Kang\n', '\nPengwen Chen\n', '\nChung-Kuan Cheng\n']","8 pages, 7 figures, ISPD 2016",,http://dx.doi.org/10.1145/2872334.2872361,cs.OH,['cs.OH'],10.1145/2872334.2872361,,[]
Crowds for Clouds: Recent Trends in Humanities Research Infrastructures,http://arxiv.org/abs/1601.00533v1,2015-12-27T17:41:41Z,2015-12-27T17:41:41Z,"  Humanities have convincingly argued that they need transnational research
opportunities and through the digital transformation of their disciplines also
have the means to proceed with it on an up to now unknown scale. The digital
transformation of research and its resources means that many of the artifacts,
documents, materials, etc. that interest humanities research can now be
combined in new and innovative ways. Due to the digital transformations, (big)
data and information have become central to the study of culture and society.
Humanities research infrastructures manage, organise and distribute this kind
of information and many more data objects as they becomes relevant for social
and cultural research.
","['\nTobias Blanke\nCMB, ALPAGE\n', '\nConny Kristel\nCMB, ALPAGE\n', '\nLaurent Romary\nCMB, ALPAGE\n']",,"Agiati Benardou, Erik Champion, Costis Dallas, Lorna Hughes
  Cultural Heritage Digital Tools and Infrastructures, 2016, 978-1-4724-4712-8",http://arxiv.org/abs/1601.00533v1,cs.OH,['cs.OH'],,,"['CMB, ALPAGE', 'CMB, ALPAGE', 'CMB, ALPAGE']"
50+ Metrics for Calendar Mining,http://arxiv.org/abs/1601.06740v1,2016-01-01T07:39:55Z,2016-01-01T07:39:55Z,"  In this report we propose 50+ metrics which can be measured by organizations
in order to identify improvements in various areas such as meeting efficiency,
capacity planning or leadership skills, just to new a few. The notion of
calendar mining is introduced and support is provided for performing the
measurement by a reference data model and queries for all metrics defined.
","['\nZádor Dániel Kelemen\n', '\nDániel Miglász\n']",60 pages,,http://arxiv.org/abs/1601.06740v1,cs.OH,['cs.OH'],,,[]
"Finding Needles in a Haystack: Missing Tag Detection in Large RFID
  Systems",http://arxiv.org/abs/1512.05228v1,2015-12-16T16:17:57Z,2015-12-16T16:17:57Z,"  Radio frequency identification (RFID) technology has been widely used in
missing tag detection to reduce and avoid inventory shrinkage. In this
application, promptly finding out the missing event is of paramount importance.
However, existing missing tag detection protocols cannot efficiently handle the
presence of a large number of unexpected tags whose IDs are not known to the
reader, which shackles the time efficiency. To deal with the problem of
detecting missing tags in the presence of unexpected tags, this paper
introduces a two-phase Bloom filter-based missing tag detection protocol
(BMTD). The proposed BMTD exploits Bloom filter in sequence to first deactivate
the unexpected tags and then test the membership of the expected tags, thus
dampening the interference from the unexpected tags and considerably reducing
the detection time. Moreover, the theoretical analysis of the protocol
parameters is performed to minimize the detection time of the proposed BMTD and
achieve the required reliability simultaneously. Extensive experiments are then
conducted to evaluate the performance of the proposed BMTD. The results
demonstrate that the proposed BMTD significantly outperforms the
state-of-the-art solutions.
","['\nJihong Yu\n', '\nLin Chen\n', '\nKehao Wang\n']",,,http://arxiv.org/abs/1512.05228v1,cs.OH,['cs.OH'],,,[]
"Two improved normalized subband adaptive filter algorithms with good
  robustness against impulsive interferences",http://arxiv.org/abs/1512.05338v2,2015-12-16T04:41:30Z,2016-02-29T11:20:54Z,"  To improve the robustness of subband adaptive filter (SAF) against impulsive
interferences, we propose two modified SAF algorithms with an individual scale
function for each subband, which are derived by maximizing correntropy-based
cost function and minimizing logarithm-based cost function, respectively,
called MCC-SAF and LC-SAF. Whenever the impulsive interference happens, the
subband scale functions can sharply drop the step size, which eliminate the
influence of outliers on the tap-weight vector update. Therefore, the proposed
algorithms are robust against impulsive interferences, and exhibit the faster
convergence rate and better tracking capability than the sign SAF (SSAF)
algorithm. Besides, in impulse-free interference environments, the proposed
algorithms achieve similar convergence performance as the normalized SAF (NSAF)
algorithm. Simulation results have demonstrated the performance of our proposed
algorithms.
","['\nYi Yu\n', '\nHaiquan Zhao\n', '\nBadong Chen\n', '\nZhengyou He\n']","14 pages,8 figures,accepted by Circuits, Systems, and Signal
  Processing on Feb 23, 2016",,http://dx.doi.org/10.1007/s00034-016-0289-4,cs.OH,['cs.OH'],10.1007/s00034-016-0289-4,,[]
"On A Testing and Implementation of Quantum Gate and Measurement Emulator
  (QGAME)",http://arxiv.org/abs/1512.05499v1,2015-12-17T09:08:10Z,2015-12-17T09:08:10Z,"  Today, people are looking forward to get an awesome computational power. This
kind of desire can be answered by quantum computing. By adopting quantum
mechanics theory, it can generate a very fast computation result. As known,
quantum mechanics can establish that particle can also become wave; it shows
that electron can be in duality. Through this theory, even a human
teleportation is issued can be really happened in the future. However, it needs
a high requirement of hardware support to implement the real quantum computing.
That is why it is difficult to bring quantum computing into reality. This
research presents a study about quantum computing. Here it is studied, a
specialty of quantum computing, like superposition, as if the classical
computer can do it. Since there was a marvellous research about quantum
computer simulation that runs on classical computer, this research provides an
analysis about our testing and implementation of Quantum Gate and Measurement
Emulator (QGAME). Our analysis, testing and implementation are based on a
method that always use in the software engineering field.
","['\nA. B. Mutiara\n', '\nR. Refianti\n', '\nJ. S. K. Karamoy\n']","10 pages, 6 figures","International Journal of Engineering and Technology (IJET), Vol. 5
  No. 3 Jun-Jul 2013, pp. 2186-2195",http://arxiv.org/abs/1512.05499v1,cs.OH,['cs.OH'],,,[]
Energy Consumption Forecasting for Smart Meters,http://arxiv.org/abs/1512.05979v1,2015-12-18T14:59:26Z,2015-12-18T14:59:26Z,"  Earth, water, air, food, shelter and energy are essential factors required
for human being to survive on the planet. Among this energy plays a key role in
our day to day living including giving lighting, cooling and heating of
shelter, preparation of food. Due to this interdependency, energy, specifically
electricity, production and distribution became a high tech industry. Unlike
other industries, the key differentiator of electricity industry is the product
itself. It can be produced but cannot be stored for future; production and
consumption happen almost in near real-time. This particular peculiarity of the
industry is the key driver for Machine Learning and Data Science based
innovations in this industry. There is always a gap between the demand and
supply in the electricity market across the globe. To fill the gap and improve
the service efficiency through providing necessary supply to the market,
commercial as well as federal electricity companies employ forecasting
techniques to predict the future demand and try to meet the demand and provide
curtailment guidelines to optimise the electricity consumption/demand. In this
paper the authors examine the application of Machine Learning algorithms,
specifically Boosted Decision Tree Regression, to the modelling and forecasting
of energy consumption for smart meters. The data used for this exercise is
obtained from DECC data website. Along with this data, the methodology has been
tested in Smart Meter data obtained from EMA Singapore. This paper focuses on
feature engineering for time series forecasting using regression algorithms and
deriving a methodology to create personalised electricity plans offers for
household users based on usage history.
","['\nAnshul Bansal\n', '\nSusheel Kaushik Rompikuntla\n', '\nJaganadh Gopinadhan\n', '\nAmanpreet Kaur\n', '\nZahoor Ahamed Kazi\n']","Presented at BAI Conference 2015 at IIM Bangalore, India",,http://arxiv.org/abs/1512.05979v1,cs.OH,['cs.OH'],,,[]
"Cloud Computation and Google Earth Visualization of Heat/Cold Waves: A
  Nonanticipative Long-Range Forecasting Case Study",http://arxiv.org/abs/1512.06017v1,2015-12-18T16:24:13Z,2015-12-18T16:24:13Z,"  Long-range forecasting of heat/cold waves is a topical issue nowadays. High
computational complexity of the design of numerical and statistical models is a
bottleneck for the forecast process. In this work, Windows Server 2012 R2
virtual machines are used as a high-performance tool for the speed-up of the
computational process. Six D-series and one standard tier A-series virtual
machines were hosted in Microsoft Azure public cloud for this purpose.
Visualization of the forecasted data is based on the Google Earth Pro virtual
globe in ASP.NET web-site against http://gearth.azurewebsites.net (prototype),
where KMZ file represents geographic placemarks. The long-range predictions of
the heat/cold waves are computed for several specifically located places based
on nonanticipative analog algorithm. The arguments of forecast models are
datasets from around the world, which reflects the concept of teleconnections.
This methodology does not require the probability distribution to design the
forecast models and/or calculate the predictions. Heat weaves at Annaba
(Algeria) are discussed in detail. Up to 36.4% of heat waves are specifically
predicted. Up to 33.3% of cold waves are specifically predicted for other four
locations around the world. The proposed approach is 100% accurate if the signs
of predicted and actual values are compared according to climatological
baseline. These high-accuracy predictions were achieved due to the
interdisciplinary approach, but advanced computer science techniques, public
cloud computing and Google Earth Pro virtual globe mainly, form the major part
of the work.
",['\nDmytro Zubov\n'],"10 pages, 2 figures, 4 tables, 30 references. arXiv admin note: text
  overlap with arXiv:1507.03283",,http://arxiv.org/abs/1512.06017v1,cs.OH,"['cs.OH', '68U35']",,,[]
"A Novel Approach to Compress Centralized Text Data using Indexed
  Dictionary",http://arxiv.org/abs/1512.07153v1,2015-12-22T16:45:12Z,2015-12-22T16:45:12Z,"  Data compression is very important feature in terms of saving the memory
space. In this proposal, an indexed dictionary based compression is used for
text data, where the word's reference in dictionary is used for compression.
This approach is not file based, a common dictionary is used for compression.
Which contains the words, the position of the word in dictionary is one of the
key parts of encoded frame which is compressed form of the text word. This is
loss-less compression. This compression approach is also take cares of small
words like one or two characters words which usually decrease the efficiency of
compression algorithms. This approach is also deals with file having special
characters as a word. Special character words, alpha numeric words, normal
texted words and small words all deals differently which makes this approach
more efficient. Since a centralized dictionary is used for data compression,
therefore, this approach is not preferred for transfer compressed file, while
it is suitable to store text data in compressed form in hard disk drive and
centralized storage or cloud drive for memory utilization.
","['\nVivek Dimri\n', '\nProf. Ranjit Biswas\n']","Paper Accepted in journal: IJASCSE Volume 4, Issue 12 (December 2015)
  http://www.ijascse.org","IJASCSE Volume 4, Issue 12 (December 2015)",http://arxiv.org/abs/1512.07153v1,cs.OH,['cs.OH'],,,[]
"NexMon: A Cookbook for Firmware Modifications on Smartphones to Enable
  Monitor Mode",http://arxiv.org/abs/1601.07077v1,2015-12-24T17:19:20Z,2015-12-24T17:19:20Z,"  Full control over a Wi-Fi chip for research purposes is often limited by its
firmware, which makes it hard to evolve communication protocols and test
schemes in practical environments. Monitor mode, which allows eavesdropping on
all frames on a wireless communication channel, is a first step to lower this
barrier. Use cases include, but are not limited to, network packet analyses,
security research and testing of new medium access control layer protocols.
Monitor mode is generally offered by SoftMAC drivers that implement the media
access control sublayer management entity (MLME) in the driver rather than in
the Wi-Fi chip. On smartphones, however, mostly FullMAC chips are used to
reduce power consumption, as MLME tasks do not need to wake up the main
processor. Even though, monitor mode is also possible in FullMAC scenarios, it
is generally not implemented in today's Wi-Fi firmwares used in smartphones.
This work focuses on bringing monitor mode to Nexus 5 smartphones to enhance
the interoperability between applications that require monitor mode and BCM4339
Wi-Fi chips. The implementation is based on our new C-based programming
framework to extend existing Wi-Fi firmwares.
","['\nMatthias Schulz\n', '\nDaniel Wegemer\n', '\nMatthias Hollick\n']",Project website: https://seemoo.tu-darmstadt.de/nexmon,,http://arxiv.org/abs/1601.07077v1,cs.OH,['cs.OH'],,,[]
Emerging Cloud Computing Security Threats,http://arxiv.org/abs/1512.01701v1,2015-12-05T20:55:58Z,2015-12-05T20:55:58Z,"  Cloud computing is one of the latest emerging innovations of the modern
internet and technological landscape. With everyone from the White house to
major online technological leaders like Amazon and Google using or offering
cloud computing services it is truly presents itself as an exciting and
innovative method to store and use data on the internet.
",['\nKamal Ahmat\n'],,,http://arxiv.org/abs/1512.01701v1,cs.OH,['cs.OH'],,,[]
"Motion model transitions in GPS-IMU sensor fusion for user tracking in
  augmented reality",http://arxiv.org/abs/1512.02758v1,2015-12-09T05:51:11Z,2015-12-09T05:51:11Z,"  Finding the position of the user is an important processing step for
augmented reality (AR) applications. This paper investigates the use of
different motion models in order to choose the most suitable one, and
eventually reduce the Kalman filter errors in sensor fusion for such
applications where the accuracy of user tracking is crucial. A Deterministic
Finite Automaton (DFA) was employed using the innovation parameters of the
filter. Results show that the approach presented here reduces the filter error
compared to a static model and prevents filter divergence. The approach was
tested on a simple AR game in order to justify the accuracy and performance of
the algorithm.
",['\nErkan Bostanci\n'],"8 pages, journal",,http://arxiv.org/abs/1512.02758v1,cs.OH,['cs.OH'],,,[]
"Web application for size and topology optimization of trusses and gusset
  plates",http://arxiv.org/abs/1512.02881v1,2015-12-08T05:45:32Z,2015-12-08T05:45:32Z,"  With its ever growing popularity, providing Internet based applications tuned
towards practical applications is on the rise. Advantages such as no external
plugins and additional software, ease of use, updating and maintenance have
increased the popularity of web applications. In this work, a web-based
application has been developed which can perform size optimization of truss
structure as a whole as well as topology optimization of individual gusset
plate of each joint based on specified joint displacements and load conditions.
This application is developed using cutting-edge web technologies such as
Three.js and HTML5. The client side boasts of an intuitive interface which in
addition to its modeling capabilities also recommends configurations based on
user input, provides analysis options and finally displays the results. The
server side, using a combination of Scilab and DAKOTA, computes solution and
also provides the user with comparisons of the optimal design with that
conforming to Indian Standard (IS 800-2007). It is a freely available one-stop
web-based application to perform optimal and/or code based design of trusses.
","['\nShankarjee Krishnamoorthi\n', '\nGaurav Srivastava\n', '\nAmar Mandhyan\n']","17 pages, 8 figures, submitted to Structural Engineering and
  Mechanics",,http://arxiv.org/abs/1512.02881v1,cs.OH,['cs.OH'],,,[]
"Simulation and Analysis of Container Freight Train Operations at Port
  Botany",http://arxiv.org/abs/1512.03476v2,2015-12-10T22:52:18Z,2015-12-22T01:23:14Z,"  Over two million containers crossed the docks at Sydney's Port Botany in
2011/12; a figure that is forecast increase more than threefold by the end of
the next decade. To cope with such large growth in volumes the NSW Government
plans to double rail mode share at the port by the year 2020. Conventional
wisdom from industry and the media says that existing infrastructure cannot
handle such volumes. In this paper we use a combination of data analytics and
simulation to examine operations at the port and evaluate the efficacy of
current infrastructure to handle projected growth in volumes. Contrary to
conventional wisdom we find that current rail resources appear distinctly
under-utilised. Moreover: (i) the peak rail capacity of Port Botany is 1.78
million TEU per annum; over six times higher than 2011/12 rail volumes; (ii)
there are no infrastructural impediments to the achievement of peak rail
capacity; (iii) operational changes, not infrastructural investment, are the
key to unlocking the potential of the port; (iv) Port Botany is well positioned
to handle projected increases in container volumes over the next decade and
beyond, including the 28% rail mode share target established by the New South
Wales State Government.
","['\nDaniel Guimarans\n', '\nDaniel Harabor\n', '\nPascal van Hentenryck\n']",,,http://arxiv.org/abs/1512.03476v2,cs.OH,['cs.OH'],,,[]
Design and Implementation of an Antenna Model for the Cooja simulator,http://arxiv.org/abs/1610.06129v1,2015-12-08T07:57:03Z,2015-12-08T07:57:03Z,"  COOJA is a network simulator developed for wireless sensor networks. It can
be used for high-level algorithm development as well as low-level device driver
implementations for accurate simulation of wireless sensor networks before
deployment. However, in a simulation Cooja assumes that the nodes are only
equipped with omnidirectional antennas. There is currently no support for
directional antennas. Due to the growing interest in the use of directional or
smart antennas in wireless sensor networks, a model that can support
directional antennas is essential for the realistic simulations of protocols
relying on directional communication. This paper presents work on extending
COOJA with a directional antenna model.
",['\nVishwesh Rege\n'],"accepted for publication in International Conference on Electronics
  and Communication Systems (ICECS) - 2016",,http://arxiv.org/abs/1610.06129v1,cs.OH,['cs.OH'],,,[]
Pairwise Comparisons Rating Scale Paradox,http://arxiv.org/abs/1511.07540v2,2015-11-24T02:13:12Z,2015-12-01T17:09:36Z,"  This study demonstrates that incorrect data are entered into a pairwise
comparisons matrix for processing into weights for the data collected by a
rating scale. Unprocessed rating scale data lead to a paradox. A solution to
it, based on normalization, is proposed. This is an essential correction for
virtually all pairwise comparisons methods using rating scales. The
illustration of the relative error currently, taking place, is discussed.
",['\nW. W. Koczkodaj\n'],"12 pages, 3 figure, 1 table, progress report, (practically) ready for
  submission, call for cooperation, call for corrections of formerly published
  results (especially related to AHP) which may go into tens of thousands",,http://arxiv.org/abs/1511.07540v2,cs.OH,['cs.OH'],,,[]
"moGrams: a network-based methodology for visualizing the set of
  non-dominated solutions in multiobjective optimization",http://arxiv.org/abs/1511.08178v1,2015-11-24T12:11:43Z,2015-11-24T12:11:43Z,"  An appropriate visualization of multiobjective non-dominated solutions is a
valuable asset for decision making. Although there are methods for visualizing
the solutions in the design space, they do not provide any information about
their relationship. In this work, we propose a novel methodology that allows
the visualization of the non-dominated solutions in the design space and their
relationships by means of a network. The nodes represent the solutions in the
objective space, while the edges show the relationships between the solutions
in the design space. Our proposal (called moGrams) thus provides a joint
visualization of both objective and design spaces. It aims at helping the
decision maker to get more understanding of the problem so that (s)he can
choose the more appropriate final solution. moGrams can be applied to any
multicriteria problem in which the solutions are related by a similarity
metric. Besides, the decision maker interaction is facilitated by modifying the
network based on the current preferences to obtain a clearer view. An
exhaustive experimental study is performed using three multiobjective problems
in order to show both the usefulness and versatility of moGrams. The results
exhibit interesting characteristics of our methodology for visualizing and
analyzing solutions of multiobjective problems.
","['\nKrzysztof Trawiński\n', '\nManuel Chica\n', '\nDavid P. Pancho\n', '\nSergio Damas\n', '\nOscar Cordón\n']",,,http://arxiv.org/abs/1511.08178v1,cs.OH,['cs.OH'],,,[]
The Quasi cellular nets-based models of transport and logistic systems,http://arxiv.org/abs/1512.00312v1,2015-11-27T01:49:02Z,2015-11-27T01:49:02Z,"  There are many systems in different subjects such as industry, medicine,
transport, social and others, can be discribed on their dynamic of flows.
Nowadays models of flows consist of micro- and macro-models. In practice there
is a problem of convertation from different levels of simulation. In the
different articles author descriptes quasi cellular nets. Quasi cellular nets
are new type of discrete structures without signature. It may be used for
simulation instruments. This structures can simulate flows on micro- and macro
levels on the single model structure. In this article described using quasi
cellular nets in transport and logistics of open-cast mining.
",['\nAnton Aristov\n'],,"Reports of the XXIII XXIII International Scientific Symposium
  Miner's Week, 2015, Moscow : NUST MISIS., PP. 280-287",http://arxiv.org/abs/1512.00312v1,cs.OH,"['cs.OH', '68R99']",,,[]
"Computation of Transition Adjacency Relations Based on Complete Prefix
  Unfolding (Technical Report)",http://arxiv.org/abs/1512.00428v6,2015-11-28T14:09:10Z,2020-03-10T13:42:08Z,"  An increasing number of works have devoted to the application of Transition
Adjacency Relation (TAR) as a means to capture behavioral features of business
process models. In this paper, we systematically study the efficient TAR
derivation from process models using unfolding technique which previously has
been used to address the state space explosion when dealing with concurrent
behaviors of a Petri net. We reveal and formally describe the equivalence
between TAR and Event Adjacency Relation (EAR), the manifestation of TAR in the
Complete Prefix Unfolding (CPU) of a Petri net. By computing TARs from CPU
using this equivalence, we can alleviate the concurrency caused state-explosion
issues. Furthermore, structural boosting rules are categorized, proved and
added to the TAR computing algorithm. Formal proofs of correctness and
generality of CPU-based TAR computation are provided for the first time by this
work, and they significantly expand the range of Petri nets from which TARs can
be efficiently derived. Experiments on both industrial and synthesized process
models show the effectiveness of proposed CPU-based algorithms as well as the
observation that they scale well with the increase in size and concurrency of
business process models.
","['\nJisheng Pei\n', '\nLijie Wen\n', '\nAkhil Kumar\n', '\nXiaojun Ye\n']",Related to recent submission to TSC Journal,,http://arxiv.org/abs/1512.00428v6,cs.OH,['cs.OH'],,,[]
"Introduction of the Residue Number Arithmetic Logic Unit With Brief
  Computational Complexity Analysis",http://arxiv.org/abs/1512.00911v1,2015-12-03T00:20:50Z,2015-12-03T00:20:50Z,"  Digital System Research has pioneered the mathematics and design for a new
class of computing machine using residue numbers. Unlike prior art, the new
breakthrough provides methods and apparatus for general purpose computation
using several new residue based fractional representations. The result is that
fractional arithmetic may be performed without carry. Additionally, fractional
operations such as addition, subtraction and multiplication of a fraction by an
integer occur in a single clock period, regardless of word size. Fractional
multiplication is of the order O(p), where p equals the number of residues.
More significantly, complex operations, such as sum of products, may be
performed in an extended format, where fractional products are performed and
summed using single clock instructions, regardless of word width, and where a
normalization operation with an execution time of the order O(p) is performed
as a final step.
",['\nEric B. Olsen\n'],"20 pages, 2 figures, 5 tables, 3 graphs",,http://arxiv.org/abs/1512.00911v1,cs.OH,['cs.OH'],,,[]
"Multi-Number CVT-XOR Arithmetic Operations in any Base System and its
  Significant Properties",http://arxiv.org/abs/1601.04249v1,2015-11-30T13:19:36Z,2015-11-30T13:19:36Z,"  Carry Value Transformation (CVT) is a model of discrete dynamical system
which is one special case of Integral Value Transformations (IVTs). Earlier in
[5] it has been proved that sum of two non-negative integers is equal to the
sum of their CVT and XOR values in any base system. In the present study, this
phenomenon is extended to perform CVT and XOR operations for many non-negative
integers in any base system. To achieve that both the definition of CVT and XOR
are modified over the set of multiple integers instead of two. Also some
important properties of these operations have been studied. With the help of
cellular automata the adder circuit designed in [14] on using CVT-XOR
recurrence formula is used to design a parallel adder circuit for multiple
numbers in binary number system.
","['\nJayanta Kumar Das\n', '\nPabitra Pal Choudhury\n', '\nSudhakar Sahoo\n']","Pages-4, Tables-4, Figure-2",,http://arxiv.org/abs/1601.04249v1,cs.OH,['cs.OH'],,,[]
Preprint WebVRGIS Based Traffic Analysis and Visualization System,http://arxiv.org/abs/1511.06313v1,2015-11-19T19:16:17Z,2015-11-19T19:16:17Z,"  This is the preprint version of our paper on Advances in Engineering
Software. With several characteristics, such as large scale, diverse
predictability and timeliness, the city traffic data falls in the range of
definition of Big Data. A Virtual Reality GIS based traffic analysis and
visualization system is proposed as a promising and inspiring approach to
manage and develop traffic big data. In addition to the basic GIS interaction
functions, the proposed system also includes some intelligent visual analysis
and forecasting functions. The passenger flow forecasting algorithm is
introduced in detail.
","['\nXiaoming Li\n', '\nZhihan Lv\n', '\nWeixi Wang\n', '\nBaoyun Zhang\n', '\nJinxing Hu\n', '\nLing Yin\n', '\nShengzhong Feng\n']","This is the preprint version of our paper on Advances in Engineering
  Software. arXiv admin note: substantial text overlap with arXiv:1504.01057,
  arXiv:1504.01375",,http://arxiv.org/abs/1511.06313v1,cs.OH,['cs.OH'],,,[]
Analysis of SVN Repositories for Remote Access,http://arxiv.org/abs/1511.06722v1,2015-11-20T18:37:47Z,2015-11-20T18:37:47Z,"  Software Evolution is considered to be essential and challenging
characteristic in the field of software engineering. Version control system is
an incremental versions tracking system, introduced to avoid unnecessary
overwriting of files such as programming code, web pages and records. It also
helps to decrease the confusion affected by duplicate or outdated data. In this
proposed research SVN repository is maintained and analyzed for
msitone.wikispaces.com to minimize the efforts as well as resources for the
future users. We have used two semester data for the analysis purpose that is
observed SVN repository. The result shows that, implementing the SVN
repositories are helpful for maintenance of the Wikispaces as it also reduce
the cost, time and efforts for their evolution. Whereas without implementing
the SVN repositories Wikispaces were just supposed to be building the house by
putting each brick from start.
","['\n Sadaf\n', '\nSafeeullah Soomro\n', '\nSuhni Abbasi\n']",,,http://arxiv.org/abs/1511.06722v1,cs.OH,['cs.OH'],,,[]
WLAN Specific IoT Enable Power Efficient RAM Design on 40nm FPGA,http://arxiv.org/abs/1511.06759v1,2015-11-20T18:44:37Z,2015-11-20T18:44:37Z,"  Increasing the speed of computer is one of the important aspects of the
Random Access Memory (RAM) and for better and fast processing it should be
efficient. In this work, the main focus is to design energy efficient RAM and
it also can be accessed through internet. A 128-bit IPv6 address is added to
the RAM in order to control it via internet. Four different types of Low
Voltage CMOS (LCVMOS) IO standards are used to make it low power under five
different WLAN frequencies is taken. At WLAN frequency 2.4GHz, there is maximum
power reduction of 85% is achieved when LVCMOS12 is taken in place of LVCMOS25.
This design is implemented using Virtex-6 FPGA, Device xc6vlx75t and Package
FF484
","['\nTanesh Kumar\n', '\nFaizan Khan\n', '\nSafeeullah Soomro\n', '\nAreez Khalil Memon\n']",,,http://arxiv.org/abs/1511.06759v1,cs.OH,['cs.OH'],,,[]
"DGD Gallery: Storage, sharing, and publication of digital research data",http://arxiv.org/abs/1512.04364v1,2015-11-16T16:32:25Z,2015-11-16T16:32:25Z,"  We describe a project, called the ""Discretization in Geometry and Dynamics
Gallery"", or DGD Gallery for short, whose goal is to store geometric data and
to make it publicly available. The DGD Gallery offers an online web service for
the storage, sharing, and publication of digital research data.
","['\nMichael Joswig\n', '\nMilan Mehner\n', '\nStefan Sechelmann\n', '\nJan Techter\n', '\nAlexander I. Bobenko\n']","19 pages, 8 figures, to appear in ""Advances in Discrete Differential
  Geometry"", ed. A. I. Bobenko, Springer, 2016","""Advances in Discrete Differential Geometry"", ed. A. I. Bobenko,
  pp. 421-439, Springer, 2016",http://dx.doi.org/10.1007/978-3-662-50447-5,cs.OH,"['cs.OH', 'math.DG']",10.1007/978-3-662-50447-5,,[]
"Building a Decision Tree Model for Academic Advising Affairs Based on
  the Algorithm C 4-5",http://arxiv.org/abs/1511.04026v1,2015-11-10T19:47:05Z,2015-11-10T19:47:05Z,"  The ability to recognize students weakness and solve any problem that may
confront them in timely fashion is always a target for all educational
institutions. Thus, colleges and universities implement the so-called academic
advising affairs. On the academic advisor relies the responsibility of solving
any problem that may confront students learning progress. This paper shows how
the adviser can benefit from data mining techniques, namely decision trees
techniques. The C 4.5 algorithm is used as a method for building such trees.
The output is evaluated based on the accuracy measure, Kappa measure, and ROC
area. The difference between the registered and gained credit hours is
considered as the main attribute on which advisor can rely
",['\nMohammed Al-Sarem\n'],,,http://arxiv.org/abs/1511.04026v1,cs.OH,['cs.OH'],,,[]
Predictive and statistical analyses for academic advisory support,http://arxiv.org/abs/1601.04244v1,2015-11-07T14:06:22Z,2015-11-07T14:06:22Z,"  The ability to recognize weakness of students and solving any problem may
confront them in timely fashion is always a target of all educational
institutions. This study was designed to explore how can predictive and
statistical analysis support the academic work of adviser mainly in analysis
progress of students . The sample consisted of a total of 249 undergraduate
students: 46 % of them were Female and 54% Male. A one-way analysis of variance
and t-test were conducted to analysis if there was different behavior in
registering courses. Predictive data mining is used for support adviser in
decision making. Several classification techniques with 10-fold Cross
validation were applied. Among of them, C 4.5 constitutes the best agreement
among the finding results.
",['\nMohammed Al-Sarem\n'],,,http://arxiv.org/abs/1601.04244v1,cs.OH,['cs.OH'],,,[]
"The Virtual Experiences Lab - a platform for global collaborative
  engineering and beyond",http://arxiv.org/abs/1510.09077v1,2015-10-29T04:34:16Z,2015-10-29T04:34:16Z,"  We are developing the Virtual Experiences (Vx)Lab, a research and research
training infrastructure and capability platform for global collaboration. VxLab
comprises labs with visualisation capabilities, including underpinning
networking to global points of presence, videoconferencing and high-performance
computation, simulation and rendering, and sensors and actuators such as
robotic instruments locally and in connected remote labs. VxLab has been used
for industry projects in industrial automation, experimental research in cloud
deployment, workshops and remote capability demonstrations, teaching
advanced-level courses in games development, and student software engineering
projects. Our goal is for resources to become a ""catalyst"" for IT-driven
research results both within the university and with external industry
partners. Use cases include: multi-disciplinary collaboration, prototyping and
troubleshooting requiring multiple viewpoints and architectures, dashboards and
decision support for global remote planning and operations.
","['\nIan D. Peake\n', '\nJan Olaf Blech\n', '\nIan Thomas\n', '\nNicholas May\n', '\nHeinz W. Schmidt\n', '\nLasith Fernando\n', '\nRavi Sreenivasamurthy\n']","This is a pre-print of an extended talk abstract accepted for
  eResearch Australasia, Brisbane, October 2015",,http://arxiv.org/abs/1510.09077v1,cs.OH,['cs.OH'],,,[]
Spatial Prediction Under Location Uncertainty In Cellular Networks,http://arxiv.org/abs/1510.03638v2,2015-10-13T11:49:06Z,2016-03-14T21:50:18Z,"  Coverage optimization is an important process for the operator as it is a
crucial prerequisite towards offering a satisfactory quality of service to the
end-users. The first step of this process is coverage prediction, which can be
performed by interpolating geo-located measurements reported to the network by
mobile users' equipments. In previous works, we proposed a low complexity
coverage prediction algorithm based on the adaptation of the Geo-statistics
Fixed Rank Kriging (FRK) algorithm. We supposed that the geo-location
information reported with the radio measurements was perfect, which is not the
case in reality. In this paper, we study the impact of location uncertainty on
the coverage prediction accuracy and we extend the previously proposed
algorithm to include geo-location error in the prediction model. We validate
the proposed algorithm using both simulated and real field measurements. The
FRK extended to take into account the location uncertainty proves to enhance
the prediction accuracy while keeping a reasonable computational complexity.
","['\nHajer Braham\n', '\nSana Ben Jemaa\n', '\nGersende Fort\n', '\nEric Moulines\n', '\nBerna Sayrac\n']",,,http://arxiv.org/abs/1510.03638v2,cs.OH,['cs.OH'],,,[]
Fine-Grained Energy Modeling for the Source Code of a Mobile Application,http://arxiv.org/abs/1510.04165v2,2015-10-14T15:49:20Z,2016-05-18T12:29:40Z,"  Energy efficiency has a significant influence on user experience of
battery-driven devices such as smartphones and tablets. The goal of an energy
model for source code is to lay a foundation for the application of
energy-saving techniques during software development. The challenge is to
relate hardware energy consumption to high-level application code, considering
the complex run-time context and software stack. Traditional techniques build
the energy model by mapping a hardware energy model onto software constructs;
this approach faces obstacles when the software stack consists of a number of
abstract layers. Another approach that has been followed is to utilize hardware
or operating system features to estimate software energy information at a
coarse level of granularity such as blocks, methods or even applications. In
this paper, we explain how to construct a fine-grained energy model for the
source code, which is based on ""energy operations"" identified directly from the
source code and able to provide more valuable information for code
optimization. We apply the approach to a class of applications based on a
game-engine, and explain the wider applicability of the method.
","['\nXueliang Li\n', '\nJohn P. Gallagher\n']",10 pages,,http://arxiv.org/abs/1510.04165v2,cs.OH,['cs.OH'],,,[]
"Comparison of different Methods for Univariate Time Series Imputation in
  R",http://arxiv.org/abs/1510.03924v1,2015-10-13T23:16:10Z,2015-10-13T23:16:10Z,"  Missing values in datasets are a well-known problem and there are quite a lot
of R packages offering imputation functions. But while imputation in general is
well covered within R, it is hard to find functions for imputation of
univariate time series. The problem is, most standard imputation techniques can
not be applied directly. Most algorithms rely on inter-attribute correlations,
while univariate time series imputation needs to employ time dependencies. This
paper provides an overview of univariate time series imputation in general and
an in-detail insight into the respective implementations within R packages.
Furthermore, we experimentally compare the R functions on different time series
using four different ratios of missing data. Our results show that either an
interpolation with seasonal kalman filter from the zoo package or a linear
interpolation on seasonal loess decomposed data from the forecast package were
the most effective methods for dealing with missing data in most of the
scenarios assessed in this paper.
","['\nSteffen Moritz\n', '\nAlexis Sardá\n', '\nThomas Bartz-Beielstein\n', '\nMartin Zaefferer\n', '\nJörg Stork\n']",,,http://arxiv.org/abs/1510.03924v1,stat.AP,"['stat.AP', 'cs.OH']",,,[]
Implications of Burn-In Stress on NBTI Degradation,http://arxiv.org/abs/1510.01370v1,2015-10-05T21:24:18Z,2015-10-05T21:24:18Z,"  Burn-in is accepted as a way to evaluate ageing effects in an accelerated
manner. It has been suggested that burn-in stress may have a significant effect
on the Negative Bias Temperature Instability (NBTI) of subthreshold CMOS
circuits. This paper analyses the effect of burn-in on NBTI in the context of a
Digital to Analogue Converter (DAC) circuit. Analogue circuits require matched
device pairs; NBTI may cause mismatches and hence circuit failure. The NBTI
degradation observed in the simulation analysis indicates that under severe
stress conditions, a significant voltage threshold mismatch in the DAC beyond
the design specification of 2 mV limit can result. Experimental results confirm
the sensitivity of the DAC circuit design to NBTI resulting from burn-in.
","['\nMohd Azman Abdul Latif\n', '\nNoohul Basheer Zain Ali\n', '\nFawnizu Azmadi Hussin\n', '\nMark Zwolinski\n']",,,http://dx.doi.org/10.13140/RG.2.1.5057.0965,cs.OH,['cs.OH'],10.13140/RG.2.1.5057.0965,,[]
Model-independent comparison of simulation output,http://arxiv.org/abs/1509.09174v4,2015-09-30T13:48:38Z,2017-01-06T12:33:05Z,"  Computational models of complex systems are usually elaborate and sensitive
to implementation details, characteristics which often affect their
verification and validation. Model replication is a possible solution to this
issue. It avoids biases associated with the language or toolkit used to develop
the original model, not only promoting its verification and validation, but
also fostering the credibility of the underlying conceptual model. However,
different model implementations must be compared to assess their equivalence.
The problem is, given two or more implementations of a stochastic model, how to
prove that they display similar behavior? In this paper, we present a model
comparison technique, which uses principal component analysis to convert
simulation output into a set of linearly uncorrelated statistical measures,
analyzable in a consistent, model-independent fashion. It is appropriate for
ascertaining distributional equivalence of a model replication with its
original implementation. Besides model-independence, this technique has three
other desirable properties: a) it automatically selects output features that
best explain implementation differences; b) it does not depend on the
distributional properties of simulation output; and, c) it simplifies the
modelers' work, as it can be used directly on simulation outputs. The proposed
technique is shown to produce similar results to the manual or empirical
selection of output features when applied to a well-studied reference model.
","['\nNuno Fachada\n', '\nVitor V. Lopes\n', '\nRui C. Martins\n', '\nAgostinho C. Rosa\n']","The peer-reviewed version of this paper is published in Simulation
  Modelling Practice and Theory at
  http://dx.doi.org/10.1016/j.simpat.2016.12.013 . This version is typeset by
  the authors and differs only in pagination and typographical detail","Simulation Modelling Practice and Theory, 72C, pp. 131-149, 2017",http://dx.doi.org/10.1016/j.simpat.2016.12.013,cs.OH,"['cs.OH', '68U20', 'D.2.4; I.2.2; I.5.2; I.6.4; I.6.6']",10.1016/j.simpat.2016.12.013,,[]
"On The Evolution Of User Support Topics in Computational Science and
  Engineering Software",http://arxiv.org/abs/1510.01122v1,2015-10-05T12:19:46Z,2015-10-05T12:19:46Z,"  We investigate ten years of user support emails in the large-scale solver
library PETSc in order to identify changes in user requests. For this purpose
we assign each email thread to one or several categories describing the type of
support request. We find that despite several changes in hardware architecture
as well programming models, the relative share of emails for the individual
categories does not show a notable change over time. This is particularly
remarkable as the total communication volume has increased four-fold in the
considered time frame, indicating a considerable growth of the user base. Our
data also demonstrates that user support cannot be substituted with what is
often referred to as 'better documentation' and that the involvement of core
developers in user support is essential.
","['\nK. Rupp\n', '\nS. Balay\n', '\nJ. Brown\n', '\nM. Knepley\n', '\nL. C. McInnes\n', '\nB. Smith\n']","2 pages, 1 figure, whitepaper for the workshop ""Computational Science
  & Engineering Software Sustainability and Productivity Challenges""",,http://arxiv.org/abs/1510.01122v1,cs.OH,"['cs.OH', 'cs.MS', '68N01', 'D.2.7']",,,[]
Evolvable Autonomic Management,http://arxiv.org/abs/1510.01179v1,2015-09-23T06:50:21Z,2015-09-23T06:50:21Z,"  Autonomic management is aimed at adapting to uncertainty. Hence, it is
devised as m-connected k-dominating set problem, resembled by dominator and
dominate, such that dominators are resilient up to m-1 uncertainty among them
and dominate are resilient up to k-1 uncertainty on their way to dominators.
Therefore, an evolutionary algorithm GENESIS is proposed, which resolves
uncertainty by evolving population of solutions, while considering uncertain
constraints as sub-problems, started by initial populations by a greedy
algorithm AVIDO. Theoretical analysis first justifies original problem as
NP-hard problem. Eventually, the absence of polynomial time approximation
scheme necessitates justification of original problem as multiobjective
optimization problem. Furthermore, approximation to Pareto front is verified to
be decomposed into scalar optimization sub-problems, which lays out the
theoretical foundation for decomposition based evolutionary solution. Finally,
case-study, feasibility analysis and exemplary implication are presented for
evolvable autonomic management in combined cancer treatment with in-vivo sensor
networks.
",['\nRossi Kamal\n'],arXiv admin note: text overlap with arXiv:1508.03975,,http://arxiv.org/abs/1510.01179v1,cs.OH,['cs.OH'],,,[]
"Properties of Farey Sequence and their Applications to Digital Image
  Processing",http://arxiv.org/abs/1509.07757v1,2015-09-25T15:36:00Z,2015-09-25T15:36:00Z,"  Farey sequence has been a topic of interest to the mathematicians since the
very beginning of last century. With the emergence of various algorithms
involving the digital plane in recent times, several interesting works related
with the Farey sequence have come up. Our work is related with the problem of
searching an arbitrary fraction in a Farey sequence and its relevance to image
processing. Given an arbitrary fraction p/q (0 < p < q) and a Farey sequence Fn
of order n, we propose a novel algorithm using the Regula Falsi method and the
concept of Farey table to efficiently find the fraction of Fn closest to p/q.
All computations are in the integer domain only, which is its added benefit.
Some contemporary applications of image processing have also been shown where
such concepts can be incorporated. Experimental results have been furnished to
demonstrate its efficiency and elegance.
","['\nSoham Das\n', '\nKishaloy Halder\n', '\nSanjoy Pratihar\n', '\nPartha Bhowmick\n']",,,http://arxiv.org/abs/1509.07757v1,cs.OH,['cs.OH'],,,[]
"Yield, Area and Energy Optimization in Stt-MRAMs using failure aware ECC",http://arxiv.org/abs/1509.08806v2,2015-09-28T17:50:42Z,2016-06-16T23:41:04Z,"  Spin Transfer Torque MRAMs are attractive due to their non-volatility, high
density and zero leakage. However, STT-MRAMs suffer from poor reliability due
to shared read and write paths. Additionally, conflicting requirements for data
retention and write-ability (both related to the energy barrier height of the
magnet) makes design more challenging. Furthermore, the energy barrier height
depends on the physical dimensions of the free layer. Any variations in the
dimensions of the free layer lead to variations in the energy barrier height.
In order to address poor reliability of STT-MRAMs, usage of Error Correcting
Codes (ECC) have been proposed. Unlike traditional CMOS memory technologies,
ECC is expected to correct both soft and hard errors in STT_MRAMs. To achieve
acceptable yield with low write power, stronger ECC is required, resulting in
increased number of encoded bits and degraded memory efficiency. In this paper,
we propose Failure aware ECC (FaECC), which masks permanent faults while
maintaining the same correction capability for soft errors without increased
encoded bits. Furthermore, we investigate the impact of process variations on
run-time reliability of STT-MRAMs. We provide an analysis on the impact of
process variations on the life-time of the free layer and retention failures.
In order to analyze the effectiveness of our methodology, we developed a
cross-layer simulation framework that consists of device, circuit and array
level analysis of STT-MRAM memory arrays. Our results show that using FaECC
relaxes the requirements on the energy barrier height, which reduces the write
energy and results in smaller access transistor size and memory array area.
Keywords: STT-MRAM, reliability, Error Correcting Codes, ECC, magnetic memory
","['\nZoha Pajouhi\n', '\nXuanyao Fong\n', '\nAnand Raghunathan\n', '\nKaushik Roy\n']",This paper will be published in ACM JETC journal,,http://arxiv.org/abs/1509.08806v2,cs.OH,['cs.OH'],,,[]
Axiomatization of Inconsistency Indicators for Pairwise Comparisons,http://arxiv.org/abs/1509.03781v4,2015-09-12T21:57:17Z,2023-07-24T11:37:02Z,"  This study proposes revised axioms for defining inconsistency indicators in
pairwise comparisons. It is based on the new findings that ""PC submatrix cannot
have a worse inconsistency indicator than the PC matrix containing it"" and that
there must be a PC submatrix with the same inconsistency as the given PC
matrix.
  This study also provides better reasoning for the need of normalization. It
is a revision of axiomatization by Koczkodaj and Szwarc, 2014 which proposed
axioms expressed informally with some deficiencies addressed in this study.
","['\nW. W. Koczkodaj\n', '\nJ. -P. Magnot\n']","Project aborted, not published",,http://arxiv.org/abs/1509.03781v4,cs.OH,['cs.OH'],,,[]
Validation of daylighting model in CODYRUN building simulation code,http://arxiv.org/abs/1509.04738v1,2015-09-14T06:57:18Z,2015-09-14T06:57:18Z,"  CODYRUN is a multi-zone software integrating thermal building simulation,
airflow, and pollutant transfer. A first question thus arose as to the
integration of indoor lighting conditions into the simulation, leading to a new
model calculating natural and artificial lighting. The results of this new
daylighting module were then compared with results of other simulation codes
and experimental cases both in artificial and natural environments. Excellent
agreements were obtained, such as the values for luminous efficiencies in a
tropical and humid climate. In this paper, a comparison of the model output
with detailed measures is presented using a dedicated test cell in Reunion
Island (French overseas territory in the Indian Ocean), thus confirming the
interest for thermal and daylighting designs in low-energy buildings.
Introduction Several software packages are available for thermal and airflow
simulation in buildings. The most frequently used are ENERGY+ [1], ESP-r [2],
and TRNSYS [3]. These applications allow an increasing number of models to be
integrated, such as airflow, pollutant transport, and daylighting. In the
latter category, we may note ENERGY+, ESP-r and ECOTECT [4] software. After
more than 20 years of developing a specific code named CODYRUN, we decided to
add a lighting module to our software. This paper therefore provides some
details on this evolution and elements of validation. The CODYRUN initial
software and its validation Developed by the Physics and Mathematical
Engineering Laboratory for Energy and Environment at the University of Reunion
Island, CODYRUN [5-14] is a multi-zone software program integrating ventilation
and moisture transport transfer in buildings. The software employs a zone
approach based on nodal analysis and resolves a coupled system describing
thermal and airflow phenomena. Numerous validation tests of the CODYRUN code
were successfully applied to the software. Apart from the daylighting model,
the majority applied the BESTEST procedure [15]. The International Energy
Agency (IEA) sponsors a number of programs to improve the use and associated
technologies of energy. The National Renewable Energy Laboratory (NREL)
developed BESTEST, which is a method based on comparative testing of building
simulation programs, on the IEA's behalf. The procedure consists of a series of
test cases buildings that are designed to isolate individual aspects of
building energy and test the extremes of a program. As the modelling approach
is very different between codes, the test cases are specified so that input
equivalency can be defined thus allowing the different cases to be modelled by
most of codes. The basis for comparison is a range of results from a number of
programs considered to be a state-of-art in United States and Europe.
Associated with other specific comparisons, a very confident level of
validation was obtained for the CODYRUN initial software [8].
","['\nH. Boyer\nPIMENT\n', '\nS. Guichard\nPIMENT\n', '\nA. Jean\nPIMENT\n', '\nT. Libelle\nPIMENT\n', '\nDimitri Bigot\nPIMENT\n', '\nF. Miranville\nPIMENT\n', '\nM. Bojić\n']","ICRET 2014 : 2014 International Conference on Renewable Energy
  Technologies, Nov 2014, Hong Kong, China. 2014",,http://arxiv.org/abs/1509.04738v1,cs.OH,['cs.OH'],,,"['PIMENT', 'PIMENT', 'PIMENT', 'PIMENT', 'PIMENT', 'PIMENT']"
"Problem of optimization of a transport traffic at preliminary
  registration of queires with use of CBSMAP-model",http://arxiv.org/abs/1509.05022v1,2015-09-14T06:17:24Z,2015-09-14T06:17:24Z,"  The problem of optimization of a transport traffic at preliminary
registration of demands with use of the CBSMAP model is investigated. For the
solution of an objective application of the queueing theory and the theory of
controlled processes is supposed.
",['\nElizaveta V. Kondrashova\n'],,,http://arxiv.org/abs/1509.05022v1,cs.OH,['cs.OH'],,,[]
A balanced rail-to-rail all digital comparator using only standard cells,http://arxiv.org/abs/1509.05192v2,2015-09-17T10:11:02Z,2016-05-10T04:09:32Z,"  An all-digital comparator with full input range is presented. It outperforms
the nowaday all-digital comparators with its large rail-to-rail input range.
This is achieved by the proposed Yin-yang balance mechanism between the two
logic gates: NAND3 and OAI (Or-And-Invert). The important design considerations
to achieve this balance are presented, such as the driving strength
manipulation and the use of pre-distortion technique. Constructed only by
commercially available digital standard cells, the layout of the proposed
comparator is generated automatically by standard digital Place & Route routine
within several minutes. The Verilog code for the proposed circuit is given, and
the circuit is successfully implemented in 130nm CMOS technology with the power
consumption of 0.176mW at the clock of 330MHz.
","['\nBo Wang\n', '\nKezhi Li\n', '\nZhongjian Chen\n', '\nXinan Wang\n']","This paper has been withdrawn by the author due to the imperfection
  of the circuit, i.e., the long reset time of hundreds ns to several us",,http://arxiv.org/abs/1509.05192v2,cs.OH,['cs.OH'],,,[]
"GCC-Plugin for Automated Accelerator Generation and Integration on
  Hybrid FPGA-SoCs",http://arxiv.org/abs/1509.00025v2,2015-08-28T09:15:49Z,2015-09-23T11:35:57Z,"  In recent years, architectures combining a reconfigurable fabric and a
general purpose processor on a single chip became increasingly popular. Such
hybrid architectures allow extending embedded software with application
specific hardware accelerators to improve performance and/or energy efficiency.
Aiding system designers and programmers at handling the complexity of the
required process of hardware/software (HW/SW) partitioning is an important
issue. Current methods are often restricted, either to bare-metal systems, to
subsets of mainstream programming languages, or require special coding
guidelines, e.g., via annotations. These restrictions still represent a high
entry barrier for the wider community of programmers that new hybrid
architectures are intended for. In this paper we revisit HW/SW partitioning and
present a seamless programming flow for unrestricted, legacy C code. It
consists of a retargetable GCC plugin that automatically identifies code
sections for hardware acceleration and generates code accordingly. The proposed
workflow was evaluated on the Xilinx Zynq platform using unmodified code from
an embedded benchmark suite.
","['\nMarkus Vogt\n', '\nGerald Hempel\n', '\nJeronimo Castrillon\n', '\nChristian Hochberger\n']","Presented at Second International Workshop on FPGAs for Software
  Programmers (FSP 2015) (arXiv:1508.06320)",,http://arxiv.org/abs/1509.00025v2,cs.OH,['cs.OH'],,,[]
"A Comparison of High-Level Design Tools for SoC-FPGA on Disparity Map
  Calculation Example",http://arxiv.org/abs/1509.00036v1,2015-08-28T08:59:11Z,2015-08-28T08:59:11Z,"  Modern SoC-FPGA that consists of FPGA with embedded ARM cores is being
popularized as an embedded vision system platform. However, the design approach
of SoC-FPGA applications still follows traditional hardware-software separate
workflow, which becomes the barrier of rapid product design and iteration on
SoC-FPGA. High-Level Synthesis (HLS) and OpenCL-based system-level design
approaches provide programmers the possibility to design SoC-FGPA at
system-level with an unified development environment for both hardware and
software. To evaluate the feasibility of high-level design approach especially
for embedded vision applications, Vivado HLS and Altera SDK for OpenCL,
representative and most popular commercial tools in market, are selected as
evaluation design tools, disparity map calculation as targeting application. In
this paper, hardware accelerators of disparity map calculation are designed
with both tools and implemented on Zedboard and SoCKit development board,
respectively. Comparisons between design tools are made in aspects of
supporting directives, accelerator design process, and generated hardware
performance. The results show that both tools can generate efficient hardware
for disparity map calculation application with much less developing time.
Moreover, we can also state that, more directives (e.g., interface type, array
reshape, resource type specification) are supported, but more hardware
knowledge is required, in Vivado HLS. In contrast, Altera SDK for OpenCL is
relatively easier for software programmers who is new to hardware, but with the
price of more resources usage on FPGA for similar hardware accelerator
generation.
","['\nShaodong Qin\n', '\nMladen Berekovic\n']","Presented at Second International Workshop on FPGAs for Software
  Programmers (FSP 2015) (arXiv:1508.06320)",,http://arxiv.org/abs/1509.00036v1,cs.OH,['cs.OH'],,,[]
Target Tracking in Confined Environments with Uncertain Sensor Positions,http://arxiv.org/abs/1509.00238v1,2015-09-01T11:45:56Z,2015-09-01T11:45:56Z,"  To ensure safety in confined environments such as mines or subway tunnels, a
(wireless) sensor network can be deployed to monitor various environmental
conditions. One of its most important applications is to track personnel,
mobile equipment and vehicles. However, the state-of-the-art algorithms assume
that the positions of the sensors are perfectly known, which is not necessarily
true due to imprecise placement and/or dropping of sensors. Therefore, we
propose an automatic approach for simultaneous refinement of sensors' positions
and target tracking. We divide the considered area in a finite number of cells,
define dynamic and measurement models, and apply a discrete variant of belief
propagation which can efficiently solve this high-dimensional problem, and
handle all non-Gaussian uncertainties expected in this kind of environments.
Finally, we use ray-tracing simulation to generate an artificial mine-like
environment and generate synthetic measurement data. According to our extensive
simulation study, the proposed approach performs significantly better than
standard Bayesian target tracking and localization algorithms, and provides
robustness against outliers.
","['\nVladimir Savic\n', '\nHenk Wymeersch\n', '\nErik G. Larsson\n']","IEEE Transactions on Vehicular Technology, 2015",,http://dx.doi.org/10.1109/TVT.2015.2404132,cs.OH,['cs.OH'],10.1109/TVT.2015.2404132,,[]
Confluent Orthogonal Drawings of Syntax Diagrams,http://arxiv.org/abs/1509.00818v1,2015-09-02T18:54:49Z,2015-09-02T18:54:49Z,"  We provide a pipeline for generating syntax diagrams (also called railroad
diagrams) from context free grammars. Syntax diagrams are a graphical
representation of a context free language, which we formalize abstractly as a
set of mutually recursive nondeterministic finite automata and draw by
combining elements from the confluent drawing, layered drawing, and smooth
orthogonal drawing styles. Within our pipeline we introduce several heuristics
that modify the grammar but preserve the language, improving the aesthetics of
the final drawing.
","['\nMichael J. Bannister\n', '\nDavid A. Brown\n', '\nDavid Eppstein\n']",GD 2015,,http://arxiv.org/abs/1509.00818v1,cs.OH,"['cs.OH', 'cs.FL']",,,[]
"Missing Spectrum-Data Recovery in Cognitive Radio Networks Using
  Piecewise Constant Nonnegative Matrix Factorization",http://arxiv.org/abs/1508.07269v1,2015-08-28T16:40:50Z,2015-08-28T16:40:50Z,"  In this paper, we propose a missing spectrum data recovery technique for
cognitive radio (CR) networks using Nonnegative Matrix Factorization (NMF). It
is shown that the spectrum measurements collected from secondary users (SUs)
can be factorized as product of a channel gain matrix times an activation
matrix. Then, an NMF method with piecewise constant activation coefficients is
introduced to analyze the measurements and estimate the missing spectrum data.
The proposed optimization problem is solved by a Majorization-Minimization
technique. The numerical simulation verifies that the proposed technique is
able to accurately estimate the missing spectrum data in the presence of noise
and fading.
","['\nAlireza Zaeemzadeh\n', '\nMohsen Joneidi\n', '\nBehzad Shahrasbi\n', '\nNazanin Rahnavard\n']","6 pages, 6 figures, Accepted for presentation in MILCOM'15 Conference",,http://dx.doi.org/10.1109/MILCOM.2015.7357449,cs.OH,"['cs.OH', 'cs.IT', 'math.IT']",10.1109/MILCOM.2015.7357449,,[]
"Passenger Flow Predictions at Sydney International Airport: A
  Data-Driven Queuing Approach",http://arxiv.org/abs/1508.04839v1,2015-08-20T00:24:28Z,2015-08-20T00:24:28Z,"  Time spent in processing zones at an airport are an important part of the
passenger's airport experience. It undercuts the time spent in the rest of the
airport, and therefore the revenue that could be generated from shopping and
dining. It can also result in passengers missing flights and connections, which
has significant operational repercussions. Inadequate staffing levels are often
to blame for large congestion at an airport. In this paper, we present a
stochastic simulation that estimates the operational uncertainty in passenger
processing at immigration. Congestion and delays are estimated on arrivals and
departures based on scheduled flight departures and arrivals. We demonstrate
the use of cellular tracking data in refining the model, and an approach to
controlling congestion by adjusting staffing levels.
","['\nHarold Nikoue\n', '\nAude Marzuoli\n', '\nJohn-Paul Clarke\n', '\nEric Feron\n', '\nJim Peters\n']",,,http://arxiv.org/abs/1508.04839v1,cs.OH,['cs.OH'],,,[]
"On environments as systemic exoskeletons: Crosscutting optimizers and
  antifragility enablers",http://arxiv.org/abs/1508.01869v4,2015-08-08T08:31:52Z,2015-10-19T06:21:35Z,"  Classic approaches to General Systems Theory often adopt an individual
perspective and a limited number of systemic classes. As a result, those
classes include a wide number and variety of systems that result equivalent to
each other. This paper introduces a different approach: First, systems
belonging to a same class are further differentiated according to five major
general characteristics. This introduces a ""horizontal dimension"" to system
classification. A second component of our approach considers systems as nested
compositional hierarchies of other sub-systems. The resulting ""vertical
dimension"" further specializes the systemic classes and makes it easier to
assess similarities and differences regarding properties such as resilience,
performance, and quality-of-experience. Our approach is exemplified by
considering a telemonitoring system designed in the framework of Flemish
project ""Little Sister"". We show how our approach makes it possible to design
intelligent environments able to closely follow a system's horizontal and
vertical organization and to artificially augment its features by serving as
crosscutting optimizers and as enablers of antifragile behaviors.
",['\nVincenzo De Florio\n'],"Accepted for publication in the Journal of Reliable Intelligent
  Environments. Extends conference papers [10,12,15]. The final publication is
  available at Springer via http://dx.doi.org/10.1007/s40860-015-0006-2",,http://arxiv.org/abs/1508.01869v4,cs.OH,['cs.OH'],,,[]
Can JSP Code be Generated Using XML Tags?,http://arxiv.org/abs/1508.02557v1,2015-08-11T11:16:31Z,2015-08-11T11:16:31Z,"  Over the years, a variety of web services have started using server-side
scripting to deliver results back to a client as a paid or free service; one
such server-side scripting language is Java Server Pages (JSP). Also Extensible
markup language (XML), is being adopted by most web developers as a tool to
describe data.Therefore, we present a conversion method which uses predefined
XML tags as input and generates the corresponding JSP code. However, the end
users are required to have a basic experience with web pages. This conversion
method aims to reduce the time and effort spent by the user (web developer) to
get acquainted with JSP. The conversion process abstracts the user from the
intricacies of JSP and enables him to focus on the business logic.
","['\nNeha Bothra\n', '\nKritika Jain\n', '\nSanjay Chakraborty\n']",,,http://dx.doi.org/10.15864/ajac.v2i3.138,cs.OH,['cs.OH'],10.15864/ajac.v2i3.138,,[]
Cracking Intel Sandy Bridge's Cache Hash Function,http://arxiv.org/abs/1508.03767v1,2015-08-15T20:05:20Z,2015-08-15T20:05:20Z,"  On Intel Sandy Bridge processor, last level cache (LLC) is divided into cache
slices and all physical addresses are distributed across the cache slices using
an hash function. With this undocumented hash function existing, it is
impossible to implement cache partition based on page coloring. This article
cracks the hash functions on two types of Intel Sandy processors by converting
the problem of cracking the hash function to the problem of classifying data
blocks into different groups based on eviction relationship existing between
data blocks that are mapped to the same cache set. Based on the cracking
result, this article proves that it's possible to implement cache partition
based on page coloring on cache indexed by hashing.
","['\nZhipeng Wei\n', '\nZehan Cui\n', '\nMingyu Chen\n']",,,http://arxiv.org/abs/1508.03767v1,cs.OH,['cs.OH'],,,[]
"Determining rural areas vulnerable to illegal dumping using GIS
  techniques. Case study: Neamt county, Romania",http://arxiv.org/abs/1507.00627v1,2015-07-02T15:25:12Z,2015-07-02T15:25:12Z,"  The paper aims to mapping the potential vulnerable areas to illegal dumping
of household waste from rural areas in the extra- Carpathian region of Neamt
County. These areas are ordinary in the proximity of built-up areas and buffers
areas of 1 km were delimited for every locality. Based on various map layers in
vector formats (land use, rivers, built-up areas, roads etc) an assessment
method is performed to highlight the potential areas vulnerable to illegal
dumping inside these buffer areas at local scale. The results are correlated to
field observations and current situation of waste management systems. The maps
outline local disparities due to various geographical conditions of county.
This approach is a necessary tool in EIA studies particularly for rural waste
management systems at local and regional scale which are less studied in
current literature than urban areas.
","['\nFlorin-Constantin Mihai\n', '\nAdrian Ursu\n', '\nPavel Ichim\n', '\nDan-Adrian Chelaru\n']",,"13th International Multidisciplinary Scientific GeoConference on
  ECOLOGY, ECONOMICS, EDUCATION AND LEGISLATION, SGEM 2013 Conference
  Proceedings vol 1 : 275-282",http://dx.doi.org/10.5593/SGEM2013/BE5.V1/S20.037,cs.OH,"['cs.OH', 'H.2.8; J.2; I.4.8']",10.5593/SGEM2013/BE5.V1/S20.037,,[]
Review on the Design of Web Based SCADA Systems Based on OPC DA Protocol,http://arxiv.org/abs/1506.05069v1,2015-06-15T12:59:07Z,2015-06-15T12:59:07Z,"  One of the most familiar SCADA (supervisory control and data acquisition)
application protocols now is OPC protocol. This interface is supported by
almost all SCADA, visualization, and process control systems. There are many
research efforts tried to design and implement an approach to access an OPC DA
server through the Internet. To achieve this goal they used diverse of modern
IT technologies like XML, Web services, Java and AJAX. In this paper, we
present a complete classification of the different approaches introduced in the
literature. A comparative study is also introduced. Finally we study the
feasibility of the realization of these approaches based on the real time
constraints imposed by the nature of the problem.
","['\nHosny A. Abbas\n', '\nAhmed M. Mohamed\n']",review,,http://arxiv.org/abs/1506.05069v1,cs.OH,['cs.OH'],,,[]
Low Power Wideband Sensing for One-Bit Quantized Cognitive Radio Systems,http://arxiv.org/abs/1506.01753v1,2015-06-03T02:34:24Z,2015-06-03T02:34:24Z,"  We proposes an ultra low power wideband spectrum sensing architecture by
utilizing a one-bit quantization at the cognitive radio (CR) receiver. The
impact of this aggressive quantization is quantified and it is shown that the
proposed method is robust to low signal-to-noise ratios (SNR). We derive
closed-form expressions for both false alarm and detection probabilities. The
sensing performance and the analytical results are assessed through comparisons
with respective results from computer simulations. The results indicate that
the proposed method provides significant saving in power, complexity, and
sensing period on the account of an acceptable range of performance
degradation.
","['\nAbdelmohsen Ali\n', '\nWalaa Hamouda\n']",,,http://arxiv.org/abs/1506.01753v1,cs.OH,['cs.OH'],,,[]
"A few reflections on the quality of emergence in complex collective
  systems",http://arxiv.org/abs/1506.01821v2,2015-06-05T08:58:31Z,2016-08-11T08:28:17Z,"  A number of elements towards a classification of the quality of emergence in
emergent collective systems are provided. By using those elements, several
classes of emergent systems are exemplified, ranging from simple aggregations
of simple parts up to complex organizations of complex collective systems. In
so doing, the factors likely to play a a significant role in the persistence of
emergence and its opposite are highlighted. From this, new elements for
discussion are identified also considering elements from the System of Leibniz.
",['\nVincenzo De Florio\n'],"Paper accepted for publication in Mohammad Essaaidi, Mohamed Nemiche
  (Eds.) ""Advances in Complex Societal, Environmental and Engineered Systems"",
  Springer series ""Nonlinear Systems and Complexity"", Springer, 2016",,http://arxiv.org/abs/1506.01821v2,cs.OH,['cs.OH'],,,[]
"Increasing loyalty using predictive modeling in Business-to-Business
  Telecommunication",http://arxiv.org/abs/1506.03214v1,2015-06-10T08:41:41Z,2015-06-10T08:41:41Z,"  Customer Relationship Management (CRM) is a key element of modern marketing
strategies. One of the most practical way to build useful knowledge on
customers in a CRM system to produce scores to forecast churn behavior,
propensity to subscribe to a new service... In AMEA zone (Asia, Middle East and
Africa zone), the context of fierce competition may represent a higher
percentage, and particularly in B2B market. But by contrast, to our knowledge,
no scientific papers were dedicated and published to detail the way to improve
loyalty in B2B Telco market. If we can assume at low segments similarities
between B2B and B2C, some research is required in order to model B2B user
behavior versus B2C behavior. This problematic stands actual as ""Bring Your own
Device"" (BYOD) becomes more and more trendy. Answering business requirements,
our team applied some B2C predictive tools with adapting them to B2B in an AMEA
country Orange affiliate.
","['\nPatrick Luciano\n', '\nIsmail Rebai\n', '\nVincent Lemaire\n']",,,http://arxiv.org/abs/1506.03214v1,cs.OH,"['cs.OH', 'cs.IR']",,,[]
Limitations of PLL simulation: hidden oscillations in MatLab and SPICE,http://arxiv.org/abs/1506.02484v2,2015-06-05T05:04:00Z,2015-09-06T08:03:44Z,"  Nonlinear analysis of the phase-locked loop (PLL) based circuits is a
challenging task, thus in modern engineering literature simplified mathematical
models and simulation are widely used for their study. In this work the
limitations of numerical approach is discussed and it is shown that, e.g.
hidden oscillations may not be found by simulation. Corresponding examples in
SPICE and MatLab, which may lead to wrong conclusions concerning the
operability of PLL-based circuits, are presented.
","['\nG. Bianchi\n', '\nN. V. Kuznetsov\n', '\nG. A. Leonov\n', '\nM. V. Yuldashev\n', '\nR. V. Yuldashev\n']",,"IEEE 2015 7th International Congress on Ultra Modern
  Telecommunications and Control Systems and Workshops (ICUMT), 2015, pp. 79-84",http://dx.doi.org/10.1109/ICUMT.2015.7382409,cs.OH,"['cs.OH', 'math.DS', 'nlin.CD']",10.1109/ICUMT.2015.7382409,,[]
"Coherent 100G Nonlinear Compensation with Single-Step Digital
  Backpropagation",http://arxiv.org/abs/1507.00921v1,2015-06-09T21:11:05Z,2015-06-09T21:11:05Z,"  Enhanced-SSFM digital backpropagation (DBP) is experimentally demonstrated
and compared to conventional DBP. A 112 Gb/s PM-QPSK signal is transmitted over
a 3200 km dispersion-unmanaged link. The intradyne coherent receiver includes
single-step digital backpropagation based on the enhanced-SSFM algorithm. In
comparison, conventional DBP requires twenty steps to achieve the same
performance. An analysis of the computational complexity and structure of the
two algorithms reveals that the overall complexity and power consumption of DBP
are reduced by a factor of 16 with respect to a conventional implementation,
while the computation time is reduced by a factor of 20. As a result, the
proposed algorithm enables a practical and effective implementation of DBP in
real-time optical receivers, with only a moderate increase of the computational
complexity, power consumption, and latency with respect to a simple
feed-forward equalizer for dispersion compensation.
","['\nMarco Secondini\n', '\nSimon Rommel\n', '\nFrancesco Fresi\n', '\nEnrico Forestieri\n', '\nGianluca Meloni\n', '\nLuca Potì\n']","This work has been presented at Optical Networks Design & Modeling
  (ONDM) 2015, Pisa, Italy, May 11-14, 2015",,http://arxiv.org/abs/1507.00921v1,cs.OH,"['cs.OH', 'cs.IT', 'math.IT']",,,[]
Sensitivity Analysis of Resonant Circuits,http://arxiv.org/abs/1506.00632v1,2015-05-30T00:47:22Z,2015-05-30T00:47:22Z,"  We use first-order perturbation theory to provide a local linear relation
between the circuit parameters and the poles of an RLC network. The sensitivity
matrix, which defines this relationship, is obtained from the systems
eigenvectors and the derivative of its eigenvalues. In general, the sensitivity
matrix is related to the equilibrium fluctuations of the system. In particular,
it may be used as the basis for a statistical model to efficiently predict the
sensitivity of the circuit response to small component variations. The method
is illustrated with a calculation of conditional probabilities by Monte Carlo
Simulation.
",['\nOlivier Buu\n'],,,http://arxiv.org/abs/1506.00632v1,cs.OH,['cs.OH'],,,[]
"Lossless Layout Image Compression Algorithms for Electron-Beam
  Direct-Write Lithography",http://arxiv.org/abs/1505.06494v2,2015-05-24T22:48:00Z,2015-08-18T15:40:19Z,"  Electron-beam direct-write (EBDW) lithography systems must in the future
transmit terabits of information per second to be viable for commercial
semiconductor manufacturing. Lossless layout image compression algorithms with
high decoding throughputs and modest decoding resources are tools to address
the data transfer portion of the throughput problem. The earlier lossless
layout image compression algorithm Corner2 is designed for binary layout images
on raster-scanning systems. We propose variations of Corner2 collectively
called Corner2-EPC and Paeth-EPC which apply to electron-beam proximity
corrected layout images and offer interesting trade-offs between compression
ratios and decoding speeds. Most of our algorithms achieve better overall
compression performance than PNG, Block C4 and LineDiffEntropy while having low
decoding times and resources.
","['\nNarendra Chaudhary\n', '\nYao Luo\n', '\nSerap A. Savari\n', '\nRoger McCay\n']","This is the source file for the paper which was published in the
  Journal of Vacuum Science & Technology B (volume 33, 06FD01) on 5 August 2015","Journal of Vacuum Science & Technology B (Vol.33, Issue 6), 06FD01
  (2015)",http://dx.doi.org/10.1116/1.4927639,cs.OH,['cs.OH'],10.1116/1.4927639,,[]
A Novel Geographic Partitioning System for Anonymizing Health Care Data,http://arxiv.org/abs/1505.06939v1,2015-05-26T13:30:19Z,2015-05-26T13:30:19Z,"  With large volumes of detailed health care data being collected, there is a
high demand for the release of this data for research purposes. Hospitals and
organizations are faced with conflicting interests of releasing this data and
protecting the confidentiality of the individuals to whom the data pertains.
Similarly, there is a conflict in the need to release precise geographic
information for certain research applications and the requirement to censor or
generalize the same information for the sake of confidentiality. Ultimately the
challenge is to anonymize data in order to comply with government privacy
policies while reducing the loss in geographic information as much as possible.
In this paper, we present a novel geographic-based system for the anonymization
of health care data. This system is broken up into major components for which
different approaches may be supplied. We compare such approaches in order to
make recommendations on which of them to select to best match user
requirements.
","['\nWilliam Lee Croft\n', '\nWei Shi\n', '\nJorg-Rudiger Sack\n', '\nJean-Pierre Corriveau\n']",,,http://arxiv.org/abs/1505.06939v1,cs.OH,['cs.OH'],,,[]
Fixed Rank Kriging for Cellular Coverage Analysis,http://arxiv.org/abs/1505.07062v2,2015-05-26T17:42:16Z,2016-03-14T21:50:11Z,"  Coverage planning and optimization is one of the most crucial tasks for a
radio network operator. Efficient coverage optimization requires accurate
coverage estimation. This estimation relies on geo-located field measurements
which are gathered today during highly expensive drive tests (DT); and will be
reported in the near future by users' mobile devices thanks to the 3GPP
Minimizing Drive Tests (MDT) feature~\cite{3GPPproposal}. This feature consists
in an automatic reporting of the radio measurements associated with the
geographic location of the user's mobile device. Such a solution is still
costly in terms of battery consumption and signaling overhead. Therefore,
predicting the coverage on a location where no measurements are available
remains a key and challenging task. This paper describes a powerful tool that
gives an accurate coverage prediction on the whole area of interest: it builds
a coverage map by spatially interpolating geo-located measurements using the
Kriging technique. The paper focuses on the reduction of the computational
complexity of the Kriging algorithm by applying Fixed Rank Kriging (FRK). The
performance evaluation of the FRK algorithm both on simulated measurements and
real field measurements shows a good trade-off between prediction efficiency
and computational complexity. In order to go a step further towards the
operational application of the proposed algorithm, a multicellular use-case is
studied. Simulation results show a good performance in terms of coverage
prediction and detection of the best serving cell.
","['\nHajer Braham\n', '\nSana Ben Jemaa\n', '\nGersende Fort\n', '\nEric Moulines\n', '\nBerna Sayrac\n']",,,http://arxiv.org/abs/1505.07062v2,cs.OH,['cs.OH'],,,[]
An Analogy Based Method for Freight Forwarding Cost Estimation,http://arxiv.org/abs/1505.08088v1,2015-05-28T01:24:24Z,2015-05-28T01:24:24Z,"  The author explored estimation by analogy (EBA) as a means of estimating the
cost of international freight consignment. A version of the k-Nearest Neighbors
algorithm (k-NN) was tested by predicting job costs from a database of over
5000 actual jobs booked by an Irish freight forwarding firm over a seven year
period. The effect of a computer intensive training process on overall accuracy
of the method was found to be insignificant when the method was implemented
with four or fewer neighbors. Overall, the accuracy of the analogy based
method, while still significantly less accurate than manually working up
estimates, might be worthwhile to implement in practice, depending labor costs
in an adopting firm. A simulation model was used to compare manual versus
analytical estimation methods. The point of indifference occurs when it takes a
firm more than 1.5 worker hours to prepare a manual estimate (at current Irish
labor costs). Suggestions are given for future experiments to improve the
sampling policy of the method to improve accuracy and to improve overall
scalability.
",['\nKevin A. Straight\n'],,,http://arxiv.org/abs/1505.08088v1,cs.OH,['cs.OH'],,,[]
"Improving Time Estimation by Blind Deconvolution: with Applications to
  TOFD and Backscatter Sizing",http://arxiv.org/abs/1505.08107v1,2015-05-29T16:45:16Z,2015-05-29T16:45:16Z,"  In this paper we present a blind deconvolution scheme based on statistical
wavelet estimation. We assume no prior knowledge of the wavelet, and do not
select a reflector from the signal. Instead, the wavelet (ultrasound pulse) is
statistically estimated from the signal itself by a kurtosis-based metric. This
wavelet is then used to deconvolve the RF (radiofrequency) signal through
Wiener filtering, and the resultant zero phase trace is subjected to spectral
broadening by Autoregressive Spectral Extrapolation (ASE). These steps increase
the time resolution of diffraction techniques. Results on synthetic and real
cases show the robustness of the proposed method.
","['\nRoberto H. Herrera\n', '\nZhaorui Liu\n', '\nNatasha Raffa\n', '\nPaul Christensen\n', '\nAdrianus Elvers\n']","10 pages, 10 figures, Conference Paper",,http://arxiv.org/abs/1505.08107v1,cs.OH,['cs.OH'],,,[]
UbiBreathe: A Ubiquitous non-Invasive WiFi-based Breathing Estimator,http://arxiv.org/abs/1505.02388v1,2015-05-10T14:43:17Z,2015-05-10T14:43:17Z,"  Monitoring breathing rates and patterns helps in the diagnosis and potential
avoidance of various health problems. Current solutions for respiratory
monitoring, however, are usually invasive and/or limited to medical facilities.
In this paper, we propose a novel respiratory monitoring system, UbiBreathe,
based on ubiquitous off-the-shelf WiFi-enabled devices. Our experiments show
that the received signal strength (RSS) at a WiFi-enabled device held on a
person's chest is affected by the breathing process. This effect extends to
scenarios when the person is situated on the line-of-sight (LOS) between the
access point and the device, even without holding it. UbiBreathe leverages
these changes in the WiFi RSS patterns to enable ubiquitous non-invasive
respiratory rate estimation, as well as apnea detection.
  We propose the full architecture and design for UbiBreathe, incorporating
various modules that help reliably extract the hidden breathing signal from a
noisy WiFi RSS. The system handles various challenges such as noise
elimination, interfering humans, sudden user movements, as well as detecting
abnormal breathing situations. Our implementation of UbiBreathe using
off-the-shelf devices in a wide range of environmental conditions shows that it
can estimate different breathing rates with less than 1 breaths per minute
(bpm) error. In addition, UbiBreathe can detect apnea with more than 96%
accuracy in both the device-on-chest and hands-free scenarios. This highlights
its suitability for a new class of anywhere respiratory monitoring.
","['\nHeba Abdelnasser\n', '\nKhaled A. Harras\n', '\nMoustafa Youssef\n']",Accepted for publication in MobiHoc 2015,,http://arxiv.org/abs/1505.02388v1,cs.OH,['cs.OH'],,,[]
Advances in Computational Biology: A Real Boost or a Wishful Thinking,http://arxiv.org/abs/1505.05186v1,2015-05-18T10:51:50Z,2015-05-18T10:51:50Z,"  Computational biology is on the verge of a paradigm shift in its research
practice - from a data-based (computational) paradigm to an information-based
(cognitive) paradigm. As in the other research fields, this transition is
impeded by lack of a right understanding about what is actually hidden behind
the term ""information"". The paper is intended to clarify this issue and
introduces two new notions of ""physical information"" and ""semantic
information"", which together constitute the term ""information"". Some
implications of this introduction are considered.
",['\nEmanuel Diamant\n'],"The paper was submitted to the ICCS-2015 conference (Reykjavik,
  Iceland, June 1-3, 2015) and was rejected as irrelevant to the conference
  topic. arXiv admin note: text overlap with arXiv:1411.0054, arXiv:1502.04791,
  arXiv:1505.04785",,http://arxiv.org/abs/1505.05186v1,cs.OH,['cs.OH'],,,[]
"Server component installation and testing of the university information
  and educational environment on the Moodle LMS platform",http://arxiv.org/abs/1505.00422v1,2015-05-03T12:04:00Z,2015-05-03T12:04:00Z,"  The informational educational environment (IEE) of an institution is a
complex multilevel system which, along with methodical, organizational and
cultural resources, accumulates the intellectual and technical potential of a
university, as well as the informative and activity components of the learners
and teachers. In practice, the formation of IEE is actually based on the
creation of information technologies and their integration into the existing
educational environment of the institution. The management of this system is
carried out using specialized equipment and software. For the successful
formation and operation of IEE, in the present work we review software products
that form the basis of the organization of interactive and web interactions
between students, teachers and all participants of the educational process. We
analyse the technical capabilities that have provided users with IEE services
such as the Apache web server with connected modules PHP, MySQL, the Java
virtual machine and the Red5 server. We demonstrate the possibility of
obtaining results from the interaction of these products, and reports on users'
work in webinars, video conferences and web conferences.
","['\nIrina Erjomina\n', '\nAleksandr Rozentsvaig\n', '\nRushan Ziatdinov\n']",in Russian,"Izvestiya Sochi State University, Vol. 34, No. 1, Apr. 2015, pp.
  24-32",http://arxiv.org/abs/1505.00422v1,cs.OH,['cs.OH'],,,[]
"Preprint Virtual Reality GIS and Cloud Service Based Traffic Analysis
  Platform",http://arxiv.org/abs/1505.01056v2,2015-05-05T15:57:36Z,2015-07-29T08:32:30Z,"  This is the preprint version of our paper on The 23rd International
Conference on Geoinformatics (Geoinformatics2015). City traffic data has
several characteristics, such as large scale, diverse predictable and
real-time, which falls in the range of definition of Big Data. This paper
proposed a cloud service platform which targets for wise transportation is to
carry out unified management and mining analysis of the huge number of the
multivariate and heterogeneous dynamic transportation information, provides
real-time transportation information, increase the utilization efficiency of
transportation, promote transportation management and service level of travel
information and provide decision support of transportation management by
virtual reality as visual.
","['\nXiaoming Li\n', '\nZhihan Lv\n', '\nWeixi Wang\n', '\nChen Wu\n', '\nJinxing Hu\n']","This is the preprint version of our paper on The 23rd International
  Conference on Geoinformatics (Geoinformatics2015)",,http://arxiv.org/abs/1505.01056v2,cs.OH,['cs.OH'],,,[]
A Monte Carlo Study of Pairwise Comparisons,http://arxiv.org/abs/1505.01888v1,2015-05-07T23:12:02Z,2015-05-07T23:12:02Z,"  Consistent approximations obtained by geometric means ($GM$) and the
principal eigenvector ($EV$), turned out to be close enough for 1,000,000
not-so-inconsistent pairwise comparisons matrices. In this respect both methods
are accurate enough for most practical applications. As the enclosed Table 1
demonstrates, the biggest difference between average deviations of $GM$ and
$EV$ solutions is 0.00019 for the Euclidean metric and 0.00355 for the
Tchebychev metric.
  For practical applications, this precision is far better than expected. After
all we are talking, in most cases, about relative subjective comparisons and
one tenth of a percent is usually below our threshold of perception.
","['\nM. W. Herman\n', '\nW. W. Koczkodaj\n']",,Information Processing Letters 57 (1996) 25-29,http://arxiv.org/abs/1505.01888v1,cs.OH,['cs.OH'],,,[]
An LP-based inconsistency monitoring of pairwise comparison matrices,http://arxiv.org/abs/1505.01902v1,2015-05-08T01:25:57Z,2015-05-08T01:25:57Z,"  A distance-based inconsistency indicator, defined by the third author for the
consistency-driven pairwise comparisons method, is extended to the incomplete
case. The corresponding optimization problem is transformed into an equivalent
linear programming problem. The results can be applied in the process of
filling in the matrix as the decision maker gets automatic feedback. As soon as
a serious error occurs among the matrix elements, even due to a misprint, a
significant increase in the inconsistency index is reported. The high
inconsistency may be alarmed not only at the end of the process of filling in
the matrix but also during the completion process. Numerical examples are also
provided.
","['\nS. Bozoki\n', '\nJ. Fulop\n', '\nW. W. Koczkodaj\n']",,"Mathematical and Computer Modelling, v.54(1-2),789-793, 2011",http://dx.doi.org/10.1016/j.mcm.2011.03.026,cs.OH,"['cs.OH', '68W30']",10.1016/j.mcm.2011.03.026,,[]
"An Algorithm for the Optimal Consistent Approximation to a Pairwise
  Comparisons Matrix by Orthogonal Projections",http://arxiv.org/abs/1505.01903v1,2015-05-08T01:40:03Z,2015-05-08T01:40:03Z,"  The algorithm for finding the optimal consistent approximation of an
inconsistent pairwise comparisons matrix is based on a logarithmic
transformation of a pairwise comparisons matrix into a vector space with the
Euclidean metric. Orthogonal basis is introduced in the vector space. The
orthogonal projection of the transformed matrix onto the space formed by the
images of consistent matrices is the required consistent approximation.
","['\nW. W. Koczkodaj\n', '\nM. Orlowski\n']",,"Computers & Mathematics with Applications,34(10): 41-47, 1997",http://dx.doi.org/10.1016/S0898-1221(97)00205-8,cs.OH,['cs.OH'],10.1016/S0898-1221(97)00205-8,,[]
"FPGA-Based Bandwidth Selection for Kernel Density Estimation Using High
  Level Synthesis Approach",http://arxiv.org/abs/1505.02100v2,2015-05-08T16:54:00Z,2015-09-29T20:00:15Z,"  FPGA technology can offer significantly hi\-gher performance at much lower
power consumption than is available from CPUs and GPUs in many computational
problems. Unfortunately, programming for FPGA (using ha\-rdware description
languages, HDL) is a difficult and not-trivial task and is not intuitive for
C/C++/Java programmers. To bring the gap between programming effectiveness and
difficulty the High Level Synthesis (HLS) approach is promoting by main FPGA
vendors. Nowadays, time-intensive calculations are mainly performed on GPU/CPU
architectures, but can also be successfully performed using HLS approach. In
the paper we implement a bandwidth selection algorithm for kernel density
estimation (KDE) using HLS and show techniques which were used to optimize the
final FPGA implementation. We are also going to show that FPGA speedups,
comparing to highly optimized CPU and GPU implementations, are quite
substantial. Moreover, power consumption for FPGA devices is usually much less
than typical power consumption of the present CPUs and GPUs.
","['\nArtur Gramacki\n', '\nMarek Sawerwain\n', '\nJarosław Gramacki\n']","23 pages, 6 figures, extended version of initial paper",,http://arxiv.org/abs/1505.02100v2,cs.OH,['cs.OH'],,,[]
"How Resilient Are Our Societies? Analyses, Models, and Preliminary
  Results",http://arxiv.org/abs/1505.02759v1,2015-05-08T09:59:55Z,2015-05-08T09:59:55Z,"  Traditional social organizations such as those for the management of
healthcare and civil defence are the result of designs and realizations that
matched well with an operational context considerably different from the one we
are experiencing today: A simpler world, characterized by a greater amount of
resources to match less users producing lower peaks of requests. The new
context reveals all the fragility of our societies: unmanageability is just
around the corner unless we do not complement the ""old recipes"" with smarter
forms of social organization. Here we analyze this problem and propose a
refinement to our fractal social organizations as a model for resilient
cyber-physical societies. Evidence to our claims is provided by simulating our
model in terms of multi-agent systems.
","['\nVincenzo De Florio\n', '\nArianit Pajaziti\n']","Paper submitted for publication in the Proc. of SERENE 2015
  (http://serene.disim.univaq.it/2015/)",,http://arxiv.org/abs/1505.02759v1,cs.OH,"['cs.OH', 'cs.MA']",,,[]
"A Simple and General Problem and its Optimal Randomized Online Algorithm
  Design with Competitive Analysis",http://arxiv.org/abs/1504.05305v1,2015-04-21T04:54:49Z,2015-04-21T04:54:49Z,"  The online algorithm design was proposed to handle the caching problem when
the future information is unknown. And currently, it draws more and more
attentions from the researchers from the areas of microgrid, where the
production of renewables are unpredictable.
  In this note, we present a framework of randomized online algorithm design
for the \textit{simple and tractable} problem. This framework hopes to provide
a tractable design to design a randomized online algorithm, which can be proved
to achieve the best competitive ratio by \textit{Yao's Principle}.
",['\nYing Zhang\n'],,,http://arxiv.org/abs/1504.05305v1,cs.OH,['cs.OH'],,,[]
Signal filtering to obtain number of Hamiltonian paths,http://arxiv.org/abs/1504.05429v11,2015-04-21T13:33:53Z,2021-04-04T06:51:24Z,"  This paper consists of two parts. First, the (undirected) Hamiltonian path
problem is reduced to a signal filtering problem - number of Hamiltonian paths
becomes amplitude at zero frequency for (a combination of) sinusoidal signal
f(t) that encodes a graph. Then a 'divide and conquer' strategy to filtering
out wide bandwidth components of a signal is suggested - one filters out
angular frequency 1/2 to 1, then 1/4 to 1/2, then 1/8 to 1/4 and so on. An
actual implementation of this strategy involves careful local polynomial
extrapolation using numerical differentiation filters. When conjectures
regarding required number of samples for specified filter designs and time
complexity of obtaining filter coefficients hold, P=NP conditionally.
",['\nBryce Kim\n'],"24 pages, 2 algorithms, 2 figures. Additional category cs.CC and
  eess.SP - plan to get endorsements to get cross-listed",,http://arxiv.org/abs/1504.05429v11,cs.OH,"['cs.OH', 'F.1.3; F.2.1']",,,[]
"State of the Art of the Intra-Task Dynamic Voltage and Frequency Scaling
  Technique",http://arxiv.org/abs/1504.06177v1,2015-04-23T13:39:54Z,2015-04-23T13:39:54Z,"  In recent years there has been an increasing use of embedded systems because
of advances in technology, the reduction of the costs of electronic equipment
and mainly the popularity of mobile devices. Many of these systems implement
low power consumption policies to extend their autonomy, usually because they
have a reduced amount of resources and the great majority of them use electric
power from batteries. One way to minimize the power consumption of these
devices is through of the application of low power consumption techniques. From
the various techniques presented in the literature - the intra-task Dynamic
Voltage and Frequency Scaling (DVFS) has played an important role. The main aim
of DVFS is to allow each task to manage the minimum resources necessary for
tasks execution, this way reducing the processor power consumption and, at the
same time, respecting the task deadlines when considered a real-time system
context. Therefore, this paper aims to apply a systematic literature review
with the goal of identifying and presenting the main methods using the
intra-task DVFS technique, applied in the context of real-time systems to
reduce energy consumption on the processor. Finally, this work will show the
advantages and disadvantages of each cataloged methodology.
","['\nRawlinson S. Gonçalves\n', '\nRaimundo da Silva Barreto\n']","94 pages, in Portuguese",,http://arxiv.org/abs/1504.06177v1,cs.OH,['cs.OH'],,,[]
"The Fallacy of Favoring Gradual Replacement Mind Uploading Over
  Scan-and-Copy",http://arxiv.org/abs/1504.06320v4,2015-04-23T14:09:50Z,2018-12-17T13:59:52Z,"  Mind uploading speculation and debate often concludes that a procedure
described as gradual in-place replacement preserves personal identity while a
procedure described as destructive scan-and-copy produces some other identity
in the target substrate such that personal identity is lost along with the
biological brain. This paper demonstrates a chain of reasoning that establishes
metaphysical equivalence between these two methods in terms of preserving
personal identity.
","['\nKeith B. Wiley\n', '\nRandal A. Koene\n']","14 pages, 0 figures","Wiley, K. B. and Koene, R. A. The Fallacy of Favoring Gradual
  Replacement Mind Uploading Over Scan-and-Copy. Journal of Consciousness
  Studies (JoCS), Vol. 23, No. 3-4, pp212-235, Mar 2016",http://arxiv.org/abs/1504.06320v4,cs.OH,"['cs.OH', 'I.2.0']",,,[]
"Aashiyana: Design and Evaluation of a Smart Demand-Response System for
  Highly-stressed Grids",http://arxiv.org/abs/1504.06975v3,2015-04-27T08:57:55Z,2016-02-19T08:02:07Z,"  This paper targets the unexplored problem of demand response within the
context of power-grids that are allowed to regularly enforce blackouts as a
mean to balance supply with demand:highly-stressed grids. Currently these
utilities use as a cyclic and binary (power/no-power) schedule over consumer
groups leading to significant wastage of capacity and long hours of no-power.
We present here a novel building DLC system, Aashiyana, that can enforce
several user-defined low-power states. We evaluate distributed and centralized
load-shedding schemes using Aashiyana that can, compared to current
load-shedding strategy, reduce the number of homes with no power by 80% for
minor change in the fraction of homes with full-power.
","['\nNoman Bashir\n', '\nZohaib Sharani\n', '\nKhushboo Qayyum\n', '\nAffan A. Syed\n']",,,http://arxiv.org/abs/1504.06975v3,cs.OH,['cs.OH'],,,[]
Definition and Research of Internet Neurology,http://arxiv.org/abs/1504.02842v1,2015-04-11T06:17:58Z,2015-04-11T06:17:58Z,"  More and more scientific research shows that there is a close correlation
between the Internet and brain science. This paper presents the idea of
establishing the Internet neurology, which means to make a cross-contrast
between the two in terms of physiology and psychology, so that a complete
infrastructure system of the Internet is established, predicting the
development trend of the Internet in the future as well as the brain structure
and operation mechanism, and providing theoretical support for the generation
principle of intelligence, cognition and emotion. It also proposes the
viewpoint that the Internet can be divided into Internet neurophysiology,
Internet neuropsychology, Brain Internet physiology, Brain Internet psychology
and the Internet in cognitive science.
",['\nFeng Liu\n'],,,http://arxiv.org/abs/1504.02842v1,cs.OH,['cs.OH'],,,[]
"An Improved Variable Step-size Zero-point Attracting Projection
  Algorithm",http://arxiv.org/abs/1504.03664v1,2015-04-14T19:14:32Z,2015-04-14T19:14:32Z,"  This paper proposes an improved variable step-size (VSS) scheme for
zero-point attracting projection (ZAP) algorithm. The proposed VSS is
proportional to the sparseness difference between filter coefficients and the
true impulse response. Meanwhile, it works for both sparse and non-sparse
system identification, and simulation results demonstrate that the proposed
algorithm could provide both faster convergence rate and better tracking
ability than previous ones.
","['\nJianming Liu\n', '\nSteven L. Grant\n']","5 pages, ICASSP 2015. arXiv admin note: substantial text overlap with
  arXiv:1312.2612",,http://arxiv.org/abs/1504.03664v1,cs.OH,['cs.OH'],,,[]
"A parametric study on window-to-floor ratio of double window glazing and
  its shadowing using dynamic simulation",http://arxiv.org/abs/1504.04174v1,2015-04-15T14:31:07Z,2015-04-15T14:31:07Z,"  When incorrectly designed, windows can be responsible for unnecessary energy
consumption in a building. This may result from its dimensions, orientation and
shadowing. In a moderate climate like the Portuguese, and considering an annual
thermal comfort assessment of a space, if windows are under-dimensioned or
over-shadowed, they can contribute to the increase of heating needs. However,
when over-dimensioned or under-shadowed, they contribute to the increase of
cooling requirements. Therefore, it is important to find the optimum design
that balances orientation, dimension and shadowing, contributing to minimize
both the heating and cooling needs. This study presents a parametric analysis
of a double glazing window in its orientation and dimension, located in the
Portuguese city of Coimbra. For each window orientation and dimension, the
optimum overhang depth is determined. The objective is to minimize degree-hours
of thermal discomfort. Results show that overhangs are mainly a corrective
mechanism to over-dimensioned openings, thus allowing that building
practitioners may choose a wider range of windows dimensions.
","['\nAna Rita Amaral\n', '\nEugénio Rodrigues\n', '\nAdélio Rodrigues Gaspar\n', '\nÁlvaro Gomes\n']","7 pages, 2 figures, 3rd LAETA Young Researchers Meeting - 3EJIL,
  Coimbra, 7-8 May, 2015. arXiv admin note: text overlap with arXiv:1503.07016",,http://arxiv.org/abs/1504.04174v1,cs.OH,"['cs.OH', '68U20']",,,[]
Any non-affine one-to-one binary gate suffices for computation,http://arxiv.org/abs/1504.03376v1,2015-04-13T22:06:50Z,2015-04-13T22:06:50Z,"  Any non-affine one-to-one binary gate can be wired together with suitable
inputs to give AND, OR, NOT and fan-out gates, and so suffices to construct a
general-purpose computer.
",['\nSeth Lloyd\n'],"7 pages, plain TeX, 1992 Los Alamos Alamos preprint number
  LA-UR-92-996. Shows that any non-affine reversible binary logic gate is
  universal",,http://arxiv.org/abs/1504.03376v1,quant-ph,"['quant-ph', 'cs.OH']",,,[]
Preprint Traffic Management and Forecasting System Based on 3D GIS,http://arxiv.org/abs/1504.01375v3,2015-04-04T22:08:29Z,2015-07-29T08:43:52Z,"  This is the preprint version of our paper on 2015 15th IEEE/ACM International
Symposium on Cluster, Cloud and Grid Computing (CCGrid). This paper takes
Shenzhen Futian comprehensive transportation junction as the case, and makes
use of continuous multiple real-time dynamic traffic information to carry out
monitoring and analysis on spatial and temporal distribution of passenger flow
under different means of transportation and service capacity of junction from
multi-dimensional space-time perspectives such as different period and special
period. Virtual reality geographic information system is employed to present
the forecasting result.
","['\nXiaoming Li\n', '\nZhihan Lv\n', '\nJinxing Hu\n', '\nBaoyun Zhang\n', '\nLing Yin\n', '\nChen Zhong\n', '\nWeixi Wang\n', '\nShengzhong Feng\n']","This is the preprint version of our paper on 2015 15th IEEE/ACM
  International Symposium on Cluster, Cloud and Grid Computing (CCGrid). arXiv
  admin note: substantial text overlap with arXiv:1504.01057",,http://arxiv.org/abs/1504.01375v3,cs.OH,"['cs.OH', 'I.3.7']",,,[]
Android based security and home automation system,http://arxiv.org/abs/1504.03564v1,2015-03-28T06:06:55Z,2015-03-28T06:06:55Z,"  The smart mobile terminal operator platform Android is getting popular all
over the world with its wide variety of applications and enormous use in
numerous spheres of our daily life. Considering the fact of increasing demand
of home security and automation, an Android based control system is presented
in this paper where the proposed system can maintain the security of home main
entrance and also the car door lock. Another important feature of the designed
system is that it can control the overall appliances in a room. The mobile to
security system or home automation system interface is established through
Bluetooth. The hardware part is designed with the PIC microcontroller.
","['\nSadeque Reza Khan\n', '\nFarzana Sultana Dristy\n']","10 pages,17 figures, Journal, International Journal of Ambient
  Systems and Applications, Volume 3, 2015",,http://dx.doi.org/10.5121/ijasa.2014.3102,cs.OH,['cs.OH'],10.5121/ijasa.2014.3102,,[]
An embedded system for real-time feedback neuroscience experiments,http://arxiv.org/abs/1504.00932v1,2015-04-03T20:05:59Z,2015-04-03T20:05:59Z,"  A complete data acquisition and signal output control system for synchronous
stimuli generation, geared towards in vivo neuroscience experiments, was
developed using the Terasic DE2i-150 board. All emotions and thoughts are an
emergent property of the chemical and electrical activity of neurons. Most of
these cells are regarded as excitable cells (spiking neurons), which produce
temporally localized electric patterns (spikes). Researchers usually consider
that only the instant of occurrence (timestamp) of these spikes encodes
information. Registering neural activity evoked by stimuli demands timing
determinism and data storage capabilities that cannot be met without dedicated
hardware and a hard real-time operational system (RTOS). Indeed, research in
neuroscience usually requires dedicated electronic instrumentation for studies
in neural coding, brain machine interfaces and closed loop in vivo or in vitro
experiments. We developed a complete embedded system solution consisting of a
hardware/software co-design with the Intel Atom processor running a free RTOS
and a FPGA communicating via a PCIe-to-Avalon bridge. Our system is capable of
registering input event timestamps with 1{\mu}s precision and digitally
generating stimuli output in hard real-time. The whole system is controlled by
a Linux-based Graphical User Interface (GUI). Collected results are
simultaneously saved in a local file and broadcasted wirelessly to mobile
device web-browsers in an user-friendly graphic format, enhanced by HTML5
technology. The developed system is low-cost and highly configurable, enabling
various neuroscience experimental setups, while the commercial off-the-shelf
systems have low availability and are less flexible to adapt to specific
experimental configurations.
","['\nLirio Onofre Baptista de Almeida\n', '\nPaulo Matias\n', '\nRafael Tuma Guariento\n']","17 pages, 11 figures, IV Brazilian Symposium on Computing Systems
  Engineering",,http://dx.doi.org/10.13140/RG.2.1.4077.7769,q-bio.QM,"['q-bio.QM', 'cs.OH']",10.13140/RG.2.1.4077.7769,,[]
"Feeder Load Balancing using Fuzzy Logic and Combinatorial
  Optimization-based Implementation",http://arxiv.org/abs/1503.05273v1,2015-03-18T03:09:30Z,2015-03-18T03:09:30Z,"  The distribution system problems, such as planning, loss minimization, and
energy restoration, usually involve the phase balancing or network
reconfiguration procedures. The determination of an optimal phase balance is,
in general, a combinatorial optimization problem. This paper proposes a novel
reconfiguration of the phase balancing using the fuzzy logic and the
combinatorial optimization-based implementation step back to back. Input to the
fuzzy step is the total load per phase of the feeders. Output of the fuzzy step
is the load change values, negative value for load releasing and positive value
for load receiving. The output of the fuzzy step is the input to the load
changing system. The load changing system uses combinatorial optimization
techniques to translate the change values (kW) into number of load points and
then selects the specific load points. It also performs the inter-changing of
the load points between the releasing and the receiving phases in an optimal
fashion. Application results using the distribution feeder network of South
Africa are presented in this paper.
","['\nA. Ukil\n', '\nW. Siti\n']",11 pages,"Electric Power Systems Research, Elsevier, vol. 78, issue 11, pp.
  1922-1932, 2008",http://dx.doi.org/10.1016/j.epsr.2008.03.020,cs.OH,['cs.OH'],10.1016/j.epsr.2008.03.020,,[]
"Adjusted Haar Wavelet for Application in the Power Systems Disturbance
  Analysis",http://arxiv.org/abs/1503.05287v1,2015-03-18T05:50:39Z,2015-03-18T05:50:39Z,"  Abrupt change detection based on the wavelet transform and threshold method
is very effective in detecting the abrupt changes and hence segmenting the
signals recorded during disturbances in the electrical power network. The
wavelet method estimates the time-instants of the changes in the signal model
parameters during the pre-fault condition, after initiation of fault, after
circuit-breaker opening and auto-reclosure. Certain kinds of disturbance
signals do not show distinct abrupt changes in the signal parameters. In those
cases, the standard mother wavelets fail to achieve correct event-specific
segmentations. A new adjustment technique to the standard Haar wavelet is
proposed in this paper, by introducing 2n adjusting zeros in the Haar wavelet
scaling filter, n being a positive integer. This technique is quite effective
in segmenting those fault signals into pre- and post-fault segments, and it is
an improvement over the standard mother wavelets for this application. This
paper presents many practical examples where recorded signals from the power
network in South Africa have been used.
","['\nA. Ukil\n', '\nR. Zivanovic\n']",13 pages in final printed version,"Digital Signal Processing, Elsevier, vol. 18, issue 2, pp.
  103-115, 2008",http://dx.doi.org/10.1016/j.dsp.2007.04.001,cs.OH,['cs.OH'],10.1016/j.dsp.2007.04.001,,[]
"A software controlled voltage tuning system using multi-purpose ring
  oscillators",http://arxiv.org/abs/1503.05733v1,2015-03-19T12:16:33Z,2015-03-19T12:16:33Z,"  This paper presents a novel software driven voltage tuning method that
utilises multi-purpose Ring Oscillators (ROs) to provide process variation and
environment sensitive energy reductions. The proposed technique enables voltage
tuning based on the observed frequency of the ROs, taken as a representation of
the device speed and used to estimate a safe minimum operating voltage at a
given core frequency. A conservative linear relationship between RO frequency
and silicon speed is used to approximate the critical path of the processor.
  Using a multi-purpose RO not specifically implemented for critical path
characterisation is a unique approach to voltage tuning. The parameters
governing the relationship between RO and silicon speed are obtained through
the testing of a sample of processors from different wafer regions. These
parameters can then be used on all devices of that model. The tuning method and
software control framework is demonstrated on a sample of XMOS XS1-U8A-64
embedded microprocessors, yielding a dynamic power saving of up to 25% with no
performance reduction and no negative impact on the real-time constraints of
the embedded software running on the processor.
","['\nSteve Kerrison\n', '\nKerstin Eder\n']",,,http://arxiv.org/abs/1503.05733v1,cs.OH,['cs.OH'],,,[]
Normalization: A Preprocessing Stage,http://arxiv.org/abs/1503.06462v1,2015-03-19T06:54:53Z,2015-03-19T06:54:53Z,"  As we know that the normalization is a pre-processing stage of any type
problem statement. Especially normalization takes important role in the field
of soft computing, cloud computing etc. for manipulation of data like scale
down or scale up the range of data before it becomes used for further stage.
There are so many normalization techniques are there namely Min-Max
normalization, Z-score normalization and Decimal scaling normalization. So by
referring these normalization techniques we are going to propose one new
normalization technique namely, Integer Scaling Normalization. And we are going
to show our proposed normalization technique using various data sets.
","['\nS. Gopal Krishna Patro\n', '\nKishore Kumar Sahu\n']","4 pages, 3 figures, 3 tables",,http://arxiv.org/abs/1503.06462v1,cs.OH,['cs.OH'],,,[]
Practical Denoising of MEG Data using Wavelet Transform,http://arxiv.org/abs/1503.06618v1,2015-03-23T12:22:39Z,2015-03-23T12:22:39Z,"  Magnetoencephalography (MEG) is an important noninvasive, nonhazardous
technology for functional brain mapping, measuring the magnetic fields due to
the intracellular neuronal current flow in the brain. However, the inherent
level of noise in the data collection process is large enough to obscure the
signal(s) of interest most often. In this paper, a practical denoising
technique based on the wavelet transform and the multiresolution signal
decomposition technique is presented. The proposed technique is substantiated
by the application results using three different mother wavelets on the
recorded MEG signal.
",['\nA. Ukil\n'],8 pages. arXiv admin note: text overlap with arXiv:1503.05821,"Lecture Notes in Computer Science, Springer, vol. 4233, pp.
  578-585, 2006",http://dx.doi.org/10.1007/11893257_65,cs.OH,['cs.OH'],10.1007/11893257_65,,[]
"Optimization of gridshell bar orientation using a simplified genetic
  approach",http://arxiv.org/abs/1503.06729v1,2015-03-20T19:09:20Z,2015-03-20T19:09:20Z,"  Gridshells are defined as structures that have the shape and rigidity of a
double curvature shell but consist of a grid instead of a continuous surface.
This study concerns those obtained by elastic deformation of an initially flat
two-way grid. This paper presents a novel approach to generate gridshells on an
imposed shape under imposed boundary conditions. A numerical tool based on a
geometrical method, the compass method, is developed. It is coupled with
genetic algorithms to optimize the orientation of gridshell bars in order to
minimize the stresses and therefore to avoid bar breakage during the
construction phase. Examples of application are shown.
","['\nLina Bouhaya\nNAVIER UMR 8205\n', '\nOlivier Baverel\nNAVIER UMR 8205\n', '\nJean-François Caron\nNAVIER UMR 8205\n']",,"Structural and Multidisciplinary Optimization, Springer Verlag
  (Germany), 2014, 50 (5), pp. 839-848",http://dx.doi.org/10.1007/s00158-014-1088-9,cs.OH,['cs.OH'],10.1007/s00158-014-1088-9,,"['NAVIER UMR 8205', 'NAVIER UMR 8205', 'NAVIER UMR 8205']"
"A parametric study of window-to-floor ratio of three window types using
  dynamic simulation",http://arxiv.org/abs/1503.07016v1,2015-03-24T12:41:14Z,2015-03-24T12:41:14Z,"  The windows can be responsible for unnecessary energy consumption in a
building, if incorrectly designed, shadowed or oriented. Considering an annual
thermal comfort assessment of a space, if windows are over-dimensioned, they
can contribute to the increase of the heating needs due to heat losses, and
also to the increase of cooling needs due to over-exposure to solar radiation.
When under-dimensioned, the same space may benefit from reduced heat losses
through the glazing surface but does not benefit from solar radiation gains.
Therefore, it is important to find the optimum design that minimizes both the
heating and cooling needs. This paper presents a parametric study of window
type (single, double and triple glazing), orientation and opening size, located
in the city of Coimbra, Portugal. An annual and a seasonal assessment were
done, in order to obtain the set of optimum values around 360 degree
orientation.
","['\nAna Rita Amaral\n', '\nEugénio Rodrigues\n', '\nAdélio Rodrigues Gaspar\n', '\nÁlvaro Gomes\n']","7 pages, 2 figures, Proceedings of Energy for Sustainability 2015
  Conference: Sustainable Cities: Designing for People and the Planet, Coimbra,
  14-15 May, 2015",,http://arxiv.org/abs/1503.07016v1,cs.OH,"['cs.OH', '68U20']",,,[]
"Modeling and Improving the Energy Performance of GPS Receivers for
  Mobile Applications",http://arxiv.org/abs/1503.02656v2,2015-03-09T06:43:37Z,2015-11-30T03:16:23Z,"  Integrated GPS receivers have become a basic module in today's mobile
devices. While serving as the cornerstone for location based services, GPS
modules have a serious battery drain problem due to high computation load. This
paper aims to reveal the impact of key software parameters on hardware energy
consumption, by establishing an energy model for a standard GPS receiver
architecture as found in both academic and industrial designs. In particular,
our measurements show that the receiver's energy consumption is in large part
linear with the number of tracked satellites. This leads to a design of
selective tracking algorithm that provides similar positioning accuracy (around
12m) with a subset of selected satellites, which translates to an energy saving
of 20.9-23.1\% on the Namuru board.
","['\nKongyang Chen\n', '\nGuang Tan\n']",,,http://arxiv.org/abs/1503.02656v2,cs.OH,['cs.OH'],,,[]
When In-Memory Computing is Slower than Heavy Disk Usage,http://arxiv.org/abs/1503.02678v2,2015-03-10T05:29:01Z,2015-03-27T18:05:24Z,"  Disk access latency and transfer times are often considered to have a major
and detrimental impact on the running time of software. Developers are often
advised to favour in-memory operations and minimise disk access. Furthermore,
diskless computer architectures are being studied and designed to remove this
bottleneck all together, to improve application performance in areas such as
High Performance Computing, Big Data, and Business Intelligence. In this paper
we use code inspired by real, production software, to show that in-memory
operations are not always a guarantee for high performance, and may actually
cause a considerable slow-down. We also show how small code changes can have
dramatic effects on running times. We argue that a combination of system-level
improvements and better developer awareness and coding practices are necessary
to ensure in-memory computing can achieve its full potential.
","['\nKamran Karimi\n', '\nDiwakar Krishnamurthy\n', '\nParissa Mirjafari\n']","This paper has been withdrawn by the authors for personal reasons. No
  further revisions",,http://arxiv.org/abs/1503.02678v2,cs.OH,['cs.OH'],,,[]
Hardware Probing Interface and Test Robustness,http://arxiv.org/abs/1503.03113v1,2015-03-10T22:06:51Z,2015-03-10T22:06:51Z,"  Computerized integrity test of an electronic product hardware interface and
product probing validation are considered. Integrity testing is based on a
current voltage characteristic measurement, when a small voltage and/or current
stimuli are applied to the product pads including power supply circuitry pads,
so that the product is not normally powered on. Test fixture needles validation
is a part of a self test maintenance scenario designed to predict deterioration
of product probing.
",['\nA. M. Dorman\n'],"7 pages, 1 figures",,http://arxiv.org/abs/1503.03113v1,cs.OH,['cs.OH'],,,[]
Synthesis of all Maximum Length Cellular Automata of Cell Size up to 12,http://arxiv.org/abs/1503.04006v1,2015-03-12T12:09:37Z,2015-03-12T12:09:37Z,"  Maximum length CA has wide range of applications in design of linear block
code, cryptographic primitives and VLSI testing particularly in
Built-In-Self-Test. In this paper, an algorithm to compute all $n$-cell maximum
length CA-rule vectors is proposed. Also rule vectors for each primitive
polynomial in GF(2^2) to GF(2^{12} have been computed by simulation and they
have been listed.Programmable rule vectors based maximum length CA can be used
to design cryptographic primitives.
",['\nJaydeb Bhaumik\n'],10 pages,,http://arxiv.org/abs/1503.04006v1,cs.OH,['cs.OH'],,,[]
"Designing and Building a Three-dimensional Projective Scanner for
  Smartphones",http://arxiv.org/abs/1503.04315v1,2015-03-14T15:36:10Z,2015-03-14T15:36:10Z,"  One of the frustrating things in the digital fabrication era is that its
media are neither affordable nor easily accessible and usable.
Three-dimensional (3D) fabrication media (DFM) such as 3D Printers and 3D
Scanners have experienced an upsurge in popularity, while the latter remain
expensive and hard to function. With this paper, we aim to present you the
RhoScanner Project - a an affordable and efficient Three-dimensional Projective
Scanner for Smart-phones, hence shedding light on the extended capabilities of
digital fabrication media on popular use.
",['\nMarios Papachristou\n'],,,http://arxiv.org/abs/1503.04315v1,cs.OH,['cs.OH'],,,[]
"Towards an intelligent VNS heuristic for the k-labelled spanning forest
  problem",http://arxiv.org/abs/1503.02009v1,2015-03-05T14:10:19Z,2015-03-05T14:10:19Z,"  In a currently ongoing project, we investigate a new possibility for solving
the k-labelled spanning forest (kLSF) problem by an intelligent Variable
Neighbourhood Search (Int-VNS) metaheuristic. In the kLSF problem we are given
an undirected input graph G and an integer positive value k, and the aim is to
find a spanning forest of G having the minimum number of connected components
and the upper bound k on the number of labels to use. The problem is related to
the minimum labelling spanning tree (MLST) problem, whose goal is to get the
spanning tree of the input graph with the minimum number of labels, and has
several applications in the real world, where one aims to ensure connectivity
by means of homogeneous connections. The Int-VNS metaheuristic that we propose
for the kLSF problem is derived from the promising intelligent VNS strategy
recently proposed for the MLST problem, and integrates the basic VNS for the
kLSF problem with other complementary approaches from machine learning,
statistics and experimental algorithmics, in order to produce high-quality
performance and to completely automate the resulting strategy.
","['\nSergio Consoli\n', '\nJosè Andrès Moreno Pèrez\n', '\nNenad Mladenovic\n']","2 pages, Fifteenth International Conference on Computer Aided Systems
  Theory (EUROCAST 2015), Las Palmas de Gran Canaria, Spain","Computer Aided Systems Theory, pages 79-80 (2015)",http://arxiv.org/abs/1503.02009v1,cs.OH,"['cs.OH', 'cs.AI']",,,[]
Distributed Computation Particle PHD filter,http://arxiv.org/abs/1503.03769v1,2015-03-09T14:55:05Z,2015-03-09T14:55:05Z,"  Particle probability hypothesis density filtering has become a promising
means for multi-target tracking due to its capability of handling an unknown
and time-varying number of targets in non-linear non-Gaussian system. However,
its computational complexity grows linearly with the number of measurements and
particles assigned to each target, and this can be very time consuming
especially when numerous targets and clutter exist in the surveillance region.
Addressing this issue, we present a distributed computation particle PHD filter
for target tracking. Its framework consists of several local particle PHD
filters at each processing element and a central unit. Each processing element
takes responsibility for part particles but full measurements and provides
local estimates; central unit controls particle exchange between processing
elements and specifies a fusion rule to match and fuse the estimates from
different local filters. The proposed framework is suitable for parallel
implementation and maintains the tracking accuracy. Simulations verify the
proposed method can provide comparative accuracy as well as a significant
speedup with the standard particle PHD filter.
","['\nWang Junjie\n', '\nZhao Lingling\n', '\nSu Xiaohong\n', '\nMa Peijun\n']",,,http://arxiv.org/abs/1503.03769v1,stat.CO,"['stat.CO', 'cs.OH']",,,[]
"Euclidean Distance Matrices: Essential Theory, Algorithms and
  Applications",http://arxiv.org/abs/1502.07541v2,2015-02-26T13:18:37Z,2015-08-15T16:51:08Z,"  Euclidean distance matrices (EDM) are matrices of squared distances between
points. The definition is deceivingly simple: thanks to their many useful
properties they have found applications in psychometrics, crystallography,
machine learning, wireless sensor networks, acoustics, and more. Despite the
usefulness of EDMs, they seem to be insufficiently known in the signal
processing community. Our goal is to rectify this mishap in a concise tutorial.
We review the fundamental properties of EDMs, such as rank or
(non)definiteness. We show how various EDM properties can be used to design
algorithms for completing and denoising distance data. Along the way, we
demonstrate applications to microphone position calibration, ultrasound
tomography, room reconstruction from echoes and phase retrieval. By spelling
out the essential algorithms, we hope to fast-track the readers in applying
EDMs to their own problems. Matlab code for all the described algorithms, and
to generate the figures in the paper, is available online. Finally, we suggest
directions for further research.
","['\nIvan Dokmanic\n', '\nReza Parhizkar\n', '\nJuri Ranieri\n', '\nMartin Vetterli\n']","- 17 pages, 12 figures, to appear in IEEE Signal Processing Magazine
  - change of title in the last revision",,http://dx.doi.org/10.1109/MSP.2015.2398954,cs.OH,['cs.OH'],10.1109/MSP.2015.2398954,,[]
Globally Optimal Crowdsourcing Quality Management,http://arxiv.org/abs/1502.07710v2,2015-02-26T20:09:29Z,2015-03-01T17:53:34Z,"  We study crowdsourcing quality management, that is, given worker responses to
a set of tasks, our goal is to jointly estimate the true answers for the tasks,
as well as the quality of the workers. Prior work on this problem relies
primarily on applying Expectation-Maximization (EM) on the underlying maximum
likelihood problem to estimate true answers as well as worker quality.
Unfortunately, EM only provides a locally optimal solution rather than a
globally optimal one. Other solutions to the problem (that do not leverage EM)
fail to provide global optimality guarantees as well. In this paper, we focus
on filtering, where tasks require the evaluation of a yes/no predicate, and
rating, where tasks elicit integer scores from a finite domain. We design
algorithms for finding the global optimal estimates of correct task answers and
worker quality for the underlying maximum likelihood problem, and characterize
the complexity of these algorithms. Our algorithms conceptually consider all
mappings from tasks to true answers (typically a very large number), leveraging
two key ideas to reduce, by several orders of magnitude, the number of mappings
under consideration, while preserving optimality. We also demonstrate that
these algorithms often find more accurate estimates than EM-based algorithms.
This paper makes an important contribution towards understanding the inherent
complexity of globally optimal crowdsourcing quality management.
","['\nAkash Das Sarma\n', '\nAditya Parameswaran\n', '\nJennifer Widom\n']",,,http://arxiv.org/abs/1502.07710v2,cs.OH,['cs.OH'],,,[]
"Parameter Estimation of Jelinski-Moranda Model Based on Weighted
  Nonlinear Least Squares and Heteroscedasticity",http://arxiv.org/abs/1503.00094v1,2015-02-28T07:55:30Z,2015-02-28T07:55:30Z,"  Parameter estimation method of Jelinski-Moranda (JM) model based on weighted
nonlinear least squares (WNLS) is proposed. The formulae of resolving the
parameter WNLS estimation (WNLSE) are derived, and the empirical weight
function and heteroscedasticity problem are discussed. The effects of
optimization parameter estimation selection based on maximum likelihood
estimation (MLE) method, least squares estimation (LSE) method and weighted
nonlinear least squares estimation (WNLSE) method are also investigated. Two
strategies of heteroscedasticity decision and weighting methods embedded in JM
model prediction process are also investigated. The experimental results on
standard software reliability analysis database-Naval Tactical Data System
(NTDS) and three datasets used by J.D. Musa demonstrate that WNLSE method can
be superior to LSE and MLE under the relative error (RE) criterion.
","['\nJingwei Liu\n', '\nYi Liu\n', '\nMeizhi Xu\n']","17 pages, 10 figures",,http://arxiv.org/abs/1503.00094v1,cs.OH,['cs.OH'],,,[]
Google-based Mode Choice Modeling Approach,http://arxiv.org/abs/1503.00208v1,2015-03-01T03:16:34Z,2015-03-01T03:16:34Z,"  Microsimulation based frameworks have become very popular in many research
areas including travel demand modeling where activity-based models have been in
the center of attention for the past decade. Advanced activity-based models
synthesize the entire population of the study region and simulate their
activities in a way that they can keep track of agents resources as well as
their spatial location. However, the models that are built for these frameworks
do not take into account this information mainly because they do not have them
at the modeling stage. This paper tries to describe the importance of this
information by analyzing a travel survey and generate the actual alternatives
that individuals had when making their trips. With a focus on transit, the
study reveals how transit alternatives are limited\unavailable in certain areas
which must be taken in to account in our mode choice models. Some statistics
regarding available alternatives and the constraints people encounter when
making a choice are presented with a comprehensive choice set formation. A mode
choice model is then developed based on this approach to represent the
importance of such information.
",['\nZohreh Ghasemi\n'],,,http://arxiv.org/abs/1503.00208v1,cs.OH,['cs.OH'],,,[]
Physical Biomodeling: a new field enabled by 3-D printing in biomodeling,http://arxiv.org/abs/1502.04687v2,2015-02-16T20:24:09Z,2015-07-12T23:57:02Z,"  Accurate physical modeling with 3D-printing techniques could lead to new
approaches to study structure and dynamics of biological systems complementing
computational methods. Computational biology has become an important part of
research over the last couple of decades. Now 3D printing technology opens the
door for a new field, Physical Biomodeling, at the intersection of experimental
data, computational biology and physical modeling for study of biological
systems, such as protein folding at nano-scale. Here I explore this new domain
of precision physical modeling and correlate it with existing visualization and
computational systems and future possibilities. Dynamic physical models can be
designed to-scale that can serve as research tools in future along with
existing biocomputational tools and databases, adding a third angle to tackle
unsolved scientific problems.
",['\nPromita Chakraborty\n'],,,http://arxiv.org/abs/1502.04687v2,cs.OH,['cs.OH'],,,[]
"Privately Information Sharing with Delusive Paths for Data Forwarding in
  Vehicular Networks",http://arxiv.org/abs/1502.05551v2,2015-02-19T12:33:13Z,2016-10-24T10:00:50Z,"  We discuss how to efficiently forward data in vehicular networks. Existing
solutions do not make full use of trajectory planning of nearby vehicles, or
social attributes. The development of onboard navigation system provides
drivers some traveling route information. The main novelty of our approach is
to envision sharing partial traveling information to the encountered vehicles
for better service. Our data forwarding algorithm utilizes this lightweight
information under the delusive paths privacy preservation together with the
social community structure in vehicular networks. We assume that data
transmission is carried by vehicles and road side units (RSUs), while cellular
network manages and coordinates relevant global information. The approximate
destination set is the set of RSUs that are often passed by the destination
vehicle. RSU importance is raised by summing encounter ratios of RSUs in the
same connected component. We first define a concept of space-time
approachability which is derived from shared partial traveling route and
encounter information. It describes the capability of a vehicle to advance
messages toward destination. Then, we design a novel data forwarding algorithm,
called approachability based algorithm, which combines the space-time
approachability with the social community attribute in vehicular networks. We
evaluate our approachability based algorithm on data sets from San Francisco
Cabspotting and Shanghai Taxi Movement. Results show that the partially shared
traveling information plays a positive role in data forwarding in vehicular
networks. Approachability based data forwarding algorithm achieves a better
performance than existing social based algorithms in vehicular networks.
","['\nZhong Li\n', '\nCheng Wang\n', '\nLu Shao\n', '\nChangjun Jiang\n']",This paper has been withdrawn by the author due to patent protection,,http://arxiv.org/abs/1502.05551v2,cs.OH,['cs.OH'],,,[]
"Analysis of Lithography Based Approaches In development of Semi
  Conductors",http://arxiv.org/abs/1502.05887v2,2015-02-20T14:45:10Z,2015-02-24T17:02:13Z,"  The end of the 19th century brought about a change in the dynamics of
computing by the development of the microprocessor. Huge bedroom size computers
began being replaced by portable, smaller sized desktops. Today the world is
dominated by silicon, which has circumscribed chip development for computers
through microprocessors. Majority of the integrated circuits that are
manufactured at present are developed using the concept of Lithography. This
paper presents a detailed analysis of multiple Lithography methodologies as a
means for advanced integrated circuit development. The study paper primarily
restricts to examples in the context of Lithography, surveying the various
existing techniques of Lithography in literature, examining feasible and
efficient methods, highlighting the various pros and cons of each of them.
",['\nJatin Chopra\n'],"12 pages, 6 figures in International Journal of Computer Science &
  Infromation Technology(IJCSIT) Vol6,No6,December 2014",,http://dx.doi.org/10.5121/ijcsit2014.6604,cs.OH,['cs.OH'],10.5121/ijcsit2014.6604,,[]
"Converting ECG and other paper legated biomedical maps into digital
  signals",http://arxiv.org/abs/1502.05906v2,2015-02-20T15:35:27Z,2015-03-05T16:01:46Z,"  This paper presents a digital signal processing tool developed using
MatlabTM, which provides a very low-cost and effective strategy for
analog-to-digital conversion of legated paper biomedical maps without requiring
dedicated hardware. This software-based approach is particularly helpful for
digitalizing biomedical signals acquired from analogical devices equipped with
a plottingter. Albeit signals used in biomedical diagnosis are the primary
concern, this imaging processing tool is suitable to modernize facilities in a
non-expensive way. Legated paper ECG and EEG charts can be fast and efficiently
digitalized in order to be added in existing up-to-date medical data banks,
improving the follow-up of patients.
","['\nA. R. Gomes e Silva\n', '\nH. M. de Oliveira\n', '\nR. D. Lins\n']","6 pages, 9 figures","Lecture Notes in Computer Science, LNCS 5046, GREC 2007, pp.
  21-28, 2008",http://dx.doi.org/10.1007/978-3-540-88188-9,cs.OH,['cs.OH'],10.1007/978-3-540-88188-9,,[]
"Improved Model for Wire-Length Estimation in Stochastic Wiring
  Distribution",http://arxiv.org/abs/1502.05931v1,2015-02-19T16:59:02Z,2015-02-19T16:59:02Z,"  This paper presents a pair of improved stochastic wiring distribution model
for better estimation of on-chip wire lengths. The proposed models provide 28 -
50% reduction in error when estimating the average on-chip wire length compared
to the estimation using the existing models. The impact of Rent's exponent on
the average wire length estimation is also investigated to demonstrate
limitations of the approximations used in some of the current models. To
improve the approximations of the model a new threshold for Rent's constant is
recommended. Simulation results demonstrate that proposed models with the new
threshold reduce the error of estimation by 38 to 75 percent compared to the
previous works.
","['\nMohamed S. Hefeida\n', '\nMasud H. Chowdhury\n']",,,http://arxiv.org/abs/1502.05931v1,cs.OH,['cs.OH'],,,[]
What Is an Emerging Technology?,http://arxiv.org/abs/1503.00673v4,2015-02-13T17:28:50Z,2016-01-04T13:16:10Z,"  There is considerable and growing interest in the emergence of novel
technologies, especially from the policy-making perspective. Yet as an area of
study, emerging technologies lacks key foundational elements, namely a
consensus on what classifies a technology as 'emergent' and strong research
designs that operationalize central theoretical concepts. The present paper
aims to fill this gap by developing a definition of 'emerging technologies' and
linking this conceptual effort with the development of a framework for the
operationalisation of technological emergence. The definition is developed by
combining a basic understanding of the term and in particular the concept of
'emergence' with a review of key innovation studies dealing with definitional
issues of technological emergence. The resulting definition identifies five
attributes that feature in the emergence of novel technologies. These are: (i)
radical novelty, (ii) relatively fast growth, (iii) coherence, (iv) prominent
impact, and (v) uncertainty and ambiguity. The framework for operationalising
emerging technologies is then elaborated on the basis of the proposed
attributes. To do so, we identify and review major empirical approaches (mainly
in, although not limited to, the scientometric domain) for the detection and
study of emerging technologies (these include indicators and trend analysis,
citation analysis, co-word analysis, overlay mapping, and combinations thereof)
and elaborate on how these can be used to operationalise the different
attributes of emergence.
","['\nDaniele Rotolo\n', '\nDiana Hicks\n', '\nBen R. Martin\n']",Research Policy (in press),,http://arxiv.org/abs/1503.00673v4,cs.OH,['cs.OH'],,,[]
Fault Analysis Using Gegenbauer Multiresolution Analysis,http://arxiv.org/abs/1503.00698v1,2015-02-12T14:09:35Z,2015-02-12T14:09:35Z,"  This paper exploits the multiresolution analysis in the fault analysis on
transmission lines. Faults were simulated using the ATP (Alternative Transient
Program), considering signals at 128/cycle. A nonorthogonal multiresolution
analysis was provided by Gegenbauer scaling and wavelet filters. In the cases
where the signal reconstruction is not required, orthogonality may be
immaterial. Gegenbauer filter banks are thereby offered in this paper as a tool
for analyzing fault signals on transmission lines. Results are compared to
those ones derived from a 4-coefficient Daubechies filter. The main advantages
in favor of Gegenbauer filters are their smaller computational effort and their
constant group delay, as they are symmetric filters.
","['\nL. R. Soares\n', '\nH. M. de Oliveira\n']","6 pages, 12 figures. In: Transmission and Distribution IEEE/PES/T&D
  Latin America, Sao Paulo, Brazil, 2004",,http://dx.doi.org/10.1109/TDC.2004.1432473,math.CA,"['math.CA', 'cs.OH']",10.1109/TDC.2004.1432473,,[]
"ECPR: Environment- and Context-aware Combined Power and Rate Distributed
  Congestion Control for Vehicular Communications",http://arxiv.org/abs/1502.00054v2,2015-01-31T01:34:02Z,2016-06-05T20:03:33Z,"  Safety and efficiency applications in vehicular networks rely on the exchange
of periodic messages between vehicles. These messages contain position, speed,
heading, and other vital information that makes the vehicles aware of their
surroundings. The drawback of exchanging periodic cooperative messages is that
they generate significant channel load. Decentralized Congestion Control (DCC)
algorithms have been proposed to minimize the channel load. However, while the
rationale for periodic message exchange is to improve awareness, existing DCC
algorithms do not use awareness as a metric for deciding when, at what power,
and at what rate the periodic messages need to be sent in order to make sure
all vehicles are informed. We propose an environment- and context-aware DCC
algorithm combines power and rate control in order to improve cooperative
awareness by adapting to both specific propagation environments (e.g., urban
intersections, open highways, suburban roads) as well as application
requirements (e.g., different target cooperative awareness range). Studying
various operational conditions (e.g., speed, direction, and application
requirement), ECPR adjusts the transmit power of the messages in order to reach
the desired awareness ratio at the target distance while at the same time
controlling the channel load using an adaptive rate control algorithm. By
performing extensive simulations, including realistic propagation as well as
environment modeling and realistic vehicle operational environments (varying
demand on both awareness range and rate), we show that ECPR can increase
awareness by 20% while keeping the channel load and interference at almost the
same level. When permitted by the awareness requirements, ECPR can improve the
average message rate by 18% compared to algorithms that perform rate adaptation
only.
","['\nBengi Aygun\n', '\nMate Boban\n', '\nAlexander M. Wyglinski\n']","37 Pages, 12 Figures, 5 Tables, Elsevier Computer Communications, May
  2016",,http://dx.doi.org/10.1016/j.comcom.2016.05.015,cs.OH,['cs.OH'],10.1016/j.comcom.2016.05.015,,[]
"E-BLOW: E-Beam Lithography Overlapping aware Stencil Planning for MCC
  System",http://arxiv.org/abs/1502.00621v1,2015-02-01T18:01:50Z,2015-02-01T18:01:50Z,"  Electron beam lithography (EBL) is a promising maskless solution for the
technology beyond 14nm logic node. To overcome its throughput limitation,
industry has proposed character projection (CP) technique, where some complex
shapes (characters) can be printed in one shot. Recently the traditional EBL
system is extended into multi-column cell (MCC) system to further improve the
throughput. In MCC system, several independent CPs are used to further speed-up
the writing process. Because of the area constraint of stencil, MCC system
needs to be packed/planned carefully to take advantage of the characters. In
this paper, we prove that the overlapping aware stencil planning (OSP) problem
is NP-hard. To solve OSP problem in MCC system, we present a tool, E-BLOW, with
several novel speedup techniques, such as successive relaxation, dynamic
programming, and KD-Tree based clustering. Experimental results show that,
compared with previous works, E-BLOW demonstrates better performance for both
conventional EBL system and MCC system.
","['\nBei Yu\n', '\nKun Yuan\n', '\nJhih-Rong Gao\n', '\nDavid Z. Pan\n']",arXiv admin note: text overlap with arXiv:1402.2435,,http://arxiv.org/abs/1502.00621v1,cs.OH,['cs.OH'],,,[]
"CHAOS: Accurate and Realtime Detection of Aging-Oriented Failure Using
  Entropy",http://arxiv.org/abs/1502.00781v1,2015-02-03T08:50:52Z,2015-02-03T08:50:52Z,"  Even well-designed software systems suffer from chronic performance
degradation, also named ""software aging"", due to internal (e.g. software bugs)
and external (e.g. resource exhaustion) impairments. These chronic problems
often fly under the radar of software monitoring systems before causing severe
impacts (e.g. system failure). Therefore it's a challenging issue how to timely
detect these problems to prevent system crash. Although a large quantity of
approaches have been proposed to solve this issue, the accuracy and
effectiveness of these approaches are still far from satisfactory due to the
insufficiency of aging indicators adopted by them. In this paper, we present a
novel entropy-based aging indicator, Multidimensional Multi-scale Entropy
(MMSE). MMSE employs the complexity embedded in runtime performance metrics to
indicate software aging and leverages multi-scale and multi-dimension
integration to tolerate system fluctuations. Via theoretical proof and
experimental evaluation, we demonstrate that MMSE satisfies Stability,
Monotonicity and Integration which we conjecture that an ideal aging indicator
should have. Based upon MMSE, we develop three failure detection approaches
encapsulated in a proof-of-concept named CHAOS. The experimental evaluations in
a Video on Demand (VoD) system and in a real-world production system,
AntVision, show that CHAOS can detect the failure-prone state in an
extraordinarily high accuracy and a near 0 Ahead-Time-To-Failure (ATTF).
Compared to previous approaches, CHAOS improves the detection accuracy by about
5 times and reduces the ATTF even by 3 orders of magnitude. In addition, CHAOS
is light-weight enough to satisfy the realtime requirement.
","['\nPengfei Chen\n', '\nYong Qi\n', '\nDi Hou\n']",15,,http://arxiv.org/abs/1502.00781v1,cs.OH,['cs.OH'],,,[]
"Etude des déterminants psychologiques de la persistance dans l'usage
  d'un jeu sérieux : évaluation de l'environnement optimal d'apprentissage
  avec Mecagenius?",http://arxiv.org/abs/1502.01818v1,2015-02-06T07:30:53Z,2015-02-06T07:30:53Z,"  The aim of this paper is to show the relevance of motivational key concepts
in evaluating the use of serious game. This research involves 115 students
training with Mecagenius (serious game in mechanical engineering). The results
of the exploratory study also confirm the relevance of the use of flow in
Education scale (EduFlow) to evaluate the optimal learning experienc ewith a
serious game. It also appears that EduFlow is related to specific actions
within the school context such as self-efficacy, motivational climate and
interest.
","['\nJean Heutte\nLGMT\n', '\nMichel Galaup\nLGMT\n', '\nCatherine Lelardeux\nLGMT\n', '\nPierre Lagarrigue\nLGMT\n', '\nFabien Fenouillet\n']",in French,"STICEF, ATIEF, 2014, Evaluation dans les jeux s\'erieux, 21,
  pp.ISSN : 1764-7223, http://sticef.org/",http://arxiv.org/abs/1502.01818v1,cs.OH,['cs.OH'],,,"['LGMT', 'LGMT', 'LGMT', 'LGMT']"
"FRAME: Fast and Realistic Attacker Modeling and Evaluation for Temporal
  Logical Correlation in Static Noise",http://arxiv.org/abs/1502.02236v1,2015-02-08T10:44:55Z,2015-02-08T10:44:55Z,"  We propose a method called Fast and Realistic Attacker Modeling and
Evaluation (FRAME) that can reduce pessimism in static noise analysis by
exploiting temporal logical correlation of attackers and using novel techniques
termed envelopes and $\sigma$ functions. Unlike conventional pruning-based
approaches, FRAME efficiently considers all relevant attackers, thereby
producing more realistic results. FRAME was tested with complex industrial
design and successfully reduced the pessimism of conventional techniques by
30.4% on average, with little computational overhead.
","['\nSungroh Yoon\n', '\nNahmsuk Oh\n', '\nPeivand Tehrani\n', '\nEui-Young Chung\n', '\nGiovanni De Micheli\n']",,,http://arxiv.org/abs/1502.02236v1,cs.OH,"['cs.OH', 'B.7.2']",,,[]
"Extended Report: Fine-grained Recognition of Abnormal Behaviors for
  Early Detection of Mild Cognitive Impairment",http://arxiv.org/abs/1501.05581v1,2015-01-22T17:34:16Z,2015-01-22T17:34:16Z,"  According to the World Health Organization, the rate of people aged 60 or
more is growing faster than any other age group in almost every country, and
this trend is not going to change in a near future. Since senior citizens are
at high risk of non communicable diseases requiring long-term care, this trend
will challenge the sustainability of the entire health system. Pervasive
computing can provide innovative methods and tools for early detecting the
onset of health issues. In this paper we propose a novel method to detect
abnormal behaviors of elderly people living at home. The method relies on
medical models, provided by cognitive neuroscience researchers, describing
abnormal activity routines that may indicate the onset of early symptoms of
mild cognitive impairment. A non-intrusive sensor-based infrastructure acquires
low-level data about the interaction of the individual with home appliances and
furniture, as well as data from environmental sensors. Based on those data, a
novel hybrid statistical-symbolical technique is used to detect the abnormal
behaviors of the patient, which are communicated to the medical center.
Differently from related works, our method can detect abnormal behaviors at a
fine-grained level, thus providing an important tool to support the medical
diagnosis. In order to evaluate our method we have developed a prototype of the
system and acquired a large dataset of abnormal behaviors carried out in an
instrumented smart home. Experimental results show that our technique is able
to detect most anomalies while generating a small number of false positives.
","['\nDaniele Riboni\n', '\nClaudio Bettini\n', '\nGabriele Civitarese\n', '\nZaffar Haider Janjua\n', '\nRim Helaoui\n']",,,http://arxiv.org/abs/1501.05581v1,cs.OH,['cs.OH'],,,[]
Sparsity based Efficient Cross-Correlation Techniques in Sensor Networks,http://arxiv.org/abs/1501.06473v3,2015-01-26T16:38:34Z,2016-06-11T05:21:06Z,"  Cross-correlation is a popular signal processing technique used in numerous
location tracking systems for obtaining reliable range information. However,
its efficient design and practical implementation has not yet been achieved on
mote platforms that are typical in wireless sensor network due to resource
constrains. In this paper, we propose SparseS-XCorr: cross-correlation via
structured sparse representation, a new computing framework for ranging based
on L1-minimization and structured sparsity. The key idea is to compress the
ranging signal samples on the mote by efficient random projections and transfer
them to a central device; where a convex optimization process estimates the
range by exploiting the sparse signal structure in the proposed correlation
dictionary. Through theoretical validation, extensive empirical studies and
experiments on an end-to-end acoustic ranging system implemented on resource
limited off-the-shelf sensor nodes, we show that the proposed framework can
achieve up to two orders of magnitude better performance compared to other
approaches such as working on DCT domain and downsampling. Compared to the
standard cross-correlation, it is able to obtain range estimates with a bias of
2-6cm with 30% and approximately 100cm with 5% compressed measurements. Its
structured sparsity model is able to improve the ranging accuracy by 40% under
challenging recovery conditions (such as high compression factor and low
signal-to-noise ratio) by overcoming limitations due to dictionary coherence.
","['\nPrasant Misra\n', '\nWen Hu\n', '\nMingrui Yang\n', '\nMarco Duarte\n', '\nSanjay Jha\n']",,,http://arxiv.org/abs/1501.06473v3,cs.OH,['cs.OH'],,,[]
"A Simulation Modeling Approach for Optimization of Storage Space
  Allocation in Container Terminal",http://arxiv.org/abs/1501.06802v1,2015-01-27T16:10:23Z,2015-01-27T16:10:23Z,"  Container handling problems at container terminals are NP-hard problems. This
paper presents an approach using discrete-event simulation modeling to optimize
solution for storage space allocation problem, taking into account all various
interrelated container terminal handling activities. The proposed approach is
applied on a real case study data of container terminal at Alexandria port. The
computational results show the effectiveness of the proposed model for
optimization of storage space allocation in container terminal where 54%
reduction in containers handling time in port is achieved.
","['\nGamal Abd El-Nasser A. Said\n', '\nEl-Sayed M. El-Horbaty\n']","International Journal of Computer, Information, Systems and Control
  Engineering Vol:9 No:1, 2015","Information, Systems and Control Engineering Vol. 9, No. 1, 2015,
  pp. 168-173",http://arxiv.org/abs/1501.06802v1,cs.OH,['cs.OH'],,,[]
Contemporary Internet of Things platforms,http://arxiv.org/abs/1501.07438v1,2015-01-29T12:51:41Z,2015-01-29T12:51:41Z,"  This document regroups a representative, but non-exhaustive, list of
contemporary IoT platforms. The platforms are ordered alphabetically. The aim
of this document is to provide the a quick review of current IoT platforms, as
well as relevant information.
","['\nJulien Mineraud\n', '\nOleksiy Mazhelis\n', '\nXiang Su\n', '\nSasu Tarkoma\n']","6 pages, 0 figure, techreport, review of IoT platforms",,http://arxiv.org/abs/1501.07438v1,cs.OH,['cs.OH'],,,[]
Complexity of Power Draws for Load Disaggregation,http://arxiv.org/abs/1501.02954v1,2015-01-13T11:09:51Z,2015-01-13T11:09:51Z,"  Non-Intrusive Load Monitoring (NILM) is a technology offering methods to
identify appliances in homes based on their consumption characteristics and the
total household demand. Recently, many different novel NILM approaches were
introduced, tested on real-world data and evaluated with a common evaluation
metric. However, the fair comparison between different NILM approaches even
with the usage of the same evaluation metric is nearly impossible due to
incomplete or missing problem definitions. Each NILM approach typically is
evaluated under different test scenarios. Test results are thus influenced by
the considered appliances, the number of used appliances, the device type
representing the appliance and the pre-processing stages denoising the
consumption data. This paper introduces a novel complexity measure of
aggregated consumption data providing an assessment of the problem complexity
affected by the used appliances, the appliance characteristics and the
appliance usage over time. We test our load disaggregation complexity on
different real-world datasets and with a state-of-the-art NILM approach. The
introduced disaggregation complexity measure is able to classify the
disaggregation problem based on the used appliance set and the considered
measurement noise.
","['\nDominik Egarter\n', '\nManfred Pöchacker\n', '\nWilfried Elmenreich\n']",,,http://arxiv.org/abs/1501.02954v1,cs.OH,['cs.OH'],,,[]
"Modified Design of Microstrip Patch Antenna for WiMAX Communication
  System",http://arxiv.org/abs/1412.8561v1,2014-12-30T04:20:31Z,2014-12-30T04:20:31Z,"  In this paper, a new design for U-shaped microstrip patch antenna is
proposed, which can be used in WiMAX communication systems. The aim of this
paper is to optimize the performance of microstrip patch antenna. Nowadays,
WiMAX communication applications are widely using U-shaped microstrip patch
antenna and it has become very popular. Our proposed antenna design uses 4-4.5
GHZ frequency band and it is working at narrowband within this band. RT/DUROID
5880 material is used for creating the substrate of the microstrip antenna.
This modified design of the microstrip patch antenna gives high performance in
terms of gain and return loss.
","['\nNidhi Kumari Lal\n', '\nAshutosh Kumar Singh\n']",,,http://dx.doi.org/10.9781/ijimai.2014.315,cs.OH,['cs.OH'],10.9781/ijimai.2014.315,,[]
Mapping and Matching Algorithms: Data Mining by Adaptive Graphs,http://arxiv.org/abs/1501.00491v1,2015-01-02T18:05:04Z,2015-01-02T18:05:04Z,"  Assume we have two bijective functions $U(x)$ and $M(x)$ with $M(x)\neq U(x)$
for all $x$ and $M,N: \N \rightarrow \N$ . Every day and in different
locations, we see the different results of $U$ and $M$ without seeing $x$. We
are not assured about the time stamp nor the order within the day but at least
the location is fully defined. We want to find the matching between $U(x)$ and
$M(x)$ (i.e., we will not know $x$). We formulate this problem as an adaptive
graph mining: we develop the theory, the solution, and the implementation. This
work stems from a practical problem thus our definitions. The solution is
simple, clear, and the implementation parallel and efficient. In our
experience, the problem and the solution are novel and we want to share our
finding.
","[""\nPaolo D'Alberto\n"", '\nVeronica Milenkly\n']",,,http://arxiv.org/abs/1501.00491v1,cs.OH,['cs.OH'],,,[]
PC Guided Automatic Vehicle System,http://arxiv.org/abs/1501.01109v1,2015-01-06T08:27:46Z,2015-01-06T08:27:46Z,"  The main objective of this paper is to design and develop an automatic
vehicle, fully controlled by a computer system. The vehicle designed in the
present work can move in a pre-determined path and work automatically without
the need of any human operator and it also controlled by human operator. Such a
vehicle is capable of performing wide variety of difficult tasks in space
research, domestic, scientific and industrial fields. For this purpose, an IBM
compatible PC with Pentium microprocessor has been used which performed the
function of the system controller. Its parallel printer port has been used as
data communication port to interface the vehicle. A suitable software program
has been developed for the system controller to send commands to the vehicle.
","['\nM. A. A. Mashud\n', '\nM. R. Hossain\n', '\nMustari Zaman\n', '\nM. A. Razzaque\n']","10 pages, International Journal on Cybernetics & Informatics
  (IJCI);2014",,http://dx.doi.org/10.5121/ijci.2014.3601,cs.OH,['cs.OH'],10.5121/ijci.2014.3601,,[]
Micro-location for Internet of Things equipped Smart Buildings,http://arxiv.org/abs/1501.01539v2,2015-01-07T16:13:48Z,2017-05-25T11:45:05Z,"  Micro-location is the process of locating any entity with high accuracy
(possibly in centimeters), while geofencing is the process of creating a
virtual fence around a so-called Point of Interest (PoI). In this paper, we
present an insight into various micro-location enabling technologies and
services. We also discuss how these can accelerate the incorporation of
Internet of Things (IoT) in smart buildings. We argue that micro-location based
location-aware solutions can play a significant role in facilitating the
tenants of an IoT equipped smart building. Also, such advanced technologies
will enable the smart building control system through minimal actions performed
by the tenants. We also highlight the existing and envisioned services to be
provided by using micro-location enabling technologies. We describe the
challenges and propose some potential solutions such that micro-location
enabling technologies and services are thoroughly integrated with IoT equipped
smart building.
","['\nFaheem Zafari\n', '\nIoannis Papapanagiotou\n', '\nKonstantinos Christidis\n']","""This work has been published in IEEE Internet of Things Journal","IEEE Internet of Things Journal Volume: 3, Issue: 1, Feb. 2016",http://dx.doi.org/10.1109/JIOT.2015.2442956,cs.OH,['cs.OH'],10.1109/JIOT.2015.2442956,,[]
"Design, Analysis, and Simulation of a Pipe-Welding Robot with Fixed
  Plinth",http://arxiv.org/abs/1501.01930v1,2015-01-06T05:14:45Z,2015-01-06T05:14:45Z,"  Industrial requirements concerning the increased efficiency and high rate of
manufacturing result in the development of manufacturer robots, and a vast
group of these types of robots is used for welding. This study presented the
design, analysis, and simulation of a pipe-welding robot with fixed plinth for
a constant circular welding around the pipes. Design of a welding robot capable
of keeping the electrode orientation, welding speed, and distance between
electrode and pipe surface constant can improve the quality of welding; thus, a
five-linked articulated robot was designed for this purpose. Solving of direct
and diverse kinematics and dynamics equations of the robot was done by means of
Matlab software. The robot was also simulated using a program written in Matlab
and the diagrams of angles, velocities, and accelerations of all the arms, and
the applied force and torque of each arm required for drive the mechanism were
obtained.
","['\nAnahita Emami\n', '\nSeyedmeysam Khaleghian\n', '\nMohammad Mahjoob Jahromi\n']","6 pages, 11 figures, 3rd International Conference on Manufacturing
  Engineering, ICME2011, Tehran, Iran",,http://arxiv.org/abs/1501.01930v1,cs.OH,['cs.OH'],,,[]
GREAT Process Modeller user manual,http://arxiv.org/abs/1502.07693v1,2015-01-07T12:35:11Z,2015-01-07T12:35:11Z,"  This report contains instructions to install, uninstall and use GREAT Process
Modeller, a tool that supports Communication Analysis, a communication-oriented
business process modelling method. GREAT allows creating communicative event
diagrams (i.e. business process models), specifying message structures (which
describe the messages associated to each communicative event), and
automatically generating a class diagram (representing the data model of an
information system that would support such organisational communication). This
report briefly describes the methodological background of the tool. This
handbook explains the modelling techniques in detail: Espa\~na, S., A.
Gonz\'alez, \'O. Pastor and M. Ruiz (2012). Communication Analysis modelling
techniques. Technical report ProS-TR-2012-02, PROS Research Centre, Universitat
Polit\`ecnica de Val\`encia, Spain, arXiv:1205.0987.
","['\nUrko Rueda\n', '\nSergio España\n', '\nMarcela Ruiz\n']",8 pages,,http://arxiv.org/abs/1502.07693v1,cs.OH,"['cs.OH', '68N01', 'D.2.1']",,,[]
"Multiple-Campaign Ad-Targeting Deployment: Parallel Response Modeling,
  Calibration and Scoring Without Personal User Information",http://arxiv.org/abs/1501.02185v2,2015-01-02T18:02:52Z,2015-05-11T00:30:00Z,"  We present a vertical introduction to campaign optimization; that is, the
ability to predict the user response to an ad campaign without any users'
profiles on average and for each exposed ad. In practice, we present an
approach to build a polytomous model, multi response, composed by several
hundred binary models using generalized linear models. The theory has been
introduced twenty years ago and it has been applied in different fields since
then. Here, we show how we optimize hundreds campaigns and how this large
number of campaigns may overcome a few characteristic caveats of single
campaign optimization. We discuss the problem and solution of training and
calibration at scale. We present statistical performance as {\em coverage},
{\em precision} and {\em recall} used in classification. We present also a
discussion about the potential performance as throughput: how many decisions
can be done per second streaming the bid auctions also by using dedicated
hardware.
","[""\nPaolo D'Alberto\n""]",,,http://arxiv.org/abs/1501.02185v2,stat.AP,"['stat.AP', 'cs.OH']",,,[]
"Image Processing Code for Sharpening Photoelastic Fringe Patterns and
  Its Usage in Determination of Stress Intensity Factors in a Sample Contact
  Problem",http://arxiv.org/abs/1501.02245v1,2015-01-06T06:13:59Z,2015-01-06T06:13:59Z,"  This study presented a type of image processing code which is used for
sharpening photoelastic fringe patterns of transparent materials in
photoelastic experiences to determine the stress distribution. C-Sharp software
was utilized for coding the algorithm of this image processing method. For
evaluation of this code, the results of a photoelastic experience of a sample
contact problem between a half-plane with an oblique edge crack and a tilted
wedge using this image processing method was compared with the FEM results of
the same problem in order to obtain the stress intensity factors (SIF) of the
specimen. A good agreement between experimental results extracted from this
method of image processing and computational results was observed.
","['\nSeyedmeysam Khaleghian\n', '\nAnahita Emami\n', '\nNasser Soltani\n']","4 pages, 5 figures, ICME 2011, Tehran, Iran",,http://arxiv.org/abs/1501.02245v1,cond-mat.mtrl-sci,"['cond-mat.mtrl-sci', 'cs.OH', 'physics.optics']",,,[]
"Cloud Enabled Emergency Navigation Using Faster-than-real-time
  Simulation",http://arxiv.org/abs/1412.7059v2,2014-12-22T17:08:59Z,2015-01-03T15:41:35Z,"  State-of-the-art emergency navigation approaches are designed to evacuate
civilians during a disaster based on real-time decisions using a pre-defined
algorithm and live sensory data. Hence, casualties caused by the poor decisions
and guidance are only apparent at the end of the evacuation process and cannot
then be remedied. Previous research shows that the performance of routing
algorithms for evacuation purposes are sensitive to the initial distribution of
evacuees, the occupancy levels, the type of disaster and its as well its
locations. Thus an algorithm that performs well in one scenario may achieve bad
results in another scenario. This problem is especially serious in
heuristic-based routing algorithms for evacuees where results are affected by
the choice of certain parameters. Therefore, this paper proposes a
simulation-based evacuee routing algorithm that optimises evacuation by making
use of the high computational power of cloud servers. Rather than guiding
evacuees with a predetermined routing algorithm, a robust Cognitive Packet
Network based algorithm is first evaluated via a cloud-based simulator in a
faster-than-real-time manner, and any ""simulated casualties"" are then re-routed
using a variant of Dijkstra's algorithm to obtain new safe paths for them to
exits. This approach can be iterated as long as corrective action is still
possible.
","['\nHuibo Bi\n', '\nErol Gelenbe\n']",Submitted to PerNEM'15 for review,,http://arxiv.org/abs/1412.7059v2,cs.OH,['cs.OH'],,,[]
Time-Symmetric Physics: A Radical Approach to the Decoherence Problem,http://arxiv.org/abs/1501.00027v1,2014-12-23T21:30:24Z,2014-12-23T21:30:24Z,"  The most powerful form of quantum learning system possible would somehow
learn the parameters W of a quantum system f(X, W), for f representing the
largest, most powerful set of possible input-output relations. This paper
addresses the issue of how to enlarge the set represented by f, by using a new
formulation of time-symmetric physics to model analog quantum computers based
on spin and by exploring possible sources of backwards-time free energy so as
to address problems of decoherence and dissipation.
",['\nPaul J. Werbos\n'],"5 pages, 2 figures. Invited keynote talk for Quantum Computing
  workshop at Pacific Rim AI (PRICAI) conference, December 2014. Simple summary
  and link to the slides and audio of the talk posted at
  http://www.werbos.com/quantum.htm",,http://arxiv.org/abs/1501.00027v1,cs.OH,"['cs.OH', 'quant-ph']",,,[]
"Autonomous Load Disaggregation Approach based on Active Power
  Measurements",http://arxiv.org/abs/1412.2877v2,2014-12-09T07:50:57Z,2014-12-14T11:59:05Z,"  With the help of smart metering valuable information of the appliance usage
can be retrieved. In detail, non-intrusive load monitoring (NILM), also called
load disaggregation, tries to identify appliances in the power draw of an
household. In this paper an unsupervised load disaggregation approach is
proposed that works without a priori knowledge about appliances. The proposed
algorithm works autonomously in real time. The number of used appliances and
the corresponding appliance models are learned in operation and are
progressively updated. The proposed algorithm is considering each useful and
suitable detected power state. The algorithm tries to detect power states
corresponding to on/off appliances as well as to multi-state appliances based
on active power measurements in 1s resolution. We evaluated the novel
introduced load disaggregation approach on real world data by testing the
possibility to disaggregate energy demand on appliance level.
","['\nDominik Egarter\n', '\nWilfried Elmenreich\n']",,,http://dx.doi.org/10.1109/PERCOMW.2015.7134051,cs.OH,['cs.OH'],10.1109/PERCOMW.2015.7134051,,[]
Personal Multi-threading,http://arxiv.org/abs/1412.3579v1,2014-12-11T09:06:40Z,2014-12-11T09:06:40Z,"  Multi-threading allows agents to pursue a heterogeneous collection of tasks
in an orderly manner. The view of multi-threading that emerges from thread
algebra is applied to the case where a single agent, who may be human,
maintains a hierarchical multithread as an architecture of its own activities.
",['\nJan A. Bergstra\n'],,,http://arxiv.org/abs/1412.3579v1,cs.OH,['cs.OH'],,,[]
Domotic Embedded System,http://arxiv.org/abs/1412.4406v1,2014-12-14T20:52:32Z,2014-12-14T20:52:32Z,"  This paper presents an original domotic embedded system for room temperature
monitoring. The OpenRemote is the main software interface between the user and
the system, but other software components and communication protocols are used,
such as 1-Wire protocol for temperature monitoring devices, RS-232 for the
central PC unit and OWFS software for remote control using Android mobile
devices. The system architecture consists in hardware and software components
to remote control a room temperature parameter for energy efficiency
increasing.
",['\nLidia Dobrescu\n'],,,http://arxiv.org/abs/1412.4406v1,cs.OH,['cs.OH'],,,[]
Neuronal noise as a physical resource for human cognition,http://arxiv.org/abs/1412.4920v1,2014-12-16T08:42:05Z,2014-12-16T08:42:05Z,"  A new class of energy-efficient digital microprocessor is being developed
which is susceptible to thermal noise and consequently operates in
probabilistic rather than conventional deterministic mode. Hybrid computing
systems which combine probabilistic and deterministic processors can provide
robust and efficient tools for computational problems that hitherto would be
intractable by conventional deterministic algorithm. These developments suggest
a revised perspective on the consequences of ion-channel noise in slender
axons, often regarded as a hindrance to neuronal computations. It is proposed
that the human brain is such an energy-efficient hybrid computational system
whose remarkable characteristics emerge from constructive synergies between
probabilistic and deterministic modes of operation. In particular, the capacity
for intuition and creative problem solving appears to arise naturally from such
a hybrid system. Bearing in mind that physical thermal noise is both pure and
available at no cost, our proposal has implications for attempts to emulate the
energy-efficient human brain on conventional energy-intensive deterministic
supercomputers.
","['\nT. N. Palmer\n', ""\nM. O'Shea\n""]",,,http://arxiv.org/abs/1412.4920v1,q-bio.NC,"['q-bio.NC', 'cs.OH']",,,[]
Qubit Data Structures for Analyzing Computing Systems,http://arxiv.org/abs/1412.1402v1,2014-12-02T12:30:24Z,2014-12-02T12:30:24Z,"  Qubit models and methods for improving the performance of software and
hardware for analyzing digital devices through increasing the dimension of the
data structures and memory are proposed. The basic concepts, terminology and
definitions necessary for the implementation of quantum computing when
analyzing virtual computers are introduced. The investigation results
concerning design and modeling computer systems in a cyberspace based on the
use of two-component structure <memory - transactions> are presented.
","['\nVladimir Hahanov\n', '\nWajeb Gharibi\n', '\nSvetlana Chumachenko\n', '\nEugenia Litvinova\n']","9 pages,4 figures, Proceeding of the Third International Conference
  on Data Mining & Knowledge Management Process (CDKP 2014)",,http://arxiv.org/abs/1412.1402v1,cs.OH,['cs.OH'],,,[]
A dynamic mechanism of Alzheimer based on artificial neural network,http://arxiv.org/abs/1411.4221v1,2014-11-16T06:39:23Z,2014-11-16T06:39:23Z,"  In this paper, we provide another angle to analyze the reasons why Alzheimer
Disease exists. We analyze the dynamic mechanism of Alzheimer Disease based on
the cognitive model that established from artificial neural network. We can
provide some theoretic explanations to Alzheimer Disease through the analyzing
of this model.
",['\nZhi Cheng\n'],"9 pages, 5 figures",,http://arxiv.org/abs/1411.4221v1,cs.OH,['cs.OH'],,,[]
Hostile Intent Enumeration using Soft Computing Techniques,http://arxiv.org/abs/1411.5228v1,2014-11-19T13:54:17Z,2014-11-19T13:54:17Z,"  In any tactical scenario, the successful quantification and triangulation of
potential hostile elements is instrumental to minimize any casualties which
might be incurred. The most commonly deployed infrastructures to cater to this
have mostly been surveillance systems which only extract some data pertaining
to the targets of interest in the area of observation and convey the
information to the human operators. Accordingly, with the ever increasing rate
at which warfare tactics are evolving, there has been a growing need for
smarter solutions to this problem of hostile intent enumeration. Recently, a
number of developments have been made to ameliorate the efficacy and the
certitude with which this task is performed. This paper discusses two of the
most prominent approaches which address this problem and posits the outline of
a novel solution which seeks to address the shortcomings faced by the existing
approaches.
","['\nSouham Biswas\n', '\nManisha J. Nene\n']","Published in the International Journal for Computer Science Issues
  (IJCSI Volume 11, Issue 5, September 2014)",,http://arxiv.org/abs/1411.5228v1,cs.OH,['cs.OH'],,,[]
"The Solving of the Problems with Random Division of an Interval with Use
  of Computer Analytic Programs",http://arxiv.org/abs/1411.6580v2,2014-11-20T22:45:04Z,2014-12-03T09:49:19Z,"  An original approach to solving rather difficult probabilistic problems
arising in studying the readout of random discrete fields and having no exact
analytical solutions at the moment is proposed. Several algorithms for direct,
iterative, and combinatorial-recursive calculations of multidimensional
integral expressions, which can describe partial solutions of these problems,
are presented (these solutions are further used to search for the common closed
analytical regularities). The huge volume of necessary calculations forced us
to formalize completely the algorithms and to transfer all the burden of
routine analytical transforms to a computer. The calculations performed helped
us to establish (and to prove later) a number of new earlier unknown
probabilistic formulas responsible for random division of an interval. One more
important feature of this study is the fact that we introduced a new concept of
'three-dimensional generalized Catalan numbers' and found their explicit form
in solving problems associated with random division of an interval.
","['\nAleksander Reznik\n', '\nVitaly Efimov\n', '\nAleksander Soloview\n', '\nAndrey Torgov\n']",Corrected typos,,http://arxiv.org/abs/1411.6580v2,cs.OH,"['cs.OH', 'math.PR']",,,[]
"Semantic-based Detection of Segment Outliers and Unusual Events for
  Wireless Sensor Networks",http://arxiv.org/abs/1411.2188v1,2014-11-09T02:58:33Z,2014-11-09T02:58:33Z,"  Environmental scientists have increasingly been deploying wireless sensor
networks to capture valuable data that measures and records precise information
about our environment. One of the major challenges associated with wireless
sensor networks is the quality of the data and more specifically the detection
of segment outliers and unusual events. Most previous research has focused on
detecting outliers that are errors that are caused by unreliable sensors and
sensor nodes. However, there is an urgent need for the development of new tools
capable of identifying, tagging and visualizing erroneous segment outliers and
unusual events from sensor data streams. In this paper, we present a SOUE
Detector (Segment Outlier and Unusual Event-Detector) system for wireless
sensor networks that combines statistical analyses using Dynamic Time Warping
(DTW) with domain expert knowledge (captured via an ontology and semantic
inferencing rules). The resulting Web portal enables scientist to efficiently
search across a collection of wireless sensor data streams and identify,
retrieve and display segment outliers (both erroneous and genuine) within the
data streams. In this paper, we firstly describe the detection algorithms, the
implementation details and the functionality of the SOUE Detector system.
Secondly we evaluate our approach using data that comprises sensor observations
collected from a sensor network deployed in the Springbrook National Park in
Queensland, Australia. The experimental results show that the SOUE-Detector can
efficiently detect segment outliers and unusual events with high levels of
precision and recall.
","['\nLianli Gao\n', '\nMichael Bruenig\n', '\nJane Hunter\n']","Dynamic Time Warping, Data Quality, Wireless Sensor Networks, Outlier
  Detection, Unusual Event Detection, Semantic Web","Semantic based Detection of Segment Outliers and Unusual Events
  for Wireless Sensor Networks, International Conference on Information
  Quality, Little Rock, United States, pp.127-144 (2013)",http://arxiv.org/abs/1411.2188v1,cs.OH,"['cs.OH', '68Uxx']",,,[]
"High gain two-stage amplifier with positive capacitive feedback
  compensation",http://arxiv.org/abs/1411.3506v1,2014-11-13T11:31:03Z,2014-11-13T11:31:03Z,"  A novel topology for a high gain two-stage amplifier is proposed. The
proposed circuit is designed in a way that the non-dominant pole is at output
of the first stage. A positive capacitive feedback (PCF) around the second
stage introduces a left half plane (LHP) zero which cancels the phase shift
introduced by the non-dominant pole, considerably. The dominant pole is at the
output node which means that increasing the load capacitance has minimal effect
on stability. Moreover, a simple and effective method is proposed to enhance
slew rate. Simulation shows that slew rate is improved by a factor of 2.44
using the proposed method. The proposed amplifier is designed in a 0.18um CMOS
process. It consumes 0.86mW power from a 1.8V power supply and occupies
3038.5um2 of chip area. The DC gain is 82.7dB and gain bandwidth (GBW) is 88.9
MHz when driving a 5pF capacitive load. Also low frequency CMRR and PSRR+ are
127dB and 83.2dB, respectively. They are 24.8dB and 24.2dB at GBW frequency,
which are relatively high and are other important properties of the proposed
amplifier. Moreover, Simulations show convenient performance of the circuit in
process corners and also presence of mismatch.
","['\nAlireza Mesri\n', '\nMahmoud Mahdipour Pirbazari\n', '\nKhayrollah Hadidi\n', '\nAbdollah Khoei\n']","26 pages, 12 figures, 7 tables Accepted for publication in IET
  Circuits, Devices and Systems","Circuits, Devices & Systems, IET (Volume:9 , Issue: 3 ), 2015",http://dx.doi.org/10.1049/iet-cds.2014.0139,cs.OH,['cs.OH'],10.1049/iet-cds.2014.0139,,[]
"Systems, Resilience, and Organization: Analogies and Points of Contact
  with Hierarchy Theory",http://arxiv.org/abs/1411.0092v3,2014-11-01T09:47:47Z,2017-03-08T07:40:40Z,"  Aim of this paper is to provide preliminary elements for discussion about the
implications of the Hierarchy Theory of Evolution on the design and evolution
of artificial systems and socio-technical organizations. In order to achieve
this goal, a number of analogies are drawn between the System of Leibniz; the
socio-technical architecture known as Fractal Social Organization; resilience
and related disciplines; and Hierarchy Theory. In so doing we hope to provide
elements for reflection and, hopefully, enrich the discussion on the above
topics with considerations pertaining to related fields and disciplines,
including computer science, management science, cybernetics, social systems,
and general systems theory.
",['\nVincenzo De Florio\n'],"To appear in the Proceedings of ANTIFRAGILE'17, 4th International
  Workshop on Computational Antifragility and Antifragile Engineering",,http://arxiv.org/abs/1411.0092v3,cs.OH,['cs.OH'],,,[]
On The Dynamical Nature Of Computation,http://arxiv.org/abs/1410.8402v1,2014-10-30T15:21:34Z,2014-10-30T15:21:34Z,"  Dynamical Systems theory generally deals with fixed point iterations of
continuous functions. Computation by Turing machine although is a fixed point
iteration but is not continuous. This specific category of fixed point
iterations can only be studied using their orbits. Therefore the standard
notion of chaos is not immediately applicable. However, when a suitable
definition is used, it is found that the notion of chaos and fractal sets
exists even in computation. It is found that a non terminating Computation will
be almost surely chaotic, and autonomous learning will almost surely identify
fractal only sets.
","['\nNabarun Mondal\n', '\nPartha P. Ghosh\n']","arXiv admin note: substantial text overlap with arXiv:1407.7417,
  arXiv:1111.4949",,http://arxiv.org/abs/1410.8402v1,cs.OH,"['cs.OH', '03D10, 65P20, 68Q05, 68Q87, 68T05']",,,[]
Photomapping Using Aerial Vehicle,http://arxiv.org/abs/1410.8509v2,2014-10-30T19:29:29Z,2014-11-10T19:45:39Z,"  Creating a photomap plays a critical role in navigation. Therefore, flying
vehicles are usually used to create topdown maps of the environment. In this
report we used two different aerial vehicles to create a map in a simulated
environment
",['\nSu Kim\n'],,,http://arxiv.org/abs/1410.8509v2,cs.OH,['cs.OH'],,,[]
Adaptive two-dimensional wavelet transformation based on the Haar system,http://arxiv.org/abs/1410.0705v1,2014-10-03T18:00:53Z,2014-10-03T18:00:53Z,"  The purpose is to study qualitative and quantitative rates of image
compression by using different Haar wavelet banks. The experimental results of
adaptive compression are provided. The paper deals with specific examples of
orthogonal Haar bases generated by multiresolution analysis. Bases consist of
three piecewise constant wavelet functions with a support $[0,1] \times [0,1]
$.
",['\nMikhail Prisheltsev\n'],,,http://arxiv.org/abs/1410.0705v1,cs.OH,['cs.OH'],,,[]
"Programing implementation of the Quine-McCluskey method for minimization
  of Boolean expression",http://arxiv.org/abs/1410.1059v1,2014-10-04T16:44:51Z,2014-10-04T16:44:51Z,"  A Boolean function is a function that produces a Boolean value output by
logical calculation of Boolean inputs. It plays key roles in programing
algorithms and design of circuits. Minimization of Boolean function is able to
optimize the algorithms and circuits. Quine-McCluskey (QM) method is one of the
most powerful techniques to simplify Boolean expressions. Compared to other
techniques, QM method is more executable and can handle more variables. In
addition, QM method is easier to be implemented in computer programs, which
makes it an efficient technique. There are several versions of QM simulation
codes online, whereas some of them appear to have limitations of variables
numbers or lack the consideration of Dont-Care conditions. Here a QM simulation
code based on C programing is introduced. Theoretically it is able to handle
any number of variables and has taken the Dont-Care conditions into account.
",['\nJiangbo Huang\n'],"22 pages, 14 figures",,http://arxiv.org/abs/1410.1059v1,cs.OH,['cs.OH'],,,[]
"A Novel Design of IEEE 802.15.4 and Solar Based Autonomous Water Quality
  Monitoring Prototype using ECHERP",http://arxiv.org/abs/1410.1773v1,2014-10-02T12:52:37Z,2014-10-02T12:52:37Z,"  The recently advancement in Wireless Sensor Network (WSN) technology has
brought new distributed sensing applications such as water quality monitoring.
With sensing capabilities and using parameters like pH, conductivity and
temperature, the quality of water can be known. This paper proposes a novel
design based on IEEE 802.15.4 (Zig-Bee protocol) and solar energy called
Autonomous Water Quality Monitoring Prototype (AWQMP). The prototype is
designed to use ECHERP routing protocol and Adruino Mega 2560, an open-source
electronic prototyping platform for data acquisition. AWQMP is expected to give
real time data acquirement and to reduce the cost of manual water quality
monitoring due to its autonomous characteristic. Moreover, the proposed
prototype will help to study the behavior of aquatic animals in deployed water
bodies.
",['\nFredrick Romanus Ishengoma\n'],"12 pages, International Journal of Computer Science and Network
  Solutions, Volume 2, Issue 1, January 2014",,http://arxiv.org/abs/1410.1773v1,cs.OH,['cs.OH'],,,[]
"Area Versus Speed Trade-off Analysis of a WiMAX Deinterleaver Circuit
  Design",http://arxiv.org/abs/1410.2889v1,2014-10-10T06:14:09Z,2014-10-10T06:14:09Z,"  Trade-off is one of the main design parameters in the field of electronic
circuit design. Whereas smaller electronics devices which use less hardware due
to techniques like hardware multiplexing or due to smaller devices created due
to techniques developed by nanotechnology and MEMS, are more appealing, a
trade-off between area, power and speed is inevitable. This paper analyses the
trade-off in the design of WiMAX deinterleaver. The main aim is to reduce the
hardware utilization in a deinterleaver but speed and power consumption are
important parameters which cannot be overlooked.
",['\nOmar Rafique\n'],"four pages, two figures and six tables",,http://arxiv.org/abs/1410.2889v1,cs.OH,['cs.OH'],,,[]
"3D simulation of complex shading affecting PV systems taking benefit
  from the power of graphics cards developed for the video game industry",http://arxiv.org/abs/1410.5780v1,2014-10-03T22:25:22Z,2014-10-03T22:25:22Z,"  Shading reduces the power output of a photovoltaic (PV) system. The design
engineering of PV systems requires modeling and evaluating shading losses. Some
PV systems are affected by complex shading scenes whose resulting PV energy
losses are very difficult to evaluate with current modeling tools. Several
specialized PV design and simulation software include the possibility to
evaluate shading losses. They generally possess a Graphical User Interface
(GUI) through which the user can draw a 3D shading scene, and then evaluate its
corresponding PV energy losses. The complexity of the objects that these tools
can handle is relatively limited. We have created a software solution, 3DPV,
which allows evaluating the energy losses induced by complex 3D scenes on PV
generators. The 3D objects can be imported from specialized 3D modeling
software or from a 3D object library. The shadows cast by this 3D scene on the
PV generator are then directly evaluated from the Graphics Processing Unit
(GPU). Thanks to the recent development of GPUs for the video game industry,
the shadows can be evaluated with a very high spatial resolution that reaches
well beyond the PV cell level, in very short calculation times. A PV simulation
model then translates the geometrical shading into PV energy output losses.
3DPV has been implemented using WebGL, which allows it to run directly from a
Web browser, without requiring any local installation from the user. This also
allows taken full benefits from the information already available from
Internet, such as the 3D object libraries. This contribution describes, step by
step, the method that allows 3DPV to evaluate the PV energy losses caused by
complex shading. We then illustrate the results of this methodology to several
application cases that are encountered in the world of PV systems design.
","['\nJesus Robledo\n', '\nJonathan Leloux\n', '\nEduardo Lorenzo\n']","5 page, 9 figures, conference proceedings, 29th European Photovoltaic
  Solar Energy Conference and Exhibition, Amsterdam, 2014",,http://dx.doi.org/10.13140/2.1.3722.1129,cs.OH,['cs.OH'],10.13140/2.1.3722.1129,,[]
Filtering from Observations on Stiefel Manifolds,http://arxiv.org/abs/1409.7442v1,2014-09-25T23:56:39Z,2014-09-25T23:56:39Z,"  This paper considers the problem of optimal filtering for partially observed
signals taking values on the rotation group. More precisely, one or more
components are considered not to be available in the measurement of the
attitude of a 3D rigid body. In such cases, the observed signal takes its
values on a Stiefel manifold. It is demonstrated how to filter the observed
signal through the anti-development built from observations. A particle filter
implementation is proposed to perform the estimation of the signal partially
observed and corrupted by noise. The sampling issue is also addressed and
interpolation methods are introduced. Illustration of the proposed technique on
synthetic data demonstrates the ability of the approach to estimate the angular
velocity of a partially observed 3D system partially observed.
","['\nJeremie Boulanger\n', '\nSalem Said\n', '\nNicolas Le Bihan\n', '\nJonathan Manton\n']",17 pages,,http://arxiv.org/abs/1409.7442v1,cs.OH,['cs.OH'],,,[]
An Application of Topological Data Analysis to Hockey Analytics,http://arxiv.org/abs/1409.7635v1,2014-09-25T00:42:06Z,2014-09-25T00:42:06Z,"  This paper applies the major computational tool from Topological Data
Analysis (TDA), persistent homology, to discover patterns in the data related
to professional sports teams. I will use official game data from the
North-American National Hockey League (NHL) 2013-2014 season to discover the
correlation between the composition of NHL teams with the currently preferred
offensive performance markers. Specifically, I develop and use the program
TeamPlex (based on the JavaPlex software library) to generate the persistence
bar-codes. TeamPlex is applied to players as data points in a multidimensional
(up to 12-D) data space where each coordinate corresponds to a selected
performance marker.
  The conclusion is that team's offensive performance (measured by the popular
characteristic used in NHL called the Corsi number) correlates with two
bar-code characteristics: greater \textit{sparsity} reflected in the longer
bars in dimension 0 and lower \textit{tunneling} reflected in the low
number/length of the 1-dimensional classes. The methodology can be used by team
managers in identifying deficiencies in the present composition of the team and
analyzing player trades and acquisitions. We give an example of a proposed
trade which should improve the Corsi number of the team.
",['\nDaniel Goldfarb\n'],"19 pages, 15 figures, 4 tables",,http://arxiv.org/abs/1409.7635v1,cs.OH,['cs.OH'],,,[]
Modeling In vivo Wireless Path Loss,http://arxiv.org/abs/1409.7971v1,2014-09-29T00:04:50Z,2014-09-29T00:04:50Z,"  Our long-term research goal is to model the in vivo wireless channel. As a
first step towards this goal, in this paper we performed in vivo path loss
measurements at 2.4GHz and make a comparison with free space path loss. We
calculate the path loss by using the electric field radiated by a
Hertzian-Dipole located inside the abdominal cavity. The simulations quantify
and confirm that the path loss falls more rapidly inside the body than outside
the body. We also observe fluctuations of the path loss caused by the
inhomogeneity of the human body. In comparison with the path loss measured with
monopole antennas, we conclude that the significant variations in Received
Signal Strength is caused by both the angular dependent path loss and the
significantly modified in vivo antenna effects.
","['\nYang Liu\n', '\nThomas P. Ketterl\n', '\nGabriel E. Arrobo\n', '\nRichard D. Gitlin\n']",,,http://dx.doi.org/10.1109/IMWS-BIO.2014.7032404,cs.OH,['cs.OH'],10.1109/IMWS-BIO.2014.7032404,,[]
A Smart Cushion for Real-Time Heart Rate Monitoring,http://arxiv.org/abs/1409.8021v1,2014-09-29T08:24:29Z,2014-09-29T08:24:29Z,"  This paper presents a smart cushion for real time heart rate monitoring. The
cushion comprises of an integrated micro-bending fiber sensor, which records
the BCG (Ballistocardiogram) signal without direct skin-electrode contact, and
an optical transceiver that does signal amplification, digitization, and
pre-filtering. To remove the artifacts and extract heart rate from BCG signal,
a computationally efficient heart rate detection algorithm is developed. The
system doesn't require any pre-training and is highly responsive with the
outputs updated every 3 sec and initial response within first 10 sec. Tests
conducted on human subjects show the detected heart rate closely matches the
one from a commercial SpO2 device.
","['\nChacko John Deepu\n', '\nZhihao Chen\n', '\nJu Teng Teo\n', '\nSoon Huat Ng\n', '\nXiefeng Yang\n', '\nYong Lian\n']",2012 IEEE Biomedical Circuits and Systems Conference,,http://dx.doi.org/10.1109/BioCAS.2012.6418512,cs.OH,['cs.OH'],10.1109/BioCAS.2012.6418512,,[]
"Asynchronous Linear Modulation Classification with Multiple Sensors via
  Generalized EM Algorithm",http://arxiv.org/abs/1409.8370v2,2014-09-30T03:01:50Z,2015-02-03T22:01:23Z,"  In this paper, we consider the problem of automatic modulation classification
with multiple sensors in the presence of unknown time offset, phase offset and
received signal amplitude. We develop a novel hybrid maximum likelihood (HML)
classification scheme based on a generalized expectation maximization (GEM)
algorithm. GEM is capable of finding ML estimates numerically that are
extremely hard to obtain otherwise. Assuming a good initialization technique is
available for GEM, we show that the classification performance can be greatly
improved with multiple sensors compared to that with a single sensor,
especially when the signal-to-noise ratio (SNR) is low. We further demonstrate
the superior performance of our approach when simulated annealing (SA) with
uniform as well as nonuniform grids is employed for initialization of GEM in
low SNR regions. The proposed GEM based approach employs only a small number of
samples (in the order of hundreds) at a given sensor node to perform both time
and phase synchronization, signal power estimation, followed by modulation
classification. We provide simulation results to show the computational
efficiency and effectiveness of the proposed algorithm.
","['\nO. Ozdemir\n', '\nT. Wimalajeewa\n', '\nB. Dulek\n', '\nP. K. Varshney\n', '\nW. Su\n']",,,http://arxiv.org/abs/1409.8370v2,cs.OH,['cs.OH'],,,[]
"An Efficient Topology-Based Algorithm for Transient Analysis of Power
  Grid",http://arxiv.org/abs/1409.7166v2,2014-09-25T06:42:28Z,2015-07-08T05:48:08Z,"  In the design flow of integrated circuits, chip-level verification is an
important step that sanity checks the performance is as expected. Power grid
verification is one of the most expensive and time-consuming steps of
chip-level verification, due to its extremely large size. Efficient power grid
analysis technology is highly demanded as it saves computing resources and
enables faster iteration. In this paper, a topology-base power grid transient
analysis algorithm is proposed. Nodal analysis is adopted to analyze the
topology which is mathematically equivalent to iteratively solving a positive
semi-definite linear equation. The convergence of the method is proved.
","['\nJim Jing-Yan Wang\n', '\nLan Yang\n', '\nJingbin Wang\n', '\nLorenzo Azevedo\n']",,,http://arxiv.org/abs/1409.7166v2,cs.OH,"['cs.OH', 'cs.SI']",,,[]
"Requisite Variety, Autopoiesis, and Self-organization",http://arxiv.org/abs/1409.7475v2,2014-09-26T06:19:46Z,2014-11-06T00:29:07Z,"  Ashby's law of requisite variety states that a controller must have at least
as much variety (complexity) as the controlled. Maturana and Varela proposed
autopoiesis (self-production) to define living systems. Living systems also
require to fulfill the law of requisite variety. A measure of autopoiesis has
been proposed as the ratio between the complexity of a system and the
complexity of its environment. Self-organization can be used as a concept to
guide the design of systems towards higher values of autopoiesis, with the
potential of making technology more ""living"", i.e. adaptive and robust.
",['\nCarlos Gershenson\n'],Invited keynote at WOSC 2014,"Kybernetes, 44(6-7):866-873. 2015",http://arxiv.org/abs/1409.7475v2,nlin.AO,"['nlin.AO', 'cs.OH']",,,[]
GraphState - a tool for graph identification and labelling,http://arxiv.org/abs/1409.8227v1,2014-09-29T18:34:45Z,2014-09-29T18:34:45Z,"  We present python libraries for Feynman graphs manipulation. The key feature
of these libraries is usage of generalization of graph representation offered
by B. G. Nickel et al. In this approach graph is represented in some unique
'canonical' form that depends only on its combinatorial type. The uniqueness of
graph representation gives an efficient way for isomorphism finding, searching
for subgraphs and other graph manipulation tasks. Though offered libraries were
originally designed for Feynman graphs, they might be useful for more general
graph problems.
","['\nD. Batkovich\n', '\nYu. Kirienko\n', '\nM. Kompaniets\n', '\nS. Novikov\n']",13 pages,,http://arxiv.org/abs/1409.8227v1,hep-ph,"['hep-ph', 'cs.OH', 'hep-th']",,,[]
Co-Emulation of Scan-Chain Based Designs Utilizing SCE-MI Infrastructure,http://arxiv.org/abs/1409.3276v1,2014-09-10T23:25:57Z,2014-09-10T23:25:57Z,"  As the complexity of the scan algorithm is dependent on the number of design
registers, large SoC scan designs can no longer be verified in RTL simulation
unless partitioned into smaller sub-blocks. This paper proposes a methodology
to decrease scan-chain verification time utilizing SCE-MI, a widely used
communication protocol for emulation, and an FPGA-based emulation platform. A
high-level (SystemC) testbench and FPGA synthesizable hardware transactor
models are developed for the scan-chain ISCAS89 S400 benchmark circuit for
high-speed communication between the host CPU workstation and the FPGA
emulator. The emulation results are compared to other verification
methodologies (RTL Simulation, Simulation Acceleration, and Transaction-based
emulation), and found to be 82% faster than regular RTL simulation. In
addition, the emulation runs in the MHz speed range, allowing the incorporation
of software applications, drivers, and operating systems, as opposed to the Hz
range in RTL simulation or sub-megahertz range as accomplished in
transaction-based emulation. In addition, the integration of scan testing and
acceleration/emulation platforms allows more complex DFT methods to be
developed and tested on a large scale system, decreasing the time to market for
products.
","['\nBill Jason Tomas\n', '\nYingtao Jiang\n', '\nMei Yang\n']",,,http://arxiv.org/abs/1409.3276v1,cs.OH,['cs.OH'],,,[]
YoMo - The Arduino based Smart Metering Board,http://arxiv.org/abs/1409.3404v1,2014-09-11T11:57:04Z,2014-09-11T11:57:04Z,"  Smart meters are an enabling technology for many smart grid applications.
This paper introduces a design for a low-cost smart meter system as well as the
fundamentals of smart metering. The smart meter platform, provided as open
hardware, is designed with a connector interface compatible to the Arduino
platform, thus opening the possibilities for smart meters with flexible
hardware and computation features, starting from low-cost 8 bit micro
controllers up to powerful single board computers that can run Linux. The
metering platform features a current transformer which allows a non-intrusive
installation of the current measurement unit. The suggested design can switch
loads, offers a variable sampling frequency, and provides measurement data such
as active power, reactive and apparent power. Results indicate that measurement
accuracy and resolution of the proposed metering platform are sufficient for a
range of different applications and loads from a few watts up to five
kilowatts.
","['\nChristoph Klemenjak\n', '\nDominik Egarter\n', '\nWilfried Elmenreich\n']",,,http://arxiv.org/abs/1409.3404v1,cs.OH,['cs.OH'],,,[]
Computational Gravitational Dynamics with Modern Numerical Accelerators,http://arxiv.org/abs/1409.5474v1,2014-09-18T22:05:32Z,2014-09-18T22:05:32Z,"  We review the recent optimizations of gravitational $N$-body kernels for
running them on graphics processing units (GPUs), on single hosts and massive
parallel platforms. For each of the two main $N$-body techniques, direct
summation and tree-codes, we discuss the optimization strategy, which is
different for each algorithm. Because both the accuracy as well as the
performance characteristics differ, hybridizing the two algorithms is essential
when simulating a large $N$-body system with high-density structures containing
few particles, and with low-density structures containing many particles. We
demonstrate how this can be realized by splitting the underlying Hamiltonian,
and we subsequently demonstrate the efficiency and accuracy of the hybrid code
by simulating a group of 11 merging galaxies with massive black holes in the
nuclei.
","['\nSimon Portegies Zwart\n', '\nJeroen Bédorf\n']",Accepted for publication in IEEE Computer,,http://arxiv.org/abs/1409.5474v1,astro-ph.IM,"['astro-ph.IM', 'astro-ph.GA', 'cs.OH']",,,[]
Explicit Integration with GPU Acceleration for Large Kinetic Networks,http://arxiv.org/abs/1409.5826v2,2014-09-19T22:27:30Z,2015-01-10T17:59:54Z,"  We demonstrate the first implementation of recently-developed fast explicit
kinetic integration algorithms on modern graphics processing unit (GPU)
accelerators. Taking as a generic test case a Type Ia supernova explosion with
an extremely stiff thermonuclear network having 150 isotopic species and 1604
reactions coupled to hydrodynamics using operator splitting, we demonstrate the
capability to solve of order 100 realistic kinetic networks in parallel in the
same time that standard implicit methods can solve a single such network on a
CPU. This orders-of-magnitude decrease in compute time for solving systems of
realistic kinetic networks implies that important coupled, multiphysics
problems in various scientific and technical fields that were intractible, or
could be simulated only with highly schematic kinetic networks, are now
computationally feasible.
","['\nBenjamin Brock\n', '\nAndrew Belt\n', '\nJay Jay Billings\n', '\nMike Guidry\n']","20 pages, 8 figures, submitted to Journal of Computational Physics","J. Comp. Phys. 302, 591 (2015)",http://arxiv.org/abs/1409.5826v2,physics.comp-ph,"['physics.comp-ph', 'astro-ph.SR', 'cs.OH']",,,[]
Population spatialization and synthesis with open data,http://arxiv.org/abs/1409.0612v1,2014-09-02T06:32:39Z,2014-09-02T06:32:39Z,"  Individuals together with their locations & attributes are essential to feed
micro-level applied urban models (for example, spatial micro-simulation and
agent-based modeling) for policy evaluation. Existed studies on population
spatialization and population synthesis are generally separated. In developing
countries like China, population distribution in a fine scale, as the input for
population synthesis, is not universally available. With the open-government
initiatives in China and the emerging Web 2.0 techniques, more and more open
data are becoming achievable. In this paper, we propose an automatic process
using open data for population spatialization and synthesis. Specifically, the
road network in OpenStreetMap is used to identify and delineate parcel
geometries, while crowd-sourced POIs are gathered to infer urban parcels with a
vector cellular automata model. Housing-related online Check-in records are
then applied to distinguish residential parcels from all of the identified
urban parcels. Finally the published census data, in which the sub-district
level of attributes distribution and relationships are available, is used for
synthesizing population attributes with a previously developed tool Agenter
(Long and Shen, 2013). The results are validated with ground truth
manually-prepared dataset by planners from Beijing Institute of City Planning.
","['\nYing Long\n', '\nZhenjiang Shen\n']",14 pages,,http://arxiv.org/abs/1409.0612v1,cs.OH,['cs.OH'],,,[]
"Evaluating the Electrification of Vehicle Fleets Using the Veins
  Framework",http://arxiv.org/abs/1409.1003v1,2014-09-03T09:17:14Z,2014-09-03T09:17:14Z,"  The case study discussed in this paper involves a company maintaining a
vehicle fleet of one hundred vehicles. In this article we will discuss how we
extend and deploy the Veins framework, which couples OMNeT++ and SUMO, to help
in the process of electrifying this vehicle fleet, i.e., replacing combustion
engine cars with electric vehicles to save money and lower CO$_2$ emissions.
","['\nSebastian Schellenberg\n', '\nRüdiger Berndt\n', '\nReinhard German\n', '\nDavid Eckhoff\n']","Published in: A. F\""orster, C. Sommer, T. Steinbach, M. W\""ahlisch
  (Eds.), Proc. of 1st OMNeT++ Community Summit, Hamburg, Germany, September 2,
  2014, arXiv:1409.0093, 2014",,http://arxiv.org/abs/1409.1003v1,cs.OH,['cs.OH'],,,[]
Exercises for Children with Dyslalia-Software Infrastructure,http://arxiv.org/abs/1409.1699v1,2014-09-05T09:24:27Z,2014-09-05T09:24:27Z,"  In order to help children with dyslalia we created a set of software
exercises. This set has a unitary software block (data base, programming
language, programming philosophy). In this paper we present this software
infrastructure with its advantage and disadvantage. The exercises are part of a
software system named LOGOMON. Therefore, besides horizontal compatibilities
(between exercises) vertical compatibilities are also presented (with LOGOMON
system). Concerning database tables used for modulus of exercises, a part of
them is ""inherited"" from LOGOMON application and another are specific for
exercises application. We also need to specify that there were necessary minor
changes of database tables used by LOGOMON. As programming language we used C#,
implemented in Visual Studio 2005. We developed specific interfaces elements
and classes. We also used multimedia resources that were necessary for
exercises (images, correct pronouncing obtained from speech therapist
recording, video clips). Another section of this application is related to
loading of exercises on mobile devices (Pocket PC). A part of code has been
imported directly, but there were a lot of files that need to be rewritten.
Anyway, the multimedia resources were used without any processing.
","['\nCristian-Eduard Belciug\n', '\nOvidiu-Andrei Schipor\n', '\nMirela Danubianu\n']","5 pages, 3 figures","Distributed Systems, University Of Suceava, 2007",http://arxiv.org/abs/1409.1699v1,cs.OH,['cs.OH'],,,[]
V2V Propagation Modeling with Imperfect RSSI Samples,http://arxiv.org/abs/1409.1846v1,2014-09-05T15:58:56Z,2014-09-05T15:58:56Z,"  We describe three in-field data collection efforts yielding a large database
of RSSI values vs. time or distance from vehicles communicating with each other
via DSRC. We show several data processing schemes we have devised to develop
Vehicle-to-Vehicle (V2V) propagation models from such data. The database is
limited in several important ways, not least, the presence of a high noise
floor that limits the distance over which good modeling is feasible. Another is
the presence of interference from multiple active transmitters. Our methodology
makes it possible to obtain, despite these limitations, accurate models of
median path loss vs. distance, shadow fading, and fast fading caused by
multipath. We aim not to develop a new V2V model, but to show the methods
enabling such a model to be obtained from in-field RSSI data.
","['\nSilvija Kokalj-Filipovic\n', '\nLarry Greenstein\n', '\nBin Cheng\n', '\nMarco Gruteser\n']",,,http://arxiv.org/abs/1409.1846v1,cs.OH,['cs.OH'],,,[]
"Design and Realization of an S-Band Microwave Low-Noise Amplifier for
  Wireless RF Subsystems",http://arxiv.org/abs/1409.2141v2,2014-09-07T17:19:40Z,2014-10-17T15:56:39Z,"  This study undertakes the theoretical design, CAD modeling, realization, and
performance analysis of a microwave low-noise amplifier (LNA) which has been
accurately developed for operation at 3.0 GHz (S-band). The objective of this
research is to thoroughly analyze and develop a reliable microstrip LNA
intended for a potential employment in wireless communication systems, and
satellite applications. The S-band microwave LNA demonstrates the
appropriateness to develop a high-performance and well-established device
realization for wireless RF systems. The microwave amplifier simulations have
been conducted using the latest version of the AWR Design Environment software.
","['\nArdavan Rahimian\n', '\nDavood Momeni Pakdehi\n']",,,http://arxiv.org/abs/1409.2141v2,cs.OH,['cs.OH'],,,[]
Crawford-Sobel meet Lloyd-Max on the grid,http://arxiv.org/abs/1410.6831v1,2014-09-05T12:45:14Z,2014-09-05T12:45:14Z,"  The main contribution of this work is twofold. First, we apply, for the first
time, a framework borrowed from economics to a problem in the smart grid
namely, the design of signaling schemes between a consumer and an electricity
aggregator when these have non-aligned objectives. The consumer's objective is
to meet its need in terms of power and send a request (a message) to the
aggregator which does not correspond, in general, to its actual need. The
aggregator, which receives this request, not only wants to satisfy it but also
wants to manage the cost induced by the residential electricity distribution
network. Second, we establish connections between the exploited framework and
the quantization problem. Although the model assumed for the payoff functions
for the consumer and aggregator is quite simple, it allows one to extract
insights of practical interest from the analysis conducted. This allows us to
establish a direct connection with quantization, and more importantly, to open
a much more general challenge for source and channel coding.
","['\nBenjamin Larrousse\n', '\nOlivier Beaude\n', '\nSamson Lasaulce\n']","ICASSP 2014, 5 pages","IEEE International Conference on Acoustics, Speech and Signal
  Processing (ICASSP), Florence, Italy, May 2014",http://dx.doi.org/10.1109/ICASSP.2014.6854781,cs.OH,"['cs.OH', 'cs.GT']",10.1109/ICASSP.2014.6854781,,[]
Stress-Minimizing Orthogonal Layout of Data Flow Diagrams with Ports,http://arxiv.org/abs/1408.4626v1,2014-08-20T12:26:46Z,2014-08-20T12:26:46Z,"  We present a fundamentally different approach to orthogonal layout of data
flow diagrams with ports. This is based on extending constrained stress
majorization to cater for ports and flow layout. Because we are minimizing
stress we are able to better display global structure, as measured by several
criteria such as stress, edge-length variance, and aspect ratio. Compared to
the layered approach, our layouts tend to exhibit symmetries, and eliminate
inter-layer whitespace, making the diagrams more compact.
","['\nUlf Rüegg\n', '\nSteve Kieffer\n', '\nTim Dwyer\n', '\nKim Marriott\n', '\nMichael Wybrow\n']",,,http://arxiv.org/abs/1408.4626v1,cs.OH,['cs.OH'],,,[]
Tools and Techniques for Efficient High-Level System Design on FPGAs,http://arxiv.org/abs/1408.4797v1,2014-08-20T16:08:57Z,2014-08-20T16:08:57Z,"  In order for FPGAs to be successful outside traditional markets, tools which
enable software programmers to achieve high levels of system performance while
abstracting away the FPGA-specific details are needed. DSPB Builder Advanced
(DSPBA) is one such tool. DSPBA provides model-based design environment using
Matlab's Simulink frontend that decouples the fully-algorithmic design
description from the details of FPGA system generation. DSPBA offers several
levels of debugging: from Simulink scopes to bit-accurate-simulation and silver
reference models. It also offers the most comprehensive set of fixed-point,
floating-point and signal-processing IPs available today. The combination of 7
floating-point precisions, fused-datapath support, custom operator support and
automated folding allows exploring the best tradeoffs between accuracy, size
and throughput. The DSPBA backend protects users from the details of
device-dependent operator mapping offering both efficiency and prompt support
for new devices and features such as the Arria10 floating-point cores. The
collection of features available in DSPBA allows both unexperienced and expert
users to efficiently migrate performance-crucial systems to the FPGA
architecture.
","['\nAdrian J. Chung\n', '\nKathryn Cobden\n', '\nMark Jervis\n', '\nMartin Langhammer\n', '\nBogdan Pasca\n']","Presented at First International Workshop on FPGAs for Software
  Programmers (FSP 2014) (arXiv:1408.4423)",,http://arxiv.org/abs/1408.4797v1,cs.OH,['cs.OH'],,,[]
"OpenHEC: A Framework for Application Programmers to Design FPGA-based
  Systems",http://arxiv.org/abs/1408.5347v1,2014-08-21T11:42:28Z,2014-08-21T11:42:28Z,"  Today, there is a trend to incorporate more intelligence (e.g., vision
capabilities) into a wide range of devices, which makes high performance a
necessity for computing systems. Furthermore, for embedded systems, low power
consumption should be generally considered together with high computing
performance. FPGAs, as programmable logic devices able to support different
types of fine-grained parallelisms, their power and performance advantages were
recognized widely. However, designing applications on FPGA-based systems is
traditionally far from a task can be carried out by software programmers.
Generally, hardware engineers and even system-level software engineers have
more hardware/architectural knowledge but fewer algorithm and application
knowledge. Thus, it is critical for computing systems to allow
application-level programmers to realize their idea conveniently, which is
popular in computing systems based on the general processor. In this paper, the
OpenHEC (Open Framework for High-Efficiency Computing) framework is proposed to
provide a design framework for application-level software programmers to use
FPGA-based platforms. It frees users from hardware and architectural details to
let them focus more on algorithms/applications. This framework was integrated
with the commercial Xilinx ISE/Vivado to make it to be used immediately. After
implementing a widely-used feature detection algorithm on OpenHEC from the
perspective of software programmers, it shows this framework is applicable for
application programmers with little hardware knowledge.
","['\nZhilei Chai\n', '\nZhibin Wang\n', '\nWenmin Yang\n', '\nShuai Ding\n', '\nYuanpu Zhang\n']","Presented at First International Workshop on FPGAs for Software
  Programmers (FSP 2014) (arXiv:1408.4423)",,http://arxiv.org/abs/1408.5347v1,cs.OH,['cs.OH'],,,[]
High-Level Design of Portable and Scalable FPGA Accelerators,http://arxiv.org/abs/1408.5383v1,2014-08-21T11:02:28Z,2014-08-21T11:02:28Z,"  This paper presents our approach for making FPGA accelerators accessible to
software (SW) programmers. It is intended as a starting point for
collaborations with other groups pursuing similar objectives. We report on our
current SAccO platform (Scalable Accelerator platform Osnabr\""uck) and the
planned project extending this platform.
","['\nMarkus Weinhardt\n', '\nRainer Höckmann\n', '\nThomas Kinder\n']","Presented at First International Workshop on FPGAs for Software
  Programmers (FSP 2014) (arXiv:1408.4423)",,http://arxiv.org/abs/1408.5383v1,cs.OH,['cs.OH'],,,[]
"Stream Processor Generator for HPC to Embedded Applications on
  FPGA-based System Platform",http://arxiv.org/abs/1408.5386v1,2014-08-21T11:17:32Z,2014-08-21T11:17:32Z,"  This paper presents a stream processor generator, called SPGen, for
FPGA-based system-on-chip platforms. In our research project, we use an FPGA as
a common platform for applications ranging from HPC to embedded/robotics
computing. Pipelining in application-specific stream processors brings FPGAs
power-efficient and high-performance computing. However, poor productivity in
developing custom pipelines prevents the reconfigurable platform from being
widely and easily used. SPGen aims at assisting developers to design and
implement high-throughput stream processors by generating their HDL codes with
our domain-specific high-level stream processing description, called SPD.With
an example of fluid dynamics computation, we validate SPD for describing a real
application and verify SPGen for synthesis with a pipelined data-flow graph. We
also demonstrate that SPGen allows us to easily explore a design space for
finding better implementation than a hand-designed one.
","['\nKentaro Sano\n', '\nHayato Suzuki\n', '\nRyo Ito\n', '\nTomohiro Ueno\n', '\nSatoru Yamamoto\n']","Presented at First International Workshop on FPGAs for Software
  Programmers (FSP 2014) (arXiv:1408.4423)",,http://arxiv.org/abs/1408.5386v1,cs.OH,['cs.OH'],,,[]
High-Level Synthesis Case Study: Implementation of a Memcached Server,http://arxiv.org/abs/1408.5387v1,2014-08-21T12:03:57Z,2014-08-21T12:03:57Z,"  High-Level Synthesis (HLS) aspires to raise the level of abstraction in
hardware design without sacrificing hardware efficiency. It has so far been
successfully employed in signal and video processing but has found only limited
use in other areas. This paper utilizes a commercial HLS tool, namely Vivado(R)
HLS, to implement the processing of a common data center application, the
Key-Value Store (KVS) application memcached, as a deeply pipelined dataflow
architecture. We compared our results to a fully equivalent RTL implementation
done previously in our group and found that it matches its performance, yields
tangible improvements in latency (between 7-30%) and resource consumption (22%
in LUTs and 35% in registers), all while requiring 3x less lines of code and 2x
less development time. The implementation was validated in hardware on a
Xilinx(R) VC709 development board, meeting timing requirements for 10Gbps line
rate processing.
","['\nKimon Karras\n', '\nMichaela Blott\n', '\nKees Vissers\n']","Presented at First International Workshop on FPGAs for Software
  Programmers (FSP 2014) (arXiv:1408.4423)",,http://arxiv.org/abs/1408.5387v1,cs.OH,['cs.OH'],,,[]
SFA Referee Allocation Scheme,http://arxiv.org/abs/1408.6661v1,2014-08-28T09:52:19Z,2014-08-28T09:52:19Z,"  For many sports, the allocation of officials to matches is performed manually
and is a very time consuming procedure. For the Scottish Football Association
(SFA), the allocation of referees and other officials to matches is governed by
a number of rules specifying the expertise required from the different types of
official at each level, e.g. Scottish Premiership League referee must be a
grade 1 with high experience. The allocation requires an SFA secretary to
expend several hours to find suitable officials, contact them and assign them.
Most of the time, the secretary is a volunteer who performs the allocation as a
hobby and it would be useful to reduce his costs and time.
  The project aims to reduce the burden on SFA, and potentially other
secretaries, by developing a program to assign SFA officials. A suitable
algorithm must be devised to search through the set of data about matches and
officials and find a potential allocation. The program then updates the
database with the new data, and provides a web interface for both secretaries
and officials.
  A prototype system using the new greedy algorithm has been implemented and
evaluated with SFA secretaries. A final usable referee allocation system has
been designed that uses the greedy algorithm, and is extended after evaluation
of the prototype. The final allocation system based provides both a command
line and a web interface and has also been evaluated by SFA secretaries. In
their letters of recommendation in Appendix F the SFA secretaries indicate that
the final allocation system it will be used again in the future.
",['\nNikolaos Polatidis\n'],,,http://arxiv.org/abs/1408.6661v1,cs.OH,['cs.OH'],,,[]
"A New Approach to Customization of Collision Warning Systems to
  Individual Drivers",http://arxiv.org/abs/1408.4111v2,2014-08-15T23:39:22Z,2015-12-17T00:30:41Z,"  This paper discusses the need for individualizing safety systems and proposes
an approach including the Real-Time estimation of the distribution of brake
response times for an individual driver. While maintaining high level of
safety, the collision warning system should send ""tailored"" responses to the
driver. This method could be the first step to show that safety applications
would potentially benefit from customizing to individual drivers'
characteristics using VANET. Our simulation results show that, as one of the
imminent and preliminary outcomes of the new improved system, the number of
false alarms will be reduced by more than 40%. We think this tactic can reach
to even beyond the safety applications for designing the future innovative
systems.
","['\nAli Rakhshan\n', '\nEvan Ray\n', '\nHossein Pishro-Nik\n']",,,http://arxiv.org/abs/1408.4111v2,cs.OH,"['cs.OH', 'stat.AP']",,,[]
"Study on FLOWSIM and its Application for Isolated Signal-ized
  Intersection Assessment",http://arxiv.org/abs/1407.8244v1,2014-07-31T00:36:28Z,2014-07-31T00:36:28Z,"  Recently the traffic related problems have become strategically important,
due to the continuously increasing vehicle number. As a result, microscopic
simulation software has become an efficient method in traffic engineering for
its cost-effectiveness and safety characteristics. In this paper, a new fuzzy
logic based simulation software (FLOWSIM) is introduced, which can reflect the
mixed traffic flow phenomenon in China better. The fuzzy logic based
car-following model and lane-changing model are explained in detail.
Furthermore, its applications for mixed traffic flow management in mid-size
cities and for signalized intersection management assessment in large cities
are illustrated by examples in China. Finally, further study objectives are
discussed.
","['\nYuhan Jia\n', '\nJianping Wu\n', '\nYiman Du\n', '\nGeqi Qi\n']",,,http://arxiv.org/abs/1407.8244v1,cs.OH,['cs.OH'],,,[]
"Triple Patterning Lithography (TPL) Layout Decomposition using
  End-Cutting (JM3 Special Session)",http://arxiv.org/abs/1408.0407v1,2014-08-02T19:19:36Z,2014-08-02T19:19:36Z,"  Triple patterning lithography (TPL) is one of the most promising techniques
in the 14nm logic node and beyond. Conventional LELELE type TPL technology
suffers from native conflict and overlapping problems. Recently, as an
alternative process, triple patterning lithography with end cutting (LELE-EC)
was proposed to overcome the limitations of LELELE manufacturing. In LELE-EC
process the first two masks are LELE type double patterning, while the third
mask is used to generate the end-cuts. Although the layout decomposition
problem for LELELE has been well-studied in the literature, only few attempts
have been made to address the LELE-EC layout decomposition problem. In this
paper we propose the comprehensive study for LELE-EC layout decomposition.
Conflict graph and end-cut graph are constructed to extract all the geometrical
relationships of both input layout and end-cut candidates. Based on these
graphs, integer linear programming (ILP) is formulated to minimize the conflict
number and the stitch number. The experimental results demonstrate the
effectiveness of the proposed algorithms.
","['\nBei Yu\n', '\nSubhendu Roy\n', '\nJhih-Rong Gao\n', '\nDavid Z. Pan\n']","J. Micro/Nanolith. MEMS MOEMS 14(1) XXXXXX (Jan-Mar 2015). arXiv
  admin note: text overlap with arXiv:1402.2425",,http://arxiv.org/abs/1408.0407v1,cs.OH,['cs.OH'],,,[]
"Precision of Pulse-Coupled Oscillator Synchronization on FPGA-Based
  Radios",http://arxiv.org/abs/1408.0652v2,2014-08-04T11:55:57Z,2014-12-16T08:29:12Z,"  The precision of synchronization algorithms based on the theory of
pulse-coupled oscillators is evaluated on FPGA-based radios for the first time.
Measurements show that such algorithms can reach precision in the low
microsecond range when being implemented in the physical layer. Furthermore, we
propose an algorithm extension accounting for phase rate deviations of the
hardware and show that an improved precision below one microsecond is possible
with this extension in the given setup. The resulting algorithm can thus be
applied in ad hoc wireless systems for fully distributed synchronization of
transmission slots or sleep cycles, in particular, if centralized
synchronization is impossible.
","['\nGünther Brandner\n', '\nJohannes Klinglmayr\n', '\nUdo Schilcher\n', '\nDominik Egarter\n', '\nChristian Bettstetter\n']",,"International ITG Conference on Systems, Communications and Coding
  (SCC) 2015",http://arxiv.org/abs/1408.0652v2,cs.OH,['cs.OH'],,,[]
"e-Installation: Synesthetic Documentation of Media Art via Telepresence
  Technologies",http://arxiv.org/abs/1408.1362v1,2014-08-06T17:28:59Z,2014-08-06T17:28:59Z,"  In this paper, a new synesthetic documentation method that contributes to
media art conservation is presented. This new method is called e-Installation
in analogy to the idea of the e-Book as the electronic version of a real book.
An e-Installation is a virtualized media artwork that reproduces all
synesthesia, interaction, and meaning levels of the artwork. Advanced 3D
modeling and telepresence technologies with a very high level of immersion
allow the virtual re-enactment of works of media art that are no longer
performable or rarely exhibited. The virtual re-enactment of a media artwork
can be designed with a scalable level of complexity depending on whether it
addresses professionals such as curators, art restorers, and art theorists or
the general public. An e-Installation is independent from the artwork's
physical location and can be accessed via head-mounted display or similar data
goggles, computer browser, or even mobile devices. In combination with
informational and preventive conservation measures, the e-Installation offers
an intermediate and long-term solution to archive, disseminate, and pass down
the milestones of media art history as a synesthetic documentation when the
original work can no longer be repaired or exhibited in its full function.
","['\nJesús Muñoz Morcillo\n', '\nFlorian Faion\n', '\nAntonio Zea\n', '\nUwe D. Hanebeck\n', '\nCaroline Y. Robertson-von Trotha\n']","3 figures, 14 pages",,http://arxiv.org/abs/1408.1362v1,cs.OH,['cs.OH'],,,[]
Data Transfer between Two USB Flash SCSI Disks using a Touch Screen,http://arxiv.org/abs/1407.5008v1,2014-07-17T14:56:00Z,2014-07-17T14:56:00Z,"  Under normal circumstances, as an intermediate device, if we want to move or
copy data from one mass storage device to another, we use a computer in the
form of desktops, laptops, etc. We need a device which can be used as an
intermediate device, also which is a complete blend of hardware & software.
This device is a gadget that can be used to transfer data between two flash
SCSI devices via a touch screen. This is a user friendly device which uses the
most popular bus USB (Universal Serial Bus) with Type-A connector. It is
governed by the USB 2.0 Protocol. One of the major advantage of this device is
its portability.
","['\nAnurag A. Chakravorty\n', '\nRaghwendra J. Suryawanshi\n']","5 pages, 7 figures, Published with International Journal of
  Engineering Trends and Technology (IJETT)","IJETT, V13(2),71-75 July 2014",http://dx.doi.org/10.14445/22315381/IJETT-V13P215,cs.OH,['cs.OH'],10.14445/22315381/IJETT-V13P215,,[]
"M$^2$I: Channel Modeling for Metamaterial-Enhanced Magnetic Induction
  Communications",http://arxiv.org/abs/1407.5040v2,2014-07-17T13:38:09Z,2016-04-04T21:13:12Z,"  Magnetic Induction (MI) communication technique has shown great potentials in
complex and RF-challenging environments, such as underground and underwater,
due to its advantage over EM wave-based techniques in penetrating lossy medium.
However, the transmission distance of MI techniques is limited since magnetic
field attenuates very fast in the near field. To this end, this paper proposes
Metamaterial-enhanced Magnetic Induction (M$^2$I) communication mechanism,
where a MI coil antenna is enclosed by a metamaterial shell that can enhance
the magnetic fields around the MI transceivers. As a result, the M$^2$I
communication system can achieve tens of meters communication range by using
pocket-sized antennas. In this paper, an analytical channel model is developed
to explore the fundamentals of the M$^2$I mechanism, in the aspects of
communication range and channel capacity, and the susceptibility to various
hostile and complex environments. The theoretical model is validated through
the finite element simulation software, Comsol Multiphysics. Proof-of-concept
experiments are also conducted to validate the feasibility of M$^2$I.
","['\nHongzhi Guo\n', '\nZhi Sun\n', '\nJingbo Sun\n', '\nNatalia M. Litchinitser\n']",,"IEEE Transactions on Antennas and Propagation, Vol. 63, No. 11,
  pp. 5072-5087, November 2015",http://dx.doi.org/10.1109/TAP.2015.2480095,cs.OH,['cs.OH'],10.1109/TAP.2015.2480095,,[]
"Supporting Read/Write Applications in Embedded Real-time Systems via
  Suspension-aware Analysis",http://arxiv.org/abs/1407.5126v1,2014-07-18T22:30:33Z,2014-07-18T22:30:33Z,"  In many embedded real-time systems, applications often interact with I/O
devices via read/write operations, which may incur considerable suspension
delays. Unfortunately, prior analysis methods for validating timing correctness
in embedded systems become quite pessimistic when suspension delays are
present. In this paper, we consider the problem of supporting two common types
of I/O applications in a multiprocessor system, that is, write-only
applications and read-write applications. For the write-only application model,
we present a much improved analysis technique that results in only O(m)
suspension-related utilization loss, where m is the number of processors. For
the second application model, we present a flexible I/O placement strategy and
a corresponding new scheduling algorithm, which can completely circumvent the
negative impact due to read- and write-induced suspension delays. We illustrate
the feasibility of the proposed I/O-placement-based schedule via a case study
implementation. Furthermore, experiments presented herein show that the
improvement with respect to system utilization over prior methods is often
significant.
","['\nGuangmo Tong\n', '\nCong Liu\n']",,,http://arxiv.org/abs/1407.5126v1,cs.OH,['cs.OH'],,,[]
A Novel Hybrid Algorithm for Permutation Flow Shop Scheduling,http://arxiv.org/abs/1407.5931v1,2014-07-21T16:57:04Z,2014-07-21T16:57:04Z,"  In the present scenario the recent engineering and industrial built-up units
are facing hodgepodge of problems in a lot of aspects such as machining time,
electricity, man power, raw material and customers constraints. The job-shop
scheduling is one of the most significant industrial behaviours, particularly
in manufacturing planning. This paper proposes the permutation flow shop
sequencing problem with the objective of makespan minimization using the new
modified proposed method of johnsons algorithm as well as the guptas heuristic
algorithm. This paper involves the determination of the order of processing of
n jobs in m machines. Although since the problem is known to be np-hard for
three or more machines, that produces near optimal solution of the given
problem. The proposed method is very simple and easy to understand followed by
a numerical illustration is given.
","['\nSandeep Kumar\n', '\nPooja Jadon\n']",,,http://arxiv.org/abs/1407.5931v1,cs.OH,['cs.OH'],,,[]
Adaptive Fault Diagnosis using Self-Referential Reasoning,http://arxiv.org/abs/1407.6255v1,2014-07-17T19:27:03Z,2014-07-17T19:27:03Z,"  The problem is to determine which processors are reliable in a remote
location by asking ""Yes or No"" questions. The processors are of three types:
those that always tell the truth, those that always lie, and those the
sometimes tell the truth and sometimes lie. Using self-referential reasoning,
along with earlier techniques, we can regard both the truth-tellers and liars
as reliable and thus the tackle situations when fewer than half the processors
are truth-tellers.
",['\nRobert Cowen\n'],"This article will appear in a book on Self-Referential Reasoning
  dedicated to Raymond Smullyan",,http://arxiv.org/abs/1407.6255v1,cs.OH,['cs.OH'],,,[]
"Simulation and optimization of container terminal operations: a case
  study",http://arxiv.org/abs/1407.6257v1,2014-07-22T02:20:28Z,2014-07-22T02:20:28Z,"  Container terminals are facing a set of interrelated problems. Container
handling problems at container terminals are NP-hard problems. The docking time
of container ships at the port must be optimized. In this paper we have built a
simulation model that integrates all the activities of a container terminal.
The proposed approach is applied on a real case study data of container
terminal at El-Dekheilla port. The results show that the proposed approach
reduced the ship turnaround time in port where 51% reduction in ship service
time (loading/unloading) in port is achieved.
","['\nGamal Abd El-Nasser A. Said\n', '\nAbeer M. Mahmoud\n', '\nEl-Sayed M. El-Horbaty\n']","10 pages, 3 figures","International Journal of Computer Science Engineering and
  Information Technology Research(IJCSEITR),Vol.4,Issue 4,2014,pp.85-94",http://arxiv.org/abs/1407.6257v1,cs.OH,['cs.OH'],,,[]
Solving container terminals problems using computer-based modeling,http://arxiv.org/abs/1407.6384v1,2014-07-22T02:16:04Z,2014-07-22T02:16:04Z,"  This paper addresses the applications of different techniques for solving
container terminals problems. We have built a simulation model that can be used
to analyze the performance of container terminal operations. The proposed
approach is intended to be applied for a real case study in Alexandria
Container Terminal (ACT) at Alexandria port. The implementation of our approach
shows that a proposed model increases the efficiency of Alexandria container
terminal at Alexandria port.
","['\nGamal Abd El-Nasser A. Said\n', '\nAbeer M. Mahmoud\n', '\nEl-Sayed M. El-Horbaty\n']","11 pages, 6 figures. arXiv admin note: substantial text overlap with
  arXiv:1407.6257","International Journal of Computer Science and Engineering(IJCSE),
  Vol.3, Issue 3,2014, pp.91-100",http://arxiv.org/abs/1407.6384v1,cs.OH,['cs.OH'],,,[]
Sequential Data Mining using Correlation Matrix Memory,http://arxiv.org/abs/1407.2206v1,2014-07-06T15:21:35Z,2014-07-06T15:21:35Z,"  This paper proposes a method for sequential data mining using correlation
matrix memory. Here, we use the concept of the Logical Match to mine the
indices of the sequential pattern. We demonstrate the uniqueness of the method
with both the artificial and the real datum taken from NCBI databank.
","['\nSanil Shanker KP\n', '\nAaron Turner\n', '\nElizabeth Sherly\n', '\nJim Austin\n']","Networking and Information Technology (ICNIT), 2010 International
  Conference on",,http://dx.doi.org/10.1109/ICNIT.2010.5508469,cs.OH,['cs.OH'],10.1109/ICNIT.2010.5508469,,[]
Intelligent Fatigue Detection and Automatic Vehicle Control System,http://arxiv.org/abs/1407.2412v1,2014-07-09T10:04:10Z,2014-07-09T10:04:10Z,"  This paper describes method for detecting the early signs of fatigue in train
drivers. As soon as the train driver is falling in symptoms of fatigue
immediate message will be transfer to the control room indicating the status of
the drivers. In addition of the advance technology of heart rate sensors is
also added in the system for correct detection of status of driver if in either
case driver is falling to fatigue due to any sever medical problems .The
fatigue is detected in the system by the image processing method of comparing
the image(frames) in the video and by using the human features we are able to
estimate the indirect way of detecting fatigue. The technique also focuses on
modes of person when driving the train i.e. awake, drowsy state or sleepy and
sleep state. The system is very efficient to detect the fatigue and control the
train also train can be controlled if it cross any such signal by which the
train may collide on another train.
","['\nMonali Gulhane\n', '\nP. S. Mohod\n']",,,http://arxiv.org/abs/1407.2412v1,cs.OH,['cs.OH'],,,[]
"A New Fuzzy DEMATEL-TODIM Hybrid Method for evaluation criteria of
  Knowledge management in supply chain",http://arxiv.org/abs/1407.3657v1,2014-07-11T11:34:45Z,2014-07-11T11:34:45Z,"  Knowledge management (KM) adoption in the supply chain network needs a good
investment as well as few changes in the culture of the entire SC. Knowledge
management is the process of creating, distributing and transferring
information. The goal of this study is to Rank KM criteria in supply chain
network in Iran which is important for firms these days. Criterion used in this
paper were extracted from the literature review and were confirmed by supply
chain experts. The proposed approach for ranking and finding out about these
criterion is hybrid fuzzy DEMATEL-TODIM, with using fuzzy number as data for
our studies we could avoid uncertainty. The data was gathered from PhD. And Ms.
Students in industrial engineering of Kharrazmi university of Tehran and PhD.
And Ms. Students of the management department of Semnan university. A new
hybrid approach was used for achieving the results of this study. This new
hybrid approach ranks data criteria respect to each other, then by using TODIM
for ranking respect to the best situation (gains), the rates of criterion were
determined which is a very important advantage
","['\nMahdi Mahmoodi\nKharazmi University of Tehran\n', '\nGelayol Safavi Jahromi\nSemnan University\n']",,,http://arxiv.org/abs/1407.3657v1,cs.OH,['cs.OH'],,,"['Kharazmi University of Tehran', 'Semnan University']"
Compressive Periodogram Reconstruction Using Uniform Binning,http://arxiv.org/abs/1407.4017v2,2014-07-09T16:40:57Z,2014-12-19T15:50:35Z,"  In this paper, two problems that show great similarities are examined. The
first problem is the reconstruction of the angular-domain periodogram from
spatial-domain signals received at different time indices. The second one is
the reconstruction of the frequency-domain periodogram from time-domain signals
received at different wireless sensors. We split the entire angular or
frequency band into uniform bins. The bin size is set such that the received
spectra at two frequencies or angles, whose distance is equal to or larger than
the size of a bin, are uncorrelated. These problems in the two different
domains lead to a similar circulant structure in the so-called coset
correlation matrix. This circulant structure allows for a strong compression
and a simple least-squares reconstruction method. The latter is possible under
the full column rank condition of the system matrix, which can be achieved by
designing the spatial or temporal sampling patterns based on a circular sparse
ruler. We analyze the statistical performance of the compressively
reconstructed periodogram including bias and variance. We further consider the
case when the bins are so small that the received spectra at two frequencies or
angles, with a spacing between them larger than the size of the bin, can still
be correlated. In this case, the resulting coset correlation matrix is
generally not circulant and thus a special approach is required.
","['\nDyonisius Dony Ariananda\n', '\nDaniel Romero\n', '\nGeert Leus\n']",Submitted to IEEE Transactions on Signal Processing,,http://dx.doi.org/10.1109/TSP.2015.2430838,cs.OH,"['cs.OH', 'math.SP']",10.1109/TSP.2015.2430838,,[]
E-Learning Quality Criteria and Aspects,http://arxiv.org/abs/1406.7744v1,2014-06-26T11:41:51Z,2014-06-26T11:41:51Z,"  As IT grows the impact of new technology reflects in more or less every
field. Education also gets new dimensions with the advancement in IT sector.
Nowadays education is not limited to books and black boards only it gets a new
way i.e. electronic media. Although with e-learning, the education having
broader phenomena, yet it is in budding stage. Quality is a crucial issue for
education as well as e-learning. It is required to serve qualitative and
standardization education. Quality cannot be expressed and set by a simple
definition, since in itself quality is a very abstract notion. The specified
context and the perspectives of users need to be taken into account when
defining quality in e-learning. It is also essential to classify suitable
criteria to address quality.
","['\nReema Ajmera\n', '\nDinesh Kumar dharamdasani\n']",,,http://arxiv.org/abs/1406.7744v1,cs.OH,['cs.OH'],,,[]
How to Track Online SLA,http://arxiv.org/abs/1407.0697v1,2014-07-02T16:37:18Z,2014-07-02T16:37:18Z,"  SLA (Service level agreement) is defined by an organization to fulfil its
client requirements, the time within which the deliverables should be turned
over to the clients. Tracking of SLA can be done manually by checking the
status, priority of any particular task. Manual SLA tracking takes time as one
has to go over each and every task that needs to be completed. For instance,
you ordered a product from a website and you are not happy with the quality of
the product and want to replace the same on urgent basis, You send mail to the
customer support department, the query/complaint will be submitted in a queue
and will be processed basis of its priority and urgency (The SLA for responding
back to customers concern are listed in the policy). This online SLA tracking
system will ensure that no queries/complaints are missed and are processed in
an organized manner as per their priority and the date by when it should be
handled. The portal will provide the status of the complaints for that
particular day and the ones which have been pending since last week. The
information can be refreshed as per the client need (within what time frame the
complaint should be addressed).
","['\nAnuradha Rana\n', '\nPratima Sharma\n']",,,http://arxiv.org/abs/1407.0697v1,cs.OH,['cs.OH'],,,[]
"A Dynamic Simulation-Optimization Model for Adaptive Management of Urban
  Water Distribution System Contamination Threats",http://arxiv.org/abs/1407.0424v1,2014-07-01T23:08:14Z,2014-07-01T23:08:14Z,"  Urban water distribution systems hold a critical and strategic position in
preserving public health and industrial growth. Despite the ubiquity of these
urban systems, aging infrastructure, and increased risk of terrorism, decision
support models for a timely and adaptive contamination emergency response still
remain at an undeveloped stage. Emergency response is characterized as a
progressive, interactive, and adaptive process that involves parallel
activities of processing streaming information and executing response actions.
This study develops a dynamic decision support model that adaptively simulates
the time-varying emergency environment and tracks changing best health
protection response measures at every stage of an emergency in real-time.
Feedback mechanisms between the contaminated network, emergency managers, and
consumers are incorporated in a dynamic simulation model to capture
time-varying characteristics of an emergency environment. An
evolutionary-computation-based dynamic optimization model is developed to
adaptively identify time-dependant optimal health protection measures during an
emergency. This dynamic simulation-optimization model treats perceived
contaminant source attributes as time-varying parameters to account for
perceived contamination source updates as more data stream in over time.
Performance of the developed dynamic decision support model is analyzed and
demonstrated using a mid-size virtual city that resembles the dynamics and
complexity of real-world urban systems. This adaptive emergency response
optimization model is intended to be a major component of an all-inclusive
cyberinfrastructure for efficient contamination threat management, which is
currently under development.
","['\nAmin Rasekh\n', '\nKelly Brumbelow\n']",,,http://arxiv.org/abs/1407.0424v1,cs.OH,"['cs.OH', 'cs.NE']",,,[]
"High Performance Network-on-Chips (NoCs) Design: Performance Modeling,
  Routing Algorithm and Architecture Optimization",http://arxiv.org/abs/1406.3790v1,2014-06-15T01:45:47Z,2014-06-15T01:45:47Z,"  With technology scaling down, hundreds and thousands processing elements
(PEs) can be integrated on a single chip. Network-on-chip (NoC) has been
proposed as an efficient solution to handle this distinctive challenge. In this
thesis, we have explored the high performance NoC design for MPSoC and CMP
structures from the performance modeling in the offline design phase to the
routing algorithm and NoC architecture optimization. More specifically, we
first deal with the issue of how to estimate an NoC design fast and accurately
in the synthesis inner loop. For this purpose, we propose a machine learning
based latency regression model to evaluate the NoC designs with respect to
different configurations. Then, for high performance NoC designs, we tackle one
of the most important problems, i.e., the routing algorithms design. For
avoiding temperature hotspots, a thermal-aware routing algorithm is proposed to
achieve an even temperature profile for application-specific Network-on-chips
(NoCs). For improving the reliability, a routing algorithm to achieve maximum
performance under fault is proposed. Finally, in the architecture level, we
propose two new NoC structures using bi-directional links for the performance
optimization. In particular, we propose a flit-level speedup scheme to enhance
the network-on-chip(NoC) performance utilizing bidirectional channels. We also
propose a flexible NoC architecture which takes advantage of a dynamic
distributed routing algorithm and improves the NoC communication performance
with moderate energy overhead. From the simulation results on both synthetic
traffic and real workload traces, significant performance improvement in terms
of latency and throughput can be achieved.
",['\nZhiliang Qian\n'],"A Ph.D. thesis of Zhiliang Qian in the Hong Kong University of
  Science and Technology; Thesis supervisor: Prof. Chi-Ying Tsui",,http://arxiv.org/abs/1406.3790v1,cs.OH,['cs.OH'],,,[]
Implementation of Tic-Tac-Toe Game in LabVIEW,http://arxiv.org/abs/1406.5177v1,2014-06-19T16:58:16Z,2014-06-19T16:58:16Z,"  Tic-Tac-Toe game can be played by two players where the square block (3 x 3)
can be filled with a cross (X) or a circle (O). The game will toggle between
the players by giving the chance for each player to mark their move. When one
of the players make a combination of 3 same markers in a horizontal, vertical
or diagonal line the program will display which player has won, whether X or O.
In this paper, we implement a 3x3 tic-tac-toe game in LabVIEW. The game is
designed so that two players can play tic-tac-toe using LabVIEW software. The
program will contain a display function and a select function to place the
symbol as well as toggle between the symbols allowing each player a turn to
play the game. The program will update after each player makes their move and
check for the conditions of game as it goes on. Overall program works without
any bugs and is able to use
","['\nLalitha Saroja Thota\n', '\nManal Elsayeed\n', '\nNaseema Shaik\n', '\nTayf Abdullah Ghawa\n', '\nAhlam Rashed\n', '\nMona Refdan\n', '\nWejdan Mohammed\n', '\nRawan Ali\n', '\nSuresh Babu Changalasetty\n']","7 pages, 17 figures, Published with International Journal of Computer
  Trends and Technology (IJCTT)""","International Journal of Computer Trends and Technology (IJCTT)
  V12(2):63-70, June 2014",http://dx.doi.org/10.14445/22312803/IJCTT-V12P114,cs.OH,['cs.OH'],10.14445/22312803/IJCTT-V12P114,,[]
"Midiendo la calidad de la informacion gestionada: algunas reflexiones
  conceptuales-metodologicas",http://arxiv.org/abs/1406.6277v1,2014-06-20T20:15:15Z,2014-06-20T20:15:15Z,"  The study, based on a documental classic analysis, presents conceptual and
methodological guidelines concerning the design of methodologies that help to
measure the quality of information that is managed in organizations. It is
described the process of information management and the importance of
implementing quality principles in it. There are exposed the four dimensions of
information quality as part of an indicators integration which characterize the
informational contents. There are defined each of the phases in the
methodological design to evaluate the information. There also are indicated the
implications of this activity for information professionals.
",['\nC. L. González-Valiente\n'],in Spanish,"Biblios. 2014; 52, 27-35",http://dx.doi.org/10.5195/biblios.2014.101,cs.OH,['cs.OH'],10.5195/biblios.2014.101,,[]
"Low-Complexity Variable Forgetting Factor Constrained Constant Modulus
  RLS Algorithm for Adaptive Beamforming",http://arxiv.org/abs/1406.6998v1,2014-06-22T15:37:21Z,2014-06-22T15:37:21Z,"  In this paper, a recursive least squares (RLS) based blind adaptive
beamforming algorithm that features a new variable forgetting factor (VFF)
mechanism is presented. The beamformer is designed according to the constrained
constant modulus (CCM) criterion, and the proposed adaptive algorithm operates
in the generalized sidelobe canceler (GSC) structure. A detailed study of its
operating properties is carried out, including a convexity analysis and a mean
squared error (MSE) analysis of its steady-state behavior. The results of
numerical experiments demonstrate that the proposed VFF mechanism achieves a
superior learning and tracking performance compared to other VFF mechanisms.
","['\nQ. Boya\n', '\nY. Cai\n', '\nB. Champagne\n', '\nR. C. de Lamare\n', '\nM. Zhao\n']","10 pages, 4 figures, Elsevier Signal Processing, 2014",,http://arxiv.org/abs/1406.6998v1,cs.OH,['cs.OH'],,,[]
"A Wavelet Based Algorithm for the Identification of Oscillatory
  Event-Related Potential Components",http://arxiv.org/abs/1407.2227v1,2014-06-20T15:31:48Z,2014-06-20T15:31:48Z,"  Event Related Potentials (ERPs) are very feeble alterations in the ongoing
Electroencephalogram (EEG) and their detection is a challenging problem. Based
on the unique time-based parameters derived from wavelet coefficients and the
asymmetry property of wavelets a novel algorithm to separate ERP components in
single-trial EEG data is described. Though illustrated as a specific
application to N170 ERP detection, the algorithm is a generalized approach that
can be easily adapted to isolate different kinds of ERP components. The
algorithm detected the N170 ERP component with a high level of accuracy. We
demonstrate that the asymmetry method is more accurate than the matching
wavelet algorithm and t-CWT method by 48.67 and 8.03 percent respectively. This
paper provides an off-line demonstration of the algorithm and considers issues
related to the extension of the algorithm to real-time applications.
","['\nArun Kumar A\n', '\nNinan Sajeeth Philip\n', '\nVincent J Samar\n', '\nJames A Desjardins\n', '\nSidney J Segalowitz\n']",Journal of neuroscience methods 06/2014,,http://dx.doi.org/10.1016/j.jneumeth.2014.06.004,cs.OH,"['cs.OH', 'q-bio.NC']",10.1016/j.jneumeth.2014.06.004,,[]
"A.Q.M.E.I.S.: Air Quality Meteorological and Enviromental Information
  System in Western Macedonia, Hellas",http://arxiv.org/abs/1406.0975v1,2014-06-04T08:42:35Z,2014-06-04T08:42:35Z,"  An operational monitoring, as well as high resolution local-scale
meteorological and air quality forecasting information system for Western
Macedonia, Hellas, has been developed and is operated by the Laboratory of
Atmospheric Pollution and Environmental Physics / TEI Western Macedonia since
2002, continuously improved. In this paper the novelty of information system is
presented, in a dynamic, easily accessible and user-friendly manner. It
consists of a structured system that users have access to and they can
manipulate thoroughly, as well as of a system for accessing and managing
results of measurements in a direct and dynamic way. It provides updates about
the weather and pollution forecast for the next few days (based on current day
information) in Western Macedonia. These forecasts are displayed through
dynamic-interactive web charts and the visual illustration of the atmospheric
pollution of the region in a map using images and animation images.
","['\nIoannis A. Skordas\n', '\nGeorge F. Fragulis\n', '\nAthanassios G. Triantafyllou\n']",,,http://arxiv.org/abs/1406.0975v1,cs.OH,['cs.OH'],,,[]
Load Hiding of Household's Power Demand,http://arxiv.org/abs/1406.2534v1,2014-06-10T13:00:37Z,2014-06-10T13:00:37Z,"  With the development and introduction of smart metering, the energy
information for costumers will change from infrequent manual meter readings to
fine-grained energy consumption data. On the one hand these fine-grained
measurements will lead to an improvement in costumers' energy habits, but on
the other hand the fined-grained data produces information about a household
and also households' inhabitants, which are the basis for many future privacy
issues. To ensure household privacy and smart meter information owned by the
household inhabitants, load hiding techniques were introduced to obfuscate the
load demand visible at the household energy meter. In this work, a
state-of-the-art battery-based load hiding (BLH) technique, which uses a
controllable battery to disguise the power consumption and a novel load hiding
technique called load-based load hiding (LLH) are presented. An LLH system uses
an controllable household appliance to obfuscate the household's power demand.
We evaluate and compare both load hiding techniques on real household data and
show that both techniques can strengthen household privacy but only LLH can
increase appliance level privacy.
","['\nDominik Egarter\n', '\nChristoph Prokop\n', '\nWilfried Elmenreich\n']",,"2014 IEEE International Conference on Smart Grid Communications
  (SmartGridComm), Venice, pp. 854-859",http://dx.doi.org/10.1109/SmartGridComm.2014.7007755,cs.OH,['cs.OH'],10.1109/SmartGridComm.2014.7007755,,[]
Faithful Glitch Propagation in Binary Circuit Models,http://arxiv.org/abs/1406.2544v1,2014-06-10T13:37:19Z,2014-06-10T13:37:19Z,"  Modern digital circuit design relies on fast digital timing simulation tools
and, hence, on accurate binary-valued circuit models that faithfully model
signal propagation, even throughout a complex design. Unfortunately, it was
recently proved [F\""ugger et al., ASYNC'13] that no existing binary-valued
circuit model proposed so far, including the two most commonly used pure and
inertial delay channels, faithfully captures glitch propagation: For the simple
Short-Pulse Filtration (SPF) problem, which is related to a circuit's ability
to suppress a single glitch, we showed that the quite broad class of bounded
single-history channels either contradict the unsolvability of SPF in bounded
time or the solvability of SPF in unbounded time in physical circuits.
  In this paper, we propose a class of binary circuit models that do not suffer
from this deficiency: Like bounded single-history channels, our involution
channels involve delays that may depend on the time of the previous output
transition. Their characteristic property are delay functions which are based
on involutions, i.e., functions that form their own inverse. A concrete example
of such a delay function, which is derived from a generalized first-order
analog circuit model, reveals that this is not an unrealistic assumption. We
prove that, in sharp contrast to what is possible with bounded single-history
channels, SPF cannot be solved in bounded time due to the nonexistence of a
lower bound on the delay of involution channels, whereas it is easy to provide
an unbounded SPF implementation. It hence follows that binary-valued circuit
models based on involution channels allow to solve SPF precisely when this is
possible in physical circuits. To the best of our knowledge, our model is hence
the very first candidate for a model that indeed guarantees faithful glitch
propagation.
","['\nMatthias Függer\n', '\nRobert Najvirt\n', '\nThomas Nowak\n', '\nUlrich Schmid\n']","18 pages, 7 figures",,http://arxiv.org/abs/1406.2544v1,cs.OH,['cs.OH'],,,[]
Optimal Gaussian Filter for Effective Noise Filtering,http://arxiv.org/abs/1406.3172v1,2014-06-12T09:49:17Z,2014-06-12T09:49:17Z,"  In this paper we show that the knowledge of noise statistics contaminating a
signal can be effectively used to choose an optimal Gaussian filter to
eliminate noise. Very specifically, we show that the additive white Gaussian
noise (AWGN) contaminating a signal can be filtered best by using a Gaussian
filter of specific characteristics. The design of the Gaussian filter bears
relationship with the noise statistics and also some basic information about
the signal. We first derive a relationship between the properties of the
Gaussian filter, noise statistics and the signal and later show through
experiments that this relationship can be used effectively to identify the
optimal Gaussian filter that can effectively filter noise.
","['\nSunil Kopparapu\n', '\nM Satish\n']","6 pages, 1 figure",,http://arxiv.org/abs/1406.3172v1,cs.OH,['cs.OH'],,,[]
Integration of Legacy Appliances into Home Energy Management Systems,http://arxiv.org/abs/1406.3252v1,2014-06-12T14:30:23Z,2014-06-12T14:30:23Z,"  The progressive installation of renewable energy sources requires the
coordination of energy consuming devices. At consumer level, this coordination
can be done by a home energy management system (HEMS). Interoperability issues
need to be solved among smart appliances as well as between smart and
non-smart, i.e., legacy devices. We expect current standardization efforts to
soon provide technologies to design smart appliances in order to cope with the
current interoperability issues. Nevertheless, common electrical devices affect
energy consumption significantly and therefore deserve consideration within
energy management applications. This paper discusses the integration of smart
and legacy devices into a generic system architecture and, subsequently,
elaborates the requirements and components which are necessary to realize such
an architecture including an application of load detection for the
identification of running loads and their integration into existing HEM
systems. We assess the feasibility of such an approach with a case study based
on a measurement campaign on real households. We show how the information of
detected appliances can be extracted in order to create device profiles
allowing for their integration and management within a HEMS.
","['\nDominik Egarter\n', '\nAndrea Monacchi\n', '\nTamer Khatib\n', '\nWilfried Elmenreich\n']",,,http://arxiv.org/abs/1406.3252v1,cs.OH,['cs.OH'],,,[]
Cognitive Coordination of Global Service Delivery,http://arxiv.org/abs/1406.0215v1,2014-06-01T23:12:06Z,2014-06-01T23:12:06Z,"  Formal coordination mechanisms are of growing importance as human-based
service delivery becomes more globalized and informal mechanisms are no longer
effective. Further it is becoming apparent that business environments,
communication among distributed teams, and work performance are all subject to
endogenous and exogenous uncertainty.
  This paper describes a stochastic model of service requests in global service
delivery and then puts forth a cognitive approach for coordination in the face
of uncertainty, based on a perception-action loop and receding horizon control.
Optimization algorithms used are a mix of myopic dynamic programming and
constraint-based programming. The coordination approach described has been
deployed by a globally integrated enterprise in a very large-scale global
delivery system and has been demonstrated to improve work efficiency by 10-15%
as compared to manual planning.
","['\nLav R. Varshney\n', '\nShivali Agarwal\n', '\nYi-Min Chee\n', '\nRenuka R. Sindhgatta\n', '\nDaniel V. Oppenheim\n', '\nJuhnyoung Lee\n', '\nKrishna Ratakonda\n']",,,http://arxiv.org/abs/1406.0215v1,cs.OH,['cs.OH'],,,[]
Optimizing the flash-RAM energy trade-off in deeply embedded systems,http://arxiv.org/abs/1406.0403v2,2014-05-30T16:48:06Z,2014-06-03T11:05:15Z,"  Deeply embedded systems often have the tightest constraints on energy
consumption, requiring that they consume tiny amounts of current and run on
batteries for years. However, they typically execute code directly from flash,
instead of the more energy efficient RAM. We implement a novel compiler
optimization that exploits the relative efficiency of RAM by statically moving
carefully selected basic blocks from flash to RAM. Our technique uses integer
linear programming, with an energy cost model to select a good set of basic
blocks to place into RAM, without impacting stack or data storage.
  We evaluate our optimization on a common ARM microcontroller and succeed in
reducing the average power consumption by up to 41% and reducing energy
consumption by up to 22%, while increasing execution time. A case study is
presented, where an application executes code then sleeps for a period of time.
For this example we show that our optimization could allow the application to
run on battery for up to 32% longer. We also show that for this scenario the
total application energy can be reduced, even if the optimization increases the
execution time of the code.
","['\nJames Pallister\n', '\nKerstin Eder\n', '\nSimon Hollis\n']",,,http://dx.doi.org/10.1109/CGO.2015.7054192,cs.OH,['cs.OH'],10.1109/CGO.2015.7054192,,[]
Mobile Application for GBAS Air Traffic Status Unit,http://arxiv.org/abs/1405.6822v1,2014-05-27T07:26:37Z,2014-05-27T07:26:37Z,"  At present, the Air Traffic Status Unit is a windows PC based application,
which receives the status of ground based augmentation system station over
Ethernet and displays on the screen. The objective of this project is to
convert the PC based Application into Mobile application using Android OS.
",['\nHiba Zaidi\n'],,,http://arxiv.org/abs/1405.6822v1,cs.OH,['cs.OH'],,,[]
Održavanje računarskih sistema,http://arxiv.org/abs/1405.7135v1,2014-05-28T07:01:00Z,2014-05-28T07:01:00Z,"  Computer hardware and software are resources without which the modern
business of any organization, from manufacturing to services, is impossible.
Not enough attention is being payed to maintenance of computer systems as an
aspect of business. This paper gives some recommendations for the selection of
the computer systems maintenance approach, based on many years of experience
maintaining these systems at the University of Zenica.
",['\nSamir Lemeš\n'],"3. konferencija Odr\v{z}avanje - Maintenance 2014 (S. Brdarevi\'c, S.
  Ja\v{s}arevi\'c, editors), ISSN 1986-583X, Zenica",,http://arxiv.org/abs/1405.7135v1,cs.OH,['cs.OH'],,,[]
"A Layered Modeling and Simulation Approach to investigate Resource-aware
  Computing in MPSoCs",http://arxiv.org/abs/1405.2917v1,2014-05-12T16:43:19Z,2014-05-12T16:43:19Z,"  Increasing complexity of modern multi-processor system on chip (MPSoC) and
the decreasing feature size have introduced new challenges. System designers
have to consider now aspects which were not part of the design process in past
times. Resource-aware Computing is one of such emerging design concerns which
can help to improve performance, dependability and resource utilization of
overall system. Resource-aware execution takes into account the resource status
when executing tasks on MPSoCs. Exploration of resource-aware computing at
early design stages of complex systems is mandatory and appropriate
methodologies to do this in an efficient manner are thus required. In this
paper, we present a modular approach which provides modeling and simulation
support for investigation of resource-aware execution in MPSoCs. The proposed
methodology enables rapid exploration of the design space by modeling and
simulating the resource-awareness in a separate layer while widely reusing the
legacy system model in the other layer. Our experiments illustrate the benefits
of our approach for the exploration of resource-aware execution on MPSoCs.
","['\nAurang Zaib\n', '\nPrashanth Raju\n', '\nThomas Wild\n', '\nAndreas Herkersdorf\n']","Presented at 1st Workshop on Resource Awareness and Adaptivity in
  Multi-Core Computing (Racing 2014) (arXiv:1405.2281)",,http://arxiv.org/abs/1405.2917v1,cs.OH,['cs.OH'],,,[]
GREEND: An Energy Consumption Dataset of Households in Italy and Austria,http://arxiv.org/abs/1405.3100v2,2014-05-13T10:51:32Z,2014-05-22T13:57:03Z,"  Home energy management systems can be used to monitor and optimize
consumption and local production from renewable energy. To assess solutions
before their deployment, researchers and designers of those systems demand for
energy consumption datasets. In this paper, we present the GREEND dataset,
containing detailed power usage information obtained through a measurement
campaign in households in Austria and Italy. We provide a description of
consumption scenarios and discuss design choices for the sensing
infrastructure. Finally, we benchmark the dataset with state-of-the-art
techniques in load disaggregation, occupancy detection and appliance usage
mining.
","['\nAndrea Monacchi\n', '\nDominik Egarter\n', '\nWilfried Elmenreich\n', ""\nSalvatore D'Alessandro\n"", '\nAndrea M. Tonello\n']",,,http://arxiv.org/abs/1405.3100v2,cs.OH,['cs.OH'],,,[]
"Recognition and Ranking Critical Success Factors of Business
  Intelligence in Hospitals -- Case Study: Hasheminejad Hospital",http://arxiv.org/abs/1405.4597v1,2014-05-19T04:26:13Z,2014-05-19T04:26:13Z,"  Business Intelligence, not as a tool of a product but as a new approach is
propounded in organizations to make tough decisions in business as shortly as
possible. Hospital managers often need business intelligence in their fiscal,
operational, and clinical reports and indices. The main goal of recognition and
ranking CSF is implementation of a business intelligent system in hospitals to
increase success factor of application of business intelligence in health and
treatment sector. This paper is an application and descriptive-analytical one,
in which we use questionnaires to gather data and we used SPSS and LISREL to
analyze them. Its statistical society is managers and personnel of Hasheminejad
hospital and case studies are selected by Cochran formula. The findings show
that all three organizational, process, and technological factors equally
affect implementation of business intelligence based on Yeoh & Koronis
approach, where the assumptions are based upon it. The proposed model for CSFs
of business intelligence in hospitals include: declaring perspective, goals and
strategies, development of human and financial resources, clarification of
organizational culture, documentation and process mature, management support,
etc. Business intelligence implementation is affected by different components.
Center of Hasheminejad hospital BI system as a leader in providing quality
health care, partially succeeded to take advantage of the benefits the
organization in passing the information revolution but the development of this
system to achieve intelligent hospital and its certainty is a high priority,
thus it can`t be said that the hospital-wide BI system is quite favorable. In
this regard, it can be concluded that Hasheminejad hospital requires practical
model for business intelligence systems development.
","['\nMarjan Naderinejad\n', '\nMohammad Jafar Tarokh\n', '\nAlireza Poorebrahimi\n']",http://airccse.org/journal/ijcsit2014_curr.html,,http://dx.doi.org/10.5121/ijcsit.2014.6208,cs.OH,['cs.OH'],10.5121/ijcsit.2014.6208,,[]
"Using the Expectation Maximization Algorithm with Heterogeneous Mixture
  Components for the Analysis of Spectrometry Data",http://arxiv.org/abs/1405.5501v1,2014-05-21T18:14:50Z,2014-05-21T18:14:50Z,"  Coupling a multi-capillary column (MCC) with an ion mobility (IM)
spectrometer (IMS) opened a multitude of new application areas for gas
analysis, especially in a medical context, as volatile organic compounds (VOCs)
in exhaled breath can hint at a person's state of health. To obtain a potential
diagnosis from a raw MCC/IMS measurement, several computational steps are
necessary, which so far have required manual interaction, e.g., human
evaluation of discovered peaks. We have recently proposed an automated pipeline
for this task that does not require human intervention during the analysis.
Nevertheless, there is a need for improved methods for each computational step.
In comparison to gas chromatography / mass spectrometry (GC/MS) data, MCC/IMS
data is easier and less expensive to obtain, but peaks are more diffuse and
there is a higher noise level. MCC/IMS measurements can be described as samples
of mixture models (i.e., of convex combinations) of two-dimensional probability
distributions. So we use the expectation-maximization (EM) algorithm to
deconvolute mixtures in order to develop methods that improve data processing
in three computational steps: denoising, baseline correction and peak
clustering. A common theme of these methods is that mixture components within
one model are not homogeneous (e.g., all Gaussian), but of different types.
Evaluation shows that the novel methods outperform the existing ones. We
provide Python software implementing all three methods and make our evaluation
data available at http://www.rahmannlab.de/research/ims.
","['\nDominik Kopczynski\n', '\nSven Rahmann\n']",,,http://arxiv.org/abs/1405.5501v1,cs.OH,['cs.OH'],,,[]
"Estimation of Optimized Energy and Latency Constraints for Task
  Allocation in 3d Network on Chip",http://arxiv.org/abs/1405.0109v1,2014-05-01T07:38:46Z,2014-05-01T07:38:46Z,"  In Network on Chip (NoC) rooted system, energy consumption is affected by
task scheduling and allocation schemes which affect the performance of the
system. In this paper we test the pre-existing proposed algorithms and
introduced a new energy skilled algorithm for 3D NoC architecture. An efficient
dynamic and cluster approaches are proposed along with the optimization using
bio-inspired algorithm. The proposed algorithm has been implemented and
evaluated on randomly generated benchmark and real life application such as
MMS, Telecom and VOPD. The algorithm has also been tested with the E3S
benchmark and has been compared with the existing mapping algorithm spiral and
crinkle and has shown better reduction in the communication energy consumption
and shows improvement in the performance of the system. On performing
experimental analysis of proposed algorithm results shows that average
reduction in energy consumption is 49%, reduction in communication cost is 48%
and average latency is 34%. Cluster based approach is mapped onto NoC using
Dynamic Diagonal Mapping (DDMap), Crinkle and Spiral algorithms and found DDmap
provides improved result. On analysis and comparison of mapping of cluster
using DDmap approach the average energy reduction is 14% and 9% with crinkle
and spiral.
","['\nVaibhav Jha\n', '\nMohit Jha\n', '\nGK Sharma\n']","20 Pages,17 Figure, International Journal of Computer Science &
  Information Technology. arXiv admin note: substantial text overlap with
  arXiv:1404.2512","International Journal of Computer Science & Information Technology
  Vol 6,No 2,pp 67-86 April 2014",http://dx.doi.org/10.5121/ijcsit.2014.6205,cs.OH,['cs.OH'],10.5121/ijcsit.2014.6205,,[]
"Coal Blending: Business Value, Analysis, and Optimization",http://arxiv.org/abs/1405.0276v2,2014-05-01T07:59:10Z,2014-05-26T05:04:28Z,"  Coal blending is a critically important process in the coal mining industry
as it directly influences the number of product tonnes and the total revenue
generated by a mine site. Coal blending represents a challenging and complex
problem with numerous blending possibilities, multiple constraints and
competing objectives. At many mine sites, blending decisions are made using
heuristics that have been developed through experience or made by using
computer assisted control algorithms or linear programming. While current
blending procedures have achieved profitable outcomes in the past, they often
result in a sub-optimal utilization of high quality coal. This sub-optimality
has a considerable negative impact on mine site productivity as it can reduce
the amount of lower quality ROM that is blended and sold. This article reviews
the coal blending problem and discusses some of the difficult trade-offs and
challenges that arise in trying to address this problem. We highlight some of
the risks from making simplifying assumptions and the limitations of current
software optimization systems. We conclude by explaining how the mining
industry would significantly benefit from research and development into
optimization algorithms and technologies that are better able to combine
computer optimization algorithm capabilities with the important insights of
engineers and quality control specialists.
","['\nJames Whitacre\n', '\nSven Schellenberg\n', '\nAntony Iorio\n']",,,http://arxiv.org/abs/1405.0276v2,cs.OH,['cs.OH'],,,[]
"Rapture in the Cartesian Wall between Real World Entities and their
  Abstract Models",http://arxiv.org/abs/1405.1085v1,2014-05-05T21:40:58Z,2014-05-05T21:40:58Z,"  This short paper envisages that the advancements made with respect to Big
Data (BD), High Performance Computing, etc. would give rise to a new paradigm
of concrete information models, which would closely replicate the real world
and the consequences such as self-verifying information models, BD warehouses
as intermediaries between data sources and information systems, etc.
",['\nNarada Wickramage\n'],"International Conference on Future Trends in Computing and
  Communication Technologies",,http://arxiv.org/abs/1405.1085v1,cs.OH,['cs.OH'],,,[]
A New Multi-Tiered Solid State Disk Using Slc/Mlc Combined Flash Memory,http://arxiv.org/abs/1405.2157v1,2014-05-09T06:58:18Z,2014-05-09T06:58:18Z,"  Storing digital information, ensuring the accuracy, steady and uninterrupted
access to the data are considered as fundamental challenges in enterprise-class
organizations and companies. In recent years, new types of storage systems such
as solid state disks (SSD) have been introduced. Unlike hard disks that have
mechanical structure, SSDs are based on flash memory and thus have electronic
structure. Generally a SSD consists of a number of flash memory chips, some
buffers of the volatile memory type, and an embedded microprocessor, which have
been interconnected by a port. This microprocessor run a small file system
which called flash translation layer (FTL). This software controls and
schedules buffers, data transfers and all flash memory tasks. SSDs have some
advantages over hard disks such as high speed, low energy consumption, lower
heat and noise, resistance against damage, and smaller size. Besides, some
disadvantages such as limited endurance and high price are still challenging.
In this study, the effort is to combine two common technologies - SLC and MLC
chips - used in the manufacture of SSDs in a single SSD to decrease the side
effects of current SSDs. The idea of using multi-layer SSD is regarded as an
efficient solution in this field.
","['\nArash Batni\nFaculty of ECE, ShahidBeheshti University G.C., Tehran, Iran\n', '\nFarshad Safaei\nFaculty of ECE, ShahidBeheshti University G.C., Tehran, Iran\nInstitute for Studies in Theoretical Physics and Mathematics\n']","13 pages, 18 figures","International Journal of Computer Science, Engineering and
  Information Technology (IJCSEIT), Vol. 4, No.2, April 2014",http://arxiv.org/abs/1405.2157v1,cs.OH,['cs.OH'],,,"['Faculty of ECE, ShahidBeheshti University G.C., Tehran, Iran', 'Faculty of ECE, ShahidBeheshti University G.C., Tehran, Iran', 'Institute for Studies in Theoretical Physics and Mathematics']"
"The Eve of 3D Printing in Telemedicine: State of the Art and Future
  Challenges",http://arxiv.org/abs/1405.2305v1,2014-05-06T10:54:29Z,2014-05-06T10:54:29Z,"  3D printing has raised a lot of attention from fields outside the
manufacturing one in the last years. In this paper, we will illustrate some
recent advances of 3D printing technology, applied to the field of telemedicine
and remote patient care. The potentiality of this technology will be detailed
without lab examples. Some crucial aspect such as the regulation of these
devices and the need of some standards will also be discussed. The purpose of
this paper is to present some of the most promising applications of such
technology.
","['\nPiero Giacomelli\n', '\nAsa Smedberg\n']","5 pages submitted to The Sixth International Conference on eHealth,
  Telemedicine, and Social Medicine (eTELEMED 2014)",,http://arxiv.org/abs/1405.2305v1,cs.OH,['cs.OH'],,,[]
"Training-Free Non-Intrusive Load Monitoring of Electric Vehicle Charging
  with Low Sampling Rate",http://arxiv.org/abs/1404.5020v2,2014-04-20T06:18:38Z,2014-08-07T02:10:44Z,"  Non-intrusive load monitoring (NILM) is an important topic in smart-grid and
smart-home. Many energy disaggregation algorithms have been proposed to detect
various individual appliances from one aggregated signal observation. However,
few works studied the energy disaggregation of plug-in electric vehicle (EV)
charging in the residential environment since EVs charging at home has emerged
only recently. Recent studies showed that EV charging has a large impact on
smart-grid especially in summer. Therefore, EV charging monitoring has become a
more important and urgent missing piece in energy disaggregation. In this
paper, we present a novel method to disaggregate EV charging signals from
aggregated real power signals. The proposed method can effectively mitigate
interference coming from air-conditioner (AC), enabling accurate EV charging
detection and energy estimation under the presence of AC power signals.
Besides, the proposed algorithm requires no training, demands a light
computational load, delivers high estimation accuracy, and works well for data
recorded at the low sampling rate 1/60 Hz. When the algorithm is tested on
real-world data recorded from 11 houses over about a whole year (total 125
months worth of data), the averaged error in estimating energy consumption of
EV charging is 15.7 kwh/month (while the true averaged energy consumption of EV
charging is 208.5 kwh/month), and the averaged normalized mean square error in
disaggregating EV charging load signals is 0.19.
","['\nZhilin Zhang\n', '\nJae Hyun Son\n', '\nYing Li\n', '\nMark Trayer\n', '\nZhouyue Pi\n', '\nDong Yoon Hwang\n', '\nJoong Ki Moon\n']","Accepted by The 40th Annual Conference of the IEEE Industrial
  Electronics Society (IECON 2014)",,http://dx.doi.org/10.1109/IECON.2014.7049328,cs.OH,['cs.OH'],10.1109/IECON.2014.7049328,,[]
"Design of a Low Voltage Class-AB CMOS Super Buffer Amplifier with Sub
  Threshold and Leakage Control",http://arxiv.org/abs/1404.6034v1,2014-04-24T06:04:18Z,2014-04-24T06:04:18Z,"  This paper describes a CMOS analogy voltage supper buffer designed to have
extremely low static current Consumption as well as high current drive
capability. A new technique is used to reduce the leakage power of class-AB
CMOS buffer circuits without affecting dynamic power dissipation. The name of
applied technique is TRANSISTOR GATING TECHNIQUE, which gives the high speed
buffer with the reduced low power dissipation (1.105%), low leakage and reduced
area (3.08%) also. The proposed buffer is simulated at 45nm CMOS technology and
the circuit is operated at 3.3V supply[11]. Consumption is comparable to the
switching component. Reports indicate that 40% or even higher percentage of the
total power consumption is due to the leakage of transistors. This percentage
will increase with technology scaling unless effective techniques are
introduced to bring leakage under control. This article focuses on circuit
optimization and Design automation techniques to accomplish this goal [9].
",['\nRakesh Gupta\n'],"5 pages, 6 figures, International Journal of Engineering Trends and
  Technology (IJETT)-Volume 7 Number 1 - Jan 2014",,http://dx.doi.org/10.14445/22315381/IJETT-V7P219,cs.OH,['cs.OH'],10.14445/22315381/IJETT-V7P219,,[]
Design and considerations of ADC0808 as interleaved ADCs,http://arxiv.org/abs/1404.6040v1,2014-04-24T06:25:09Z,2014-04-24T06:25:09Z,"  Here in this paper we are presenting a digital system background technique
for correcting the time offset error rate and gain mismatches in a time
interleaved analog to digital converter system for N channel communication
using 8 bit ADC0808 ICs. A time interleaved A to D converter system is an
effective way to implement a high sampling rate ADC with relatively slow
circuits. This paper analyses the benefits and derives an upper band on the
performance by considering kT/C noise and slewing requirement of the circuit
driving the system. In the system, several channel ADCs operate at interleaved
sampling times as if they were effectively a single ADC operating at a much
higher sampling rate. A timing mismatch calibration technique is proposed that
covers linear and non linear channel mismatches, unifies, and extends the
channel models. A novel foreground channel mismatch identification method has
been developed, which can be used to fully characterize dynamic linear
mismatches. A background identification method provides accurate timing
mismatch estimates.
","['\nRajiv Kumar\n', '\nRakesh Gupta\n']","11 Pages, 13 figures, Internation Journal of Advanced Scientific and
  Technical Research Issue 4, Volume 1, January - February 2014",,http://arxiv.org/abs/1404.6040v1,cs.OH,['cs.OH'],,,[]
Big Spectrum Data: The New Resource for Cognitive Wireless Networking,http://arxiv.org/abs/1404.6508v1,2014-04-23T03:38:10Z,2014-04-23T03:38:10Z,"  The era of Big Data is here now, which has brought both unprecedented
opportunities and critical challenges. In this article, from a perspective of
cognitive wireless networking, we start with a definition of Big Spectrum Data
by analyzing its characteristics in terms of six Vs, i.e., volume, variety,
velocity, veracity, viability, and value. We then present a high-level tutorial
on research frontiers in Big Spectrum Data analytics to guide the development
of practical algorithms. We also highlight Big Spectrum Data as the new
resource for cognitive wireless networking by presenting the emerging use
cases.
","['\nGuoru Ding\n', '\nQihui Wu\n', '\nJinlong Wang\n', '\nYu-Dong Yao\n']",,,http://arxiv.org/abs/1404.6508v1,cs.OH,['cs.OH'],,,[]
Data Driven Energy Efficiency in Buildings,http://arxiv.org/abs/1404.7227v3,2014-04-29T03:45:59Z,2014-09-02T21:53:26Z,"  Buildings across the world contribute significantly to the overall energy
consumption and are thus stakeholders in grid operations. Towards the
development of a smart grid, utilities and governments across the world are
encouraging smart meter deployments. High resolution (often at every 15
minutes) data from these smart meters can be used to understand and optimize
energy consumptions in buildings. In addition to smart meters, buildings are
also increasingly managed with Building Management Systems (BMS) which control
different sub-systems such as lighting and heating, ventilation, and air
conditioning (HVAC). With the advent of these smart meters, increased usage of
BMS and easy availability and widespread installation of ambient sensors, there
is a deluge of building energy data. This data has been leveraged for a variety
of applications such as demand response, appliance fault detection and
optimizing HVAC schedules. Beyond the traditional use of such data sets, they
can be put to effective use towards making buildings smarter and hence driving
every possible bit of energy efficiency. Effective use of this data entails
several critical areas from sensing to decision making and participatory
involvement of occupants. Picking from wide literature in building energy
efficiency, we identify five crust areas (also referred to as 5 Is) for
realizing data driven energy efficiency in buildings : i) instrument optimally;
ii) interconnect sub-systems; iii) inferred decision making; iv) involve
occupants and v) intelligent operations. We classify prior work as per these 5
Is and dis-cuss challenges, opportunities and applications across them.
Building upon these 5 Is we discuss a well studied problem in building energy
efficiency -non-intrusive load monitoring (NILM) and how research in this area
spans across the 5 Is.
","['\nNipun Batra\n', '\nAmarjeet Singh\n', '\nPushpendra Singh\n', '\nHaimonti Dutta\n', '\nVenkatesh Sarangan\n', '\nMani Srivastava\n']",PhD. Qualifiers report presented at IIIT Delhi,,http://arxiv.org/abs/1404.7227v3,cs.OH,['cs.OH'],,,[]
"Revised Version of a JCIT Paper-Comparison of Feature Point Extraction
  Algorithms for Vision Based Autonomous Aerial Refueling",http://arxiv.org/abs/1405.6163v2,2014-04-28T16:12:43Z,2014-06-04T11:30:23Z,"  This is a revised version of our paper published in Journal of Convergence
Information Technology(JCIT): ""Comparison of Feature Point Extraction
Algorithms for Vision Based Autonomous Aerial Refueling"". We corrected some
errors including measurement unit errors, spelling errors and so on. Since the
published papers in JCIT are not allowed to be modified, we submit the revised
version to arXiv.org to make the paper more rigorous and not to confuse other
researchers.
","['\nBorui Li\n', '\nChundi Mu\n', '\nTao Wang\n', '\nQian Peng\n']",,"Journal of Convergence Information Technology. 2012, 7(20):
  108-118",http://dx.doi.org/10.4156/jcit.vol7.issue20.14,cs.OH,['cs.OH'],10.4156/jcit.vol7.issue20.14,,[]
Implementation of Sensor Network using Efficient CAN Interface,http://arxiv.org/abs/1404.6612v1,2014-04-26T05:57:42Z,2014-04-26T05:57:42Z,"  Sensors monitored by centralized system, that may be used for controlling and
monitoring industrial parameters (Temp, Pressure, Speed, Torque) by using CAN
interface. In this paper we presents a comprehensive overview of controller
area networks, their architecture, protocol, and standards. Also, this paper
gives an overview of CAN applications, in both the industrial and nonindustrial
fields. Due to CAN reliability, efficiency and robustness, we also propose the
extension of CAN applications to sensor network. In this paper, a framework of
sensor network for monitoring industrial parameters is explained where sensors
are physically distributed and CAN is used to exchange system information. CAN
(Controller Area Network) is a high integrity serial bus protocol that is
designed to operate at high speeds ranging from 20kbit/s to 1Mbit/s which
provide an efficient, reliable and very economical link
","['\nYeshwant Deodhe\n', '\nSwapnil Jain\n', '\nRavindra Gimonkar\n']",5 PAGES PAPER,,http://arxiv.org/abs/1404.6612v1,cs.OH,"['cs.OH', 'cs.NI']",,,[]
"Real-Time Estimation of the Distribution of Brake Response Times for an
  Individual Driver Using Vehicular Ad Hoc Network",http://arxiv.org/abs/1405.6161v1,2014-04-29T19:49:28Z,2014-04-29T19:49:28Z,"  Adapting the functioning of the collision warning systems to the specific
drivers' characteristics is of great benefit to drivers. For example, by
customizing collision warning algorithms we can minimize false alarms, thereby
reducing injuries and deaths in highway traffic accidents. In order to take the
behaviors of individual drivers into account, the system needs to have a
Real-Time estimation of the distribution of brake response times for an
individual driver. In this paper, we propose a method for doing this estimation
which is not computationally intensive and can take advantage of the
information contained in all data points.
","['\nAli Rakhshan\n', '\nEvan Ray\n', '\nHossein Pishro-Nik\n']",,,http://arxiv.org/abs/1405.6161v1,cs.OH,"['cs.OH', 'cs.SY']",,,[]
Teaching Network Storage Technology Assessment Outcomes and Directions,http://arxiv.org/abs/1404.2346v1,2014-04-09T01:32:03Z,2014-04-09T01:32:03Z,"  The paper presents academic content, delivery and assessment mechanisms used,
available resources including initial lessons from teaching Networked Storage
Technology as a special topics course to students enrolled in two specific
programs - IT and CS. The course is based on the EMC s vendor-neutral Storage
Technology Fundamentals course. Furthermore, this manuscript provides a
detailed review of how the course fits into our curriculum, particularly, how
it helps achieving the 2008 ABET assessment requirements.
","['\nDr. V. Jovanovic\n', '\nDr. Timur Mirzoev\n']",,"ACM 2008, New York, NY, USA ISBN: 978-1-60558-329-7",http://arxiv.org/abs/1404.2346v1,cs.OH,['cs.OH'],,,[]
Automation Security,http://arxiv.org/abs/1404.2347v1,2014-04-09T01:41:35Z,2014-04-09T01:41:35Z,"  Web-based Automated Process Control systems are a new type of applications
that use the Internet to control industrial processes with the access to the
real-time data. Supervisory control and data acquisition (SCADA) networks
contain computers and applications that perform key functions in providing
essential services and commodities (e.g., electricity, natural gas, gasoline,
water, waste treatment, transportation) to all Americans. As such, they are
part of the nation s critical infrastructure and require protection from a
variety of threats that exist in cyber space today.
",['\nDr. Timur Mirzoev\n'],,"Proceedings of the Second International Seminar, Saint Petersburg
  State University of Aerospace Instrumentation. January 2007",http://arxiv.org/abs/1404.2347v1,cs.OH,['cs.OH'],,,[]
"Substrate integrated waveguide power divider, circulator and coupler in
  [10-15] GHz band",http://arxiv.org/abs/1404.2888v1,2014-04-10T17:45:37Z,2014-04-10T17:45:37Z,"  The Substrate Integrated Waveguide (SIW) technology is an attractive approach
for the design of high performance microwave and millimeter wave components, as
it combines the advantages of planar technology, such as low fabrication costs,
with the low loss inherent to the waveguide solution. In this study, a
substrate integrated waveguide power divider, circulator and coupler are
conceived and optimized in [10-15] GHz band by Ansoft HFSS code. Thus, results
of this modeling are presented, discussed and allow to integrate these devices
in planar circuits.
",['\nBouchra Rahali Mohammed Feham\n'],,"International Journal of Information Sciences and Techniques
  (IJIST) Vol.4, No.1/2, March 2014",http://arxiv.org/abs/1404.2888v1,cs.OH,['cs.OH'],,,[]
"Elevation Contour Analysis and Water body Extraction for Finding Water
  Scarcity Locations using DEM",http://arxiv.org/abs/1404.3002v2,2014-04-11T04:59:58Z,2017-12-06T04:46:51Z,"  The present study was aimed to create new methods for extraction and analysis
of land elevation contour lines, automatic extraction of water bodies (river
basins and lakes), from the digital elevation models (DEM) of a test area. And
extraction of villages which are fell under critical water scarcity regions for
agriculture and drinking water with respect to their elevation data and
available natural water resources.
","['\nB. G. Kodge\n', '\nP. S. Hiremath\n']","Due to some unprojectioned spatial data, and wrong contour outputs,
  the paper is withdrawn","World Journal of Science and Technology 2011, 1(12): 29-34 World
  Journal of Science and Technology 2011, 1(12)",http://arxiv.org/abs/1404.3002v2,cs.OH,['cs.OH'],,,[]
"Computer Simulation Codes for the Quine-McCluskey Method of Logic
  Minimization",http://arxiv.org/abs/1404.3349v2,2014-04-13T06:58:49Z,2014-05-20T09:29:49Z,"  The Quine-McCluskey method is useful in minimizing logic expressions for
larger number of variables when compared with minimization by Karnaugh Map or
Boolean algebra. In this paper, we have tried to put together all of the
computer codes which are available on the internet, edited and modified them as
well as rewritten some parts of those collected codes our self, which are used
in the implementation of the Quine- McCluskey method. A brief introduction and
the logic of this method are discussed following which the codes have been
provided. The Quine-McCluskey Method has been implemented using computer
languages like C and C++ using some amount of variations. Our effort is to list
them all, so that the readers well versed in any of the particular computer
language will find it easy to follow the code written in that particular
language.
",['\nSourangsu Banerji\n'],"45 pages, 8 tables",,http://arxiv.org/abs/1404.3349v2,cs.OH,['cs.OH'],,,[]
Big Data: Overview,http://arxiv.org/abs/1404.4136v1,2014-04-16T04:58:57Z,2014-04-16T04:58:57Z,"  Big data is data that exceeds the processing capacity of traditional
databases. The data is too big to be processed by a single machine. New and
innovative methods are required to process and store such large volumes of
data. This paper provides an overview on big data, its importance in our live
and some technologies to handle big data.
","['\nRicha Gupta\n', '\nSunny Gupta\n', '\nAnuradha Singhal\n']","3 pages, 1 figure","International Journal of Computer Trends and Technology (IJCTT)
  9(5):1-3, March 2014",http://dx.doi.org/10.14445/22312803/IJCTT-V9150,cs.OH,['cs.OH'],10.14445/22312803/IJCTT-V9150,,[]
Web Mining using Artificial Ant Colonies: A Survey,http://arxiv.org/abs/1404.4139v1,2014-04-16T05:15:04Z,2014-04-16T05:15:04Z,"  Web mining has been very crucial to any organization as it provides useful
insights to business patterns. It helps the company to understand its customers
better. As the web is growing in pace, so is its importance and hence it
becomes all the more necessary to find useful patterns. Here in this paper, web
mining using ant colony optimization has been reviewed with some of its
experimental results.
",['\nRicha Gupta\n'],"6 pages, 3 figures","International Journal of Computer Trends and Technology (IJCTT)
  10(1): 1-6, April 2014",http://dx.doi.org/10.14445/22312803/IJCTT-V10P103,cs.OH,['cs.OH'],10.14445/22312803/IJCTT-V10P103,,[]
Journey from Data Mining to Web Mining to Big Data,http://arxiv.org/abs/1404.4140v1,2014-04-16T05:20:28Z,2014-04-16T05:20:28Z,"  This paper describes the journey of big data starting from data mining to web
mining to big data. It discusses each of this method in brief and also provides
their applications. It states the importance of mining big data today using
fast and novel approaches.
",['\nRicha Gupta\n'],3 pages,"International Journal of Computer Trends and Technology (IJCTT)
  10(1): 1-3, April 2014. Published by Seventh Sense Research Group",http://dx.doi.org/10.14445/22312803/IJCTT-V10P104,cs.OH,['cs.OH'],10.14445/22312803/IJCTT-V10P104,,[]
Trust Evaluation using an Improved Context Similarity Measurement,http://arxiv.org/abs/1404.4592v1,2014-04-17T17:54:15Z,2014-04-17T17:54:15Z,"  In context-aware trust evaluation, using ontology tree is a popular approach
to represent the relation between contexts. Usually, similarity between two
contexts is computed using these trees. Therefore, the performance of trust
evaluation highly depends on the quality of ontology trees. Fairness or
granularity consistency is one of the major limitations affecting the quality
of ontology tree. This limitation refers to inequality of semantic similarity
in the most ontology trees. In other words, semantic similarity of every two
adjacent nodes is unequal in these trees. It deteriorates the performance of
contexts similarity computation. We overcome this limitation by weighting tree
edges based on their semantic similarity. Weight of each edge is computed using
Normalized Similarity Score (NSS) method. This method is based on frequencies
of concepts (words) co-occurrences in the pages indexed by search engines. Our
experiments represent the better performance of the proposed approach in
comparison with established trust evaluation approaches. The suggested approach
can enhance efficiency of any solution which models semantic relations by
ontology tree.
","['\nMohsen Raeesi\n', '\nMohammad Amin Morid\n', '\nMehdi Shajari\n']","19 pages, 13 figures","International Journal of Business Information Systems Strategies
  (IJBISS), Volume 3, Number 1, February 2014",http://arxiv.org/abs/1404.4592v1,cs.OH,['cs.OH'],,,[]
"Use of ARAS 360 to Facilitate Rapid Development of Articulated Total
  Body Biomechanical Physics Simulations",http://arxiv.org/abs/1405.2063v1,2014-04-17T19:32:40Z,2014-04-17T19:32:40Z,"  The development of 3-dimensional environments to be used within a
biomechanical physics simulation framework, such as Articulated Total Body, can
be laborious and time intensive. This brief article demonstrates how the ARAS
360 software package can aid the user by speeding up development time.
",['\nBob J. Scurlock\n'],,,http://arxiv.org/abs/1405.2063v1,cs.OH,['cs.OH'],,,[]
"Multiple-Population Moment Estimation: Exploiting Inter-Population
  Correlation for Efficient Moment Estimation in Analog/Mixed-Signal Validation",http://arxiv.org/abs/1403.7872v1,2014-03-31T05:23:09Z,2014-03-31T05:23:09Z,"  Moment estimation is an important problem during circuit validation, in both
pre-Silicon and post-Silicon stages. From the estimated moments, the
probability of failure and parametric yield can be estimated at each circuit
configuration and corner, and these metrics are used for design optimization
and making product qualification decisions. The problem is especially difficult
if only a very small sample size is allowed for measurement or simulation, as
is the case for complex analog/mixed-signal circuits. In this paper, we propose
an efficient moment estimation method, called Multiple-Population Moment
Estimation (MPME), that significantly improves estimation accuracy under small
sample size. The key idea is to leverage the data collected under different
corners/configurations to improve the accuracy of moment estimation at each
individual corner/configuration. Mathematically, we employ the hierarchical
Bayesian framework to exploit the underlying correlation in the data. We apply
the proposed method to several datasets including post-silicon measurements of
a commercial high-speed I/O link, and demonstrate an average error reduction of
up to 2$\times$, which can be equivalently translated to significant reduction
of validation time and cost.
","['\nChenjie Gu\n', '\nManzil Zaheer\n', '\nXin Li\n']",,,http://arxiv.org/abs/1403.7872v1,cs.OH,['cs.OH'],,,[]
"Development of Wearable Systems for Ubiquitous Healthcare Service
  Provisioning",http://arxiv.org/abs/1404.0158v1,2014-04-01T08:18:09Z,2014-04-01T08:18:09Z,"  This paper reports on the development of a wearable system using wireless
biomedical sensors for ubiquitous healthcare service provisioning. The
prototype system is developed to address current healthcare challenges such as
increasing cost of services, inability to access diverse services, low quality
services and increasing population of elderly as experienced globally. The
biomedical sensors proactively collect physiological data of remote patients to
recommend diagnostic services. The prototype system is designed to monitor
oxygen saturation level (SpO2), Heart Rate (HR), activity and location of the
elderly. Physiological data collected are uploaded to a Health Server (HS) via
GPRS/Internet for analysis.
","['\nO. O. Ogunduyile\n', '\nO. O. Olugbara\n', '\nM. Lall\n']","6 pages, 3 figures, APCBEE Procedia 7, 2013. arXiv admin note:
  substantial text overlap with arXiv:1309.1542",,http://arxiv.org/abs/1404.0158v1,cs.OH,['cs.OH'],,,[]
"The UK-DALE dataset, domestic appliance-level electricity demand and
  whole-house demand from five UK homes",http://arxiv.org/abs/1404.0284v3,2014-04-01T15:49:00Z,2015-03-19T10:45:57Z,"  Many countries are rolling out smart electricity meters. These measure a
home's total power demand. However, research into consumer behaviour suggests
that consumers are best able to improve their energy efficiency when provided
with itemised, appliance-by-appliance consumption information. Energy
disaggregation is a computational technique for estimating
appliance-by-appliance energy consumption from a whole-house meter signal.
  To conduct research on disaggregation algorithms, researchers require data
describing not just the aggregate demand per building but also the `ground
truth' demand of individual appliances. In this context, we present UK-DALE: an
open-access dataset from the UK recording Domestic Appliance-Level Electricity
at a sample rate of 16 kHz for the whole-house and at 1/6 Hz for individual
appliances. This is the first open access UK dataset at this temporal
resolution. We recorded from five houses, one of which was recorded for 655
days, the longest duration we are aware of for any energy dataset at this
sample rate. We also describe the low-cost, open-source, wireless system we
built for collecting our dataset.
","['\nJack Kelly\n', '\nWilliam Knottenbelt\n']",,Scientific Data 2 (2015) Article number: 150007 (2015),http://dx.doi.org/10.1038/sdata.2015.7,cs.OH,['cs.OH'],10.1038/sdata.2015.7,,[]
Design of Reversible Counter,http://arxiv.org/abs/1404.1271v1,2014-03-29T17:12:37Z,2014-03-29T17:12:37Z,"  This article presents a research work on the design and synthesis of
sequential circuits and flip-flops that are available in digital arena; and
describes a new synthesis design of reversible counter that is optimized in
terms of quantum cost, delay and garbage outputs compared to the existing
designs. We proposed a new model of reversible T flip-flop in designing
reversible counter
","['\nMd. Selim Al Mamun\n', '\nB. K. Karmaker\n']","International Journal of Advanced Computer Science and
  Applications(IJACSA), Volume 5 Issue 1, 2014",,http://dx.doi.org/10.14569/IJACSA.2014.050117,cs.OH,['cs.OH'],10.14569/IJACSA.2014.050117,,[]
Steerable Antennas for Automotive Communication Systems,http://arxiv.org/abs/1404.1286v1,2014-04-03T11:15:38Z,2014-04-03T11:15:38Z,"  This research project undertakes a comprehensive analysis of RF beamforming
techniques for design, simulation, fabrication, and measurement of Butler
Matrix and Rotman Lens beamforming networks. It is aimed to develop novel and
well-established designs for steerable antenna systems that can be used in
vehicular telematics and automotive communication systems based on microwave
and millimeter-wave techniques.
",['\nArdavan Rahimian\n'],"MEng Dissertation, School of EECE, University of Birmingham [Rohde &
  Schwarz Technology Prize Winner]",,http://arxiv.org/abs/1404.1286v1,cs.OH,['cs.OH'],,,[]
"Energy and Latency Aware Application Mapping Algorithm & Optimization
  for Homogeneous 3D Network on Chip",http://arxiv.org/abs/1404.2512v1,2014-04-07T15:10:08Z,2014-04-07T15:10:08Z,"  Energy efficiency is one of the most critical issue in design of System on
Chip. In Network On Chip (NoC) based system, energy consumption is influenced
dramatically by mapping of Intellectual Property (IP) which affect the
performance of the system. In this paper we test the antecedently extant
proposed algorithms and introduced a new energy proficient algorithm stand for
3D NoC architecture. In addition a hybrid method has also been implemented
using bioinspired optimization (particle swarm optimization) technique. The
proposed algorithm has been implemented and evaluated on randomly generated
benchmark and real life application such as MMS, Telecom and VOPD. The
algorithm has also been tested with the E3S benchmark and has been compared
with the existing algorithm (spiral and crinkle) and has shown better reduction
in the communication energy consumption and shows improvement in the
performance of the system. Comparing our work with spiral and crinkle,
experimental result shows that the average reduction in communication energy
consumption is 19% with spiral and 17% with crinkle mapping algorithms, while
reduction in communication cost is 24% and 21% whereas reduction in latency is
of 24% and 22% with spiral and crinkle. Optimizing our work and the existing
methods using bio-inspired technique and having the comparison among them an
average energy reduction is found to be of 18% and 24%.
","['\nVaibhav Jha\n', '\nSunny Deol\n', '\nMohit Jha\n', '\nGK Sharma\n']","15 pages, 11 figure, CCSEA 2014",,http://dx.doi.org/10.5121/csit.2014.4302,cs.OH,['cs.OH'],10.5121/csit.2014.4302,,[]
"Load flow analysis of radial distribution network using linear data
  structure",http://arxiv.org/abs/1403.4702v1,2014-03-19T06:13:22Z,2014-03-19T06:13:22Z,"  Distribution systems hold a very significant position in the power system
since it is the main point of link between bulk power and consumers. A planned
and effective distribution network is the key to cope up with the ever
increasing demand for domestic, industrial and commercial load. The load-flow
study of radial distribution network is of prime importance for effective
planning of load transfers. Power companies are interested in finding the most
efficient configuration for minimization of real power loses and load balancing
among distribution feeders to save energy and enhance the over all performance
of distribution system.
  This thesis presents a fast and efficient method for load-flow analysis of
radial distribution networks. The proposed method is based on linear data
structure. The order of time and space complexity is reported here. There is
significant saving in no. of steps execution. Using graph theory concept and
exploiting multi-cores architecture, the proposed method for load flow can be
further investigated for obtaining more optimized solutions.
",['\nRitu Parasher\n'],,,http://arxiv.org/abs/1403.4702v1,cs.OH,['cs.OH'],,,[]
Mapping parcel-level urban areas for a large geographical area,http://arxiv.org/abs/1403.5864v1,2014-03-24T06:39:17Z,2014-03-24T06:39:17Z,"  As a vital indicator for measuring urban development, urban areas are
expected to be identified explicitly and conveniently with widely available
dataset thereby benefiting the planning decisions and relevant urban studies.
Existing approaches to identify urban areas normally based on mid-resolution
sensing dataset, socioeconomic information (e.g. population density) generally
associate with low-resolution in space, e.g. cells with several square
kilometers or even larger towns/wards. Yet, few of them pay attention to
defining urban areas with micro data in a fine-scaled manner with large extend
scale by incorporating the morphological and functional characteristics. This
paper investigates an automated framework to delineate urban areas in the
parcel level, using increasingly available ordnance surveys for generating all
parcels (or geo-units) and ubiquitous points of interest (POIs) for inferring
density of each parcel. A vector cellular automata model was adopted for
identifying urban parcels from all generated parcels, taking into account
density, neighborhood condition, and other spatial variables of each parcel. We
applied this approach for mapping urban areas of all 654 Chinese cities and
compared them with those interpreted from mid-resolution remote sensing images
and inferred by population density and road intersections. Our proposed
framework is proved to be more straight-forward, time-saving and fine-scaled,
compared with other existing ones, and reclaim the need for consistency,
efficiency and availability in defining urban areas with well-consideration of
omnipresent spatial and functional factors across cities.
","['\nYing Long\n', '\nYao Shen\n']","21 pages, 9 figures, 3 tables",,http://dx.doi.org/10.1080/00045608.2015.1095062,cs.OH,['cs.OH'],10.1080/00045608.2015.1095062,,[]
"Marine Buoy Location Finding and Tracking System for Linux Supporting
  Mobiles",http://arxiv.org/abs/1403.6919v1,2014-03-27T05:57:10Z,2014-03-27T05:57:10Z,"  Marine buoy is an important part of underwater acoustic communication system.
It is of great significance to track and locate it. It is widely used in ocean
environment three - dimensional monitoring, underwater multimedia
communication, underwater mobile carrier navigation and positioning, marine
resources detection, remote control of submarine topography mapping and
offshore oil industry, data acquisition, etc. This paper describes the
application of the monitoring service of GPRS / GPS module at Marine buoy. It
can achieve real - time location of underwater acoustic communication devices
and route tracking to avoid the loss of the device, as well as assist to
retrieve the lost device.
","['\nHarikrishnan. R\n', '\nShajna S. Hammed\n', '\nP. Malini\n']","3pages, 5figures","International Journal of Engineering Trends and Technology Volume
  9 Number 8 - Mar 2014",http://arxiv.org/abs/1403.6919v1,cs.OH,['cs.OH'],,,[]
"The Unambiguous Distance in a Phase-based Ranging System with Hopping
  Frequencies",http://arxiv.org/abs/1403.1923v1,2014-03-08T02:55:14Z,2014-03-08T02:55:14Z,"  It is a challenge to specify unambiguous distance (UD) in a phase-based
ranging system with hopping frequencies (PRSHF). In this letter, we propose to
characterize the UD in a PRSHF by the probability that it takes on its maximum
value. We obtain a very simple and elegant expression of the probability with
growth estimation techniques from analytic number theory. It is revealed that
the UD in a PRSHF usually takes on the maximum value with as few as 10
frequencies in measurement, almost independent of the specific distribution of
available bandwidth.
","['\nYue Zhang\n', '\nWangdong Qi\n', '\nSu Zhang\n']","6 pages, 1 figure",,http://arxiv.org/abs/1403.1923v1,cs.OH,['cs.OH'],,,[]
"ARM 7 Based Controller Area Network for Accident Avoidance in
  Automobiles",http://arxiv.org/abs/1403.3156v1,2014-03-13T03:49:31Z,2014-03-13T03:49:31Z,"  Based on requirements of modern vehicle, in- vehicle Controller Area Network
(CAN) architecture has been implemented. In order to reduce point to point
wiring harness in vehicle automation, CAN is suggested as a means for data
communication within the vehicle environment. The benefits of CAN bus based
network over traditional point to point schemes will offer increased
flexibility and expandability for future technology insertions.
  This paper describes system which uses sensors to measure various parameters
of the car like speed, distance from the other car, presence of alcohol in car
and accidental change of lane and sends a warning signal to the driver if any
of the parameter goes out of range to avoid accidents . In addition to this if
accident occurs in any remote area then using bump sensor accident is detected
and SMS is send immediately using GSM. A situation that provides a good example
of how the system works is when a driver is about to change lanes, and there is
a car in his blind spot. The sensors will detect that car and inform the driver
before he starts turning, preventing him from potentially getting into a
serious accident.
","['\nKashyap Joshi\n', '\nVipul Gohil\n']","6 pages, 6 figure , ""Published with International Journal of
  Engineering Trends and Technology (IJETT)"". http://www.ijettjournal.org.
  published by seventh sense. research group","IJETT,V9(2),61-65 March 2014. ISSN:2231-5381",http://dx.doi.org/10.14445/22315381/IJETT-V9P212,cs.OH,['cs.OH'],10.14445/22315381/IJETT-V9P212,,[]
Distributed Transformer Monitoring System Based On Zigbee Technology,http://arxiv.org/abs/1403.3547v1,2014-03-14T12:08:22Z,2014-03-14T12:08:22Z,"  A Distributed transformer networks remote monitoring system is developed and
constructed,for monitor and record the parameters like temperature, oil level
status, of a distribution transformer.The system consists of a micro controller
based circuit,with solid-state components for handling sensors,power
back-up,real time clock and data communication module which based on ZigBee
protocol.
","['\nRakesh Kumar Pandey\n', '\nDilip Kumar\n']","3 Pages, 5 Figures, International Journal of Engineering Trends and
  Technology (IJETT)-Volume4 Issue5-May (2013)",,http://arxiv.org/abs/1403.3547v1,cs.OH,['cs.OH'],,,[]
Recommendation System for Outfit Selection (RSOS),http://arxiv.org/abs/1402.6692v1,2014-02-26T12:44:11Z,2014-02-26T12:44:11Z,"  We propose a system which will be able to recommend the user to choose
appropriate outfits suits to their personality. The necessity of this system is
to reduce the outfit selection and purchasing time; this will also help to
create tailor made outfits as per the personality traits. The guidelines for
selection of their respective outfits are based upon various bodily parameters
that evolve with the learning of available labeled and unlabeled data. The
system is based on two modules of processes; first one is to recognize the
features for usage of outfits like traditional, western, functional, daytime or
night etc, second is to calculate the body measurement parameters. The proposed
system will have image capturing by using HAAR feature or input device for
getting body parameters. We intend to classify and extract the best possible
outfits from the system by using HIGEN MINER algorithm. The applications of
outfit selection will be ranging from manual gender selection, image processing
with body feature extractions, Value comparison with database by using
different statistical techniques and data mining algorithms. After that it will
recommend best outfits as per body parameters, inputs and availability
","['\nShiv H. Sutar\n', '\nAkshata H. Khade\n']","6 pages,9 figures,5 tables",,http://arxiv.org/abs/1402.6692v1,cs.OH,['cs.OH'],,,[]
Complex Beauty,http://arxiv.org/abs/1402.6712v1,2014-02-24T09:52:05Z,2014-02-24T09:52:05Z,"  Complex systems and their underlying convoluted networks are ubiquitous, all
we need is an eye for them. They pose problems of organized complexity which
cannot be approached with a reductionist method. Complexity science and its
emergent sister network science both come to grips with the inherent complexity
of complex systems with an holistic strategy. The relevance of complexity,
however, transcends the sciences. Complex systems and networks are the focal
point of a philosophical, cultural and artistic turn of our tightly
interrelated and interdependent postmodern society. Here I take a different,
aesthetic perspective on complexity. I argue that complex systems can be
beautiful and can the object of artification - the neologism refers to
processes in which something that is not regarded as art in the traditional
sense of the word is changed into art. Complex systems and networks are
powerful sources of inspiration for the generative designer, for the artful
data visualizer, as well as for the traditional artist. I finally discuss the
benefits of a cross-fertilization between science and art.
",['\nMassimo Franceschet\n'],,,http://arxiv.org/abs/1402.6712v1,cs.OH,['cs.OH'],,,[]
Three Experiments to Analyze the Nature of the Heat Spreader,http://arxiv.org/abs/1402.6903v1,2014-02-27T13:45:16Z,2014-02-27T13:45:16Z,"  In this paper, we describe ongoing work to investigate the properties of the
heat spreader, and its implication on architecture research. In specific, we
conduct two experiments to quantify the heat distribution across the surface of
a spreader during normal operation. The first experiment uses T-type
thermocouples, to find the temperature difference across different points on
the spreader. We observe about a 6 degree celsius temperature difference on
average. In the second experiment, we try to capture the temperature gradients
using an infrared camera. However, this experiment was inconclusive because of
some practical constraints such as the low emissivity of the spreader. We
conclude that to properly model the spreader, it is necessary to conduct
detailed finite element simulations. We describe a method to accurately measure
the thermal conductivity of the heat spreader such that it can be used to
compute the steady state temperature distribution across the spreader.
","['\nSeema Sethia\n', '\nShouri Chatterjee\n', '\nSunil Kale\n', '\nAmit Gupta\n', '\nSmruti R. Sarangi\n']","4 pages, 9 figures",,http://arxiv.org/abs/1402.6903v1,cs.OH,['cs.OH'],,,[]
"A Three-State Received Signal Strength Model for Device-free
  Localization",http://arxiv.org/abs/1402.7019v1,2014-02-24T18:31:56Z,2014-02-24T18:31:56Z,"  The indoor radio propagation channel is typically modeled as a two-state
time-variant process where one of the states represents the channel when the
environment is static, whereas the other state characterizes the medium when it
is altered by people. In this paper, the aforementioned process is augmented
with an additional state. It is shown that the changes in received signal
strength are dictated by: i) electronic noise, when a person is not present in
the monitored area; ii) reflection, when a person is moving in the close
vicinity of the line-of-sight; iii) shadowing, when a person is obstructing the
line-of-sight component of the transmitter-receiver pair. Statistical and
spatial models for the three states are derived and the models are empirically
validated. Based on the models, a simplistic device-free localization
application is designed which aims to: first, estimate the temporal state of
the channel using a hidden Markov model; second, track a person using a
particle filter. The results suggest that the tracking accuracy is enhanced by
at least 65% while the link's sensitivity region is increased by 100% or more
with respect to empirical models presented in earlier works.
","['\nOssi Kaltiokallio\n', '\nHüseyin Yiğitler\n', '\nRiku Jäntti\n']",,,http://arxiv.org/abs/1402.7019v1,cs.OH,['cs.OH'],,,[]
"On the Behavioral Interpretation of System-Environment Fit and
  Auto-Resilience",http://arxiv.org/abs/1403.0339v1,2014-03-03T08:42:24Z,2014-03-03T08:42:24Z,"  Already 71 years ago Rosenblueth, Wiener, and Bigelow introduced the concept
of the ""behavioristic study of natural events"" and proposed a classification of
systems according to the quality of the behaviors they are able to exercise. In
this paper we consider the problem of the resilience of a system when deployed
in a changing environment, which we tackle by considering the behaviors both
the system organs and the environment mutually exercise. We then introduce a
partial order and a metric space for those behaviors, and we use them to define
a behavioral interpretation of the concept of system-environment fit. Moreover
we suggest that behaviors based on the extrapolation of future environmental
requirements would allow systems to proactively improve their own
system-environment fit and optimally evolve their resilience. Finally we
describe how we plan to express a complex optimization strategy in terms of the
concepts introduced in this paper.
",['\nVincenzo De Florio\n'],"Draft submitted for publication in the Proceedings of the IEEE 2014
  Conference on Norbert Wiener in the 21st Century",,http://dx.doi.org/10.1109/NORBERT.2014.6893945,cs.OH,['cs.OH'],10.1109/NORBERT.2014.6893945,,[]
"Frequency-Shift Filtering for OFDM Signal Recovery in Narrowband Power
  Line Communications",http://arxiv.org/abs/1403.1061v1,2014-03-05T10:02:43Z,2014-03-05T10:02:43Z,"  Power line communications (PLC) has been drawing considerable interest in
recent years due to the growing interest in smart grid implementation.
Specifically, network control and grid applications are allocated the frequency
band of 0-500 kHz, commonly referred to as the narrowband PLC channel. This
frequency band is characterized by strong periodic noise which results in low
signal to noise ratio (SNR). In this work we propose a receiver which uses
frequency shift filtering to exploit the cyclostationary properties of both the
narrowband power line noise, as well as the information signal, digitally
modulated using orthogonal frequency division multiplexing. An adaptive
implementation for the proposed receiver is presented as well. The proposed
receiver is compared to existing receivers via analysis and simulation. The
results show that the receiver proposed in this work obtains a substantial
performance gain over previously proposed receivers, without requiring any
coordination with the transmitter.
","['\nNir Shlezinger\n', '\nRon Dabora\n']",Accepted to the ieee transactions on communications,,http://arxiv.org/abs/1403.1061v1,cs.OH,['cs.OH'],,,[]
Adaptive Minimum-Maximum Exclusive Mean Filter for Impulse Noise Removal,http://arxiv.org/abs/1405.6174v2,2014-02-26T08:51:52Z,2014-11-02T01:10:46Z,"  Many filters are proposed for impulse noise removal. However, they are hard
to keep excellent denoising performance with high computational efficiency. In
response to this difficulty, this paper presents a novel fast filter, adaptive
minimum-maximum exclusive mean (AMMEM) filter to remove impulse noise. Although
the AMMEM filter is a variety of the maximum-minimum exclusive mean (MMEM)
filter, however, the AMMEM filter inherits the advantages, and overcomes the
drawbacks, compared with the MMEM filter. To increase the various performances
of noise removal, the AMMEM filter uses an adaptive size window, introduces two
flexible factors, projection factor P and detection factor T, and limits the
calculation scope of the AVG. The experimental results show the AMMEM filter
makes a significant improvement in terms of noise detection, image restoration,
and computational efficiency. Even at noise level as high as 95%, the AMMEM
filter still can restore the images with good visual effect.
","['\nShuliang Wang\n', '\nZhe Zhou\n', '\nWenzhong Shi\n']","This paper has been withdrawn by the author due to a crucial error in
  experiment",,http://arxiv.org/abs/1405.6174v2,cs.OH,['cs.OH'],,,[]
Evaluating ECG Capturing Using Sound-Card of PC/Laptop,http://arxiv.org/abs/1402.3651v1,2014-02-15T05:25:56Z,2014-02-15T05:25:56Z,"  The purpose of the Evaluating ECG capturing using sound-card of PC/Laptop is
provided portable and low cost ECG monitoring system using laptop and mobile
phones. There is no need to interface micro controller or any other device to
transmit ECG data. This research is based on hardware design, implementation,
signal capturing and Evaluation of an ECG processing and analyzing system which
attend the physicians in heart disease diagnosis. Some important modification
is given in design part to avoid all definitive ECG instrument problems faced
in previous designs. Moreover, attenuate power frequency noise and noise that
produces from patient's body have required additional developments. The
hardware design has basically three units: transduction and conditioning Unit,
interfacing unit and data processing unit.The most focusing factor is the ECG
signal/data transmits in laptop/PC via microphone pin. The live simulation is
possible using SOUNDSCOPE software in PC/Laptop. The software program that is
written in MATLAB and LAB-View performs data acquisition (record, stored,
filtration) and several tasks such as QRS detection, calculate heart rate.
","['\nBhavikkumar Patel\n', '\nDhrumil Shah\n']",22 Pages,,http://arxiv.org/abs/1402.3651v1,cs.OH,['cs.OH'],,,[]
QTC3D: Extending the Qualitative Trajectory Calculus to Three Dimensions,http://arxiv.org/abs/1402.3779v1,2014-02-16T10:05:25Z,2014-02-16T10:05:25Z,"  Spatial interactions between agents (humans, animals, or machines) carry
information of high value to human or electronic observers. However, not all
the information contained in a pair of continuous trajectories is important and
thus the need for qualitative descriptions of interaction trajectories arises.
The Qualitative Trajectory Calculus (QTC) (Van de Weghe, 2004) is a promising
development towards this goal. Numerous variants of QTC have been proposed in
the past and QTC has been applied towards analyzing various interaction
domains. However, an inherent limitation of those QTC variations that deal with
lateral movements is that they are limited to two-dimensional motion;
therefore, complex three-dimensional interactions, such as those occurring
between flying planes or birds, cannot be captured. Towards that purpose, in
this paper QTC3Dis presented: a novel qualitative trajectory calculus that can
deal with full three-dimensional interactions. QTC3D is based on
transformations of the Frenet-Serret frames accompanying the trajectories of
the moving objects. Apart from the theoretical exposition, including definition
and properties, as well as computational aspects, we also present an
application of QTC3D towards modeling bird flight. Thus, the power of QTC is
now extended to the full dimensionality of physical space, enabling succinct
yet rich representations of spatial interactions between agents.
","['\nNikolaos Mavridis\n', '\nNicola Bellotto\n', '\nKonstantinos Iliopoulos\n', '\nNico Van de Weghe\n']",,,http://arxiv.org/abs/1402.3779v1,cs.OH,['cs.OH'],,,[]
Planar Shielded-Loop Resonators,http://arxiv.org/abs/1402.1219v1,2014-02-06T00:50:00Z,2014-02-06T00:50:00Z,"  The design and analysis of planar shielded-loop resonators for use in
wireless non-radiative power transfer systems is presented. The difficulties
associated with coaxial shielded-loop resonators for wireless power transfer
are discussed and planar alternatives are proposed. The currents along these
planar structures are analyzed and first-order design equations are presented
in the form of a circuit model. In addition, the planar structures are
simulated and fabricated. Planar shielded-loop resonators are compact and
simple to fabricate. Moreover, they are well-suited for printed circuit board
designs or integrated circuits
","['\nBrian B. Tierney\n', '\nAnthony Grbic\n']",,,http://dx.doi.org/10.1109/TAP.2014.2314305,cs.OH,['cs.OH'],10.1109/TAP.2014.2314305,,[]
Deployment of an Innovative Resource Choice Method for Process Planning,http://arxiv.org/abs/1402.1438v1,2014-02-05T14:34:31Z,2014-02-05T14:34:31Z,"  Designers, process planners and manufacturers naturally consider different
concepts for a same object. The stiffness of production means and the design
specification requirements mark out process planners as responsible of the
coherent integration of all constraints. First, this paper details an
innovative solution of resource choice, applied for aircraft manufacturing
parts. In a second part, key concepts are instanced for the considered
industrial domain. Finally, a digital mock up validates the solution viability
and demonstrates the possibility of an in-process knowledge capitalisation and
use. Formalising the link between Design and Manufacturing allows to hope
enhancements of simultaneous Product / Process developments.
","['\nAlexandre Candlot\nIRCCyN\n', '\nNicolas Perry\nLGM2B\n', '\nAlain Bernard\nIRCCyN\n', '\nSamar Ammar-Khodja\nIRCCyN\n']",,"CIRP Journal of Manufacturing Systems 35, 5 (2006) 487-506",http://arxiv.org/abs/1402.1438v1,cs.OH,['cs.OH'],,,"['IRCCyN', 'LGM2B', 'IRCCyN', 'IRCCyN']"
Real Time Industrial Monitoring System,http://arxiv.org/abs/1402.1693v1,2014-02-07T16:56:55Z,2014-02-07T16:56:55Z,"  Industries are the biggest workplace all over the world, also there are large
number of peoples involves as a worker and most of them are work as a machine
operator. There are many systems developed for industrial work place, some of
them, monitors machine processes and some do monitoring and control of machine
parameters. Such as speed, temperature, production batch count etc. However
there is no such system available that provides monitoring of operator during
their work is in progress at workplace. This paper proposes the monitoring of
the operators and the machines, by Real time Operator -Machine Allocation and
monitoring system (Omams). Omams allocates a work machine to worker at entry
point itself. It uses automation with RFID and one of the standards of wireless
communication method. The system can be industry specific. Through this
research paper our approach is to make fair allocation of machine to the
operator in industry and reduce hassle for efficiency calculations.
","['\nRahul D. Chavhan\n', '\nSachin U. Chavhan\n', '\nGanesh B. Chavan\n']",5 pages 7 figures,,http://arxiv.org/abs/1402.1693v1,cs.OH,['cs.OH'],,,[]
Tasks for Temporal Graph Visualisation,http://arxiv.org/abs/1402.2867v1,2014-02-10T09:54:05Z,2014-02-10T09:54:05Z,"  In [1], we describe the design and development of a task taxonomy for
temporal graph visualisation. This paper details the full instantiation of that
task taxonomy. Our task taxonomy is based on the Andrienko framework [2], which
uses a systematic approach to develop a formal task framework for visual tasks
specifically associated with Exploratory Data Analysis. The Andrienko framework
is intended to be applicable to all types of data, however, it does not
consider relational (graph) data. We therefore extended both their data model
and task framework for temporal graph data, and instantiated the extended
version to produce a comprehensive list of tasks of interest during exploratory
analysis of temporal graph data. As expected, our instantiation of the
framework resulted in a very large task list; with more than 144 variations of
attribute based tasks alone, it is too large to fit in a standard journal
paper, hence we provide the detailed listing in this document.
","['\nNatalie Kerracher\n', '\nJessie Kennedy\n', '\nKevin Chalmers\n']",42 pages,,http://arxiv.org/abs/1402.2867v1,cs.OH,['cs.OH'],,,[]
"Invisibility System Using Image Processing and Optical Camouflage
  Technology",http://arxiv.org/abs/1404.4107v1,2014-02-08T12:31:45Z,2014-02-08T12:31:45Z,"  Invisible persons are seen in fiction stories only, but in the real world it
is proved that invisibility is possible. This paper describes the creation of
invisibility with the help of technologies like Optical camouflage; Image based
rendering and Retro reflective projection. The object that needs to be made
transparent or invisible is painted or covered with retro reflective material.
Then a projector projects the background image on it making the masking object
virtually transparent. Capturing the background image requires a video camera,
which sits behind the person wearing the cloak. The video from the camera must
be in a digital format so it can be sent to a computer for image processing
using image based rendering technical. There are some useful applications for
this simple but astonishing technology.
","['\nVasireddy Srikanth\n', '\nPillem Ramesh\n']","IJETT, 2013",,http://arxiv.org/abs/1404.4107v1,cs.OH,['cs.OH'],,,[]
GreenMail: Reducing Email Service's Carbon Emission with Minimum Cost,http://arxiv.org/abs/1401.5546v1,2014-01-22T03:24:12Z,2014-01-22T03:24:12Z,"  Internet services contribute a large fraction of worldwide carbon emission
nowadays, in a context of increasing number of companies tending to provide and
more and more developers use Internet services. Noticeably, a trend is those
service providers are trying to reduce their carbon emissions by utilizing
on-site or off-site renewable energy in their datacenters in order to attract
more customers. With such efforts have been paid, there are still some users
who are aggressively calling for even cleaner Internet services. For example,
over 500,000 Facebook users petitioned the social networking site to use
renewable energy to power its datacenter. However, it seems impossible for such
demand to be satisfied merely from the inside of those production datacenters,
considering the transition cost and stability. Outside the existing Internet
services, on the other hand, may easily set up a proxy service to attract those
renewable-energy-sensitive users, by 1) using carbon neutral or even
over-offsetting cloud instances to bridge the end user and traditional Internet
services; and 2) estimating and offsetting the carbon emissions from the
traditional Internet services. In our paper, we proposed GreenMail, which is a
general IMAP proxy caching system that connects email users and traditional
email services. GreenMail runs on green web hosts to cache users' emails on
green cloud instances. Besides, it offsets the carbon emitted by traditional
backend email services. With GreenMail, users could set a carbon emission
constraint and use traditional email service without breaking any code
modification of user side and email server side.
",['\nChen Li\n'],Master's Thesis,,http://arxiv.org/abs/1401.5546v1,cs.OH,['cs.OH'],,,[]
"Specification of the State Lifetime in the DEVS Formalism by Fuzzy
  Controller",http://arxiv.org/abs/1401.5638v1,2014-01-22T12:00:23Z,2014-01-22T12:00:23Z,"  This paper aims to develop a new approach to assess the duration of state in
the DEVS formalism by fuzzy controller. The idea is to define a set of fuzzy
rules obtained from observers or expert knowledge and to specify a fuzzy model
which computes this duration, this latter is fed into the simulator to specify
the new value in the model. In conventional model, each state is defined by a
mean lifetime value whereas our method, calculates for each state the new
lifetime according to inputs values. A wildfire case study is presented at the
end of the paper. It is a challenging task due to its complex behavior,
dynamical weather condition, and various variables involved. A global
specification of the fuzzy controller and the forest fire model are presented
in the DEVS formalism and comparison between conventional and fuzzy method is
illustrated.
","['\nDahmani Youcef\n', '\nHamri Maamar\n']","IJAIT International Journal of Advanced Information Technology
  (IJAIT) Vol. 3, No.2, April 2013",,http://dx.doi.org/10.5121/ijait.2013.3201,cs.OH,['cs.OH'],10.5121/ijait.2013.3201,,[]
A Software Design through Electrical System for a Building,http://arxiv.org/abs/1401.7148v1,2014-01-28T11:55:00Z,2014-01-28T11:55:00Z,"  Computer aided design of lighting systems made new installations of lighting
dimensioning and verification of existing lighting systems for both indoor and
outdoor lighting systems.The design of the building light system was in a
dedicated software, named DiaLux, version 4.11.
","['\nAnghel Drugarin\n', '\nCornelia Victoria\n']","Politehnica Timisoara Press, nov.2013",,http://arxiv.org/abs/1401.7148v1,cs.OH,['cs.OH'],,,[]
Modeling Life as Cognitive Info-Computation,http://arxiv.org/abs/1401.7191v1,2014-01-28T14:14:18Z,2014-01-28T14:14:18Z,"  This article presents a naturalist approach to cognition understood as a
network of info-computational, autopoietic processes in living systems. It
provides a conceptual framework for the unified view of cognition as evolved
from the simplest to the most complex organisms, based on new empirical and
theoretical results. It addresses three fundamental questions: what cognition
is, how cognition works and what cognition does at different levels of
complexity of living organisms. By explicating the info-computational character
of cognition, its evolution, agent-dependency and generative mechanisms we can
better understand its life-sustaining and life-propagating role. The
info-computational approach contributes to rethinking cognition as a process of
natural computation in living beings that can be applied for cognitive
computation in artificial systems.
",['\nGordana Dodig-Crnkovic\n'],Manuscript submitted to Computability in Europe CiE 2014,,http://arxiv.org/abs/1401.7191v1,cs.OH,['cs.OH'],,,[]
ICT technologies for the refurbishment of wooden structure buildings,http://arxiv.org/abs/1401.8136v1,2014-01-31T11:27:10Z,2014-01-31T11:27:10Z,"  Nowadays, one would think that after years of massive concrete and steel
construction in Spain, there are not many wood structure buildings left to be
refurbished except for some palaces or cathedrals. However, if we go for a walk
and have a look at the old part of any city, we will realize that still most of
the buildings have a wood structure. In spite of the fact that the majority of
urban regulations forbid their demolition, other bad practices such as casting
and overloading the wood structure are very common. Considering that we want to
reach a standard of sustainable construction, the economical and environmental
costs, which are implied by the deficient refurbishment makes it well worth a
previous study of the structure, which in most cases represents less than a 1%
of the total budget. The main goal of this paper is to present most relevant
parts of the whole process of diagnosis of a wood structure building by means
of Non-Destructive Testing Techniques. Among the ones to be considered, we
could mention the analysis of the building and its surroundings, on-site
inspection of the building, structural diagnosis, definition of the corrective
actions to be taken, definition of treatments, quality control and a
maintenance plan. For the on-site inspection of the building, in the paper we
will highlight the use of Non-Destructive methods such as resistograph
drilling, X-ray imaging, ultrasound-based testing or moisture measurement. We
will provide practical examples of all this. The aim of this paper is to give
the audience an overall idea on how a pre-assessment work can enhance the
refurbishment of a wood structure building while reducing costs and
environmental impact.
","['\nIvan Arakistain\n', '\nJose Miguel Abascal\n', '\nOriol Munne\n']",,,http://arxiv.org/abs/1401.8136v1,cs.OH,['cs.OH'],,,[]
Introducing E-maintenance 2.0,http://arxiv.org/abs/1401.8252v1,2014-01-31T18:38:56Z,2014-01-31T18:38:56Z,"  While research literature is still debating e-maintenance definition, a new
reality is emerging in business world confirming the enterprise 2.0 model.
Executives are more and more forced to stop running against current trend
towards social media and instead envisage harnessing its power within the
enterprise. Maintenance can't be an exception for long and has to take
advantage of new opportunities created by social technological innovations. In
this paper, a combination of pure E perspective and 2.0 perspective is proposed
to avoid a lock-in and allow continous evolution of e-maintenance within the
new context of business: A combination of data centric models and people
oriented applications to form a collaborative environment in order to conceive
and achieve global goals of maintenance.New challenges are also to be expected
as to the effecient integration of enterprise 2.0 tools within current
e-maintenance platforms and futher research work is still to be done in this
area.
","['\nAbdessamad Mouzoune\n', '\nSaoudi Taibi\n']","11 pages, 1 figure","International Journal of Computer Science and Business
  Informatics, Vol. 9, No. 1, pp. 80-90",http://arxiv.org/abs/1401.8252v1,cs.OH,['cs.OH'],,,[]
"Pi Fractions for Generating Uniformly Distributed Sampling Points in
  Global Search and Optimization Algorithms",http://arxiv.org/abs/1401.3038v2,2014-01-14T00:19:26Z,2014-03-17T14:43:04Z,"  Pi Fractions are used to create deterministic uniformly distributed
pseudorandom decision space sample points for a global search and optimization
algorithm. These fractions appear to be uniformly distributed on [0,1] and can
be used in any stochastic algorithm rendering it effectively deterministic
without compromising its ability to explore the decision space. Pi Fractions
are generated using the BBP Pi digit extraction algorithm. The Pi Fraction
approach is tested using genetic algorithm Pi-GASR with very good results. A Pi
Fraction data file is available upon request.
",['\nRichard A. Formato\n'],Discussion of bidimensional correlation has been added,,http://arxiv.org/abs/1401.3038v2,cs.OH,['cs.OH'],,,[]
"Comprehensive Analysis and Measurement of Frequency-Tuned and
  Impedance-Tuned Wireless Non-Radiative Power Transfer Systems",http://arxiv.org/abs/1401.3324v1,2014-01-14T20:37:27Z,2014-01-14T20:37:27Z,"  This paper theoretically and experimentally investigates frequency-tuned and
impedance-tuned wireless non-radiative power transfer (WNPT) systems.
Closed-form expressions for the efficiencies of both systems, as a function of
frequency and system (circuit) parameters, are presented. In the
frequency-tuned system, the operating frequency is adjusted to compensate for
changes in mutual inductance that occur for variations of transmitter and
receiver loop positions. Frequency-tuning is employed for a range of distances
over which the loops are strongly coupled. In contrast, the impedance-tuned
system employs varactor-based matching networks to compensate for changes in
mutual inductance and achieve a simultaneous conjugate impedance match over a
range of distances. The frequency-tuned system is simpler to implement, while
the impedance-tuned system is more complex but can achieve higher efficiencies.
Both of the experimental WNPT systems studied employ resonant shielded loops as
transmitting and receiving devices.
","['\nJason D. Heebl\n', '\nErin M. Thomas\n', '\nRobert P. Penno\n', '\nAnthony Grbic\n']","11 pages, 16 figures. This article has been accepted into IEEE
  Antennas and Propagation Magazine",,http://dx.doi.org/10.1109/MAP.2014.6971924,cs.OH,['cs.OH'],10.1109/MAP.2014.6971924,,[]
"Advanced Self-interference Cancellation and Multiantenna Techniques for
  Full-Duplex Radios",http://arxiv.org/abs/1401.3331v1,2014-01-14T12:41:22Z,2014-01-14T12:41:22Z,"  In an in-band full-duplex system, radios transmit and receive simultaneously
in the same frequency band at the same time, providing a radical improvement in
spectral efficiency over a half-duplex system. However, in order to design such
a system, it is necessary to mitigate the self-interference due to simultaneous
transmission and reception, which seriously limits the maximum transmit power
of the full-duplex device. Especially, large differences in power levels in the
receiver front-end sets stringent requirements for the linearity of the
transceiver electronics. We present an advanced architecture for a compact
full-duplex multiantenna transceiver combining antenna design with analog and
digital cancellation, including both linear and nonlinear signal processing.
","['\nDani Korpi\n', '\nSathya Venkatasubramanian\n', '\nTaneli Riihonen\n', '\nLauri Anttila\n', '\nStrasdosky Otewa\n', '\nClemens Icheln\n', '\nKatsuyuki Haneda\n', '\nSergei Tretyakov\n', '\nMikko Valkama\n', '\nRisto Wichman\n']","Presented in 47th Annual Asilomar Conference on Signals, Systems, and
  Computers, 2013",,http://arxiv.org/abs/1401.3331v1,cs.OH,['cs.OH'],,,[]
"Accelerating SystemVerilog UVM Based VIP to Improve Methodology for
  Verification of Image Signal Processing Designs Using HW Emulator",http://arxiv.org/abs/1401.3554v1,2014-01-15T11:58:56Z,2014-01-15T11:58:56Z,"  In this paper we present the development of Acceleratable UVCs from standard
UVCs in SystemVerilog and their usage in UVM based Verification Environment of
Image Signal Processing designs to increase run time performance. This paper
covers development of Acceleratable UVCs from standard UVCs for internal
control and data buses of ST imaging group by partitioning of transaction-level
components and cycle-accurate signal-level components between the software
simulator and hardware accelerator respectively. Standard Co-Emulation API:
Modeling Interface (SCE-MI) compliant, transaction-level communications link
between test benches running on a host system and Emulation machine is
established. Accelerated Verification IPs are used at UVM based Verification
Environment of Image Signal Processing designs both with simulator and emulator
as UVM acceleration is an extension of the standard simulation-only UVM and is
fully backward compatible with it. Acceleratable UVCs significantly reduces
development schedule risks while leveraging transaction models used during
simulation.
  In this paper, we discuss our experiences on UVM based methodology adoption
on TestBench-Xpress(TBX) based technology step by step. We are also doing
comparison between the run time performance results from earlier simulator-only
environment and the new, hardware-accelerated environment. Although this paper
focuses on Acceleratable UVCs development and their usage for image signal
processing designs, Same concept can be extended for non-image signal
processing designs.
  KEYWORDS- SystemVerilog, Universal Verification Methodology (UVM),
TestBench-Xpress (TBX), Universal Verification Component (UVC), Standard
Co-Emulation API: Modelling Interface (SCE-MI), Acceleratable UVC, Emulator,
XRTL Tasks/Functions (xtf), Transactor interface (tif), Verification IP (VIP).
","['\nAbhishek Jain\n', '\nPiyush Kumar Gupta\n', '\nDr. Hima Gupta\n', '\nSachish Dhar\n']",International Journal of VLSI design & Communication Systems (VLSICS),,http://arxiv.org/abs/1401.3554v1,cs.OH,['cs.OH'],,,[]
"Transport Information System using Query Centric Cyber Physical Systems
  (QCPS)",http://arxiv.org/abs/1401.3623v1,2014-01-15T15:19:42Z,2014-01-15T15:19:42Z,"  To incorporate the computation and communication with the physical world,
next generation architecture i.e. CPS is viewed as a new technology. To improve
the better interaction with the physical world or to perk up the electricity
delivery usage, various CPS based approaches have been introduced. Recently
several GPS equipped smart phones and sensor based frameworks have been
proposed which provide various services i.e. environment estimation, road
safety improvement but encounter certain limitations like elevated energy
consumption and high computation cost. To meet the high reliability and safety
requirements, this paper introduces a novel approach based on QCPS model which
provides several users services (discussed in this paper). Further, this paper
proposed a Transport Information System (TIS), which provide the communication
with lower cost overhead by arranging the similar sensors in the form of grids.
Each grid has a coordinator which interacts with cloud to process the user
query. In order to evaluate the performance of proposed approach we have
implemented a test bed of 16 wireless sensor nodes and have shown the
performance in terms of computation and communication cost.
","['\nAnkit Mundra\n', '\nGeetanjali Rathee\n', '\nMeenu Chawla\n', '\nNitin Rakesh\n', '\nAshsutosh Soni\n']","5 pages, 4 Figures","International Journal of Computer Applications 85(3):12-16,
  January 2014. Published by Foundation of Computer Science, New York, USA",http://dx.doi.org/10.5120/14820-3050,cs.OH,['cs.OH'],10.5120/14820-3050,,[]
"The Energy/Frequency Convexity Rule: Modeling and Experimental
  Validation on Mobile Devices",http://arxiv.org/abs/1401.4655v1,2014-01-19T11:16:00Z,2014-01-19T11:16:00Z,"  This paper provides both theoretical and experimental evidence for the
existence of an Energy/Frequency Convexity Rule, which relates energy
consumption and CPU frequency on mobile devices. We monitored a typical
smartphone running a specific computing-intensive kernel of multiple nested
loops written in C using a high-resolution power gauge. Data gathered during a
week-long acquisition campaign suggest that energy consumed per input element
is strongly correlated with CPU frequency, and, more interestingly, the curve
exhibits a clear minimum over a 0.2 GHz to 1.6 GHz window. We provide and
motivate an analytical model for this behavior, which fits well with the data.
Our work should be of clear interest to researchers focusing on energy usage
and minimization for mobile devices, and provide new insights for optimization
opportunities.
","['\nKarel De Vogeleer\n', '\nGerard Memmi\n', '\nPierre Jouvelot\n', '\nFabien Coelho\n']",,,http://arxiv.org/abs/1401.4655v1,cs.OH,['cs.OH'],,,[]
"Antifragility = Elasticity + Resilience + Machine Learning: Models and
  Algorithms for Open System Fidelity",http://arxiv.org/abs/1401.4862v1,2014-01-20T11:19:32Z,2014-01-20T11:19:32Z,"  We introduce a model of the fidelity of open systems - fidelity being
interpreted here as the compliance between corresponding figures of interest in
two separate but communicating domains. A special case of fidelity is given by
real-timeliness and synchrony, in which the figure of interest is the physical
and the system's notion of time. Our model covers two orthogonal aspects of
fidelity, the first one focusing on a system's steady state and the second one
capturing that system's dynamic and behavioural characteristics. We discuss how
the two aspects correspond respectively to elasticity and resilience and we
highlight each aspect's qualities and limitations. Finally we sketch the
elements of a new model coupling both of the first model's aspects and
complementing them with machine learning. Finally, a conjecture is put forward
that the new model may represent a first step towards compositional criteria
for antifragile systems.
",['\nVincenzo De Florio\n'],"Preliminary version submitted to the 1st International Workshop ""From
  Dependable to Resilient, from Resilient to Antifragile Ambients and Systems""
  (ANTIFRAGILE 2014), https://sites.google.com/site/resilience2antifragile/",,http://arxiv.org/abs/1401.4862v1,cs.OH,['cs.OH'],,,[]
"Design and implementation of the Customer Experience Data Mart in the
  Telecommunication Industry: Application Order-To-Payment end to end process",http://arxiv.org/abs/1401.0534v1,2013-12-31T14:33:40Z,2013-12-31T14:33:40Z,"  Facing the new market challenges, service providers are looking for solutions
to improve three major business areas namely the Customer Experience, The
Operational Efficiency and Revenue and Margin. To meet the business requiements
related to these areas, service providers are going through three major
transformation programs namely the Business Support Systems transformation
program for Customer related aspects, the Operations Support System
transformation program for mainly service Fulfillment and Assurance and
Resource, Fulfillment and Assurance, and Time To Market Transformation program
for Products ans Services development and management. These transformations are
about making a transition from a current situation with all its views to a
desired one. The information view transformation is about reorganizing and
reengineering the existing information to be used for the day to day activities
and reporting to support decision making. For reporting purpose, service
providers have to invest in Business Intelligence solutions. For which the main
purpose is to provide the right information in a timely manner to efficiently
support the decision making. One of the key BI challenges is to model an
information structure where to host all the information coming from multiple
sources. The purpose of this paper is to suggest a step by step methodology to
design a Telco Data Mart, one of the fundamental BI components.
Order-To-Payment, an end to end customer process, will be used as an
application for this methodology. Our methodology consists on bringing together
the concepts of business intelligence and the telecom business frameworks
developed by the TM Forum: the Business Process Framework, the Information
Framework, and the Business Metrics. The advantage of this solution is its
ability to adapt to any telecom enterprise architecture since it's built around
the business standards.
","['\nMounire Benhima\n', '\nJohn P. Reilly\n', '\nZaineb Naamane\n', '\nMeriam Kharbat\n', '\nMohammed Issam Kabbaj\n', '\nOussama Esqalli\n']","25 pages, 24 figures, 17 tables, IJCSI","IJCSI Journal, Volume 10, Issue 3, May 2013",http://arxiv.org/abs/1401.0534v1,cs.OH,['cs.OH'],,,[]
Smart Grid Demand Monitoring Model,http://arxiv.org/abs/1401.1451v1,2014-01-07T17:30:29Z,2014-01-07T17:30:29Z,"  This paper is in related to the demand genrated by the consumer for a time
for the power which is being viewed by taking some measures to solve the demand
need.
","['\nKalpana Kandpal\n', '\nAnjali Singhal\n']","3 pages, 1 figure, International Journal",,http://arxiv.org/abs/1401.1451v1,cs.OH,['cs.OH'],,,[]
Advanced Data Processing in the Business Network System,http://arxiv.org/abs/1312.7436v1,2013-12-28T14:16:58Z,2013-12-28T14:16:58Z,"  The discovery, representation and reconstruction of Business Networks (BN)
from Network Mining (NM) raw data is a difficult problem for enterprises. This
is due to huge amounts of e.g. complex business processes within and across
enterprise boundaries, heterogeneous technology stacks, and fragmented data. To
remain competitive, visibility into the enterprise and partner networks on
different, interrelated abstraction levels is desirable.
  We show the query and data processing capabilities of a novel data discovery,
mining and network inference system, called Business Network System (BNS) that
reconstructs the BN--integration and business process networks - from raw data,
hidden in the enterprises' landscapes. The paper covers both the foundation and
the key data processing characteristics features of BNS, including its
underlying technologies, its overall system architecture, and data provenance
approach.
",['\nDaniel Ritter\n'],"5 pages, 2nd International Conference on Knowledge Discovery (ICKD),
  Copenhagen, 2013. Proceedings in the International Journal of Machine
  Learning and Computing (IJMLC)","International Journal of Machine Learning and Computing, Volume 3,
  Number 2, 2013",http://dx.doi.org/10.7763/IJMLC,cs.OH,['cs.OH'],10.7763/IJMLC,,[]
"A Novel Carrier Waveform Inter-Displacement Modulation Method in
  Underwater Communication Channel",http://arxiv.org/abs/1312.7441v1,2013-12-28T15:17:35Z,2013-12-28T15:17:35Z,"  As the main way of underwater wireless communication, underwater acoustic
communication is one of the focuses of ocean research. Compared with the free
space wireless communication channel, the underwater acoustic channel suffers
from more severe multipath effect, the less available bandwidth and the even
complex noise. The underwater acoustic channel is one of the most complicated
wireless communication channels. To achieve a reliable underwater acoustic
communication, Phase Shift Keying (PSK) modulation and Passive Time Reversal
Mirror (PTRM) equalization are considered to be a suitable scheme. However, due
to the serious distortion of the received signal caused by the channel, this
scheme suffers from a high Bit Error Rate (BER) under the condition of the low
Signal to Noise Ratio (SNR). To solve this problem, we proposes a Carrier
Waveform Inter-Displacement (CWID) modulation method based on the Linear
Frequency Modulation (LFM) PSK and PTRM scheme. The new communication scheme
reduces BER by increasing the difference from the carrier waveform for
different symbols. Simulation results show the effectiveness and superiority of
the proposed method.
","['\nHai-Peng Ren\n', '\nYang Zhao\n']","8 pages, 11 figures","Telecommunication systems, 2022",http://dx.doi.org/10.1007/s11235-022-00902-5,cs.OH,['cs.OH'],10.1007/s11235-022-00902-5,,[]
"Beyond Reuse Distance Analysis: Dynamic Analysis for Characterization of
  Data Locality Potential",http://arxiv.org/abs/1401.5024v1,2013-12-21T08:06:18Z,2013-12-21T08:06:18Z,"  Emerging computer architectures will feature drastically decreased flops/byte
(ratio of peak processing rate to memory bandwidth) as highlighted by recent
studies on Exascale architectural trends. Further, flops are getting cheaper
while the energy cost of data movement is increasingly dominant. The
understanding and characterization of data locality properties of computations
is critical in order to guide efforts to enhance data locality. Reuse distance
analysis of memory address traces is a valuable tool to perform data locality
characterization of programs. A single reuse distance analysis can be used to
estimate the number of cache misses in a fully associative LRU cache of any
size, thereby providing estimates on the minimum bandwidth requirements at
different levels of the memory hierarchy to avoid being bandwidth bound.
However, such an analysis only holds for the particular execution order that
produced the trace. It cannot estimate potential improvement in data locality
through dependence preserving transformations that change the execution
schedule of the operations in the computation. In this article, we develop a
novel dynamic analysis approach to characterize the inherent locality
properties of a computation and thereby assess the potential for data locality
enhancement via dependence preserving transformations. The execution trace of a
code is analyzed to extract a computational directed acyclic graph (CDAG) of
the data dependences. The CDAG is then partitioned into convex subsets, and the
convex partitioning is used to reorder the operations in the execution trace to
enhance data locality. The approach enables us to go beyond reuse distance
analysis of a single specific order of execution of the operations of a
computation in characterization of its data locality properties. It can serve a
valuable role in identifying promising code regions for manual transformation,
as well as assessing the effectiveness of compiler transformations for data
locality enhancement. We demonstrate the effectiveness of the approach using a
number of benchmarks, including case studies where the potential shown by the
analysis is exploited to achieve lower data movement costs and better
performance.
","['\nNaznin Fauzia\nOSU\n', '\nVenmugil Elango\nOSU\n', '\nMahesh Ravishankar\nOSU\n', '\nJ. Ramanujam\nECE\n', '\nFabrice Rastello\nLIP\n', '\nAtanas Rountev\nOSU\n', '\nLouis-Noël Pouchet\nUCLA-CS\n', '\nP. Sadayappan\nCSE\n']",Transaction on Architecture and Code Optimization (2014),,http://arxiv.org/abs/1401.5024v1,cs.OH,['cs.OH'],,,"['OSU', 'OSU', 'OSU', 'ECE', 'LIP', 'OSU', 'UCLA-CS', 'CSE']"
A New Variable Step-size Zero-point Attracting Projection Algorithm,http://arxiv.org/abs/1312.2612v1,2013-12-09T21:56:07Z,2013-12-09T21:56:07Z,"  This paper proposes a new variable step-size (VSS) scheme for the recently
introduced zero-point attracting projection (ZAP) algorithm. The proposed
variable step-size ZAPs are based on the gradient of the estimated filter
coefficients sparseness that is approximated by the difference between the
sparseness measure of current filter coefficients and an averaged sparseness
measure. Simulation results demonstrate that the proposed approach provides
both faster convergence rate and better tracking ability than previous ones.
","['\nJianming Liu\n', '\nSteven L Grant\n']","5 pages, Asilomar 2013",,http://arxiv.org/abs/1312.2612v1,cs.OH,['cs.OH'],,,[]
Personalized real time weather forecasting,http://arxiv.org/abs/1312.2808v1,2013-12-10T14:19:37Z,2013-12-10T14:19:37Z,"  Temperature forecasting and rain forecasting in today's environment is
playing a major role in many fields like transportation, tour planning and
agriculture. The purpose of this paper is to provide a real time forecasting to
the user according to their current position and requirement. The simplest
method of forecasting the weather, persistence, relies upon today's conditions
to forecast the conditions tomorrow i.e. analyzing historical data for
predicting future weather conditions. The weather data used for the DM research
include daily temperature, daily pressure and monthly rainfall.
","['\nAbhishek Kumar SIngh\n', '\nAditi Sharma\n', '\nRahul Mishra\n']",Published in IJACSA 5 pages Weather Forcasting. Paper 26,IJACSA 2013 vol 4 issue 11,http://arxiv.org/abs/1312.2808v1,cs.OH,['cs.OH'],,,[]
Abridged Petri Nets,http://arxiv.org/abs/1312.2865v1,2013-12-10T16:38:27Z,2013-12-10T16:38:27Z,"  A new graphical framework, Abridged Petri Nets (APNs) is introduced for
bottom-up modeling of complex stochastic systems. APNs are similar to
Stochastic Petri Nets (SPNs) in as much as they both rely on component-based
representation of system state space, in contrast to Markov chains that
explicitly model the states of an entire system. In both frameworks, so-called
tokens (denoted as small circles) represent individual entities comprising the
system; however, SPN graphs contain two distinct types of nodes (called places
and transitions) with transitions serving the purpose of routing tokens among
places. As a result, a pair of place nodes in SPNs can be linked to each other
only via a transient stop, a transition node. In contrast, APN graphs link
place nodes directly by arcs (transitions), similar to state space diagrams for
Markov chains, and separate transition nodes are not needed.
  Tokens in APN are distinct and have labels that can assume both discrete
values (""colors"") and continuous values (""ages""), both of which can change
during simulation. Component interactions are modeled in APNs using triggers,
which are either inhibitors or enablers (the inhibitors' opposites).
Hierarchical construction of APNs rely on using stacks (layers) of submodels
with automatically matching color policies. As a result, APNs provide at least
the same modeling power as SPNs, but, as demonstrated by means of several
examples, the resulting models are often more compact and transparent,
therefore facilitating more efficient performance evaluation of complex
systems.
",['\nVitali Volovoi\n'],17 figures,,http://arxiv.org/abs/1312.2865v1,cs.OH,['cs.OH'],,,[]
"Maturity Model for IT Service Outsourcing in Higher Education
  Institutions",http://arxiv.org/abs/1312.2868v1,2013-12-10T16:46:13Z,2013-12-10T16:46:13Z,"  The current success of organizations depends on the successful implementation
of Information and Comunication Technologies (ICTs). Good governance and ICT
management are essential for delivering value, managing technological risks,
managing resources and performance measurement. In addition, outsourcing is a
strategic option which complements IT services provided internally in
organizations. This paper proposes the design of a new holistic maturity model
based on standards ISO/IEC 20000 and ISO/IEC 38500, the frameworks and best
practices of ITIL and COBIT, with a specific focus on IT outsourcing. This
model is validated by practices in the field of higher education, using a
questionnaire and a metrics table among other measurement tools. Models,
standards and guidelines are proposed in the model for facilitating adaptation
to universities and achieving excellence in the outsourcing of IT services. The
applicability of the model allows an effective transition to a model of good
governance and management of outsourced IT services which, aligned with the
core business of universities (teaching, research and innovation), affect the
effectiveness and efficiency of its management, optimizes its value and
minimizes risks.
","['\nVictoriano Valencia García\n', '\nEugenio J. Fernández Vicente\n', '\nLuis Usero Aragonés\n']","7 pages, 3 tables, 1 figure","International Journal of Advanced Computer Science and
  Applications (IJACSA) Vol. 4, No. 10, 2013",http://dx.doi.org/10.14569/IJACSA.2013.041007,cs.OH,['cs.OH'],10.14569/IJACSA.2013.041007,,[]
"Various models of process of the learning, based on the numerical
  solution of the differential equations",http://arxiv.org/abs/1312.3116v1,2013-12-11T11:13:13Z,2013-12-11T11:13:13Z,"  The principles on which can be based computer model of process of training
are formulated. Are considered: 1) the unicomponent model, which is recognizing
that educational information consists of equal elements; 2) the multicomponent
model, which is considering that knowledge is assimilate with a various
strength, and on lesson weak knowledge becomes strong; 3) the generalized
multicomponent model which considers change of working capacity of the pupil
and various complexity of studied elements of a training material. Typical
results of imitating modeling of learning process are presented in article.
",['\nR. V. Mayer\n'],"8 pages, 5 figures, on russian; Modern scientific researches and
  innovations 2013 oktober 10",,http://arxiv.org/abs/1312.3116v1,cs.OH,"['cs.OH', '68U20', 'I.6.3']",,,[]
Vulnerability of LTE to Hostile Interference,http://arxiv.org/abs/1312.3681v2,2013-12-13T00:05:06Z,2014-03-11T17:08:49Z,"  LTE is well on its way to becoming the primary cellular standard, due to its
performance and low cost. Over the next decade we will become dependent on LTE,
which is why we must ensure it is secure and available when we need it.
Unfortunately, like any wireless technology, disruption through radio jamming
is possible. This paper investigates the extent to which LTE is vulnerable to
intentional jamming, by analyzing the components of the LTE downlink and uplink
signals. The LTE physical layer consists of several physical channels and
signals, most of which are vital to the operation of the link. By taking into
account the density of these physical channels and signals with respect to the
entire frame, as well as the modulation and coding schemes involved, we come up
with a series of vulnerability metrics in the form of jammer to signal ratios.
The ``weakest links'' of the LTE signals are then identified, and used to
establish the overall vulnerability of LTE to hostile interference.
","['\nMarc Lichtman\n', '\nJeffrey H. Reed\n', '\nT. Charles Clancy\n', '\nMark Norton\n']","4 pages, see below for citation. M. Lichtman, J. Reed, M. Norton, T.
  Clancy, ""Vulnerability of LTE to Hostile Interference'', IEEE Global
  Conference on Signal and Information Processing (GlobalSIP), Dec 2013",,http://dx.doi.org/10.1109/GlobalSIP.2013.6736871,cs.OH,['cs.OH'],10.1109/GlobalSIP.2013.6736871,,[]
"The solution of complex problems on calculation of the electrostatic
  fields on lessons on computer modeling",http://arxiv.org/abs/1312.3700v1,2013-12-13T04:35:49Z,2013-12-13T04:35:49Z,"  In article the following tasks on computer modeling of electric fields are
analyzed: 1) calculation of distribution of potential for the field created by
two parallel plates and charged bodies in the non-uniform environment; 2)
calculation of distribution of potential and force lines of electric field in
which are brought the cylinder, a pipe, a plate, a rectangular parallelepiped
from dielectric, and also the metal cylinder; 3) calculation of distribution of
potential in the one-dimensional nonuniform environment; 4) the solution of the
equation of Poisson in spherical coordinates; 5) calculation of distribution of
potential in cylindrical coordinates with the subsequent creation of
equipotential surfaces and force lines.
",['\nRobert V Mayer\n'],"in Russian, 10 pages, 5 figures, 3 programs on Pascal. Modern
  scientific researches and innovations 2013 12 December",,http://arxiv.org/abs/1312.3700v1,cs.OH,"['cs.OH', '68U20', 'I.6.0']",,,[]
"Technical Report: A New Multi-Device Wireless Power Transfer Scheme
  Using an Intermediate Energy Storage Circuit",http://arxiv.org/abs/1312.4410v3,2013-12-09T10:36:36Z,2013-12-26T03:59:34Z,"  A new multi-device wireless power transfer scheme that reduces the overall
charging time is presented. The proposed scheme employs the intermediated
energy storage (IES) circuit which consists of a constant power driving circuit
and a super-capacitor. By utilizing the characteristic of high power density of
the super-capacitor, the receiver can receive and store the energy in short
duration and supply to the battery for long time. This enables the overlap of
charging duration between all receivers. As a result, the overall charging time
can be reduced.
","['\nChangseok Yoon\n', '\nSung Sik Nam\n', '\nSung Ho Cho\n']",,,http://arxiv.org/abs/1312.4410v3,cs.OH,['cs.OH'],,,[]
"A Fraud Detection Visualization System Utilizing Radial Drawings and
  Heat-maps",http://arxiv.org/abs/1311.7259v1,2013-11-28T10:18:22Z,2013-11-28T10:18:22Z,"  We present a prototype system developed in cooperation with a business
organization that combines information visualization and pattern-matching
techniques to detect fraudulent activity by employees. The system is built upon
common fraud patterns searched while trying to detect occupational fraud
suggested by internal auditors of a business company. The main visualization of
the system consists of a multi-layer radial drawing that represents the
activity of the employees and clients. Each layer represents a different
examined pattern whereas heat-maps indicating suspicious activity are
incorporated in the visualization. The data are first preprocessed based on a
decision tree generated by the examined patterns and each employee is assigned
a value indicating whether or not there exist indications of fraud. The
visualization is presented as an animation and the employees are visualized one
by one according to their severity values together with their related clients.
","['\nEvmorfia N. Argyriou\n', '\nAntonios Symvonis\n', '\nVassilis Vassiliou\n']",,,http://arxiv.org/abs/1311.7259v1,cs.OH,['cs.OH'],,,[]
"Theoretical Foundation for Research in Communication using Information
  and Communication Technology Devices in Healthcare: An Interdisciplinary
  Scoping Review",http://arxiv.org/abs/1312.0520v1,2013-12-02T17:22:38Z,2013-12-02T17:22:38Z,"  Faulty communication between team members is one of the most important
factors preventing substantial improvement in patient safety. Aviation, nuclear
power and defense have been able to improve their safety record by adopting
theory and model based solutions. In contrast, healthcare's thrust towards
modern communication devices is largely devoid of theoretical foundation. The
objective of this scoping review is to compile communication theories,
frameworks, and models used by high risk organizations outside healthcare to
study and resolve workplace communication issues. The healthcare databases
searched included Medline, CINAHL, EMBASE, and PsycInfo. In addition, we
searched engineering and science literature to include articles in the fields
of information sciences, computer sciences, nuclear power generation, aviation,
the military and other domains such as sociology that address the science and
theory of communication. Comprehensive searching was also done in the
communication studies literature. We also reviewed conference proceedings and
grey literature and conducted citation tracking. Our initial systematic search
yielded 15,365 articles. Hand searching and reviewing references resulted in a
set of 181 articles. 144 full text articles were read and 40 of them were
selected to be included in the review. We were able to identify 14 theories and
12 models which could be applied in hospital communication research. However,
it must be noted that most of them have not yet been applied in biomedical
research in hospital communication and as such their applicability can only be
suggested-a gap which future research may be able to address. Formulation of a
custom model representing the unique features and complexities of communication
within hospitals is recommended.
","['\nArun Keepanasseril\n', '\nKathleen Ann McKibbon\n', '\nAlfonso Iorio\n']",Pages: 35,,http://arxiv.org/abs/1312.0520v1,cs.OH,['cs.OH'],,,[]
Cyclostationary Spectrum Sensing in Cognitive Radios Using FRESH Filters,http://arxiv.org/abs/1312.5257v1,2013-12-06T18:03:10Z,2013-12-06T18:03:10Z,"  This paper deals with spectrum sensing in Cognitive Radios to enable
unlicensed secondary users to opportunistically access a licensed band. The
ability to detect the presence of a primary user at a low signal to noise ratio
(SNR) is a challenging prerequisite to spectrum sensing and earlier proposed
techniques like energy detection and cyclostationary detection have only been
partially successful. This paper proposes the use of FRESH (FREquency SHift)
filters [1] to enable spectrum sensing at low SNR by optimally estimating a
cyclostationary signal using its spectral coherence properties. We establish
the mean square error convergence of the adaptive FRESH filter through
simulation. Subsequently, we formulate a cyclostationarity based binary
hypothesis test on the filtered signal and observe the resultant detection
performance. Simulation results show that the proposed approach performs better
than energy detection and cyclostationary detection techniques for spectrum
sensing.
","['\nHemant Saggar\n', '\nD. K. Mehra\n']","Presented at Advances in Wireless Cellular Telecommunications:
  Technologies & Services, 1st ICEIT National Conference on, April 14-15, 2011,
  New Delhi",,http://arxiv.org/abs/1312.5257v1,cs.OH,['cs.OH'],,,[]
Unconventional research in USSR and Russia: short overview,http://arxiv.org/abs/1312.1148v2,2013-12-04T13:16:25Z,2013-12-05T07:58:33Z,"  This work briefly surveys unconventional research in Russia from the end of
the 19th until the beginning of the 21th centuries in areas related to
generation and detection of a 'high-penetrating' emission of non-biological
origin. The overview is based on open scientific and journalistic materials.
The unique character of this research and its history, originating from
governmental programs of the USSR, is shown. Relations to modern studies on
biological effects of weak electromagnetic emission, several areas of
bioinformatics and theories of physical vacuum are discussed.
",['\nSerge Kernbach\n'],,,http://arxiv.org/abs/1312.1148v2,cs.OH,"['cs.OH', 'physics.hist-ph']",,,[]
"Exact Reconstruction of Spatially Undersampled Signals in Evolutionary
  Systems",http://arxiv.org/abs/1312.3203v1,2013-12-04T19:52:24Z,2013-12-04T19:52:24Z,"  We consider the problem of spatiotemporal sampling in which an initial state
$f$ of an evolution process $f_t=A_tf$ is to be recovered from a combined set
of coarse samples from varying time levels $\{t_1,\dots,t_N\}$. This new way of
sampling, which we call dynamical sampling, differs from standard sampling
since at any fixed time $t_i$ there are not enough samples to recover the
function $f$ or the state $f_{t_i}$. Although dynamical sampling is an inverse
problem, it differs from the typical inverse problems in which $f$ is to be
recovered from $A_Tf$ for a single time $T$. In this paper, we consider signals
that are modeled by $\ell^2(\mathbb Z)$ or a shift invariant space $V\subset
L^2(\mathbb R)$.
","['\nAkram Aldroubi\n', '\nJacqueline Davis\n', '\nIlya Krishtal\n']",,,http://dx.doi.org/10.1007/s00041-014-9359-9,cs.OH,"['cs.OH', 'math.FA']",10.1007/s00041-014-9359-9,,[]
Routing Diverse Evacuees with Cognitive Packets,http://arxiv.org/abs/1311.4818v2,2013-11-19T17:55:55Z,2014-01-10T12:44:30Z,"  This paper explores the idea of smart building evacuation when evacuees can
belong to different categories with respect to their ability to move and their
health conditions. This leads to new algorithms that use the Cognitive Packet
Network concept to tailor different quality of service needs to different
evacuees. These ideas are implemented in a simulated environment and evaluated
with regard to their effectiveness.
","['\nHuibo Bi\n', '\nErol Gelenbe\n']","7 pages, 7 figures",,http://arxiv.org/abs/1311.4818v2,cs.OH,['cs.OH'],,,[]
"Classification of ST and Q Type MI variant using thresholding and
  neighbourhood estimation method after cross wavelet based analysis",http://arxiv.org/abs/1311.5639v1,2013-11-22T02:50:18Z,2013-11-22T02:50:18Z,"  This paper proposes a cross wavelet transform based method for
Electrocardiogram signal analysis where parameters are identified from wavelet
cross spectrum and wavelet cross coherence of ECG patterns. Most of the ECG
analysing systems use explicit time plane features for cardiac pattern
classification. Application of this proposed technique for classification
eliminates the need for extraction of various explicit time plane features and
hence reduces the complexity of the system. The cross-correlation is the
measure of similarity between two waveforms or two time series and the cross
examination reveals localized similarities in time and scale. Parameters
extracted from Wavelet Cross Spectrum (WCS) and Wavelet Coherence (WCOH) is
used for classification. A pathologically varying pattern in QT zone of
inferior lead III shows the presence of Inferior Myocardial Infarction (IMI).
The Cross Wavelet Transform and Wavelet Coherence is used for the cross
examination of single normal and abnormal (IMI) beats. A normal template beat
is selected as the absolute normal pattern. Computation of the WCS and WCOH of
the selected normal template and various other normal and abnormal beats
reveals the existence of variation among patterns under study. The Wavelet
cross spectrum and Wavelet coherence of various ECG patterns shows
distinguishing characteristics over two specific regions R1 and R2, where R1 is
the QRS complex location and R2 is the T wave region. Parameters are identified
for classification of Type 1 IMI (non Q type, with ST elevation and attenuated
QRS complex) and Type 2 IMI (Q type MI with deep Q and inverted T) and normal
subjects. Accuracy of the proposed classification method is obtained as 99.43%
for normal and abnormal class and 88.5% and 87.02% for Type I and Type II
respectively.
","['\nSwati Banerjee\n', '\nMadhuchhanda Mitra\n']","arXiv admin note: text overlap with arXiv:astro-ph/0301002 by other
  authors",,http://arxiv.org/abs/1311.5639v1,cs.OH,['cs.OH'],,,[]
"Criticality estimation of IT business functions with the Business
  Continuity Testing Points method for implementing effective recovery
  exercises of crisis scenarios",http://arxiv.org/abs/1311.5677v1,2013-11-22T08:53:49Z,2013-11-22T08:53:49Z,"  The primary goal of the present paper is the introduction of a new approach
of defining IT unit business functions exact criticality levels and
respectively categorize them to the appropriate recovery tests, prior to their
thorough documentation which includes actual desired recovery time frames. The
method is entitled as Business Continuity Testing Points and it is based on the
concept of Use Case Points, a fundamental project estimation tool utilized for
sizing of object-oriented system development. The aim of the contribution is to
ameliorate the existing manual way of determining recovery time of IT business
functions that is based exclusively on experience of IT personnel, by
introducing a calculation method of multiple factors that can negatively affect
the recovery process. The elimination of damage as a result of tested immediate
response action in a crisis situation that disrupts core IT operations
constitutes the aimed advantage of the proposed contribution
","['\nAthanasios Podaras\n', '\nTomas Zizka\n']","9 pages. International Journal of Computer Science Issues, 2013",,http://arxiv.org/abs/1311.5677v1,cs.OH,['cs.OH'],,,[]
Simulation Model Of Functional Stability Of Business Processes,http://arxiv.org/abs/1311.6550v1,2013-11-26T03:51:27Z,2013-11-26T03:51:27Z,"  Functioning of business processes of high-tech enterprise is in a constant
interaction with the environment. Herewith a wide range of such interaction
represents a variety of conflicts affecting the achievement of the goals of
business processes. All these things lead to the disruption of functioning of
business processes. That's why modern enterprises should have mechanisms to
provide a new property of business processes - ability to maintain and/or
restore functions in various adverse effects. This property is called
functional stability of business processes (FSBP). In this article we offer,
showcase and test the new approach to assessing the results of business process
re-engineering by simulating their functional stability before and after
re-engineering.
","['\nYuri Monakhov\n', '\nOlga Fayman\n']","10 pages, 12 figures","Int. Journal of Engineering Research and Application ISSN :
  2248-9622, Vol. 3, Issue 6, Nov-Dec 2013, pp. 819-828",http://arxiv.org/abs/1311.6550v1,cs.OH,['cs.OH'],,,[]
"FELFCNCA: Fast & Efficient Log File Compression Using Non Linear
  Cellular Automata Classifier",http://arxiv.org/abs/1312.1889v1,2013-11-18T15:09:16Z,2013-11-18T15:09:16Z,"  Log Files are created for Traffic Analysis, Maintenance, Software debugging,
customer management at multiple places like System Services, User Monitoring
Applications, Network servers, database management systems which must be kept
for long periods of time. These Log files may grow to huge sizes in this
complex systems and environments. For storage and convenience log files must be
compressed. Most of the existing algorithms do not take temporal redundancy
specific Log Files into consideration. We propose a Non Linear based Classifier
which introduces a multidimensional log file compression scheme described in
eight variants, differing in complexity and attained compression ratios. The
FELFCNCA scheme introduces a transformation for log file whose compressible
output is far better than general purpose algorithms. This proposed method was
found lossless and fully automatic. It does not impose any constraint on the
size of log file
","['\nP. Kiran Sree\n', '\nInampudi Ramesh Babu\n', '\nSSSN Usha Devi N\n']","International Journal on Communications (IJC) Volume 1 Issue 1,
  December 2012 http://www.seipub.org/ijc",,http://arxiv.org/abs/1312.1889v1,cs.OH,['cs.OH'],,,[]
Unfaithful Glitch Propagation in Existing Binary Circuit Models,http://arxiv.org/abs/1311.1423v1,2013-11-06T15:39:45Z,2013-11-06T15:39:45Z,"  We show that no existing continuous-time, binary value-domain model for
digital circuits is able to correctly capture glitch propagation. Prominent
examples of such models are based on pure delay channels (P), inertial delay
channels (I), or the elaborate PID channels proposed by Bellido-D\'iaz et al.
We accomplish our goal by considering the solvability/non-solvability border of
a simple problem called Short-Pulse Filtration (SPF), which is closely related
to arbitration and synchronization. On one hand, we prove that SPF is solvable
in bounded time in any such model that provides channels with non-constant
delay, like I and PID. This is in opposition to the impossibility of solving
bounded SPF in real (physical) circuit models. On the other hand, for binary
circuit models with constant-delay channels, we prove that SPF cannot be solved
even in unbounded time; again in opposition to physical circuit models.
Consequently, indeed none of the binary value-domain models proposed so far
(and that we are aware of) faithfully captures glitch propagation of real
circuits. We finally show that these modeling mismatches do not hold for the
weaker eventual SPF problem.
","['\nMatthias Függer\n', '\nThomas Nowak\n', '\nUlrich Schmid\n']","23 pages, 15 figures","Proc. 19th IEEE International Symposium on Asynchronous Circuits
  and Systems (ASYNC 2013), IEEE Press, New York City, 2013, pp. 191-199",http://dx.doi.org/10.1109/ASYNC.2013.9,cs.OH,['cs.OH'],10.1109/ASYNC.2013.9,,[]
"Tasks and architecture of documentation subsystem in multi-level
  modeling environment MARS",http://arxiv.org/abs/1311.1587v1,2013-11-07T06:20:34Z,2013-11-07T06:20:34Z,"  The article describes the automated documentation system designed to generate
reports on research conducted by computer complex technical objects and systems
in multi-level modeling environment {\guillemotleft}MARS{\guillemotright}. We
defined the purposes, tasks and abilities of documentation system and examined
the types and structure of documents, and gave an example of its practical use
","['\nT. V. Gandzha\n', '\nS. A Panov\n']",in Russian,,http://arxiv.org/abs/1311.1587v1,cs.OH,['cs.OH'],,,[]
Une représentation en graphe pour l'enseignement de XML,http://arxiv.org/abs/1311.1793v2,2013-11-07T19:48:54Z,2013-11-15T07:34:30Z,"  Currently, XML is a format widely used. In the context of computer science
teaching, it is necessary to introduce students to this format and, especially,
at its eco-system. We have developed a model to support the teaching of XML. We
propose to represent an XML schema as a graph highlighting the structural
characteristics of the valide documents. We present in this report different
graphic elements of the model and the improvements it brings to data modeling
in XML.---XML est un format actuellement tr\`es utilis\'e. Dans le cadre des
formations en informatique, il est indispensable d'initier les \'etudiants \`a
ce format et, surtout, \`a tout son \'eco-syst\`eme. Nous avons donc mis au
point un mod\`ele permettant d'appuyer l'enseignement de XML. Ce mod\`ele
propose de repr\'esenter un sch\'ema XML sous la forme d'un graphe mettant en
valeur les caract\'eristiques structurelles des documents valides. Nous
pr\'esentons dans ce rapport les diff\'erents \'el\'ements graphique du
mod\`ele et les am\'eliorations qu'il apporte \`a la mod\'elisation de
donn\'ees en XML.
",['\nEmmanuel Desmontils\nLINA\n'],"22 pages, in French",,http://arxiv.org/abs/1311.1793v2,cs.OH,['cs.OH'],,,['LINA']
"The structure and functions of an automated project management system
  for the centers of scientific and technical creativity of students",http://arxiv.org/abs/1311.2056v1,2013-11-07T06:22:09Z,2013-11-07T06:22:09Z,"  This article discusses the possibility of automating of the student's
projecting through the use of automated project management system. There are
described the purpose, structure and formalism of automated workplace of
student-designer (AWSD), and shown its structural-functional diagram.
","['\nV. M. Dmitriev\n', '\nT. V. Gandzha\n', '\nV. V. Gandzha\n', '\nS. A. Panov\n']",in Russian,,http://arxiv.org/abs/1311.2056v1,cs.OH,['cs.OH'],,,[]
Smart: Semantically mashup rest web services,http://arxiv.org/abs/1311.3078v1,2013-11-13T10:55:15Z,2013-11-13T10:55:15Z,"  A mashup is a combination of information from more than one source, mixed up
in a way to create something new, or at least useful. Anyone can find mashups
on the internet, but these are always specifically designed for a predefined
purpose. To change that fact, we implemented a new platform we called the SMART
platform. SMART enables the user to make his own choices as for the REST web
services he needs to call in order to build an intelligent personalized mashup,
from a Google-like simple search interface, without needing any programming
skills. In order to achieve this goal, we defined an ontology that can hold
REST web services descriptions. These descriptions encapsulate mainly, the
input type needed for a service, its output type, and the kind of relation that
ties the input to the output. Then, by matching the user input query keywords,
with the REST web services definitions in our ontology, we can find registered
services individuals in this ontology, and construct the raw REST query for
each service found. The wrap up from the keywords, into semantic definitions,
in order to find the matching service individual, then the wrap down from the
semantic service description of the found individual, to the raw REST call, and
finally the wrap up of the result again into semantic individuals, is done for
two main purposes: the first to let the user use simple keywords in order to
build complex mashups, and the second to benefit from the ontology inference
engine in a way, where services instances can be tied together into an
intelligent mashup, simply by making each service output individuals, stand as
the next service input.
",['\nRima Kilany Maroun Chamoun\n'],,,http://arxiv.org/abs/1311.3078v1,cs.OH,['cs.OH'],,,[]
Current Services In Cloud Computing: A Survey,http://arxiv.org/abs/1311.3319v1,2013-11-13T21:59:21Z,2013-11-13T21:59:21Z,"  Due to the fast development of the Cloud Computing technologies, the rapid
increase of cloud services are became very remarkable. The fact of integration
of these services with many of the modern enterprises cannot be ignored.
Microsoft, Google, Amazon, SalesForce.com and the other leading IT companies
are entered the field of developing these services. This paper presents a
comprehensive survey of current cloud services, which are divided into eleven
categories. Also the most famous providers for these services are listed.
Finally, the Deployment Models of Cloud Computing are mentioned and briefly
discussed.
","['\nMohamed Magdy Mosbah\n', '\nHany Soliman\n', '\nMohamad Abou El-Nasr\n']",8 pages,"International Journal of Computer Science, Engineering and
  Information Technology (IJCSEIT), Vol.3,No.5,October 2013",http://arxiv.org/abs/1311.3319v1,cs.OH,['cs.OH'],,,[]
Wireless Computing and IT Ecosystems,http://arxiv.org/abs/1311.3548v1,2013-11-14T15:51:41Z,2013-11-14T15:51:41Z,"  We have evolved an IT system that is ubiquitous and pervasive and integrated
into most aspects of our lives. Many of us are working on 4th and 5th level
refinements in efficiency and functionality. But, we stand on the shoulders of
those who came before and this restricts our freedom of action. The prior work
has left us with an ecosystem which is the living embodiment of our
state-of-the-art. While we work on integration, refinement, broader application
and efficiency, the results must move seamlessly into the ecosystem.
Fundamental concepts are being researched in the lab and may rebuild the world
we all live in, until that happens, we must work within the ecosystem.
",['\nWilliam R Simpson\n'],"6 pages, 2 figures, keynote at WIMON13 June 2013, Istanbul",,http://arxiv.org/abs/1311.3548v1,cs.OH,['cs.OH'],,,[]
Intuitionistic Neutrosophic Soft Set,http://arxiv.org/abs/1311.3562v1,2013-11-14T16:12:25Z,2013-11-14T16:12:25Z,"  In this paper we study the concept of intuitionistic neutrosophic set of
Bhowmik and Pal. We have introduced this concept in soft sets and defined
intuitionistic neutrosophic soft set. Some definitions and operations have been
introduced on intuitionistic neutrosophic soft set. Some properties of this
concept have been established.
","['\nBroumi Said\n', '\nFlorentin Smarandache\n']","10 pages. arXiv admin note: substantial text overlap with
  arXiv:1305.2724","ISSN 1746-7659, England, UK Journal of Information and Computing
  Science Vol. 8, No. 2, 2013, pp.130-140",http://arxiv.org/abs/1311.3562v1,cs.OH,['cs.OH'],,,[]
"Spreading huge free software without internet connection, via
  self-replicating USB keys",http://arxiv.org/abs/1311.6754v1,2013-11-13T15:52:33Z,2013-11-13T15:52:33Z,"  We describe and discuss an affordable way to spread huge software without
relying on internet connection, via the use of self-replicating live USB keys.
",['\nThierry Monteil\n'],"5 pages, accepted to Extremecom 2013",,http://arxiv.org/abs/1311.6754v1,cs.OH,"['cs.OH', 'C.2.1; C.2.4; J.2']",,,[]
"A customized flocking algorithm for swarms of sensors tracking a swarm
  of targets",http://arxiv.org/abs/1311.6981v1,2013-11-13T04:46:57Z,2013-11-13T04:46:57Z,"  Wireless mobile sensor networks (WMSNs) are groups of mobile sensing agents
with multi-modal sensing capabilities that communicate over wireless networks.
WMSNs have more flexibility in terms of deployment and exploration abilities
over static sensor networks. Sensor networks have a wide range of applications
in security and surveillance systems, environmental monitoring, data gathering
for network-centric healthcare systems, monitoring seismic activities and
atmospheric events, tracking traffic congestion and air pollution levels,
localization of autonomous vehicles in intelligent transportation systems, and
detecting failures of sensing, storage, and switching components of smart
grids. The above applications require target tracking for processes and events
of interest occurring in an environment. Various methods and approaches have
been proposed in order to track one or more targets in a pre-defined area.
Usually, this turns out to be a complicated job involving higher order
mathematics coupled with artificial intelligence due to the dynamic nature of
the targets. To optimize the resources we need to have an approach that works
in a more straightforward manner while resulting in fairly satisfactory data.
In this paper we have discussed the various cases that might arise while
flocking a group of sensors to track targets in a given environment. The
approach has been developed from scratch although some basic assumptions have
been made keeping in mind some previous theories. This paper outlines a
customized approach for feasibly tracking swarms of targets in a specific area
so as to minimize the resources and optimize tracking efficiency.
","['\nAnupam Shukla\n', '\nGaurav Ojha\n', '\nSachin Acharya\n', '\nShubham Jain\n']","13 Pages, 11 Figures, SAI 2013",,http://arxiv.org/abs/1311.6981v1,cs.OH,"['cs.OH', 'cs.NI']",,,[]
A Quantitative Approach to Painting Styles,http://arxiv.org/abs/1403.4512v1,2013-11-14T00:15:47Z,2013-11-14T00:15:47Z,"  This research extends a method previously applied to music and
philosophy,representing the evolution of art as a time-series where relations
like dialectics are measured quantitatively. For that, a corpus of paintings of
12 well-known artists from baroque and modern art is analyzed. A set of 93
features is extracted and the features which most contributed to the
classification of painters are selected. The projection space obtained provides
the basis to the analysis of measurements. This quantitative measures underlie
revealing observations about the evolution of painting styles, specially when
compared with other humanity fields already analyzed: while music evolved along
a master-apprentice tradition (high dialectics) and philosophy by opposition,
painting presents another pattern: constant increasing skewness, low opposition
between members of the same movement and opposition peaks in the transition
between movements. Differences between baroque and modern movements are also
observed in the projected ""painting space"": while baroque paintings are
presented as an overlapped cluster, the modern paintings present minor
overlapping and are disposed more widely in the projection than the baroque
counterparts. This finding suggests that baroque painters shared aesthetics
while modern painters tend to ""break rules"" and develop their own style.
","['\nVilson Vieira\n', '\nRenato Fabbri\n', '\nDavid Sbrissa\n', '\nLuciano da Fontoura Costa\n', '\nGonzalo Travieso\n']",,,http://arxiv.org/abs/1403.4512v1,stat.AP,"['stat.AP', 'cs.OH']",,,[]
"On the Optimum Energy Efficiency for Flat-fading Channels with
  Rate-dependent Circuit Power",http://arxiv.org/abs/1310.8342v1,2013-10-30T22:53:35Z,2013-10-30T22:53:35Z,"  This paper investigates the optimum energy efficiency (EE) and the
corresponding spectral efficiency (SE) for a communication link operating over
a flat-fading channel. The EE is evaluated by the total energy consumption for
transmitting per message bit. Three channel cases are considered, namely static
channel with channel state information available at transmitter (CSIT),
fast-varying (FV) channel with channel distribution information available at
transmitter (CDIT), and FV channel with CSIT. A general circuit power model is
considered. For all the three channel cases, the tradeoff between the EE and SE
is studied. It is shown that the EE improves strictly as the SE increases from
0 to the optimum SE, and then strictly degrades as the SE increases beyond the
optimum SE. The impact of {\kappa}, {\rho} and other system parameters on the
optimum EE and corresponding SE is investigated to obtain insight.Some of the
important and interesting results for all the channel cases include: (1) when
{\kappa} increases the SE corresponding to the optimum EE should keep unchanged
if {\phi}(R) = R, but reduced if {\phi}(R) is strictly convex of R; (2) when
the rate-independent circuit power {\rho} increases, the SE corresponding to
the optimum EE has to be increased. A polynomial-complexity algorithm is
developed with the bisection method to find the optimum SE. The insight is
corroborated and the optimum EE for the three cases are compared by simulation
results.
","['\nTao Wang\n', '\nLuc Vandendorpe\n']","12 pages, 7 figures, to appear in IEEE Transactions on Communications",,http://arxiv.org/abs/1310.8342v1,cs.OH,['cs.OH'],,,[]
Automatic Airspace Sectorisation: A Survey,http://arxiv.org/abs/1311.0653v1,2013-11-04T11:48:05Z,2013-11-04T11:48:05Z,"  Airspace sectorisation provides a partition of a given airspace into sectors,
subject to geometric constraints and workload constraints, so that some cost
metric is minimised. We survey the algorithmic aspects of methods for automatic
airspace sectorisation, for an intended readership of experts on air traffic
management.
","['\nPierre Flener\n', '\nJustin Pearson\n']",,,http://arxiv.org/abs/1311.0653v1,cs.OH,['cs.OH'],,,[]
Ear-Phone: A Context-Aware Noise Mapping using Smart Phones,http://arxiv.org/abs/1310.4270v1,2013-10-16T05:04:28Z,2013-10-16T05:04:28Z,"  A noise map facilitates the monitoring of environmental noise pollution in
urban areas. However, state-of-the-art techniques for rendering noise maps in
urban areas are expensive and rarely updated, as they rely on population and
traffic models rather than on real data. Smart phone based urban sensing can be
leveraged to create an open and inexpensive platform for rendering up-to- date
noise maps. In this paper, we present the design, implementation and
performance evaluation of an end-to-end, context-aware, noise mapping system
called Ear-Phone. Ear-Phone investigates the use of different interpolation and
regularization methods to address the fundamental problem of recovering the
noise map from incomplete and random samples obtained by crowdsourcing data
collection. Ear-Phone, implemented on Nokia N95, N97 and HP iPAQ, HTC One
mobile devices, also addresses the challenge of collecting accurate noise
pollution readings at a mobile device. A major challenge of using smart phones
as sensors is that even at the same location, the sensor reading may vary
depending on the phone orientation and user context (for example, whether the
user is carrying the phone in a bag or holding it in her palm). To address this
problem, Ear-Phone leverages context-aware sensing. We develop classifiers to
accurately determine the phone sensing context. Upon context discovery,
Ear-Phone automatically decides whether to sense or not. Ear-phone also
implements in-situ calibration which performs simple calibration that can be
carried out without any technical skills whatsoever required on the user's
part. Extensive simulations and outdoor experiments demonstrate that Ear-Phone
is a feasible platform to assess noise pollution, incurring reasonable system
resource consumption at mobile devices and providing high reconstruction
accuracy of the noise map.
","['\nRajib Rana\n', '\nChun Tung Chou\n', '\nNirupama Bulusu\n', '\nSalil Kanhere\n', '\nWen Hu\n']",,,http://arxiv.org/abs/1310.4270v1,cs.OH,['cs.OH'],,,[]
"Gait Velocity Estimation using time interleaved between Consecutive
  Passive IR Sensor Activations",http://arxiv.org/abs/1310.4880v2,2013-10-18T01:21:25Z,2015-12-01T02:54:40Z,"  Gait velocity has been consistently shown to be an important indicator and
predictor of health status, especially in older adults. It is often assessed
clinically, but the assessments occur infrequently and do not allow optimal
detection of key health changes when they occur. In this paper, we show that
the time gap between activations of a pair of Passive Infrared (PIR) motion
sensors installed in the consecutively visited room pair carry rich latent
information about a person's gait velocity. We name this time gap transition
time and show that despite a six second refractory period of the PIR sensors,
transition time can be used to obtain an accurate representation of gait
velocity.
  Using a Support Vector Regression (SVR) approach to model the relationship
between transition time and gait velocity, we show that gait velocity can be
estimated with an average error less than 2.5 cm/sec. This is demonstrated with
data collected over a 5 year period from 74 older adults monitored in their own
homes.
  This method is simple and cost effective and has advantages over competing
approaches such as: obtaining 20 to 100x more gait velocity measurements per
day and offering the fusion of location-specific information with time stamped
gait estimates. These advantages allow stable estimates of gait parameters
(maximum or average speed, variability) at shorter time scales than current
approaches. This also provides a pervasive in-home method for context-aware
gait velocity sensing that allows for monitoring of gait trajectories in space
and time.
","['\nRajib Rana\n', '\nDaniel Austin\n', '\nPeter G. Jacobs\n', '\nMohanraj Karunanithi\n', '\nJeffrey Kaye\n']",,,http://arxiv.org/abs/1310.4880v2,cs.OH,['cs.OH'],,,[]
Flickers Forecasting In CRT Using Stochastic Analysis,http://arxiv.org/abs/1310.5478v1,2013-10-21T09:38:24Z,2013-10-21T09:38:24Z,"  Videos are composed of sequence of interrelated frames. There is a minute
difference among frames. Flicker is an error which is found in every video. It
is like a checker box in a video, there are several reasons behind flickers
generation, one of the main reasons is refresh rate of the monitor and second
reason is number of frames per second in a video. The main objective of this
study is to propose and develop a framework that identifies flicker location
and minimizes the flickers rate. Analysis shows that flickers can be minimize
by adjusting the persistence of pixel and higher refresh rate of CRT monitor.
Further we have compared different isotopes of phosphorous pixels and generate
its graphs. This paper highlighted the cause of flicker and its avoidance
.Statistical research proves that proposed algorithm improves the video quality
and reduce flickers ratio up to 90%.
","['\nAdnan Alam Khan\n', '\nSafeeullah Soomro\n', '\nAbdul Ghafoor Memon\n']",,Sindh Univ. Res. Jour. (Sci. Ser.) Vol.44 (3) 473-478 (2012),http://arxiv.org/abs/1310.5478v1,cs.OH,['cs.OH'],,,[]
"Using CamiTK for rapid prototyping of interactive Computer Assisted
  Medical Intervention applications",http://arxiv.org/abs/1310.5497v1,2013-10-21T10:40:02Z,2013-10-21T10:40:02Z,"  Computer Assisted Medical Intervention (CAMI hereafter) is a complex
multi-disciplinary field. CAMI research requires the collaboration of experts
in several fields as diverse as medicine, computer science, mathematics,
instrumentation, signal processing, mechanics, modeling, automatics, optics,
etc.
","['\nEmmanuel Promayon\nTIMC-IMAG\n', '\nCeline Fouard\nTIMC-IMAG\n', '\nMathieu Bailet\nTIMC-IMAG\n', '\nAurelien Deram\nTIMC-IMAG\n', '\nGaelle Fiard\nTIMC-IMAG\n', '\nNikolai Hungr\nTIMC-IMAG\n', '\nVincent Luboz\nTIMC-IMAG\n', '\nYohan Payan\nTIMC-IMAG\n', '\nJohan Sarrazin\nTIMC-IMAG\n', '\nNicolas Saubat\nTIMC-IMAG\n', '\nSonia Yuki Selmi\nTIMC-IMAG\n', '\nSandrine Voros\nTIMC-IMAG\n', '\nPhilippe Cinquin\nTIMC-IMAG\n', '\nJocelyne Troccaz\nTIMC-IMAG\n']",,"Conference proceedings : Annual International Conference of the
  IEEE Engineering in Medicine and Biology Society. 2013 (2013) 4933-6",http://dx.doi.org/10.1109/EMBC.2013.6610654,cs.OH,['cs.OH'],10.1109/EMBC.2013.6610654,,"['TIMC-IMAG', 'TIMC-IMAG', 'TIMC-IMAG', 'TIMC-IMAG', 'TIMC-IMAG', 'TIMC-IMAG', 'TIMC-IMAG', 'TIMC-IMAG', 'TIMC-IMAG', 'TIMC-IMAG', 'TIMC-IMAG', 'TIMC-IMAG', 'TIMC-IMAG', 'TIMC-IMAG']"
The Energetic Reasoning Checker Revisited,http://arxiv.org/abs/1310.5564v1,2013-10-16T19:02:19Z,2013-10-16T19:02:19Z,"  Energetic Reasoning (ER) is a powerful filtering algorithm for the Cumulative
constraint. Unfortunately, ER is generally too costly to be used in practice.
One reason of its bad behavior is that many intervals are considered as
relevant by the checker of ER, although most of them should be ignored. In this
paper, we provide a sharp characterization that allows to reduce the number of
intervals by a factor seven. Our experiments show that associating this checker
with a Time-Table filtering algorithm leads to promising results.
","['\nAlban Derrien\nLINA, INRIA - LINA\n', '\nThierry Petit\nLINA, INRIA - LINA\n']","CP Doctoral Program 2013, Uppsala : Sweden (2013)",,http://arxiv.org/abs/1310.5564v1,cs.OH,['cs.OH'],,,"['LINA, INRIA - LINA', 'LINA, INRIA - LINA']"
Multivalued Logic Circuit Design for Binary Logic Interface,http://arxiv.org/abs/1310.5697v1,2013-10-21T12:33:42Z,2013-10-21T12:33:42Z,"  Binary logic and devices have been in used since inception with advancement
and technology and millennium gate design era. The development in binary logic
has become tedious and cumbersome. Multivalued logic enables significant more
information to be packed within a single digit. The design and development of
logic circuit becomes very compact and easier. Attempts are being made to
fabricate multivalued logic based devices. Since present devices can be
implemented only in binary system,it is necessary to evolve a system that can
built the circuit in multivalued logic system and convert in binary logic
system. In multivalued logic system logic gates differ in different logic
system, a quaternary has become mature in terms of logic algebra and gates.
Hence logic design based on above system can be done using standard procedure.
In this dissertation a logic circuit design entry based on multivalued logic
system has been taken up that can provide the ease of circuit design in
multivalued system and output as binary valued circuit. The named ""MVL-DEV""
offers editing, storage and conversion into binary facility.
","['\nHitesh Gupta\n', '\nDr. S. C. Jain\n']","72 pages,Dissertation Report",,http://arxiv.org/abs/1310.5697v1,cs.OH,['cs.OH'],,,[]
Stacked Patch Antenna With Cross Slot Electronic Band Gap Structure,http://arxiv.org/abs/1310.6259v1,2013-10-16T09:29:07Z,2013-10-16T09:29:07Z,"  A cross slotted electronic band gap (EBG) with stacked rectangular patches
shorted with a shorting pin is proposed in this paper. The study is being done
on how the various parameters are varied by changing the probe feed location.
The design is constructed by using stacking of patches, shorting pin and cross
slotted EBG to form an optimized antenna design with antenna efficiency of
approximately 99.06%. The radiation patterns are given at 2.586 GHz which can
be used for wireless communications.
","['\nN. S Raghava\n', '\nAsok De\n', '\nNitish Kataria\n', '\nSarthak Chatterjee\n']","4 pages, 4 figures, 1 table. Presented Research Paper in
  International Congress on Innovative Trends in Information Technology and
  Computing Sciences for Competitive World Order (ITITCSCWO - 2013) on 2nd and
  3rd of March 2013, at J.N.U, New Delhi. Research Paper published in
  International Journal of Information and Computation Technology(IJICT),
  Volume 3, Number 5, 2013. ISSN 0974-2239","International Journal of Information and Computation Technology,
  Volume 3, Number 5,2013",http://arxiv.org/abs/1310.6259v1,cs.OH,"['cs.OH', '94-06']",,,[]
Optical Disk with Blu-Ray Technology,http://arxiv.org/abs/1310.1551v1,2013-10-06T07:13:19Z,2013-10-06T07:13:19Z,"  Blu-ray is the name of a next-generation optical disc format jointly
developed by the Blu-ray Disc Association a group of the world's leading
consumer electronics, personal computer and media manufacturers. The format was
developed to enable recording, rewriting and playback of high-definition video,
as well as storing large amounts of data. This extra capacity combined with the
use of advanced video and audio codec will offer consumers an unprecedented HD
experience. While current optical disc technologies such as DVD and DVDRAM rely
on a red laser to read and write data, the new format uses a blue-violet laser
instead, hence the name Blu-ray. Blu ray also promises some added security,
making ways for copyright protections. Blu-ray discs can have a unique ID
written on them to have copyright protection inside the recorded streams. Blu
.ray disc takes the DVD technology one step further, just by using a laser with
a nice color.
","['\nT. Ravi Kumar\n', '\nR. V. Krishnaiah\n']","10 pages International Journal of Computer Engineering and
  Applications; 2013",,http://arxiv.org/abs/1310.1551v1,cs.OH,['cs.OH'],,,[]
Emergency and Normal Navigation in Confined Spaces,http://arxiv.org/abs/1310.2886v1,2013-10-10T17:04:08Z,2013-10-10T17:04:08Z,"  Emergency navigation algorithms direct evacuees to exits when disastrous
events such as fire take place. Due to the spread of hazards, latency in
information updating and unstable flows of civilians, emergency evacuation is
absolutely a complex transshipment problem involving numerous sources and
multiple destinations. Previous algorithms which commonly need either a full
graph search or a convergence process suffer from high computational and
communication overheads. This research report surveys the current emergency
navigation algorithms and adapts the concept of Cognitive Packet Network (CPN)
to the context of emergency evacuation. By using random neural networks, the
CPN based algorithm can explore optimal routes rapidly and adaptively in a
highly dynamic emergency environment with low expense. Simultaneously, in
emergency situations there are typically different categories of evacuees such
as people of different age groups. However, current algorithms only consider
""normal"" evacuees and do not meet the specific requirements of diverse
evacuees. Our algorithms make use of the flexibility of CPN which can operate
with different user-defined goals to customize appropriate paths for each
category. The CPN algorithm is simulated in a graph based discrete-event
simulator and Dijkstra's shortest path algorithm is taken as reference. The
results show that the CPN algorithm reaches the performance of ideal
path-finding algorithm and quality of service is improved by using specific
goal functions for diverse categories of evacuees. Finally, we present a future
plan for further research.
",['\nHuibo Bi\n'],This is a internal research report with 35 pages and 6 figures,,http://arxiv.org/abs/1310.2886v1,cs.OH,['cs.OH'],,,[]
"Implementation of the Cluster Based Tunable Sleep Transistor Cell Power
  Gating Technique for a 4x4 Multiplier Circuit",http://arxiv.org/abs/1310.3203v1,2013-10-09T20:56:25Z,2013-10-09T20:56:25Z,"  A modular, programmable, and high performance Power Gating strategy, called
cluster based tunable sleep transistor cell Power Gating, has been introduced
in the present paper with a few modifications. Furthermore, a detailed
comparison of its performance with some of the other conventional Power Gating
schemes; such as Cluster Based Sleep Transistor Design (CBSTD), Distributed
Sleep Transistor Network (DSTN) etc.; has also been presented here. Considering
the constraints of power consumption, performance, and the area overhead, while
doing the actual implementation of any Power Gating scheme, it becomes
important to deal with the various design issues like the proper sizing of the
sleep transistors (STs), controlling the voltage drop (IR drop) across the STs,
and obviously maintaining a desired performance with lower amount of delay
degradation. With this notion, we tried to find out an efficient Power Gating
strategy which can reduce the overall power consumption of any CMOS circuit by
virtue of reducing the standby mode leakage current. Taking the different
performance parameters into account, for an example circuit, which is actually
the conventional 4x4 multiplier design, we found that the modified tunable
sleep transistor cell Power Gating gives very much promising results. The
reported architecture of the 4x4 multiplier with the tunable sleep transistor
cell Power Gating, is designed using 45 nm technology and it consumes
1.3638x10-5 Watt of Average Power while being operated with the nominal case of
the bit configuration word, that is, 1000. ...........
","['\nDipankar Saha\n', '\nSubhramita Basak\n', '\nSagar Mukherjee\n', '\nSayan Chatterjee\n', '\nC. K. Sarkar\n']","another version of this work can be downloaded from ""International
  Journal of Computer Applications""","International Journal of Computer Applications, Volume 66, Number
  23, pages 35-40, Year of Publication 2013",http://dx.doi.org/10.5120/11259-6539,cs.OH,['cs.OH'],10.5120/11259-6539,,[]
"Value-chain oriented identification of indicators to establish a
  comprehensive process improvement framework",http://arxiv.org/abs/1310.3230v1,2013-10-08T18:02:45Z,2013-10-08T18:02:45Z,"  The process development and optimization potential needs to be driven by the
individial coporate value chain. The identification of this specific value
chain and the related indicators is essential to limit the scope of any
analysis and optimization to the core business The process framework consisting
of clearly defined value chain, the related processes and the corresponding
indicators is a pre-requisite for a meaningful and efficient process analysis
and continuous process optimization.
",['\nUte Riemann\n'],"Process framework, value chain, business processes, indicators","International Journal of Managing Value and Supply Chains (IJMVSC)
  Vol.4, No. 3, September 2013",http://arxiv.org/abs/1310.3230v1,cs.OH,['cs.OH'],,,[]
"Optimal Energy Consumption Model for Smart Grid Households with Energy
  Storage",http://arxiv.org/abs/1310.3424v1,2013-10-12T21:14:03Z,2013-10-12T21:14:03Z,"  In this paper, we propose to model the energy consumption of smart grid
households with energy storage systems as an intertemporal trading economy.
Intertemporal trade refers to transaction of goods across time when an agent,
at any time, is faced with the option of consuming or saving with the aim of
using the savings in the future or spending the savings from the past. Smart
homes define optimal consumption as either balancing/leveling consumption such
that the utility company is presented with a uniform demand or as minimizing
consumption costs by storing energy during off-peak time periods when prices
are lower and use the stored energy during peak time periods when prices are
higher. Due to the varying nature of energy requirements of household and
market energy prices over different time periods in a day, households face a
trade-off between consuming to meet their current energy requirements and/or
storing energy for future consumption and/or spending energy stored in the
past. These trade-offs or consumption preferences of the household are modeled
as utility functions using consumer theory. We introduce two different utility
functions, one for cost minimization and another for consumption
balancing/leveling, that are maximized subject to respective budget,
consumption, storage and savings constraints to solve for the optimum
consumption profile. The optimization problem of a household with energy
storage is formulated as a geometric program for consumption
balancing/leveling, while cost minimization is formulated as a linear
programming problem. Simulation results show that the proposed model achieves
extremely low peak to average ratio in the consumption balancing/leveling
scheme with about 8% reduction in consumption costs and the least possible
amount for electricity bill with about 12% reduction in consumption costs in
the cost minimization scheme.
","['\nJayaprakash Rajasekharan\n', '\nVisa Koivunen\n']","26 pages, 9 figures, 34 equations",,http://dx.doi.org/10.1109/JSTSP.2014.2361315,cs.OH,['cs.OH'],10.1109/JSTSP.2014.2361315,,[]
E-Business Implications for Productivity and Competitiveness,http://arxiv.org/abs/1310.7962v1,2013-09-14T18:01:29Z,2013-09-14T18:01:29Z,"  Information and Communication Technology (ICT) affects to a great extent the
output and productivity growth. Evidence suggests that investment growth in ICT
has rapidly accelerated the TFP (total factor productivity) growth within the
European Union. Such progress is particularly essential for the sectors which
themselves produce new technology, but it is dispersing to other sectors, as
well. Nevertheless, decrease in ICT investment does not necessarily decline the
ICT contribution to output and productivity growth. These variations come out
from the problems related to the particular phenomenon proper assessment, but
predominantly from the companies' special requirements, as well as the
necessary adjustments of labour employed. Hence, this paper aims at estimating
the huge distinction in terms of ICT and TFB contributions to labour
productivity growth among some of the European member states, as well as the
factors which might stand behind the particular findings.
","['\nPece Mitrevski\n', '\nOlivera Kostoska\n', '\nMarjan Angeleski\n']",,"Annals of the ""Constantin Brancusi"" University of Targu - Jiu,
  Economy Series, Issue 1/2009, ISSN 1844-7007, pp. 253-262",http://arxiv.org/abs/1310.7962v1,cs.OH,['cs.OH'],,,[]
"Analytical and experimental stability investigation of a
  hardware-in-the-loop satellite docking simulator",http://arxiv.org/abs/1309.3512v1,2013-09-13T17:11:31Z,2013-09-13T17:11:31Z,"  The European Proximity Operation Simulator (EPOS) of the DLR-German Aerospace
Center is a robotics-based simulator that aims at validating and verifying a
satellite docking phase. The generic concept features a robotics tracking
system working in closed loop with a force/torque feedback signal. Inherent
delays in the tracking system combined with typical high stiffness at contact
challenge the stability of the closed-loop system. The proposed concept of
operations is hybrid: the feedback signal is a superposition of a measured
value and of a virtual value that can be tuned in order to guarantee a desired
behavior. This paper is concerned with an analytical study of the system's
closed-loop stability, and with an experimental validation of the hybrid
concept of operations in one dimension (1D). The robotics simulator is modeled
as a second-order loop-delay system and closed-form expressions for the
critical delay and associated frequency are derived as a function of the
satellites' mass and the contact dynamics stiffness and damping parameters. A
numerical illustration sheds light on the impact of the parameters on the
stability regions. A first-order Pade approximation provides additional means
of stability investigation. Experiments were performed and tests results are
described for varying values of the mass and the damping coefficients. The
empirical determination of instability is based on the coefficient of
restitution and on the observed energy. There is a very good agreement between
the critical damping values predicted by the analysis and observed during the
tests...
","['\nM. Zebenaya\n', '\nT. Boge\n', '\nR. Krenn\n', '\nD. Choukroun\n']",16 pages,,http://arxiv.org/abs/1309.3512v1,cs.OH,['cs.OH'],,,[]
"A Simple Solution To The Uncertain Delay Problem in USRP Based SDR-Radar
  Systems",http://arxiv.org/abs/1309.4843v1,2013-09-19T03:01:39Z,2013-09-19T03:01:39Z,"  We propose a simple solution to the uncertain delay problem in USRP
(Universal Software Radio Peripheral)-based SDR (Software-Defined Radio)-radar
systems. Instead of time-synchronization as employed in (pseudo-) passive radar
configurations, which require at least two synchronized receivers, we use
direct reception signal in a single receiver system as a reference to the exact
location of the target echoes. After finding the reference position, reordering
of the echoes is conducted by circular shift so that the reference moved to the
origin. We demonstrate the effectiveness of the proposed method by simulating
the problem on Matlab and implementing a 128 length random code radar on a
USRP. The random code is constructed from zero padded Barker sequence product.
Experiments on measuring multiple echoes of the targets at precise range bins
confirm the applicability of the proposed method.
",['\nAndriyan Bayu Suksmono\n'],"Presented in MITA 2013, Bali, Indonesia",,http://arxiv.org/abs/1309.4843v1,cs.OH,['cs.OH'],,,[]
Context-dependent Trust Decisions with Subjective Logic,http://arxiv.org/abs/1309.4994v1,2013-09-19T14:29:23Z,2013-09-19T14:29:23Z,"  A decision procedure implemented over a computational trust mechanism aims to
allow for decisions to be made regarding whether some entity or information
should be trusted. As recognised in the literature, trust is contextual, and we
describe how such a context often translates into a confidence level which
should be used to modify an underlying trust value. J{\o}sang's Subjective
Logic has long been used in the trust domain, and we show that its operators
are insufficient to address this problem. We therefore provide a
decision-making approach about trust which also considers the notion of
confidence (based on context) through the introduction of a new operator. In
particular, we introduce general requirements that must be respected when
combining trustworthiness and confidence degree, and demonstrate the soundness
of our new operator with respect to these properties.
","['\nFederico Cerutti\n', '\nAlice Toniolo\n', '\nNir Oren\n', '\nTimothy J. Norman\n']","19 pages, 4 figures, technical report of the University of Aberdeen
  (preprint version)",,http://arxiv.org/abs/1309.4994v1,cs.OH,['cs.OH'],,,[]
"A Calibration Algorithm for Microelectromechanical Systems
  Accelerometers in Inertial Navigation Sensors",http://arxiv.org/abs/1309.5075v1,2013-09-19T19:08:08Z,2013-09-19T19:08:08Z,"  In the present work we develop an algorithm for calibrating MEMS sensors,
which accounts for the nonorthogonality of the accelerometers' axis, as well as
for the constant bias and scaling errors. We derive an explicit formula for
computing the calibrated acceleration, given data from the sensors. We also
study the error, that is caused by the nonorthogonality of the axis.
","['\nSvetoslav Nakov\n', '\nTihomir Ivanov\n']",,,http://arxiv.org/abs/1309.5075v1,cs.OH,['cs.OH'],,,[]
"Formal Contexts, Formal Concept Analysis, and Galois Connections",http://arxiv.org/abs/1309.5134v1,2013-09-20T01:43:48Z,2013-09-20T01:43:48Z,"  Formal concept analysis (FCA) is built on a special type of Galois
connections called polarities. We present new results in formal concept
analysis and in Galois connections by presenting new Galois connection results
and then applying these to formal concept analysis. We also approach FCA from
the perspective of collections of formal contexts. Usually, when doing FCA, a
formal context is fixed. We are interested in comparing formal contexts and
asking what criteria should be used when determining when one formal context is
better than another formal context. Interestingly, we address this issue by
studying sets of polarities.
","['\nJeffrey T. Denniston\n', '\nAustin Melton\n', '\nStephen E. Rodabaugh\n']","In Proceedings Festschrift for Dave Schmidt, arXiv:1309.4557","EPTCS 129, 2013, pp. 105-120",http://dx.doi.org/10.4204/EPTCS.129.8,cs.OH,"['cs.OH', '06A15']",10.4204/EPTCS.129.8,,[]
"Multiparameter Monitoring and Fault Indication Using Inductive Power
  Transfer System",http://arxiv.org/abs/1309.0589v1,2013-09-03T05:34:43Z,2013-09-03T05:34:43Z,"  The paper aims at demonstrating communication capabilities of IPT. For this
data communication is performed between two modules using the concept of IPT.
IPT was deemed to be the best solution to the system houses a multi parameter
acquisition module such as temperature, speed, voltage, current and data
transfer from the motor. The receiver side is another microcontroller coupled
to an inductive coil that gets the data and displays in the LCD. A brief
background to IPT Inductive Power Transfer technology and its applications is
given and the design criteria for the paper are defined in detail. To be
accurate, IPT data communication helps to reduce unnecessary wire connections
and data is transmitted without any touch. Further the paper can be enhanced by
looking for fault analysis inside the motor. This can be done by analyzing
various parameters of the motor. A novel two-way IPT communication system was
designed, which worked on the concept of pulsing the system on and off to send
data serially. The paper involves transmission of data through inductive flux
without any contact between the two modules. Further as no frequency tunings or
any calibration is required between different modules a single system can be
used with multiple clients. This reduces a lot of hazards such as interference
with other modules and RF transmitters in the vicinity.
","['\nK. P. Shaji\n', '\nI. Alsheba\n', '\nY. A. Syed Khadar\n', '\nS. Kannan\n']",6 Pages,,http://arxiv.org/abs/1309.0589v1,cs.OH,['cs.OH'],,,[]
Survey of Insurance Fraud Detection Using Data Mining Techniques,http://arxiv.org/abs/1309.0806v1,2013-09-03T06:49:43Z,2013-09-03T06:49:43Z,"  With an increase in financial accounting fraud in the current economic
scenario experienced, financial accounting fraud detection has become an
emerging topics of great importance for academics, research and industries.
Financial fraud is a deliberate act that is contrary to law, rule or policy
with intent to obtain unauthorized financial benefit and intentional
misstatements or omission of amounts by deceiving users of financial
statements, especially investors and creditors. Data mining techniques are
providing great aid in financial accounting fraud detection, since dealing with
the large data volumes and complexities of financial data are big challenges
for forensic accounting. Financial fraud can be classified into four: bank
fraud, insurance fraud, securities and commodities fraud. Fraud is nothing but
wrongful or criminal trick planned to result in financial or personal gains.
This paper describes the more details on insurance sector related frauds and
related solutions. In finance, insurance sector is doing important role and
also it is unavoidable sector of every human being.
","['\nH. Lookman Sithic\n', '\nT. Balasubramanian\n']",4 pages,,http://arxiv.org/abs/1309.0806v1,cs.OH,['cs.OH'],,,[]
"Design and Implementation of Wireless Energy Meter System for Monitoring
  the Single Phase Supply",http://arxiv.org/abs/1309.1832v1,2013-09-07T07:48:51Z,2013-09-07T07:48:51Z,"  Wireless energy meter is a system developed to serve as a basic single-phase
energy meter with advanced functionalities such as Peak hour setting, Peak load
setting Wireless reading transmission; further the system eliminates the role
of a Meter Reader.
",['\nPrashanth B. U. V\n'],"4 pages, 10 figures, journal","IJCA 41(2):26-29,March 2012",http://dx.doi.org/10.5120/5514-7511,cs.OH,['cs.OH'],10.5120/5514-7511,,[]
"DyPS: Dynamic Processor Switching for Energy-Aware Video Decoding on
  Multi-core SoCs",http://arxiv.org/abs/1309.2387v1,2013-09-10T06:48:41Z,2013-09-10T06:48:41Z,"  In addition to General Purpose Processors (GPP), Multicore SoCs equipping
modern mobile devices contain specialized Digital Signal Processor designed
with the aim to provide better performance and low energy consumption
properties. However, the experimental measurements we have achieved revealed
that system overhead, in case of DSP video decoding, causes drastic
performances drop and energy efficiency as compared to the GPP decoding. This
paper describes DyPS, a new approach for energy-aware processor switching (GPP
or DSP) according to the video quality . We show the pertinence of our solution
in the context of adaptive video decoding and describe an implementation on an
embedded Linux operating system with the help of the GStreamer framework. A
simple case study showed that DyPS achieves 30% energy saving while sustaining
the decoding performance
","['\nYahia Benmoussa\nLab-STICC\n', '\nJalil Boukhobza\nLab-STICC\n', '\nEric Senn\nLab-STICC\n', '\nDjamel Benazzouz\nMSS\n', '\nYassine Hadjadj-Aoul\nINRIA - IRISA\n']",,"EWiLi, the Embedded Operating Systems Workshop, Toulouse : France
  (2013)",http://arxiv.org/abs/1309.2387v1,cs.OH,['cs.OH'],,,"['Lab-STICC', 'Lab-STICC', 'Lab-STICC', 'MSS', 'INRIA - IRISA']"
Inadmissible Class of Boolean Functions under Stuck-at Faults,http://arxiv.org/abs/1309.3993v1,2013-09-02T08:01:55Z,2013-09-02T08:01:55Z,"  Many underlying structural and functional factors that determine the fault
behavior of a combinational network, are not yet fully understood. In this
paper, we show that there exists a large class of Boolean functions, called
root functions, which can never appear as faulty response in irredundant
two-level circuits even when any arbitrary multiple stuck-at faults are
injected. Conversely, we show that any other Boolean function can appear as a
faulty response from an irredundant realization of some root function under
certain stuck-at faults. We characterize this new class of functions and show
that for n variables, their number is exactly equal to the number of
independent dominating sets (Harary and Livingston, Appl. Math. Lett., 1993) in
a Boolean n-cube. We report some bounds and enumerate the total number of root
functions up to 6 variables. Finally, we point out several open problems and
possible applications of root functions in logic design and testing.
","['\nDebesh K. Das\n', '\nDebabani Chowdhury\n', '\nBhargab B. Bhattacharya\n', '\nTsutomu Sasao\n']",,,http://arxiv.org/abs/1309.3993v1,cs.OH,['cs.OH'],,,[]
Human Resource Management System,http://arxiv.org/abs/1309.5351v1,2013-09-02T13:11:42Z,2013-09-02T13:11:42Z,"  The paper titled HUMAN RESOURCE MANAGEMENT SYSTEM is basically concerned with
managing the Administrator of HUMAN RESOURCE Department in a company. A Human
Resource Management System, refers to the systems and processes at the
intersection between human resource management and information technology. It
merges HRM as a discipline and in particular its basic HR activities and
processes with the information technology field, whereas the programming of
data processing systems evolved into standardized routines and packages of
enterprise resource planning software. The main objective of this paper is to
reduce the effort of Administrator to keep the daily events such as attendance,
projects, works, appointments, etc. This paper deals with the process of
identifying the employees, recording their attendance hourly and calculating
their effective payable hours or days. This paper should maintain the records
of each and every employee and their time spend in to company, which can be
used for performance appraisal. Based on that transfer, removal, promotion can
be done.
","['\nA. S. Syed Navaz\n', '\nA. S. Syed Fiaz\n', '\nC. Prabhadevi\n', '\nV. Sangeetha\n', '\nS. Gopalakrishnan\n']",10 Pages,,http://arxiv.org/abs/1309.5351v1,cs.OH,['cs.OH'],,,[]
Condition-Based Maintenance using Sensor Arrays and Telematics,http://arxiv.org/abs/1309.1921v1,2013-09-08T02:10:33Z,2013-09-08T02:10:33Z,"  Emergence of uniquely addressable embeddable devices has raised the bar on
Telematics capabilities. Though the technology itself is not new, its
application has been quite limited until now. Sensor based telematics
technologies generate volumes of data that are orders of magnitude larger than
what operators have dealt with previously. Real-time big data computation
capabilities have opened the flood gates for creating new predictive analytics
capabilities into an otherwise simple data log systems, enabling real-time
control and monitoring to take preventive action in case of any anomalies.
Condition-based-maintenance, usage-based-insurance, smart metering and
demand-based load generation etc. are some of the predictive analytics use
cases for Telematics. This paper presents the approach of condition-based
maintenance using real-time sensor monitoring, Telematics and predictive data
analytics.
",['\nGopalakrishna Palem\n'],,"International Journal of Mobile Network Communications &
  Telematics. 06 (2013) 3(3):19-28",http://dx.doi.org/10.5121/ijmnct.2013.3303,cs.OH,"['cs.OH', 'physics.data-an']",10.5121/ijmnct.2013.3303,,[]
"Demodulation of Sparse PPM Signals with Low Samples Using Trained RIP
  Matrix",http://arxiv.org/abs/1309.5854v1,2013-09-01T22:14:52Z,2013-09-01T22:14:52Z,"  Compressed sensing (CS) theory considers the restricted isometry property
(RIP) as a sufficient condition for measurement matrix which guarantees the
recovery of any sparse signal from its compressed measurements. The RIP
condition also preserves enough information for classification of sparse
symbols, even with fewer measurements. In this work, we utilize RIP bound as
the cost function for training a simple neural network in order to exploit the
near optimal measurements or equivalently near optimal features for
classification of a known set of sparse symbols. As an example, we consider
demodulation of pulse position modulation (PPM) signals. The results indicate
that the proposed method has much better performance than the random
measurements and requires less samples than the optimum matched filter
demodulator, at the expense of some performance loss. Further, the proposed
approach does not need equalizer for multipath channels in contrast to the
conventional receiver.
","['\nSeyed Hossein Hosseini\n', '\nMahrokh G. Shayesteh\n', '\nMehdi Chehel Amirani\n']","4 pages, 6 figures, conference paper",,http://arxiv.org/abs/1309.5854v1,cs.OH,"['cs.OH', 'cs.IT', 'cs.LG', 'math.IT']",,,[]
Services in Android can Share Your Personal Information in Background,http://arxiv.org/abs/1308.4486v1,2013-08-21T05:05:58Z,2013-08-21T05:05:58Z,"  Mobile phones have traveled a very long journey in a very short span of time
since its inception in 1973.This wonderful toy of 20th century has started
playing significant role in daily life.More than 5 billion mobile users are
there around the world and almost 90 percent of the entire earth is under the
mobile coverage now.These days smart phones are equipped with numerous
features,faster processors and high storage capacity.Android is a latest trend
in this series whose popularity is growing by leaps and bounds.Android has a
number of components which helps Application developers to embed distinguish
features in applications.This paper explains how the Service component of
Android can share your personal information to others without users
interaction.
","['\nManoj Kumar\n', '\nSheshendra Rathi\n']","4 pages, 3 figures","(IJCSIT) International Journal of Computer Science and Information
  Technologies, Vol. 2 (5) , 2011, 2356-2359 ISSN:0975-9646",http://arxiv.org/abs/1308.4486v1,cs.OH,['cs.OH'],,,[]
How to implement Marketing 2.0 Successfully,http://arxiv.org/abs/1308.4894v1,2013-08-22T15:15:59Z,2013-08-22T15:15:59Z,"  The purpose of this research is to develop a model that would close the gap
between marketing plans and strategies from one side and the advanced online
collaboration applications platforms known as WEB 2.0 in order to implement
marketing 2.0 smoothly without disrupting the working environment. We started
by examining published articles related to marketing, Web 2.0, Customer
Relationship Management Systems, CRM, and social media in a step to conduct an
extensive review of the available literature. Then, we presented critique of
the articles we have examined. After that, we have been able to develop the
model we are proposing in this research. As this paper shows, the proposed
model will help in transforming marketing plans and strategies from its
traditional approach into, what we would like to call, marketing 2.0 approach
smoothly. There are some unavoidable limitations due to the given time and
scope constrains. The factors included in the proposed model does not cover
every related aspect, however, they cover the most important ones.
","['\nAbdulrahman Aldhaheri\n', '\nChristian Bach\n']",,"International Journal of Business and Social Science, vol. 4, no.
  10, August 2013",http://arxiv.org/abs/1308.4894v1,cs.OH,['cs.OH'],,,[]
"Numerical Analysis of Gate Conflict Duration and Passenger Transit Time
  in Airport",http://arxiv.org/abs/1308.6217v1,2013-08-28T16:55:24Z,2013-08-28T16:55:24Z,"  Robustness is as important as efficiency in air transportation. All
components in the air traffic system are connected to form an interactive
network. So, a disturbance that occurs in one component, for example, a severe
delay at an airport, can influence the entire network. Delays are easily
propagated between flights through gates, but the propagation can be reduced if
gate assignments are robust against stochastic delays. In this paper, we
analyze gate delays and suggest an approach that involves assigning gates while
making them robust against stochastic delays. We extract an example flight
schedule from data source and generate schedules with increased traffic to
analyze how the compact flight schedules impact the robustness of gate
assignment. Simulation results show that our approach improves the robustness
of gate assignment. Particularly, the robust gate assignment reduces average
duration of gate conflicts by 96.3% and the number of gate conflicts by 96.7%
compared to the baseline assignment. However, the robust gate assignment
results in longer transit time for passengers, and a trade-off between the
robustness of gate assignment and passenger transit time is presented.
","['\nSang Hyun Kim\n', '\nEric Feron\n']","Submitted to Transportation Research Part B, and presented at AIAA
  Guidance, Navigation, and Control Conference in 2011 in part",,http://dx.doi.org/10.2514/1.D0067,cs.OH,['cs.OH'],10.2514/1.D0067,,[]
"Implementation and optimization of Wavelet modulation in Additive
  Gaussian channels",http://arxiv.org/abs/1308.6251v1,2013-08-25T20:27:53Z,2013-08-25T20:27:53Z,"  In this paper, we investigate the implementation of wavelet modulation (WM)
in a digital communication system and propose novel methods to improve its
performance. We will put particular focus on the structure of an optimal
detector in AWGN channels and address two main methods for inserting the
samples of the message signal in different frequency layers. Finally, computer
based algorithms are described in order to implement and optimize receivers and
transmitters.
","['\nRad Niazadeh\n', '\nSahar Nassirpour\n', '\nMohammad B. Shamsollahi\n']",,"Advanced Communication Technology, 2009. ICACT 2009. 11th
  International Conference on, Pages 1940 - 1943",http://arxiv.org/abs/1308.6251v1,cs.OH,['cs.OH'],,,[]
3D Printing for Math Professors and Their Students,http://arxiv.org/abs/1308.3420v2,2013-08-13T01:32:22Z,2013-10-06T15:26:48Z,"  In this primer, we will describe a number of projects that can be completed
with a 3D printer, particularly by mathematics professors and their students.
For many of the projects, we will utilize Mathematica to design objects that
mathematicians may be interested in printing. Included in the projects that are
described is a method to acquire data from an XBox Kinect.
","['\nEdward Aboufadel\n', '\nSylvanna V. Krawczyk\n', '\nMelissa Sherman-Bennett\n']","18 pages, 6 figures. Version 1.1",,http://arxiv.org/abs/1308.3420v2,math.HO,"['math.HO', 'cs.OH']",,,[]
"An Integrated Geographic Information System and Marketing Information
  System Model",http://arxiv.org/abs/1307.7787v1,2013-07-30T02:33:30Z,2013-07-30T02:33:30Z,"  Maintaining competitive advantage is significant in this present day of
globalization, knowledge management and enormous economic activities. An
organization's future developments are influenced by its managements'
decisions. Businesses today are facing a lot of challenges in terms of
competition and they have to be in the lead by strengthening their research and
development strategies with the aid of cutting edge technologies. Hence
marketing intelligence is now a key to the success of any business in today's
rapidly changing business environment. With all the technologies available in
marketing research, businesses still struggle with how to gather information
and make decisions in a short time and real-time about their customers' needs
and purchasing patterns in various geographical areas.
  This paper is set out to contribute to the body of knowledge in the area of
the application of Geographic Information Systems technology solutions to
businesses by developing a model for integrating Geographic Information Systems
into existing Marketing Information Systems for effective marketing research.
This model will interconnect organizations at the highest levels, providing
reassurance to enable broad scope of checks and balances as well as benefiting
many business activities including operational, tactical and strategic decision
making due to its analytical and solution driven functions.
","['\nQuist-Aphetsi Kester\n', '\n Koumadi\n', '\nKoudjo M\n', '\nNii Narku Quaynor\n']","6 pages. International Journal of Advanced Technology & Engineering
  Research (IJATER) Volume 2 issue 6, 2013","International Journal of Advanced Technology and Engineering
  Research Vol. 2 Issue 6 (2013)",http://arxiv.org/abs/1307.7787v1,cs.OH,['cs.OH'],,,[]
Complexity Analysis in Cyclic Tag System Emulated by Rule 110,http://arxiv.org/abs/1307.7951v1,2013-07-30T12:53:54Z,2013-07-30T12:53:54Z,"  It is known that elementary cellular automaton rule 110 is capable of
supporting universal computation by emulating cyclic tag system. Since the
whole information necessary to perform computation is stored in the
configuration, it is reasonable to investigate the complexity of configuration
for the analysis of computing process. In this research we employed Lempel-Ziv
complexity as a measure of complexity and calculated it during the evolution of
emulating cyclic tag system by rule 110. As a result, we observed the stepwise
decline of complexity during the evolution. That is caused by the
transformation from table data to moving data and the elimination of table data
by a rejector.
","['\nShigeru Ninagawa\n', '\nGenaro J. Martínez\n']","AUTOMATA 2013: 19th International Workshop on Cellular Automata and
  Discrete Complex Systems, Universit\""at Giessen, Germany, September 17-19,
  2013",,http://arxiv.org/abs/1307.7951v1,cs.OH,['cs.OH'],,,[]
Improved Median Polish Kriging for Simulation Metamodeling,http://arxiv.org/abs/1307.8172v1,2013-07-30T23:48:02Z,2013-07-30T23:48:02Z,"  In simulation, Median Polish Kriging is a technique used to predict
unobserved data points in two-dimensional space. The linear behavior of the
traditional Median Polish Kriging in the estimation of the mean function in a
high grid makes the interpolation of O(1) which has a low order in the
prediction and that leads to a high prediction error. Therefore, an improvement
in the estimation of the mean function has been introduced using Biharmonic
spline interpolation and the new technique has been called Improved Median
Polish Kriging (IMPK). The IMPK has been applied to the standard coal-ash data
in two-dimension. The novel method gave much better results according to the
cross validation results that were obtained when compared with the traditional
Median Polish Kriging.
","['\nFiras Al Rekabi\n', '\nAsim El Sheikh\n']",,"International Journal of Information and Communication Technology
  Research, Volume 3 No. 5, pp. 159-165, May 2013",http://arxiv.org/abs/1307.8172v1,cs.OH,['cs.OH'],,,[]
"Sistem Informasi Penjualan Dan Perbaikan Komputer (Studi Kasus: CV
  Computer Plus Palembang)",http://arxiv.org/abs/1307.8191v1,2013-07-31T02:06:00Z,2013-07-31T02:06:00Z,"  The purpose of this research is to develop an Information System of Selling
and Services using Microsoft Visual Basic and Microsoft Access for it database.
The benefits of this research is to help CV Computer Plus in selling and
services data processing everyday. To develop this IS is used 5 (five) steps:
1) Planning, 2) Analysis, 3) Design, 4) Implementation, and 5) Evaluation. The
Information System can record the selling and services data, it also prepared
usefull reports. By using this IS, CV Computer Plus can operate their selling
and services efficiency and effectively. In the future it can be upgraded for
network application.
","['\n Syaprina\n', '\nLeon Andretti Abdillah\n', '\nNyimas Sopiah\n']","Syaprina; Abdillah, L.A.; Sopiah, N., ""Sistem Informasi penjualan dan
  perbaikan komputer,"" Jurnal Ilmiah MATRIK, vol. 10, pp. 113-124, 2008",MATRIK. 10 (2008) 113-124,http://arxiv.org/abs/1307.8191v1,cs.OH,['cs.OH'],,,[]
On Two Conversion Methods of Decimal-to-Binary,http://arxiv.org/abs/1308.0555v1,2013-08-01T06:15:23Z,2013-08-01T06:15:23Z,"  Decimal-to-binary conversion is important to modern binary computers. The
classical method to solve this problem is based on division operation. In this
paper, we investigate a decimal-to-binary conversion method based on addition
operation. The method is very easily implemented by software. The cost analysis
shows that the latter is more preferable than the classical method. Thus the
current Input/Output translation hardware to convert between the internal digit
pairs and the external standard BCD codes can be reasonably removed.
",['\nZhengjun Cao\n'],,,http://arxiv.org/abs/1308.0555v1,cs.OH,['cs.OH'],,,[]
Introduction to Management Information system,http://arxiv.org/abs/1308.1797v2,2013-08-08T09:31:55Z,2013-08-16T08:27:43Z,"  A Management Information System (MIS) is a systematic organization and
presentation of information that is generally required by the management of an
organization for taking better decisions for the organization. The MIS data may
be derived from various units of the organization or from other sources.
However it is very difficult to say the exact structure of MIS as the structure
and goals of different types of organizations are different. Hence both the
data and structure of MIS is dependent on the type of organization and often
customized to the specific requirement of the management.
",['\nUmakant Mishra\n'],"11 pages, 5 figures",,http://arxiv.org/abs/1308.1797v2,cs.OH,['cs.OH'],,,[]
"A New Mattress Development Based on Pressure Sensors for Body-contouring
  Uniform Support",http://arxiv.org/abs/1308.2196v1,2013-08-08T17:31:55Z,2013-08-08T17:31:55Z,"  For getting good sleep quality, an improved approach of new mattress
development based on the pressure sensors for body-contouring uniform support
is proposed in this paper. This method solved the problems of innerspring
mattresses that cannot allow body-contouring uniform support, and foam
mattresses that cannot provide everybody equal comfort from the same mattress.
By the buried pressure sensor array and actuator array in foam layer of a
mattress, both are connected to a controller to generate the pressure
distribution mapping of a human body on the mattress, then from the data of
this mapping, some of the actuators are driven up or down by the controller to
generate a body-contouring uniform support. By the aid of mathematical
morphology algorithms, user can also choose a different support mode by another
wireless controller with touch-screen to accommodate personal favorite firmness
of the mattress and to take his tensed mood and pressure off with good sleep
until daylight. Moreover, some other homecare functions, such as temperature
measurement, sleep on posture correction and fall down prevention, can approach
by additional hardware and software as user requirement in the future.
","['\nHsiu-Chen Hsu\n', '\nRong-Chin Lo\n']",,,http://arxiv.org/abs/1308.2196v1,cs.OH,['cs.OH'],,,[]
Forecasting Intermittent Demand by Hyperbolic-Exponential Smoothing,http://arxiv.org/abs/1307.6102v2,2013-07-23T14:36:09Z,2014-09-03T09:23:52Z,"  Croston's method is generally viewed as superior to exponential smoothing
when demand is intermittent, but it has the drawbacks of bias and an inability
to deal with obsolescence, in which an item's demand ceases altogether. Several
variants have been reported, some of which are unbiased on certain types of
demand, but only one recent variant addresses the problem of obsolescence. We
describe a new hybrid of Croston's method and Bayesian inference called
Hyperbolic-Exponential Smoothing, which is unbiased on non-intermittent and
stochastic intermittent demand, decays hyperbolically when obsolescence occurs
and performs well in experiments.
","['\nS. D. Prestwich\n', '\nS. A. Tarim\n', '\nR. Rossi\n', '\nB. Hnich\n']","Earlier versions of this work were presented at the 25th European
  Conference on Operations Research, 2012; and at the 54th Annual Conference of
  the UK Operational Research Society, 2012. A journal version is in
  preparation","International Journal of Forecasting, Elsevier, 30(4):928-933,
  2014",http://dx.doi.org/10.1016/j.ijforecast.2014.01.006,cs.OH,['cs.OH'],10.1016/j.ijforecast.2014.01.006,,[]
"Abstract Geometrical Computation 8: Small Machines, Accumulations and
  Rationality",http://arxiv.org/abs/1307.6468v1,2013-07-24T15:54:14Z,2013-07-24T15:54:14Z,"  In the context of abstract geometrical computation, computing with colored
line segments, we study the possibility of having an accumulation with small
signal machines, ie, signal machines having only a very limited number of
distinct speeds. The cases of 2 and 4 speeds are trivial: we provide a proof
that no machine can produce an accumulation in the case of 2 speeds and exhibit
an accumulation with 4 speeds. The main result is the twofold case of 3 speeds.
On the one hand, we prove that accumulations cannot happen when all ratios
between speeds and all ratios between initial distances are rational. On the
other hand, we provide examples of an accumulation in the case of an irrational
ratio between 2 speeds and in the case of an irrational ratio between two
distances in the initial configuration. This dichotomy is explained by the
presence of a phenomenon computing Euclid's algorithm (gcd): it stops if and
only if its input is commensurate (ie, of rational ratio).
","['\nFlorent Becker\n', '\nMathieu Chapelle\n', '\nJérôme Durand-Lose\n', '\nVincent Levorato\n', '\nMaxime Senot\n']",,,http://arxiv.org/abs/1307.6468v1,cs.OH,['cs.OH'],,,[]
"Design of One-Dimensional Linear Phase Digital IIR Filters Using
  Orthogonal Polynomials",http://arxiv.org/abs/1307.2817v1,2013-07-10T15:05:22Z,2013-07-10T15:05:22Z,"  In the present paper, we discuss a method to design a linear phase
1-dimensional Infinite Impulse Response (IIR) filter using orthogonal
polynomials. The filter is designed using a set of object functions. These
object functions are realized using a set of orthogonal polynomials. The method
includes placement of zeros and poles in such a way that the amplitude
characteristics are not changed while we change the phase characteristics of
the resulting IIR filter.
","['\nVinay Kumar\n', '\nSunil Bhooshan\n']","8 pages, 11 figures","ISRN Signal Processing, vol. 2012, Article ID 870276, 7 pages,
  2012",http://dx.doi.org/10.5402/2012/870276,cs.OH,['cs.OH'],10.5402/2012/870276,,[]
CMOS Low Power Cell Library For Digital Design,http://arxiv.org/abs/1307.3017v1,2013-07-11T08:48:13Z,2013-07-11T08:48:13Z,"  Historically, VLSI designers have focused on increasing the speed and
reducing the area of digital systems. However, the evolution of portable
systems and advanced Deep Sub-Micron fabrication technologies have brought
power dissipation as another critical design factor. Low power design reduces
cooling cost and increases reliability especially for high density systems.
Moreover, it reduces the weight and size of portable devices. The power
dissipation in CMOS circuits consists of static and dynamic components. Since
dynamic power is proportional to V2 dd and static power is proportional to Vdd,
lowering the supply voltage and device dimensions, the transistor threshold
voltage also has to be scaled down to achieve the required performance. In case
of static power, the power is consumed during the steady state condition i.e
when there are no input/output transitions. Static power has two sources: DC
power and Leakage power. Consecutively to facilitate voltage scaling without
disturbing the performance, threshold voltage has to be minimized. Furthermore
it leads to better noise margins and helps to avoid the hot carrier effects in
short channel devices. In this paper we have been proposed the new CMOS library
for the complex digital design using scaling the supply voltage and device
dimensions and also suggest the methods to control the leakage current to
obtain the minimum power dissipation at optimum value of supply voltage and
transistor threshold. In this paper CMOS Cell library has been implemented
using TSMC (0.18um) and TSMC (90nm) technology using HEP2 tool of IC designing
from Mentor Graphics for various analysis and simulations.
","['\nKanika Kaur\n', '\nArti Noor\n']",9 pages,,http://arxiv.org/abs/1307.3017v1,cs.OH,['cs.OH'],,,[]
Design and Implementation of Car Parking System on FPGA,http://arxiv.org/abs/1307.3051v1,2013-07-11T10:53:37Z,2013-07-11T10:53:37Z,"  As, the number of vehicles are increased day by day in rapid manner. It
causes the problem of traffic congestion, pollution (noise and air). To
overcome this problem A FPGA based parking system has been proposed. In this
paper, parking system is implemented using Finite State Machine modelling. The
system has two main modules i.e. identification module and slot checking
module. Identification module identifies the visitor. Slot checking module
checks the slot status. These modules are modeled in HDL and implemented on
FPGA. A prototype of parking system is designed with various interfaces like
sensor interfacing, stepper motor and LCD.
","['\nRamneet Kaur\n', '\nBalwinder Singh\n']",,,http://arxiv.org/abs/1307.3051v1,cs.OH,['cs.OH'],,,[]
Low Power Dual Edge-Triggered Static D Flip-Flop,http://arxiv.org/abs/1307.3075v2,2013-07-11T11:53:40Z,2014-12-08T10:19:59Z,"  This paper enumerates new architecture of low power dual-edge triggered
Flip-Flop (DETFF) designed at 180nm CMOS technology. In DETFF same data
throughput can be achieved with half of the clock frequency as compared to
single edge triggered Flip-Flop (SETFF). In this paper conventional and
proposed DETFF are presented and compared at same simulation conditions. The
post layout experimental results comparison shows that the average power
dissipation is improved by 48.17%, 41.29% and 36.84% when compared with SCDFF,
DEPFF and SEDNIFF respectively and improvement in PDP is 42.44%, 33.88% and
24.69% as compared to SCDFF, DEPFF and SEDNIFF respectively. Therefore the
proposed DETFF design is suitable for low power and small area applications.
","['\n Anurag\n', '\nGurmohan Singh\n', '\nV. Sulochana\n']","Dual-Edge Triggered, Flip-Flop, High Speed, Low Power, Static D
  Flip-Flop","International Journal of VLSI design & Communication Systems
  (VLSICS) Vol.4, No.3, June 2013",http://dx.doi.org/10.5121/vlsic.2013.4303,cs.OH,['cs.OH'],10.5121/vlsic.2013.4303,,[]
"Solving the Parity Problem with Rule 60 in Array Size of the Power of
  Two",http://arxiv.org/abs/1307.3888v1,2013-07-15T11:07:12Z,2013-07-15T11:07:12Z,"  In the parity problem, a given cellular automaton has to classify any initial
configuration into two classes according to its parity. Elementary cellular
automaton rule 60 can solve the parity problem in periodic boundary conditions
with array size of the power of two. The spectral analysis of the
configurations of rule 60 at each time step in the evolution reveals that
spatial periodicity emerges as the evolution proceeds and the patterns with
longer period split into the ones with shorter period. This phenomenon is
analogous to the cascade process in which large scale eddies split into smaller
ones in turbulence. By measuring the Lempel-Ziv complexity of configuration, we
found the stepping decrease of the complexity during the evolution. This result
might imply that a decision problem solving process is accompanied with the
decline of complexity of configuration.
",['\nShigeru Ninagawa\n'],,,http://arxiv.org/abs/1307.3888v1,cs.OH,"['cs.OH', 'nlin.CG']",,,[]
"Dipole-Loaded Monopole Optimized Using VSO, v.3",http://arxiv.org/abs/1307.0220v3,2013-06-30T16:54:24Z,2013-07-06T13:55:47Z,"  A dipole-loaded monopole antenna is optimized for uniform hemispherical
coverage using VSO, a new global search design and optimization algorithm. The
antenna's performance is compared to genetic algorithm and hill-climber
optimized loaded monopoles, and VSO is tested against two suites of benchmark
functions and several other algorithms.
",['\nRichard A. Formato\n'],"arXiv admin note: substantial text overlap with arXiv:1107.1437,
  arXiv:1103.5629, arXiv:1108.0901, arXiv:1003.1039. Version 2, 02 Jul 2013:
  minor typos corrected; hill climber material added; source code listing
  updated. Version 3, 06 Jul 2013: replaces VSO diagram/pseudocode to clarify
  algorithm's elitist nature; other minor changes",,http://arxiv.org/abs/1307.0220v3,cs.OH,['cs.OH'],,,[]
How to Build an RSS Feed using ASP,http://arxiv.org/abs/1307.0772v1,2013-07-02T17:33:37Z,2013-07-02T17:33:37Z,"  RSS is a XML based format. The Current popular version of RSS is RSS version
2.0. The purpose of adding an RSS feed to your site is to show if anything new
is added to the site. For example, if a new article or blog or news item is
added to your site that should automatically appear in the RSS feed so that the
visitors/ RSS readers will automatically get updated about this new addition.
The RSS feed is also called RSS channel.
  There are two main elements of the RSS XML file, one is the header or channel
element that describes the details about the site/feeder and other is the body
or item element that describes the consists of individual articles/entries
updated in the site. As the format of the RSS feed file is pretty simple, it
can be coded in any language, ASP, PHP or anything of that sort. We will build
an RSS feeder using classical ASP (Active Server Pages) code in this article.
",['\nUmakant Mishra\n'],"11 pages, 1 figure",,http://arxiv.org/abs/1307.0772v1,cs.OH,['cs.OH'],,,[]
Wireless sensor network technology for moisture monitoring of wood,http://arxiv.org/abs/1307.0952v1,2013-07-03T09:48:08Z,2013-07-03T09:48:08Z,"  Leaks represent a very important hazard for the buildings and they can affect
all sorts of building materials and specially wood due to its hygroscopic
properties. Excessive moisture content can affect in a negative way building
processes such as the installation of wooden floors or the use of wood as a
structural material. Moisture meters can provide prompt and non-destructive
determination of wood moisture, and as such are among the most useful tools
available to wood products manufacturers and scientists. However, a continuous
monitoring system is needed in order to avoid excessive moisture content which
can damage wooden floors as well as structural wood. Data and procedures are
presented in order to develop a suitable monitoring tool based on wireless
sensor networks to provide an electronic tool of active security both for the
installation of wooden floors and for the proper maintenance of existent
buildings which have a timber structure.
","['\nIvan Arakistain\n', '\nJose Miguel Abascal\n', '\nOriol Munne\n']",,,http://arxiv.org/abs/1307.0952v1,cs.OH,['cs.OH'],,,[]
Machining of complex-shaped parts with guidance curves,http://arxiv.org/abs/1307.1215v1,2013-07-04T06:19:13Z,2013-07-04T06:19:13Z,"  Nowadays, high-speed machining is usually used for production of hardened
material parts with complex shapes such as dies and molds. In such parts, tool
paths generated for bottom machining feature with the conventional parallel
plane strategy induced many feed rate reductions, especially when boundaries of
the feature have a lot of curvatures and are not parallel. Several machining
experiments on hardened material lead to the conclusion that a tool path
implying stable cutting conditions might guarantee a better part surface
integrity. To ensure this stability, the shape machined must be decomposed when
conventional strategies are not suitable. In this paper, an experimental
approach based on high-speed performance simulation is conducted on a master
bottom machining feature in order to highlight the influence of the curvatures
towards a suitable decomposition of machining area. The decomposition is
achieved through the construction of intermediate curves between the closed
boundaries of the feature. These intermediate curves are used as guidance curve
for the tool paths generation with an alternative machining strategy called
""guidance curve strategy"". For the construction of intermediate curves, key
parameters reflecting the influence of their proximity with each closed
boundary and the influence of the curvatures of this latter are introduced.
Based on the results, a method for defining guidance curves in four steps is
proposed.
","['\nLaurent Tapie\nIRCCyN\n', '\nBernardin Mawussi\nIRCCyN\n', '\nWalter Rubio\nIRCCyN\n', '\nBenoît Furet\nIRCCyN\n']",,"International Journal of Advanced Manufacturing Technology (2013)
  1-11",http://arxiv.org/abs/1307.1215v1,cs.OH,['cs.OH'],,,"['IRCCyN', 'IRCCyN', 'IRCCyN', 'IRCCyN']"
"A Comparative study of Analog and digital Controller On DC/DC Buck-Boost
  Converter Four Switch for Mobile Device Applications",http://arxiv.org/abs/1306.5180v1,2013-06-20T18:07:15Z,2013-06-20T18:07:15Z,"  This paper presents comparative performance between Analog and digital
controller on DC/DC buck-boost converter four switch. The design of power
electronic converter circuit with the use of closed loop scheme needs modeling
and then simulating the converter using the modeled equations. This can easily
be done with the help of state equations and MATLAB/SIMULINK as a tool for
simulation of those state equations. DC/DC Buckboost converter in this study is
operated in buck (step-down) and boost (step-up) modes.
","['\nBenlafkih Abdessamad\n', '\nKrit Salah-ddine\n', '\nChafik Elidrissi Mohamed\n']","6 pages, IJCSI International Journal of Computer Science Issues, Vol.
  10, Issue 1, No 2, January 2013",,http://arxiv.org/abs/1306.5180v1,cs.OH,"['cs.OH', 'B.1.2']",,,[]
"A Compact Dual Band Dielectric Resonator Antenna For Wireless
  Applications",http://arxiv.org/abs/1306.1335v1,2013-06-06T08:21:18Z,2013-06-06T08:21:18Z,"  This paper presents the design of a dual band rectangular Dielectric
Resonator Antenna (DRA) coupled to narrow slot aperture that is fed by
microstrip line. The fundamental TE111 mode and higher-order TE113 mode are
excited with their resonant frequencies respectively. These frequencies can be
controlled by changing the DRA dimensions. A dielectric resonator with high
permittivity is used to miniaturize the global structure. The proposed antenna
is designed to have dual band operation suitable for both DCS (1710 - 1880 MHz)
and WLAN (2400 - 2484 MHz) applications. The return loss, radiation pattern and
gain of the proposed antenna are evaluated. Reasonable agreement between
simulation and experimental results is obtained.
","['\nH. Raggad\n', '\nM. Latrach\n', '\nA. Gharsallah\n', '\nT. Razban\n']",IJCNC 2013,,http://arxiv.org/abs/1306.1335v1,cs.OH,['cs.OH'],,,[]
Stable equilibrium study cascaded one bit sigma-delta modulator,http://arxiv.org/abs/1306.1702v1,2013-06-07T12:29:52Z,2013-06-07T12:29:52Z,"  In the paper defines a boundary of stability zone for sigma-delta modulator.
The boundary depends from inner sigma-delta modulator coefficients. For
designing purposes such result could be used to find or compare some
appropriate schemes with each other. It is proved some statements and showed
that boundary could be found theoretically for any order of sigma-delta
modulator, but practically till 5-th order.
",['\nEvgenii Khokhryakov\n'],Pdf-format Russian version,Voprosi RadioElectronici ISSN 0233-9950 2 (2013) 144-154,http://arxiv.org/abs/1306.1702v1,cs.OH,"['cs.OH', '93D99', 'K.6.1']",,,[]
An Optimized Design of Reversible Sequential Digital Circuits,http://arxiv.org/abs/1306.2556v1,2013-06-10T15:32:56Z,2013-06-10T15:32:56Z,"  In the today's era, reversible logics are the promising technology for the
designing of low power digital logic system having major application in the
field of nanotechnology, quantum computation, DNA and other low power digital
circuits. Reversible logics provide zero power dissipation (Ideally) in the
digital operations. There are numbers of circuit designed by the reversible
logics and sequential circuits have their own importance in the digital
systems. In this paper authors provides a optimized approach and optimized
design for the sequential circuit (counter as an example) by using the MUX gate
(a reversible gate) which provides the better results against the previous
designs discussed in the literature. The proposed design has lower quantum
cost, garbage output, constant input and total number of logical calculations
performing by the design.
","['\nPradeep Singla\n', '\nAakash Gupta\n', '\nAshutosh Bhardwaj\n', '\nPulkit Basia\n']","Proceedings of NCECST-2013, Bareily",,http://arxiv.org/abs/1306.2556v1,cs.OH,['cs.OH'],,,[]
"Valuating Surface Surveillance Technology for Collaborative
  Multiple-Spot Control of Airport Departure Operations",http://arxiv.org/abs/1306.3426v1,2013-06-14T15:26:40Z,2013-06-14T15:26:40Z,"  Airport departure operations are a source of airline delays and passenger
frustration. Excessive surface traffic is a cause of increased controller and
pilot workload. It is also a source of increased emissions and delays, and does
not yield improved runway throughput. Leveraging the extensive past research on
airport departure management, this paper explores the environmental and safety
benefits that improved surveillance technologies can bring in the context of
gate- or spot-release strategies. The paper shows that improved surveillance
technologies can yield 4% to 6% reduction of aircraft on taxiway, and therefore
emissions, in addition to the savings currently observed by implementing
threshold starategies under evaluation at Boston Logan Airport and other busy
airports during congested periods. These calculated benefits contrast sharply
with our previous work, which relied on simplified airport ramp areas with a
single departure spot, and where fewer environmental and economic benefits of
advanced surface surveillance systems could be established. Our work is
illustrated by its application to New-York LaGuardia and Seattle Tacoma
airports.
","['\nPierrick Burgain\n', '\nSang Hyun Kim\n', '\nEric Feron\n']","Submitted to IEEE Transactions on Intelligent Transportation Systems.
  arXiv admin note: substantial text overlap with arXiv:1102.2673",,http://dx.doi.org/10.1109/TITS.2013.2286271,cs.OH,['cs.OH'],10.1109/TITS.2013.2286271,,[]
Impact of Gate Assignment on Gate-Holding Departure Control Strategies,http://arxiv.org/abs/1306.3429v1,2013-06-14T15:35:49Z,2013-06-14T15:35:49Z,"  Gate holding reduces congestion by reducing the number of aircraft present on
the airport surface at any time, while not starving the runway. Because some
departing flights are held at gates, there is a possibility that arriving
flights cannot access the gates and have to wait until the gates are cleared.
This is called a gate conflict. Robust gate assignment is an assignment that
minimizes gate conflicts by assigning gates to aircraft to maximize the time
gap between two consecutive flights at the same gate; it makes gate assignment
robust, but passengers may walk longer to transfer flights. In order to
simulate the airport departure process, a queuing model is introduced. The
model is calibrated and validated with actual data from New York La Guardia
Airport (LGA) and a U.S. hub airport. Then, the model simulates the airport
departure process with the current gate assignment and a robust gate assignment
to assess the impact of gate assignment on gate-holding departure control. The
results show that the robust gate assignment reduces the number of gate
conflicts caused by gate holding compared to the current gate assignment.
Therefore, robust gate assignment can be combined with gate-holding departure
control to improve operations at congested airports with limited gate
resources.
","['\nSang Hyun Kim\n', '\nEric Feron\n']",Submitted to IEEE Transactions on Intelligent Transportation Systems,,http://dx.doi.org/10.1109/TITS.2013.2285499,cs.OH,['cs.OH'],10.1109/TITS.2013.2285499,,[]
"Roughening Methods to Prevent Sample Impoverishment in the Particle PHD
  Filter",http://arxiv.org/abs/1306.3875v1,2013-06-14T12:16:13Z,2013-06-14T12:16:13Z,"  Mahler's PHD (Probability Hypothesis Density) filter and its particle
implementation (as called the particle PHD filter) have gained popularity to
solve general MTT (Multi-target Tracking) problems. However, the resampling
procedure used in the particle PHD filter can cause sample impoverishment. To
rejuvenate the diversity of particles, two easy-to-implement roughening
approaches are presented to enhance the particle PHD filter. One termed as
""separate-roughening"" is inspired by Gordon's roughening procedure that is
applied on the resampled particles. Another termed as ""direct-roughening"" is
implemented by increasing the simulation noise of the state propagation of
particles. Four proposals are presented to customize the roughening approach.
Simulations are presented showing that the roughening approach can benefit the
particle PHD filter, especially when the sample size is small.
","['\nTiancheng Li\n', '\nTariq P. Sattar\n', '\nQing Han\n', '\nShudong Sun\n']","16th International Conference on Information Fusion(FUSION2013), 9-12
  July 2013","Proceedings of the 16th International Conference on Information
  Fusion, 2013",http://arxiv.org/abs/1306.3875v1,cs.OH,['cs.OH'],,,[]
"Comparative Study of ERP Implementation Methodology Case Study:
  Accelerated SAP VS Dantes & Hasibuan Methodology",http://arxiv.org/abs/1305.6010v2,2013-05-26T10:10:58Z,2013-11-04T08:45:24Z,"  Enterprise Resource Planning (ERP) system is a concept of enterprise system
that describe the integration of the whole process in the organization. Study
in this field mostly about external development paradigm on information system
development. So, issue in ERP is all about how to adopt it in the organization,
not about the application development. This paper reviews two methodologies on
ERP system implementation, one is vendor perspective methodology and new
generic perspective methodology. Comparation of both methodology is done in
this study using certain metric measurements. Result is the vendor perspective
slightly superior than new generic perspective methodology.
","['\nM. Hilman\n', '\nF. Setiadi\n', '\nI. Sarika\n', '\nJ. Budiasto\n', '\nR. Alfian\n']","This paper has been withdrawn by the author due to acceptance in
  Jurnal Sistem Informasi, Vol.8, No.1, pp.8-15, April, 2012. link:
  http://jsi.cs.ui.ac.id/index.php/jsi/article/view/318",,http://arxiv.org/abs/1305.6010v2,cs.OH,['cs.OH'],,,[]
Information System as a Service: Issues and Challenges,http://arxiv.org/abs/1305.6011v2,2013-05-26T10:11:32Z,2013-11-04T08:44:13Z,"  Information system evolved as the evolution of information technology. The
current state of information technology, placed the internet as a main
resources of computing. Cloud technology as the backbone of internet has been
utilized as a powerful computing resources. Therefore, cloud introduced new
term of service oriented technology, popular with ""as a service"" kind of name.
In this paper, the service oriented paradigm will be used to address future
trend of information system. Thus, this paper try to introduce the term
""information system as a service"", holistic view of infrastructure as a
service, platform as a service, software as a service, and data as a service.
",['\nMuhammad Hilman\n'],"This paper has been withdrawn by the author due to the acceptance in
  Jurnal Sistem Informasi, Vol.8, No.2, pp.71-77, October, 2012. link:
  http://jsi.cs.ui.ac.id/index.php/jsi/article/view/328",,http://arxiv.org/abs/1305.6011v2,cs.OH,['cs.OH'],,,[]
"Calculation of geometric characteristics of land cover and urban canyon
  for multi-scale parameterization of megalopolis meteorological models",http://arxiv.org/abs/1305.6067v1,2013-05-26T20:13:04Z,2013-05-26T20:13:04Z,"  The results of studies on the development of computational techniques for
geometric and thematic characteristics of the underlying surface and urban
canyon are presented. These characteristics are intended for parameterization
of the local model of energy-mass exchange between the surface layer of the
atmosphere and the surface of the active layer of the underlying urban areas
(cities). Multiscale database of parameters of the underlying surface with a
resolution of 200, 500 and 1000 meters is obtained. The use of
micro-meteorology models that take into account the specificity of the urban
environment, coupled with mesoscale prognostic models will significantly
improve the sound quality of the meteorological fields and local operational
weather forecasting in the metropolitan areas where considering the
hydrometeorological situation is particularly important.
","['\nTimofey E. Samsonov\n', '\nVladimir N. Semin\n', '\nPavel I. Konstantinov\n', '\nMikhail I. Varentzov\n']",in Russian,,http://arxiv.org/abs/1305.6067v1,cs.OH,['cs.OH'],,,[]
"An UHF RFID Energy-Harvesting System Enhanced by a DC-DC Charge Pump in
  Silicon-on-Insulator Technology",http://arxiv.org/abs/1305.4731v2,2013-05-21T06:41:31Z,2013-06-04T07:15:01Z,"  An RF-DC converter enhanced by a DC-DC voltage booster in
silicon-on-insulator technology for UHF radio frequency identification (RFID)
energy harvesting is presented in this letter. When the received RF power level
is -14 dBm or higher, the system, fabricated on an FR4 substrate using
off-the-shelf low-cost discrete components and connected to a flexible dipole
antenna, is able to produce 2.4-V DC voltage to power general-purpose
electronic devices. As a simple proof of concept,a device comprising
microcontroller, temperature sensor, and EEPROM is considered in this work. The
experimental results demonstrate the capability of the system to autonomously
perform temperature data logging up to a distance of 5 m from a conventional
UHF RFID reader used as an RF energy source.
","['\nD. De Donno\n', '\nL. Catarinucci\n', '\nL. Tarricone\n']",,"IEEE Microwave and Wireless Components Letters, vol. 23, no. 6,
  pp. 315-317, June 2013",http://dx.doi.org/10.1109/LMWC.2013.2258002,cs.OH,['cs.OH'],10.1109/LMWC.2013.2258002,,[]
"Enabling Self-Powered Autonomous Wireless Sensors with New-Generation
  I2C-RFID Chips",http://arxiv.org/abs/1305.4732v1,2013-05-21T06:46:23Z,2013-05-21T06:46:23Z,"  A self-powered autonomous RFID device with sensing and computing capabilities
is presented in this paper. Powered by an RF energy-harvesting circuit enhanced
by a DC-DC voltage booster in silicon-on-insulator (SOI) technology, the device
relies on a microcontroller and a new generation I2C-RFID chip to wirelessly
deliver sensor data to standard RFID EPC Class-1 Generation-2 (Gen2) readers.
When the RF power received from the interrogating reader is -14 dBm or higher,
the device, fabricated on an FR4 substrate using low-cost discrete components,
is able to produce 2.4-V DC voltage to power its circuitry. The experimental
results demonstrate the effectiveness of the device to perform reliable sensor
data transmissions up to 5 meters in fully-passive mode. To the best of our
knowledge, this represents the longest read range ever reported for passive UHF
RFID sensors compliant with the EPC Gen2 standard.
","['\nD. De Donno\n', '\nL. Catarinucci\n', '\nL. Tarricone\n']",,,http://dx.doi.org/10.1109/MWSYM.2013.6697449,cs.OH,['cs.OH'],10.1109/MWSYM.2013.6697449,,[]
"GUI Based Automatic Remote Control of Gas Reduction System using PIC
  Microcontroller",http://arxiv.org/abs/1305.0668v1,2013-05-03T10:51:38Z,2013-05-03T10:51:38Z,"  The GRS is a one of the important units in Erbil Power Station EPS, which is
responsible on controlling gas pressure and gas temperature this unit
previously works manually. The local control panel for GRS system contains two
types of digital signals the first one indicated by Light Emitting Diodes LED
to point normal operations, fault and alarm, and event of operations while the
second indicated by ON-OFF switches, which consists of two types the push
buttons switch and mode selector switch. To overcome human in manual control
faults in controlling GRS systems, automation system becomes the best solution.
The purpose of this research is to design and implement embedded automation
system that can be used to control a GRS automatically through a GUI and from
remote location by using programmable interface controller (PIC16F877A). In
this research the (PIC) software which is based on (C language), developed by
Microchip (MPLAB) is used in programming a PIC microcontroller, then Visual
Basic is used in the construction of GUI, the RS-232 serial cable is used as a
connector between PIC and PC. Implement the proposed design and test it as a
first system shows all operations of GRS successful were converted into full
computerize controlling (with the ability of full automatic control) from
remote location through proposed GUI. Keywords-Peripheral Interface Controller
(PIC); Microcontroller; Graphical User Interface (GUI); Remote; Control.
","['\nAyad Ghany Ismaeel\n', '\nRaghad Zuhair Yousif\n', '\nEssa F. Abdallh\n']","11 pages, 25 figures, Tables 4, and this research obtained
  certificate from Erbil Power Station, which is the beneficiary company of it.
  Available at http://www.estij.org/papers/vol3no22013/1vol3no2.pdf, 2013",,http://arxiv.org/abs/1305.0668v1,cs.OH,['cs.OH'],,,[]
BiEntropy - The Approximate Entropy of a Finite Binary String,http://arxiv.org/abs/1305.0954v2,2013-05-04T20:27:38Z,2013-09-15T22:48:56Z,"  We design, implement and test a simple algorithm which computes the
approximate entropy of a finite binary string of arbitrary length. The
algorithm uses a weighted average of the Shannon Entropies of the string and
all but the last binary derivative of the string. We successfully test the
algorithm in the fields of Prime Number Theory (where we prove explicitly that
the sequence of prime numbers is not periodic), Human Vision, Cryptography,
Random Number Generation and Quantitative Finance.
",['\nGrenville J. Croll\n'],"16 Pages, 7 Tables, 6 Colour Figures. Presented at ANPA 34, Rowland's
  Castle, Hampshire, England, August 2013",,http://arxiv.org/abs/1305.0954v2,cs.OH,['cs.OH'],,,[]
Augmented Reality in ICT for Minimum Knowledge Loss,http://arxiv.org/abs/1305.2500v1,2013-05-11T12:32:04Z,2013-05-11T12:32:04Z,"  Informatics world digitizes the human beings, with the contribution made by
all the industrial people. In the recent survey it is proved that people are
not accustomed or they are not able to access the electronic devices to its
extreme usage. Also people are more dependent to the technologies and their
day-to-day activities are ruled by the same. In this paper we discuss on one of
the advanced technology which will soon rule the world and make the people are
more creative and at the same time hassle-free. This concept is introduced as
6th sense technology by an IIT, Mumbai student who is presently Ph.D., scholar
in MIT, USA. Similar to this research there is one more research going on under
the title Augmented Reality. This research makes a new association with the
real world to digital world and allows us to share and manipulate the
information directly with our mental thoughts. A college which implements state
of the art technology for teaching and learning, Higher College of Technology,
Muscat, (HCT) tries to identify the opportunities and limitations of
implementing this augmented reality for teaching and learning. The research
team of HCT, here, tries to give two scenarios in which augmented reality can
fit in. Since this research is in the conceptual level we are trying to
illustrate the history of this technology and how it can be adopted in the
teaching environment
","['\nRamKumar Lakshminarayanan\nDepartment of IT, HCT\n', '\nRD. Balaji\nDepartment of IT, HCT\n', '\nBinod kumar\nDepartment of IT, HCT\n', '\nMalathi Balaji\nDepartment of IT, HCT\n']",,"(IJCSIS) International Journal of Computer Science and Information
  Security, Vol. 11, No. 4, April 2013",http://arxiv.org/abs/1305.2500v1,cs.OH,['cs.OH'],,,"['Department of IT, HCT', 'Department of IT, HCT', 'Department of IT, HCT', 'Department of IT, HCT']"
Priority Based Pre-emptive Task Scheduling for Android Operating System,http://arxiv.org/abs/1304.7889v1,2013-04-30T06:00:42Z,2013-04-30T06:00:42Z,"  Android mobile operating system which is based on Linux Kernel 2.6, has open
source license and adaptability to user driven applications. As all other
operating systems it has all the basic features like process scheduling, memory
management, process management etc associated with it. Any mobile platform
works smoothly when the process scheduling is performed in a proper way. Ideal
platform is that in which no resource conflict occurs. Thus scheduling in every
manner is essential for the operating system to adapt itself with the
requirement of a particular application. In this paper, priority based
pre-emptive task scheduling is proposed for the SMS application. The idea is to
define High priority to required contacts, for ex. Contact numbers of parents
or teachers will be given High priority. If in case, any SMS from these High
priority contacts is received, the application would flash the SMS on the
active screen and redirect this High priority SMS to the Priority Inbox.
","['\nDeepali Kayande\n', '\nUrmila Shrawankar\n']",Pages: 5 Figures : 10,"International Journal of Computer Science and Telecommunications
  (IJCST), Volume 2,Issue 7,October 2011,pg 17-21",http://arxiv.org/abs/1304.7889v1,cs.OH,['cs.OH'],,,[]
"Hardware Acceleration of the Gipps Model for Real-Time Traffic
  Simulation",http://arxiv.org/abs/1304.3507v1,2013-04-11T23:36:24Z,2013-04-11T23:36:24Z,"  Traffic simulation software is becoming increasingly popular as more cities
worldwide use it to better manage their crowded traffic networks. An important
requirement for such software is the ability to produce accurate results in
real time, requiring great computation resources. This work proposes an
ASIC-based hardware accelerated approach for the AIMSUN traffic simulator,
taking advantage of repetitive tasks in the algorithm. Different system
configurations using this accelerator are also discussed. Compared with the
traditional software simulator, it has been found to improve the performance by
as much as 9x when using a single processing element approach, or more
depending on the chosen hardware configuration.
","['\nSalim Farah\n', '\nMagdy Bayoumi\n']",,,http://arxiv.org/abs/1304.3507v1,cs.OH,['cs.OH'],,,[]
"Graphical Methods for Defense Against False-data Injection Attacks on
  Power System State Estimation",http://arxiv.org/abs/1304.4151v4,2013-04-15T16:16:11Z,2014-04-09T03:36:25Z,"  The normal operation of power system relies on accurate state estimation that
faithfully reflects the physical aspects of the electrical power grids.
However, recent research shows that carefully synthesized false-data injection
attacks can bypass the security system and introduce arbitrary errors to state
estimates. In this paper, we use graphical methods to study defending
mechanisms against false-data injection attacks on power system state
estimation. By securing carefully selected meter measurements, no false data
injection attack can be launched to compromise any set of state variables. We
characterize the optimal protection problem, which protects the state variables
with minimum number of measurements, as a variant Steiner tree problem in a
graph. Based on the graphical characterization, we propose both exact and
reduced-complexity approximation algorithms. In particular, we show that the
proposed tree-pruning based approximation algorithm significantly reduces
computational complexity, while yielding negligible performance degradation
compared with the optimal algorithms. The advantageous performance of the
proposed defending mechanisms is verified in IEEE standard power system
testcases.
","['\nSuzhi Bi\nAngela\n', '\nYing Jun\nAngela\n', '\n Zhang\n']",Accepted for publication by IEEE Transactions on Smart Grid,,http://arxiv.org/abs/1304.4151v4,cs.OH,['cs.OH'],,,"['Angela', 'Angela']"
"Marketplaces for Energy Demand-Side Management based on Future-Internet
  Technology",http://arxiv.org/abs/1304.5346v1,2013-04-19T09:07:14Z,2013-04-19T09:07:14Z,"  Renewable energies become more important, and they contribute to the EU's
goals for greenhouse-gas reduction. However, their fluctuating nature calls for
demand-side-management techniques, which balance energy generation and
consumption. Such techniques are currently not broadly deployed. This paper
describes the latest results from the FINSENY project on how Future-Internet
enablers and market mechanisms can be used to realise such systems.
","['\nLuigi Briguglio\n', '\nFrank Eichinger\n', '\nMassimiliano Nigrelli\n', '\nJavier Lucio Ruiz-Andino\n']",,,http://arxiv.org/abs/1304.5346v1,cs.OH,['cs.OH'],,,[]
Patterns to analyze requirements of a Decisional Information System,http://arxiv.org/abs/1304.5389v1,2013-04-19T12:13:00Z,2013-04-19T12:13:00Z,"  The domain of analysis and conception of Decisional Information System (DIS)
is, highly, applying new techniques and methods to succeed the process of the
decision and minimizing the time of conception. Our objective in this paper is
to define a group of patterns to ensure a systematic reuse of our approach to
analyse a DIS s business requirements. We seek, through this work, to guide the
discovery of an organizations business requirements, expressed as goals by
introducing the notion of context, to promote good processes design for a DIS,
to capitalize the process and models proposed in our approach and systematize
reuse steps of this approach to analyze similar projects or adapt them as
needed. The patterns are at the same time the process s patterns and product s
patterns as they capitalize models and their associated processes. These
patterns are represented according to the PSIGMA formalism.
","['\nSabri Aziza\n', '\nKjiri Laila\n']","paper accepted alson in SEDEXS'2012: International Conference On
  Software Engineering, Databases and EXpert Systems, Settat, Maroc, 14-16 Juin
  2012, International Journal of Computer Application (IJCA), Special Issue On
  Software Engineering Databases and EXpert Systems (SEDEXS), Number 2, ISBN:
  973-93-80870-26-8, 17 Septembre 2012",,http://arxiv.org/abs/1304.5389v1,cs.OH,['cs.OH'],,,[]
"The design of high-speed data transmission method for a small nuclear
  physics DAQ system",http://arxiv.org/abs/1304.3242v1,2013-04-11T10:00:04Z,2013-04-11T10:00:04Z,"  A large number of data need to be transmitted in high-speed between Field
Programmable Gate Array (FPGA) and Advanced RISC Machines 11 micro-controller
(ARM11) when we design a small data acquisition (DAQ) system for nuclear
experiments. However, it is a complex problem to beat the target. In this
paper, we will introduce a method which can realize the high-speed data
transmission. By this way, FPGA is designed to acquire massive data from
Front-end electronics (FEE) and send it to ARM11, which will transmit the data
to other computer through the TCP/IP protocol. This paper mainly introduces the
interface design of the high-speed transmission between FPGA and ARM11, the
transmission logic of FPGA and the driver program of ARM11. The research shows
that the maximal transmission speed between FPGA and ARM11 by this way can
reach 50MB/s theoretically, while in nuclear physics experiment, the system can
acquire data with the speed of 2.2MB/s.
","['\nWenxiong Zhou\n', '\nYanyu Wang\n', '\nGangyang Nan\n', '\nJianchuan Zhang\n']",submited to Chinese Physics C,,http://arxiv.org/abs/1304.3242v1,physics.ins-det,"['physics.ins-det', 'cs.OH']",,,[]
Universality in symbolic dynamics constrained by Medvedev degrees,http://arxiv.org/abs/1304.5418v2,2013-04-19T13:42:57Z,2013-07-04T20:13:53Z,"  We define a weak notion of universality in symbolic dynamics and, by
generalizing a proof of Mike Hochman, we prove that this yields necessary
conditions on the forbidden patterns defining a universal subshift: These
forbidden patterns are necessarily in a Medvedev degree greater or equal than
the degree of the set of subshifts for which it is universal.
  We also show that this necessary condition is optimal by giving constructions
of universal subshifts in the same Medvedev degree as the subshifts they
simulate and prove that this universality can be achieved by the sofic
projective subdynamics of such a subshift as soon as the necessary conditions
are verified.
  This could be summarized as: There are obstructions for the existence of
universal subshifts due to the theory of computability and they are the only
ones.
",['\nAlexis Ballier\n'],"16 pages, 4 figures",,http://arxiv.org/abs/1304.5418v2,math.DS,"['math.DS', 'cs.OH']",,,[]
"Development of a Device for Remote Monitoring of Heart Rate and Body
  Temperature",http://arxiv.org/abs/1304.0156v1,2013-03-31T05:44:47Z,2013-03-31T05:44:47Z,"  We present a new integrated, portable device to provide a convenient solution
for remote monitoring heart rate at the fingertip and body temperature using
Ethernet technology and widely spreading internet. Now a days, heart related
disease is rising. Most of the times in these cases, patients may not realize
their actual conditions and even it is a common fact that there are no doctors
by their side, especially in rural areas, but now a days most of the diseases
are curable if detected in time.
  We have tried to make a system which may give information about one's
physical condition and help him or her to detect these deadly but curable
diseases. The system gives information of heart rate and body temperature
simultaneously acquired on the portable side in real time and transmits results
to web. In this system, the condition of heart and body temperature can be
monitored from remote places. Eventually, this device provides a low cost,
easily accessible human health monitor solution bridging the gaps between
patients and doctors.
","['\nMohammad Ashekur Rahman\n', '\nAtanu Barai\n', '\nMd. Asadul Islam\n', '\nM. M. A Hashem\n']",,"Procs. of the IEEE 2012 15th International Conference on Computer
  & Information Technology (ICCIT 2012), pp.411-416, Chittagong, Bangladesh,
  December 22-24, (2012)",http://dx.doi.org/10.1109/ICCITechn.2012.6509783,cs.OH,['cs.OH'],10.1109/ICCITechn.2012.6509783,,[]
Hubs and Authorities of the English Premier League for 2010-2011,http://arxiv.org/abs/1304.0727v1,2013-04-01T13:46:18Z,2013-04-01T13:46:18Z,"  In this work author applies well known web search algorithm Hyperlink -
Induced Topic Search (HITS) to problem of ranking football teams in English
Premier League (EPL). The algorithm allows the ranking of the teams using the
notions of hubs and authorities well known for ranking pages in the World Wide
Web. Results of the games introduced as a graph where losing team 'gives a
link' to a winning team and, if draw registered both team give links to each
other. In case of a win link is weighted as three points in adjacent matrix and
in case of draw as one point. Author uses notion of authority in order to
define team which win a game and hub as a team which lose a game, the winner of
the competition defined as the 'worst' hub, team that didn't reinforced any
other team. Using this ranking system, the champion's team, which is a 'worst
hub' must not lose, or draw games to other 'good authorities' teams. If by the
end of the competition there are teams with an equal number of wins and losses
then the team which has beaten more teams with higher authority ranks, wins.
",['\nMichael Leznik\n'],,,http://arxiv.org/abs/1304.0727v1,cs.OH,['cs.OH'],,,[]
A Simulation and Modeling of Access Points with Definition Language,http://arxiv.org/abs/1304.1836v2,2013-04-06T00:18:59Z,2013-04-11T19:06:01Z,"  This submission has been withdrawn by arXiv administrators because it
contains fictitious content and was submitted under a pseudonym, which is
against arXiv policy.
",['\nTairen Sun\n'],Withdrawn by arXiv admins,,http://arxiv.org/abs/1304.1836v2,cs.OH,['cs.OH'],,,[]
Scheduling Cutting Process for Large Paper Rolls,http://arxiv.org/abs/1304.2015v1,2013-04-07T16:10:20Z,2013-04-07T16:10:20Z,"  Paper cutting is a simple process of slicing large rolls of paper,
jumbo-reels, into various sub-rolls with variable widths based on demands risen
by customers. Since the variability is high due to collected various orders
into a pool, the process turns to be production scheduling problem, which
requires optimisation so as to minimise the final remaining amount of paper
wasted. The problem holds characteristics similar one-dimensional bin-packing
problem to some extends and differs with some respects. This paper introduces a
modelling attempt as a scheduling problem with an integer programming approach
for optimisation purposes. Then, a constructive heuristic algorithm revising
one of well-known approaches, called Best-fit algorithm, is introduced to solve
the problem. The illustrative examples provided shows the near optimum solution
provided with very low complexity .
","['\nMehmet E. Aydin\n', '\nOsman Taylan\n']","Academic Platform Journal of Engineering and Science, 2013",,http://dx.doi.org/10.5505/apjes.2013.83997,cs.OH,['cs.OH'],10.5505/apjes.2013.83997,,[]
Design and Development of a Heart Rate Measuring Device using Fingertip,http://arxiv.org/abs/1304.2475v1,2013-04-09T07:24:14Z,2013-04-09T07:24:14Z,"  In this paper, we presented the design and development of a new integrated
device for measuring heart rate using fingertip to improve estimating the heart
rate. As heart related diseases are increasing day by day, the need for an
accurate and affordable heart rate measuring device or heart monitor is
essential to ensure quality of health. However, most heart rate measuring tools
and environments are expensive and do not follow ergonomics. Our proposed Heart
Rate Measuring (HRM) device is economical and user friendly and uses optical
technology to detect the flow of blood through index finger. Three phases are
used to detect pulses on the fingertip that include pulse detection, signal
extraction, and pulse amplification. Qualitative and quantitative performance
evaluation of the device on real signals shows accuracy in heart rate
estimation, even under intense of physical activity. We compared the
performance of HRM device with Electrocardiogram reports and manual pulse
measurement of heartbeat of 90 human subjects of different ages. The results
showed that the error rate of the device is negligible.
","['\nM. M. A. Hashem\n', '\nRushdi Shams\n', '\nMd. Abdul Kader\n', '\nMd. Abu Sayed\n']",,"Procs. of the IEEE International Conference on Computer and
  Communication Engineering (ICCCE10), pp. 197-201, Kuala Lumpur, Malaysia, May
  11-13, (2010)",http://dx.doi.org/10.1109/ICCCE.2010.5556841,cs.OH,['cs.OH'],10.1109/ICCCE.2010.5556841,,[]
Object Oriented Model for Evaluation of On-Chip Networks,http://arxiv.org/abs/1304.8006v1,2013-03-23T06:57:13Z,2013-03-23T06:57:13Z,"  The Network on Chip (NoC) paradigm is rapidly replacing bus based System on
Chip (SoC) designs due to their inherent disadvantages such as non-scalability,
saturation and congestion. Currently very few tools are available for the
simulation and evaluation of on-chip architectures. This study proposes a
generic object oriented model for performance evaluation of on-chip
interconnect architectures and algorithms. The generic nature of the proposed
model can help the researchers in evaluation of any kind of on-chip switching
networks. The model was applied on 2D-Mesh and 2D-Diagonal-Mesh on-chip
switching networks for verification and selection of best out of both the
analyzed architectures. The results show the superiority of 2D-Diagonal-Mesh
over 2D-Mesh in terms of average packet delay.
","['\nSheraz Anjum\n', '\nEhsan Ullah Munir\n', '\nWaqas Anwar\n', '\nNadeem Javaid\n']",,"Research Journal of Applied Sciences, Engineering and Technology
  5(2): 353-356, 2013",http://arxiv.org/abs/1304.8006v1,cs.OH,['cs.OH'],,,[]
"Universal Numerical Encoder and Profiler Reduces Computing's Memory Wall
  with Software, FPGA, and SoC Implementations",http://arxiv.org/abs/1303.4994v1,2013-03-20T17:11:12Z,2013-03-20T17:11:12Z,"  In the multicore era, the time to computational results is increasingly
determined by how quickly operands are accessed by cores, rather than by the
speed of computation per operand. From high-performance computing (HPC) to
mobile application processors, low multicore utilization rates result from the
slowness of accessing off-chip operands, i.e. the memory wall. The APplication
AXcelerator (APAX) universal numerical encoder reduces computing's memory wall
by compressing numerical operands (integers and floats), thereby decreasing CPU
access time by 3:1 to 10:1 as operands stream between memory and cores. APAX
encodes numbers using a low-complexity algorithm designed both for time series
sensor data and for multi-dimensional data, including images. APAX encoding
parameters are determined by a profiler that quantifies the uncertainty
inherent in numerical datasets and recommends encoding parameters reflecting
this uncertainty. Compatible software, FPGA, and systemon-chip (SoC)
implementations efficiently support encoding rates between 150 MByte/sec and
1.5 GByte/sec at low power. On 25 integer and floating-point datasets, we
achieved encoding rates between 3:1 and 10:1, with average correlation of
0.999959, while accelerating computational ""time to results.""
",['\nAlbert Wegener\n'],"10 pages, 4 figures, 3 tables, 19 references",,http://arxiv.org/abs/1303.4994v1,cs.OH,"['cs.OH', 'E.4; B.2.4']",,,[]
"Principle ""synthesis"" for the solution of tasks of class NP",http://arxiv.org/abs/1303.6311v1,2013-03-25T20:54:49Z,2013-03-25T20:54:49Z,"  Initial contours of the non-standard approach to reception of the answer of
any task on discrete structures are considered: the algorithm independently
creates such answer from separate fragments.
",['\nRustem Valeyev\n'],"7 pages, 6 figures",,http://arxiv.org/abs/1303.6311v1,cs.OH,"['cs.OH', '68.11Y16']",,,[]
Proposition d'une technique de gestion de projet dans les startups,http://arxiv.org/abs/1303.2317v1,2013-03-10T12:21:42Z,2013-03-10T12:21:42Z,"  This project is part of the development of mobile CRM. It aims to develop a
management application client named NOMALYS. This application allows the
commercial and business leaders to see their CRM Mobile. We have focused in
this project on the techniques of projects management, this study allowed to
classify different techniques for managing software projects and proposed the
most closely technique that match the needs of the studied company.
",['\nDjallel Bouneffouf\n'],in French,,http://arxiv.org/abs/1303.2317v1,cs.OH,"['cs.OH', 'K.6']",,,[]
Adaptive Transmission Techniques for Mobile Satellite Links,http://arxiv.org/abs/1303.3110v1,2013-03-13T09:35:26Z,2013-03-13T09:35:26Z,"  Adapting the transmission rate in an LMS channel is a challenging task
because of the relatively fast time variations, of the long delays involved,
and of the difficulty in mapping the parameters of a time-varying channel into
communication performance. In this paper, we propose two strategies for dealing
with these impairments, namely, multi-layer coding (MLC) in the forward link,
and open-loop adaptation in the return link. Both strategies rely on
physical-layer abstraction tools for predicting the link performance. We will
show that, in both cases, it is possible to increase the average spectral
efficiency while at the same time keeping the outage probability under a given
threshold. To do so, the forward link strategy will rely on introducing some
latency in the data stream by using retransmissions. The return link, on the
other hand, will rely on a statistical characterization of a physical-layer
abstraction measure.
","['\nJesus Arnau\n', '\nAlberto Rico-Alvariño\n', '\nCarlos Mosquera\n']","Presented at the 30th AIAA International Communications Satellite
  Systems Conference (ICSSC), Ottawa, Canada, 2012. Best Professional Paper
  Award",,http://arxiv.org/abs/1303.3110v1,cs.OH,['cs.OH'],,,[]
"Performance Analysis of LMS Filter for SSPA Linearization in Different
  Modulation Conditions",http://arxiv.org/abs/1303.3263v1,2013-03-13T05:58:00Z,2013-03-13T05:58:00Z,"  The SSPA has wide application in Communication system, but its high output
power varies due to its non linear gain. Pre-distortion method plays major role
in power amplifier linearization. Polynomial is one of the methods used. The
error estimation in Polynomial method is carried out by LMS Filter. Our main
work is to analysis the error estimation performance of the LMS Filter for the
Solid state power amplifiers (SSPA) in different modulation conditions. Here we
are calculating the ACP and analyzing how effectively the memoryless non
linearity has been reduced for all digital modulation techniques. All the
analysis and results are taken using Matlab software.
","['\nJ. N. Swaminathan\n', '\nP. Kumar\n', '\nM. Vinoth\n']",Appeared in ICECIT-2012,,http://arxiv.org/abs/1303.3263v1,cs.OH,['cs.OH'],,,[]
Device-Free Person Detection and Ranging in UWB Networks,http://arxiv.org/abs/1303.4092v2,2013-03-17T19:29:42Z,2013-05-24T12:13:45Z,"  We present a novel device-free stationary person detection and ranging
method, that is applicable to ultra-wide bandwidth (UWB) networks. The method
utilizes a fixed UWB infrastructure and does not require a training database of
template waveforms. Instead, the method capitalizes on the fact that a human
presence induces small low-frequency variations that stand out against the
background signal, which is mainly affected by wideband noise. We analyze the
detection probability, and validate our findings with numerical simulations and
experiments with off-the-shelf UWB transceivers in an indoor environment.
","['\nYakup Kilic\n', '\nHenk Wymeersch\n', '\nArjan Meijerink\n', '\nMark J. Bentum\n', '\nWilliam G. Scanlon\n']",submitted to IEEE Journal of Selected Topics in Signal Processing,,http://dx.doi.org/10.1109/JSTSP.2013.2281780,cs.OH,['cs.OH'],10.1109/JSTSP.2013.2281780,,[]
Using UWB for Human Trajectory Extraction,http://arxiv.org/abs/1303.4696v1,2013-03-15T17:06:22Z,2013-03-15T17:06:22Z,"  In this paper we report on a methodology to model pedestrian behaviours
whilst aggregate variables are concerned, with potential applications to
different situations, such as evacuating a building in emergency events. The
approach consists of using UWB (ultra-wide band) based data collection to
characterise behaviour in specific scenarios. From a number of experiments
carried out, we detail the single-file scenario to demonstrate the ability of
this approach to represent macroscopic characteristics of the pedestrian flow.
Results are discussed and we can conclude that UWB-based data collection shows
great potential and suitability for human trajectory extraction, when compared
to other traditional approaches.
","['\nGonçalo Vasconcelos\n', '\nMarcelo Petry\n', '\nJoão Emílio Almeida\n', '\nRosaldo J. F. Rossetti\n', '\nAntónio Leça Coelho\n']","24th European Modeling & Simulation Symposium - EMSS 2012, Vienna,
  Austria",,http://arxiv.org/abs/1303.4696v1,cs.OH,['cs.OH'],,,[]
"Adaptive Modulation (QPSK, QAM)",http://arxiv.org/abs/1302.7145v1,2013-02-28T10:46:41Z,2013-02-28T10:46:41Z,"  In this paper, introduced below are the concepts of digital modulation used
in many communication systems today. Techniques described include quadrature
phase shift keying (QPSK) and quadrature amplitude modulation (QAM) and how
these techniques can be used to increase the capacity and speed of a wireless
network. These modulation techniques are the basis of communications for
systems like cable modems, DSL modems, CDMA, 3G, Wi-Fi* (IEEE 802.11) and
WiMAX* (IEEE 802.16).
","['\nRao Farhat Masood\nMember IEEE, MIE\n']",,,http://arxiv.org/abs/1302.7145v1,cs.OH,['cs.OH'],,,"['Member IEEE, MIE']"
"Energy Aware Task Scheduling for Soft Real Time Systems using an
  Analytical Approach for Energy Estimation",http://arxiv.org/abs/1303.0725v1,2013-03-04T15:14:24Z,2013-03-04T15:14:24Z,"  Embedded systems have pervaded all walks of our life. With the increasing
importance of mobile embedded systems and flexible applications, considerable
progress in research has been made for power management. Power constraints are
increasingly becoming the critical component of the design specifications of
these systems. It helps in pre-determining the suitable hardware architecture
for the target application. The aim of this paper is to present a technique to
estimate 'pre-run time' and 'power' of a software mapped onto a hardware
system; guaranteeing the compliance of temporal constraints while generating a
schedule of tasks of software. Real time systems must handle several
independent macro-tasks, each represented by a task graph, which includes
communications and precedence constraints. We propose a novel approach for
power estimation of embedded software using the Control Data Flow Graph (CDFG)
or task graph model. This methodology uses an existing Hierarchical Concurrent
Flow Graph (HCFG) technique for the power analysis of the CDFGs. We have
evaluated our technique for energy efficient scheduling over various task graph
benchmarks. The results obtained prove the utility and efficacy of our proposed
approach for power analysis of embedded software. We also present a methodology
to obtain an energy optimal voltage assignment and perform scheduling by taking
advantage of the relaxation in execution time of tasks.
","['\nNamita Sharma\n', '\nVineet Sahula\n', '\nC. P. Ravikumar\n']","12 pages, 4 Figures, 3 Tables","IJASCSE, VOL 1, ISSUE 4, 2012",http://arxiv.org/abs/1303.0725v1,cs.OH,['cs.OH'],,,[]
"Joint Ultra-wideband and Signal Strength-based Through-building Tracking
  for Tactical Operations",http://arxiv.org/abs/1303.1418v1,2013-03-06T18:38:48Z,2013-03-06T18:38:48Z,"  Accurate device free localization (DFL) based on received signal strength
(RSS) measurements requires placement of radio transceivers on all sides of the
target area. Accuracy degrades dramatically if sensors do not surround the
area. However, law enforcement officers sometimes face situations where it is
not possible or practical to place sensors on all sides of the target room or
building. For example, for an armed subject barricaded in a motel room, police
may be able to place sensors in adjacent rooms, but not in front of the room,
where the subject would see them. In this paper, we show that using two
ultra-wideband (UWB) impulse radios, in addition to multiple RSS sensors,
improves the localization accuracy, particularly on the axis where no sensors
are placed (which we call the x-axis). We introduce three methods for combining
the RSS and UWB data. By using UWB radios together with RSS sensors, it is
still possible to localize a person through walls even when the devices are
placed only on two sides of the target area. Including the data from the UWB
radios can reduce the localization area of uncertainty by more than 60%.
","['\nMerrick McCracken\n', '\nMaurizio Bocca\n', '\nNeal Patwari\n']","9 pages, conference submission",,http://arxiv.org/abs/1303.1418v1,cs.OH,['cs.OH'],,,[]
"A Multi-objective Perspective for Operator Scheduling using Fine-grained
  DVS Architecture",http://arxiv.org/abs/1303.1645v1,2013-03-07T10:51:08Z,2013-03-07T10:51:08Z,"  The stringent power budget of fine grained power managed digital integrated
circuits have driven chip designers to optimize power at the cost of area and
delay, which were the traditional cost criteria for circuit optimization. The
emerging scenario motivates us to revisit the classical operator scheduling
problem under the availability of DVFS enabled functional units that can
trade-off cycles with power. We study the design space defined due to this
trade-off and present a branch-and-bound(B/B) algorithm to explore this state
space and report the pareto-optimal front with respect to area and power. The
scheduling also aims at maximum resource sharing and is able to attain
sufficient area and power gains for complex benchmarks when timing constraints
are relaxed by sufficient amount. Experimental results show that the algorithm
that operates without any user constraint(area/power) is able to solve the
problem for most available benchmarks, and the use of power budget or area
budget constraints leads to significant performance gain.
","['\nRajdeep Mukherjee\n', '\nPriyankar Ghosh\n', '\nPallab Dasgupta\n', '\nAjit Pal\n']","18 pages, 6 figures, International journal of VLSI design &
  Communication Systems (VLSICS)",,http://dx.doi.org/10.5121/vlsic.2013.4109,cs.OH,"['cs.OH', 'B.5.2']",10.5121/vlsic.2013.4109,,[]
A Digital Automatic Sliding Door with a Room Light Control System,http://arxiv.org/abs/1303.1728v1,2013-03-07T16:02:28Z,2013-03-07T16:02:28Z,"  Automatic door is an automated movable barrier installed in the entry of a
room or building to restrict access, provide ease of opening a door or provide
visual privacy. As a result of enhanced civilization and modernization, the
human nature demands more comfort to his life. The man seeks ways to do things
easily and which saves time. So thus, the automatic gates are one of the
examples that human nature invent to bring comfort and ease in its daily life.
To this end, we model and design an automatic sliding door with a room light
control system to provide the mentioned needs. This was achieved by considering
some factors such as economy, availability of components and research
materials, efficiency, compatibility and portability and also durability in the
design process. The performance of the system after test met design
specifications. This system works on the principle of breaking an infrared beam
of light, sensed by a photodiode. It consists of two transmitting infrared
diodes and two receiving photo-diodes. The first one is for someone coming in
and the second one is for someone going out of the room. The photodiodes are
connected to comparators, which give a lower output when the beam is broken and
high output when transmitting normally. The general operation of the work and
performance is dependent on the presence of an intruder entering through the
door and how close he/she is in closer to the door. The door is meant to open
automatically but in a case where there is no power supply trying to force the
door open would damage the mechanical control system of the unit. The overall
work was implemented with a constructed work, tested working and perfectly
functional.
","['\nA. M. Zungeru\n', '\nP. O. Abraham-Attah\n']","17 pages, 17 figures, Journal paper","A.M. Zungeru, P.O. Abraham-Attah. A Digital Automatic Sliding Door
  with a Room Light Control System, International Journal of Information
  Technology, Modeling and Computing (IJITMC), vol. 1(1), pp. 1-17, 2012",http://arxiv.org/abs/1303.1728v1,cs.OH,['cs.OH'],,,[]
"An electronic digital combination lock: A precise and reliable security
  system",http://arxiv.org/abs/1303.1734v1,2013-03-07T16:20:22Z,2013-03-07T16:20:22Z,"  The increasing rate of crime, attacks by thieves, intruders and vandals,
despite all forms of security gadgets and locks still need the attention of
researchers to find a permanent solution to the well being of lives and
properties of individuals. To this end, we design a cheap and effective
security system for buildings, cars, safes, doors and gates, so as to prevent
unauthorized person from having access to ones properties through the use of
codes, we therefore experiment the application of electronic devices as locks.
However, a modular approach was employed in the design in which the combination
lock was divided into units and each unit designed separately before being
coupled to form a whole functional system. During the design, we conducted
Twenty tests with the first eight combinations being four in number, the next
seven tests being five and the last five combinations being six. This was done
because of the incorporation of 2 dummy switches in the combinations. From the
result obtained, combinations 8, 11, 13 gave the correct output combination.
However, 8 being the actual combination gave the required output. The general
operation of the system and performance is dependent on the key combinations.
The overall system was constructed and tested and it works perfectly.
",['\nA. M. Zungeru\n'],"13 pages, 9 figures, 1 Table","A.M. Zungeru. An electronic digital combination lock: A precise
  and reliable security system. International Journal of Security, Privacy and
  Trust Management (IJSPTM), vol. 2(1), pp. 29-41, 2013",http://dx.doi.org/10.5121/ijsptm.2013.2103,cs.OH,['cs.OH'],10.5121/ijsptm.2013.2103,,[]
"Multi-User Multi-Carrier Differential Chaos Shift Keying Communication
  System",http://arxiv.org/abs/1303.2552v3,2013-03-07T20:37:09Z,2013-03-27T13:08:40Z,"  In this paper, a multi user Multi-Carrier Differential Chaos Shift Keying
(MC-DCSK) modulation is presented. The system endeavors to provide a good
trade-off between robustness, energy efficiency and high data rate, while still
being simple. In this architecture of MC-DCSK system, for each user, chaotic
reference sequence is transmitted over a predefined subcarrier frequency.
Multiple modulated data streams are transmitted over the remaining subcarriers
allocated for each user. This transmitter structure saves energy and increases
the spectral efficiency of the conventional DCSK system.
","['\nGeorges Kaddoum\n', '\nFrancois-Dominique Richardson\n', '\nSarra Adouni\n', '\nFrancois Gagnon\n', '\nClaude Thibeault\n']","Accepted in the IEEE International Wireless Communications and Mobile
  Computing Conference (IWCMC 2013)",,http://dx.doi.org/10.1109/IWCMC.2013.6583829,cs.OH,['cs.OH'],10.1109/IWCMC.2013.6583829,,[]
Design and Development of an Ultrasonic Motion Detector,http://arxiv.org/abs/1303.1732v1,2013-03-07T16:10:33Z,2013-03-07T16:10:33Z,"  The ultrasonic motion detector devices emit ultrasonic sound energy into an
area of interest (monitored area), and this further reacts to a change in the
reflected energy pattern. The system uses a technique that is based on a
frequency shift in reflected energy to detect a movement or change in position
(motion). In this system, ultrasonic sound is transmitted from the transmitting
device which is normally in the form of energy. The transmitted sound utilizes
air as its medium and this travel in a wave type motion. The wave is reflected
back from the surroundings in the room/hallway and the device hears a pitch
characteristic of the protected environment. In this system, the wave pattern
is disturbed and reflected back more quickly, thus increasing the pitch and
signaling an alarm whenever motion is detected. The main contribution of this
work is the design of a circuit that can sense motion through movement of
anything, a low cost and portable motion detector, and the design of a circuit
that can be used to trigger another circuit whether to ON or OFF depending on
the circuit attached to it. Generally, the design is made to detect movement or
moving object in a an enclosed area. In this work, a transmitter transducer
generates a signal at a frequency of 40khz, and when the signal is blocked by
any moving object, and this in turn, triggers a buzzer via a timing circuit.
This system works on the principle of the signal interference by a moving body,
and the system is dependent on the presence of an intruder or moving object
within a monitored area. The system after design and construction was tested
and found to work in accordance with specifications.
",['\nA. M. Zungeru\n'],"7 figures, 13 pages","A.M. Zungeru. Design and Development of an Ultrasonic Motion
  Detector, International Journal of Security, Privacy and Trust Management
  (IJSPTM), vol. 2(1), pp. 1-13, 2013",http://dx.doi.org/10.5121/ijsptm.2013.2101,cs.OH,"['cs.OH', 'physics.ins-det']",10.5121/ijsptm.2013.2101,,[]
"Independent Component Analysis for Filtering Airwaves in Seabed Logging
  Application",http://arxiv.org/abs/1303.2593v1,2013-03-04T16:18:51Z,2013-03-04T16:18:51Z,"  Marine controlled source electromagnetic (CSEM) sensing method used for the
detection of hydrocarbons based reservoirs in seabed logging application does
not perform well due to the presence of the airwaves (or sea-surface). These
airwaves interfere with the signal that comes from the subsurface seafloor and
also tend to dominate in the receiver response at larger offsets. The task is
to identify these air waves and the way they interact, and to filter them out.
In this paper, a popular method for counteracting with the above stated problem
scenario is Independent Component Analysis (ICA). Independent component
analysis (ICA) is a statistical method for transforming an observed
multidimensional or multivariate dataset into its constituent components
(sources) that are statistically as independent from each other as possible.
ICA-type de-convolution algorithm that is FASTICA is considered for mixed
signals de-convolution and considered convenient depending upon the nature of
the source and noise model. The results from the FASTICA algorithm are shown
and evaluated. In this paper, we present the FASTICA algorithm for the seabed
logging application.
","['\nAdeel Ansari\n', '\nAfza Bt Shafie\n', '\nAbas B Md Said\n', '\nSeema Ansari\n']","7 pages, 13 figures","International Journal of Advanced Studies in Computers, Science
  and Engineering (IJASCSE), 2013",http://arxiv.org/abs/1303.2593v1,cs.OH,"['cs.OH', 'physics.geo-ph']",,,[]
"Mixed Maps for Kolmogoroff-Nagumo-Type Averaging on the Compact Stiefel
  Manifold",http://arxiv.org/abs/1303.1748v1,2013-03-07T16:55:03Z,2013-03-07T16:55:03Z,"  The present research work proposes a new fast fixed-point averaging algorithm
on the compact Stiefel manifold based on a mixed retraction/lifting pair.
Numerical comparisons between fixed-point algorithms based on the proposed
non-associated retraction/lifting map pair and two associated
retraction/lifting pairs confirm that the averaging algorithm based on a
combination of mixed maps is remarkably less computationally demanding than the
same averaging algorithm based on any of the constituent associated
retraction/lifting pairs.
","['\nSimone Fiori\n', '\nTetsuya Kaneko\n', '\nToshihisa Tanaka\n']",10 pages,,http://arxiv.org/abs/1303.1748v1,math.NA,"['math.NA', 'cs.OH', 'math.DG']",,,[]
"An Approach to Select Cost-Effective Risk Countermeasures Exemplified in
  CORAS",http://arxiv.org/abs/1302.4689v2,2013-02-19T17:26:05Z,2013-03-07T09:55:16Z,"  Risk is unavoidable in business and risk management is needed amongst others
to set up good security policies. Once the risks are evaluated, the next step
is to decide how they should be treated. This involves managers making
decisions on proper countermeasures to be implemented to mitigate the risks.
The countermeasure expenditure, together with its ability to mitigate risks, is
factors that affect the selection. While many approaches have been proposed to
perform risk analysis, there has been less focus on delivering the prescriptive
and specific information that managers require to select cost-effective
countermeasures. This paper proposes a generic approach to integrate the cost
assessment into risk analysis to aid such decision making. The approach makes
use of a risk model which has been annotated with potential countermeasures,
estimates for their cost and effect. A calculus is then employed to reason
about this model in order to support decision in terms of decision diagrams. We
exemplify the instantiation of the generic approach in the CORAS method for
security risk analysis.
","['\nLe Minh Sang Tran\n', '\nBjørnar Solhaug\n', '\nKetil Stølen\n']",33 pages,,http://arxiv.org/abs/1302.4689v2,cs.OH,['cs.OH'],,,[]
"Perangkat lunak bantu mengenal huruf arab melayu ke bentuk huruf latin
  bahasa Indonesia",http://arxiv.org/abs/1302.5511v1,2013-02-22T08:10:19Z,2013-02-22T08:10:19Z,"  The development of computer science has contributed greatly for increasing of
efficiency and effectively. Many areas are covered by computer science,
included education. The purpose of this research is to introduce jawi a type of
Indonesian letters. Jawis letter is one of the most popular letter in the past.
But right now few people can read and understand it. Many documents in the past
was written in Jawi. The writer develop or build the software using Pressman
method, and tools such as Microsoft Visual Basic, and Microsoft Access. This
software can introduce Jawi then people can learn it easily.
","['\nNuril Aini\n', '\nLeon Andretti Abdillah\n', '\n Jemakmun\n']",,MATRIK. 8 (2006) 317-334,http://arxiv.org/abs/1302.5511v1,cs.OH,['cs.OH'],,,[]
"Design and Analysis of a Multi-Carrier Differential Chaos Shift Keying
  Communication System",http://arxiv.org/abs/1303.3177v4,2013-02-15T19:00:32Z,2013-06-26T16:24:40Z,"  A new Multi-Carrier Differential Chaos Shift Keying (MC-DCSK) modulation is
presented in this paper. The system endeavors to provide a good trade-off
between robustness, energy efficiency and high data rate, while still being
simple compared to conventional multi-carrier spread spectrum systems. This
system can be seen as a parallel extension of the DCSK modulation where one
chaotic reference sequence is transmitted over a predefined subcarrier
frequency. Multiple modulated data streams are transmitted over the remaining
subcarriers. This transmitter structure increases the spectral efficiency of
the conventional DCSK system and uses less energy. The receiver design makes
this system easy to implement where no radio frequency (RF) delay circuit is
needed to demodulate received data. Various system design parameters are
discussed throughout the paper, including the number of subcarriers, the
spreading factor, and the transmitted energy. Once the design is explained, the
bit error rate performance of the MC-DCSK system is computed and compared to
the conventional DCSK system under an additive white Gaussian noise (AWGN) and
Rayleigh channels. Simulation results confirm the advantages of this new hybrid
design.
","['\nGeorges Kaddoum\n', '\nFrancois-Dominique Richardson\n', '\nFrancois Gagnon\n']","This paper has been accepted in IEEE trans. on Communications (will
  appear soon on IEEE explore). arXiv admin note: substantial text overlap with
  arXiv:1303.2552",,http://dx.doi.org/10.1109/TCOMM.2013.071013.130225,cs.OH,['cs.OH'],10.1109/TCOMM.2013.071013.130225,,[]
"Circuit proposition for copying the value of a resistor into a
  memristive device supported by HSPICE simulation",http://arxiv.org/abs/1302.1005v1,2013-02-05T12:00:29Z,2013-02-05T12:00:29Z,"  Memristor is the fourth fundamental passive circuit element with potential
applications in development of analog memories, artificial brains (with the
capacity of hardware training) and neuro-science. In most of these applications
the memristance of the device should be set to the desired value, which is
currently performed by trial and error. The aim of this paper is to propose a
circuit for copying the value of the given resistor into a memristive device.
HSPICE simulations are also presented to confirm the efficiency of the proposed
circuit.
","['\nFarshad Merrikh-Bayat\n', '\nNafiseh Mirebrahimi\n', '\nFarhad Bayat\n']",,,http://arxiv.org/abs/1302.1005v1,cs.OH,['cs.OH'],,,[]
"Improving Mixed-Criticality System Consistency and Behavior on
  Multiprocessor Platforms by Means of Multi-Moded Approaches",http://arxiv.org/abs/1302.1010v1,2013-02-05T12:34:18Z,2013-02-05T12:34:18Z,"  Recent research in the domain of real-time scheduling theory has tackled the
problem of scheduling mixed-criticality systems upon uniprocessor or
multiprocessor platforms, with the main objective being to respect the
timeliness of the most critical tasks, at the expense of the requirements of
the less critical ones. In particular, the less critical tasks are carelessly
discarded when the computation demand of (some of) the high critical tasks
increases. This might nevertheless result in system failure, as these less
critical tasks could be accessing data, the consistency of which should be
preserved. In this paper, we address this problem and propose a method to
cautiously handle task suspension. Furthermore, it is usually assumed that the
less critical tasks will never be re-enabled once discarded. In this paper, we
also address this concern by proposing an approach to re-enable the less
critical tasks, without jeopardizing the timeliness of the high critical ones.
The suggested approaches apply to systems having two or more criticality
levels.
","['\nFrançois Santy\n', '\nGeoffrey Nelissen\n', '\nJoël Goossens\n']",,,http://arxiv.org/abs/1302.1010v1,cs.OH,['cs.OH'],,,[]
Aplikasi belajar membaca iqro' berbasis mobile,http://arxiv.org/abs/1301.6319v1,2013-01-27T05:30:43Z,2013-01-27T05:30:43Z,"  IPTEK and IMTAQ should be followed by knowledge of the ability in reading the
hijaiyah letters as Al Qur-an base. Current people are so busy with their
activities thats way authors develop this mobile application using pocket pc.
The development of this research using waterfall model. Authors use the
programming language of Microsoft Visual BASIC.Net. Authors also use Photoshop
to prepare the image of every letter. In Indonesia, there six level in reading
Al Qur-an, but for the purpose of thi research authors only use Iqro-1 until
Iqro-4. This mobile application also enriched with the voice for every letter
image.
","['\nMuhamd Sobri\n', '\nLeon Andretti Abdillah\n']","Seminar Nasional Teknologi Informasi & Multimedia (Semnasteknomedia),
  STMIK AMIKOM Yogyakarta, 2013. 5 pages. 11 figures","M. Sobri and L. A. Abdillah, ""Aplikasi belajar membaca iqro'
  berbasis mobile,"" in Seminar Nasional Teknologi Informasi & Multimedia
  (Semnasteknomedia), STMIK AMIKOM Yogyakarta, 2013",http://arxiv.org/abs/1301.6319v1,cs.OH,['cs.OH'],,,[]
MFLP: Most Frequent Least Power Encoding,http://arxiv.org/abs/1301.6946v2,2013-01-29T15:24:10Z,2013-02-11T07:56:22Z,"  This paper has been withdrawn by the authors. In this paper, we propose a new
low power coding technique by decreasing the number of switching activities on
the buses which use transition signaling to transmit data. This approach
dedicates the symbols with less ones to high probability data. MFLP unlike the
most low power encoding does not rely on spatial redundancy. Due to this
superiority, MFLP is unique in power decreasing in the Network on Chip (NoC).
Not only does this algorithm reduce the power consumption, but also it can
compress the data. It offers a tradeoff to designers to choose between
compression and power; that is, the more power consumption decrease we need,
the less compression we earn. This coding uses tree based infrastructure in
order to decrease the number of ones to reduce the switching activities, and
the power consumption consequently. The proposed algorithm constructs the tree
with this contribution that code words with less ones are allocated to more
frequent data. The experimental results for the outside and inside of the NoC
indicate that the proposed coding algorithm reduces the switching activity up
to 30 and 45%, the link power consumption up to 35 and 46% and the total power
dissipation up to 34.9 and 16% for the outside and inside of the NoC,
respectively.
","['\nMehdi Taassori\nMember, IEEE\n', '\nMeysam Taassori\nMember, IEEE\n', '\nSener Uysal\nMember, IEEE\n']",This paper has been withdrawn by the authors,,http://arxiv.org/abs/1301.6946v2,cs.OH,['cs.OH'],,,"['Member, IEEE', 'Member, IEEE', 'Member, IEEE']"
Data Analysis on the High-Frequency Pollution Data Collected in India,http://arxiv.org/abs/1301.7231v1,2013-01-30T14:09:24Z,2013-01-30T14:09:24Z,"  Fine grained 1Hz Carbon Monoxide pollution data were collected on a busy road
in Hyderabad, India. In this paper we report the findings from analysing the
experimental data, in which it was found that the data were log-normally
distributed and nonlinear. The dominant frequencies at peak hours were caused
by the pattern of traffic flow.
","['\nLamling Venus Shum\n', '\nManik Gupta\n', '\nPachamuthu Rajalakshmi\n']","7 pages, 10 figures",,http://arxiv.org/abs/1301.7231v1,cs.OH,['cs.OH'],,,[]
"Analisis laporan tugas akhir mahasiswa Diploma I dari sudut pandang
  kaidah ilmiah dan penggunaan teknologi informasi",http://arxiv.org/abs/1302.0338v1,2013-02-02T03:37:20Z,2013-02-02T03:37:20Z,"  The purposes of this research are: 1) to analyze final report from scientific
role, 2) the use of information technology (IT), and 3) to conduct academic
athmosphere in research area. This research gives contributions to study
program of MI-DI, such as: 1) to know the pattern of student final report from
scientific role and the use of IT, 2) give input to study program for next
final report scheme, and 3) can be used for next research reference. If we look
to the quality of final report, there are several focuses to be prepared on
tittle, literature review, methodology, results of report, discussion and also
conclusion. But for the use of IT is already good but the varian is decrease.
","['\nLeon Andretti Abdillah\n', '\n Emigawaty\n']","18 pages, scientific journal",MATRIK. 11 (2009) 19-36,http://arxiv.org/abs/1302.0338v1,cs.OH,['cs.OH'],,,[]
Transfers of entanglement qudit states in quantum networks,http://arxiv.org/abs/1302.0366v1,2013-02-02T11:27:36Z,2013-02-02T11:27:36Z,"  The issue of quantum states' transfer -- in particular, for so-called Perfect
State Transfer (PST) -- in the networks represented by the spin chains seems to
be one of the major concerns in quantum computing. Especially, in the context
of future communication methods that can be used in broadly defined computer
science. The chapter presents a definition of Hamiltonian describing the
dynamics of quantum data transfer in one-dimensional spin chain, which is able
to transfer the state of unknown qudits. The main part of the chapter is the
discussion about possibility of entangled states' perfect transfer, in
particular, for the generalized Bell states for qudits. One of the sections
also contains the results of numerical experiments for the transmission of
quantum entangled state in a noisy quantum channel.
","['\nMarek Sawerwain\nInstitute of Control & Computation Engineering, University of Zielona Góra, Poland\n', '\nJoanna Wiśniewska\nInstitute of Information Systems, Faculty of Cybernetics, Military University of Technology, Poland\n']","10 pages, 4 figures",,http://arxiv.org/abs/1302.0366v1,quant-ph,"['quant-ph', 'cs.OH', 'physics.comp-ph']",,,"['Institute of Control & Computation Engineering, University of Zielona Góra, Poland', 'Institute of Information Systems, Faculty of Cybernetics, Military University of Technology, Poland']"
"Microelectromechanical Resonators for Radio Frequency Communication
  Applications",http://arxiv.org/abs/1301.2780v1,2013-01-13T14:38:35Z,2013-01-13T14:38:35Z,"  Over the past few years, microelectromechanical system (MEMS) based on-chip
resonators have shown significant potential for sensing and high frequency
signal processing applications. This is due to their excellent features like
small size, large frequency-quality factor product, low power consumption, low
cost batch fabrication, and integrability with CMOS IC technology. Radio
frequency communication circuits like reference oscillators, filters, and
mixers based on such MEMS resonators can be utilized for meeting the increasing
count of RF components likely to be demanded by the next generation
multi-band/multi-mode wireless devices. MEMS resonators can provide a feasible
alternative to the present day well established quartz crystal technology that
is riddled with major drawbacks like relatively large size, high cost, and low
compatibility with IC chips. This article presents a survey of the developments
in this field of resonant MEMS structures with detailed enumeration on the
various micromechanical resonator types, modes of vibration, equivalent
mechanical and electrical models, materials and technologies used for
fabrication, and the application of the resonators for implementing oscillators
and filters. These are followed by a discussion on the challenges for RF MEMS
technology in comparison to quartz crystal technology; like high precision,
stability, reliability, need for hermetic packaging etc. which remain to be
addressed for enabling the inclusion of micromechanical resonators into
tomorrow's highly integrated communication systems.
","['\nJoydeep Basu\n', '\nTarun Kanti Bhattacharyya\n']",,"Microsystem Technologies, Oct 2011, vol. 17(10-11), pp. 1557-1580",http://dx.doi.org/10.1007/s00542-011-1332-9,cs.OH,['cs.OH'],10.1007/s00542-011-1332-9,,[]
"Generic System Verilog Universal Verification Methodology based Reusable
  Verification Environment for Efficient Verification of Image Signal
  Processing IPs/SoCs",http://arxiv.org/abs/1301.2858v1,2013-01-14T04:18:02Z,2013-01-14T04:18:02Z,"  In this paper,we present Generic System Verilog Universal Verification
Methodology based Reusable Verification Environment for efficient verification
of Image Signal Processing IP's/SoC's. With the tight schedules on all projects
it is important to have a strong verification methodology which contributes to
First Silicon Success. Deploy methodologies which enforce full functional
coverage and verification of corner cases through pseudo random test scenarios
is required. Also, standardization of verification flow is needed. Previously,
inside imaging group of ST, Specman (e)/Verilog based Verification Environment
for IP/Subsystem level verification and C/C++/Verilog based Directed
Verification Environment for SoC Level Verification was used for Functional
Verification. Different Verification Environments were used at IP level and SoC
level. Different Verification/Validation Methodologies were used for SoC
Verification across multiple sites. Verification teams were also looking for
the ways how to catch bugs early in the design cycle? Thus, Generic System
Verilog Universal Verification Methodology (UVM) based Reusable Verification
Environment is required to avoid the problem of having so many methodologies
and provides a standard unified solution which compiles on all tools. The main
aim of development of this Generic and automatic verification environment is to
develop an efficient and unified verification environment (at IP/Subsystem/SoC
Level) which reuses the already developed Verification components and also
sequences written at IP/Subsystem level can be reused at SoC Level both with
Host BFM and actual Core using Incisive Software Extension (ISX) and Virtual
Register Interface (VRI)/Verification Abstraction Layer (VAL) approaches.
IP-XACT based tools are used for automatically configuring the environment for
various imaging IPs/SoCs.
","['\nAbhishek Jain\n', '\nGiuseppe Bonanno\n', '\nHima Gupta\n', '\nAjay Goyal\n']",International journal of VLSI design & Communication Systems (VLSICS),,http://arxiv.org/abs/1301.2858v1,cs.OH,['cs.OH'],,,[]
Green WSUS,http://arxiv.org/abs/1212.6046v1,2012-12-25T13:20:25Z,2012-12-25T13:20:25Z,"  The new era of information and communication technology (ICT) calls for a
greater understanding of the environmental impacts of recent technology. With
increasing energy cost and growing environmental concerns, green IT is
receiving more and more attention. Network and system design play a crucial
role in both computing and telecommunication systems. Significant part of this
energy cost goes to system update by downloading regularly patches and bug
fixes to solve security problems and to assure that the operating system and
other systems function properly. This paper describes a new design of Windows
Server Update Services (WSUS), system responsible of downloads of the mentioned
patches and updates from Microsoft Update website and then distributes them to
computers on a network. The general idea behind our proposed design is simple.
Instead of the periodical check done by the WSUS servers to ensure update form
Microsoft main servers, we rather propose to reverse the scenario in order to
reduce energy consumption. In the proposed design, the Microsoft main server(s)
sends signal to all WSUS servers to inform them about new updates. Once the
signal received, WSUS can contact the main server to start downloading.
","['\nSeifedine Kadry\n', '\nChibli Joumaa\n']","International Journal of Applied Science and Technology, 2012",,http://arxiv.org/abs/1212.6046v1,cs.OH,['cs.OH'],,,[]
"Design and Performance Study of Smart Antenna Systems for WIMAX
  Applications",http://arxiv.org/abs/1212.6056v1,2012-12-25T14:21:01Z,2012-12-25T14:21:01Z,"  In this paper we propose an approach that uses homodyne receivers to design
smart antenna systems. The receivers functions are to detect angles of arrivals
of seven incoming RF signals using MUSIC or ESPRIT algorithms. The
characteristics of each algorithm are critical for the systems precision as
well as receivers types. Results are deduced from the simulation of each
system, using the Advanced Design System (ADS) and MATLAB. These are compared
to results deduced from real systems in the WIMAX (3.5GHz) domains.
","['\nAyman Abdallah\n', '\nSeifedine Kadry\n', '\nChibli Joumaa\n']","Informatics in Control, Automation and Robotics, Volume 2, LNEE 133,
  Springer-Verlag Berlin Heidelberg 2011",,http://arxiv.org/abs/1212.6056v1,cs.OH,['cs.OH'],,,[]
Beamforming Techniques for Multichannel audio Signal Separation,http://arxiv.org/abs/1212.6080v1,2012-12-25T19:58:02Z,2012-12-25T19:58:02Z,"  Beamforming is a signal processing technique. It has been studied in many
areas such as radar, sonar, seismology and wireless communications, to name but
a few. It can be used for a myriad of purposes, such as detecting the presence
of a signal, estimating the direction of arrival, and enhancing a desired
signal from its measurements corrupted by noise, competing sources and
reverberation. Actually, Beamforming has been adopted by the audio research
society, mostly to separate or extract speech for noisy environment.
Beamforming techniques basically approach the problem from a spatial point of
view. A microphone array is used to form a spatial filter which can extract a
signal from a specific direction and reduce the contamination of signals from
other directions. In this paper we survey some Beamforming techniques used for
multichannel audio signal separation.
","['\nHidri Adel\n', '\nMeddeb Souad\n', '\nAbdulqadir Alaqeeli\n', '\nAmiri Hamid\n']","9 pages, 7 Figures","JDCTA: International Journal of Digital Content Technology and its
  Applications, Vol. 6, No. 20, pp. 659-667, 2012",http://arxiv.org/abs/1212.6080v1,cs.OH,['cs.OH'],,,[]
YAGI Antenna Design for Signal Phone Jammer,http://arxiv.org/abs/1212.6299v1,2012-12-27T04:21:52Z,2012-12-27T04:21:52Z,"  Mobile phone is one of the most widely used today in mobile communications.
This technology is very useful for communication but this raises several
problems in a situation where silence is required such as in libraries, places
of worship, classrooms and others. Mobile phone jammer is a device that used to
block the incoming signal to a mobile phone from the base station. If the
mobile phone jammer is turned on then it can not form the incoming or outgoing
calls even sms. In this research, we designed a Yagi antenna (900MHz) to expand
the range of jamming because Yagi has a great gain. Results of impedance by
gamma match are 50.16 Om. Obtained the value of VSWR Yagi is 1.46:1 and jamming
distance that can be taken approximately 16 meters, It is different from the
jamming distance of helical antenna on a mobile phone jammer itself is about 4
meters.
","['\nY. Fitriyani\n', '\nA. B. Mutiara\n', '\nR. Refianti\n']","6 pages, 10 figures; Journal of Theoretical and Applied Information
  Technology (JATIT), Vol.43 No.1, 2012",,http://arxiv.org/abs/1212.6299v1,cs.OH,['cs.OH'],,,[]
URI Identity and Web Architecture Revisited,http://arxiv.org/abs/1212.6372v1,2012-12-27T14:07:35Z,2012-12-27T14:07:35Z,"  This document reexamined the URI's identity issue and the debate regarding
the nature of ""information resource"". By making emphasis on the abstract nature
of resource and the role of URI as an interface to the web, this article
presented an alternative viewpoint about the architecture of the web that would
allow us to objectively and consistently treat all kinds of resources.
",['\nXiaoshu Wang\n'],"14 pages, originally published at http://dfdf.inesc-id.pt/tr/web-arch",,http://arxiv.org/abs/1212.6372v1,cs.OH,['cs.OH'],,,[]
On Neighborhood Tree Search,http://arxiv.org/abs/1212.6510v1,2012-12-28T12:31:19Z,2012-12-28T12:31:19Z,"  We consider the neighborhood tree induced by alternating the use of different
neighborhood structures within a local search descent. We investigate the issue
of designing a search strategy operating at the neighborhood tree level by
exploring different paths of the tree in a heuristic way. We show that allowing
the search to 'backtrack' to a previously visited solution and resuming the
iterative variable neighborhood descent by 'pruning' the already explored
neighborhood branches leads to the design of effective and efficient search
heuristics. We describe this idea by discussing its basic design components
within a generic algorithmic scheme and we propose some simple and intuitive
strategies to guide the search when traversing the neighborhood tree. We
conduct a thorough experimental analysis of this approach by considering two
different problem domains, namely, the Total Weighted Tardiness Problem
(SMTWTP), and the more sophisticated Location Routing Problem (LRP). We show
that independently of the considered domain, the approach is highly
competitive. In particular, we show that using different branching and
backtracking strategies when exploring the neighborhood tree allows us to
achieve different trade-offs in terms of solution quality and computing cost.
","['\nHouda Derbel\nLIFL, INRIA Lille - Nord Europe\n', '\nBilel Derbel\nLIFL, INRIA Lille - Nord Europe\n']",Genetic and Evolutionary Computation Conference (GECCO'12) (2012),,http://arxiv.org/abs/1212.6510v1,cs.OH,['cs.OH'],,,"['LIFL, INRIA Lille - Nord Europe', 'LIFL, INRIA Lille - Nord Europe']"
"Application of polynomial vector (pv) processing to improve the
  estimation performance of bio diesel in variable compression ratio diesel
  engine",http://arxiv.org/abs/1301.1261v1,2012-12-17T02:53:01Z,2012-12-17T02:53:01Z,"  This paper presents the implementation of polynomial vector back propagation
algorithm (PVBPA) for estimating the power, torque, specific fuel consumption
and presence of carbon monoxide, hydrocarbons in the emission of a direct
injection diesel engine. Experimental readings were obtained using the
biodiesel prepared form the waste low quality cooking oil collected from the
canteen of Sri Sairam Engineering College, India.. This waste cooking oil was
due to the preparation of varieties of food (vegetables fried and non
vegetarian). Over more than a week, trans esterification was done in chemical
lab and the biodiesel was obtained. The biodiesel was mixed in proportions of
10%, 20%, 30%, 40%, 50% with remaining combinations of the diesel supplied by
the Indian government. Variable compression ratio (VCR) diesel engine with
single cylinder, four stroke diesel type was used. The outputs of the engine as
power, torque and specific fuel consumption were obtained from the
computational facility attached to the engine. The data collected for different
input conditions of the engine was further used to train (PVBPA). The trained
PVBPA network was further used to predict the power, torque and brake specific
fuel consumption (SFC) for different speed, biodiesel and diesel combinations
and full load condition. The estimation performance of the PVBPA network is
discussed.
","['\nM. Suresh\n', '\nMaheswar Dutta\n', '\nS. Purushothaman\n']","8 pages, 15 figures, 3 tables",,http://arxiv.org/abs/1301.1261v1,cs.OH,['cs.OH'],,,[]
Enhanced Image Analysis Using Cached Mobile Robots,http://arxiv.org/abs/1212.2531v1,2012-12-11T16:45:11Z,2012-12-11T16:45:11Z,"  In the field of Artificial intelligence Image processing plays a vital role
in Decision making. Nowadays Mobile robots work as a Network sharing
Centralized Database. All Image inputs are compared against this database and
decision is made. In some cases the Centralized database is in other side of
the globe and Mobile robots compare Input image through satellite link this
sometime results in delays in decision making which may result in castrophe.
This Research paper is about how to make image processing in mobile robots less
time consuming and fast decision making. This research paper compares search
techniques employed currently and optimum search method which we are going to
state. Nowadays Mobile robots are extensively used in environments which are
dangerous to human beings. In this dangerous situations quick Decision making
makes the difference between Hit and Miss this can also results in Day to day
tasks performed by Mobile robots Successful or Failure.
","['\nKabeer Mohammed\n', '\nDr. Bhaskara Reddy\n']",6 Pages one flowchart,,http://arxiv.org/abs/1212.2531v1,cs.OH,['cs.OH'],,,[]
Brain Connectivity Analysis Methods for Better Understanding of Coupling,http://arxiv.org/abs/1212.3786v1,2012-12-16T13:08:34Z,2012-12-16T13:08:34Z,"  Action, cognition, emotion and perception can be mapped in the brain by using
set of techniques. Translating unimodal concepts from one modality to another
is an important step towards understanding the neural mechanisms. This paper
provides a comprehensive survey of multimodal analysis of brain signals such as
fMRI, EEG, MEG, NIRS and motivations, assumptions and pitfalls associated with
it. All these non-invasive brain modalities complement and restrain each other
and hence improve our understating of functional and neuronal organization. By
combining the various modalities together, we can exploit the strengths and
flaws of individual brain imaging methods. Integrated anatomical analysis and
functional measurements of human brain offer a powerful paradigm for the brain
mapping. Here we provide the brief review on non invasive brain modalities,
describe the future of co-analysis of these brain signals.
","['\nRevati Shriram\n', '\nDr. M. Sundhararajan\n', '\nNivedita Daimiwal\n']",7 Pages,,http://arxiv.org/abs/1212.3786v1,cs.OH,['cs.OH'],,,[]
Voltage Temperature Monitoring System (VTMS) for a BTS Room,http://arxiv.org/abs/1212.4913v1,2012-12-20T03:42:14Z,2012-12-20T03:42:14Z,"  Although Cellular communication is getting more and more popular in our
country present days, but its network improvement is hampered by the crysis of
electricity. The recent decision of present Government is that they will not
provide any electricity from the grid to any new BTS rooms of any Celluler
operator companies like Grammen Phone, Robi, Airtel etc. These companies have
to develop their own power stations either by using generators or by developing
solar plants. Now a days most of the BTS rooms, that the cellular operators are
installing with a generator and 48 volt battery backup. So for the
synchronisation of the operation of PDB, Generator and battery, they require a
device called Voltage Temperature Monitoring System or VTMS. It is a
Microcontroller based controlling unit which controlls the operation of
generator and battery when PDB in not available in the BTS room.
","['\nSadeque Reza Khan\n', '\nSiddique Reza Khan\n', '\nArifa Ferdousi\n']",,"International Journal of Instrumentation and Control Systems
  (IJICS), Volume: 02, Number:04, page: 01-10, ISSN: 2249 - 1147, year:2012",http://arxiv.org/abs/1212.4913v1,cs.OH,['cs.OH'],,,[]
Binary Sequences with Minimum Peak Sidelobe Level up to Length 68,http://arxiv.org/abs/1212.4930v1,2012-12-20T05:58:39Z,2012-12-20T05:58:39Z,"  Results of an exhaustive search for minimum peak sidelobe level binary
sequences are presented. Several techniques for efficiency implementation of
search algorithm are described. A table of number of non-equivalent optimal
binary sequences with minimum peak sidelobe (MPS) level up to length 68 is
given. This number can be used in prediction of the longest length for a given
sidelobe level of binary sequences. The examples of optimal binary MPS
sequences having high merit factor are shown.
","['\nAnatolii Leukhin\n', '\nEgor Potehin\n']","10 pages, 3figures,2 tables, submitting to International Worksho on
  Coding and Crypography, WCC 2013, April 15-19, 2013, Bergen, Norway",,http://arxiv.org/abs/1212.4930v1,cs.OH,['cs.OH'],,,[]
Guadalupe: a browser design for heterogeneous hardware,http://arxiv.org/abs/1212.5170v1,2012-12-19T18:28:59Z,2012-12-19T18:28:59Z,"  Mobile systems are embracing heterogeneous architectures by getting more
types of cores and more specialized cores, which allows applications to be
faster and more efficient. We aim at exploiting the hardware heterogeneity from
the browser without requiring any changes to either the OS or the web
applications. Our design, Guadalupe, can use hardware processing units with
different degrees of capability for matched browser services. It starts with a
weak hardware unit, determines if and when a strong unit is needed, and
seamlessly migrates to the strong one when necessary. Guadalupe not only makes
more computing resources available to mobile web browsing but also improves its
energy proportionality. Based on Chrome for Android and TI OMAP4, We provide a
prototype browser implementation for resource loading and rendering. Compared
to Chrome for Android, we show that Guadalupe browser for rendering can
increase other 3D application's frame rate by up to 767% and save 4.7% of the
entire system's energy consumption. More importantly, by using the two cases,
we demonstrate that Guadalupe creates the great opportunity for many browser
services to get better resource utilization and energy proportionality by
exploiting hardware heterogeneity.
","['\nZhen Wang\n', '\nFelix Xiaozhu Lin\n', '\nLin Zhong\n', '\nMansoor Chishtie\n']",,,http://arxiv.org/abs/1212.5170v1,cs.OH,['cs.OH'],,,[]
"Performance Improvement by Changing Modulation Methods for Software
  Defined Radios",http://arxiv.org/abs/1212.0114v1,2012-12-01T13:47:25Z,2012-12-01T13:47:25Z,"  This paper describes an automatic switching of modulation method to
reconfigure transceivers of Software Defined Radio (SDR) based wireless
communication system. The programmable architecture of Software Radio promotes
a flexible implementation of modulation methods. This flexibility also
translates into adaptively, which is used here to optimize the throughput of a
wireless network, operating under varying channel conditions. It is robust and
efficient with processing time overhead that still allows the SDR to maintain
its real-time operating objectives. This technique is studied for digital
wireless communication systems. Tests and simulations using an AWGN channel
show that the SNR threshold is 5dB for the case study.
","['\nBhalchandra B. Godbole\n', '\nDilip S. Aldar\n']",IJACSA,,http://arxiv.org/abs/1212.0114v1,cs.OH,['cs.OH'],,,[]
Centralized Integrated Spectrum Sensing for Cognitive Radios,http://arxiv.org/abs/1212.0116v1,2012-12-01T13:55:06Z,2012-12-01T13:55:06Z,"  Spectrum sensing is the challenge for cognitive radio design and
implementation, which allows the secondary user to access the primary bands
without interference with primary users. Cognitive radios should decide on the
best spectrum band to meet the Quality of service requirements over all
available spectrum bands. This paper investigates the integrated centralized
spectrum sensing techniques in multipath fading environment and the performance
was analyzed with energy detection and wavelet based sensing techniques for
unknown signal. Keywords: Cognitive Radio, Spectrum Sensing, Signal Detection,
Primary User, Secondary User
",['\nDilip s. Aldar\n'],,,http://arxiv.org/abs/1212.0116v1,cs.OH,['cs.OH'],,,[]
Review of Knowledge Management Systems As Socio-Technical System,http://arxiv.org/abs/1212.0387v1,2012-12-03T13:53:35Z,2012-12-03T13:53:35Z,"  Knowledge Management Systems as socio-technical systemperspectives has
recognized for decades. Practitioners and scholars belief Knowledge Management
is best carried out throught the optimization both technological and
social-aspect.Lacking of understand and consider both aspects could lead
organizations in misinterpretation while developing andimplementing Knowledge
Management System. There is a need for practical guidance how Knowledge
Management System should implement in organizations. We propose a framework
that could use by practitioner and manager as guidance in developing and
implementing Knowledge Management System as Socio-Technical Systems. The
framework developed base on Pan and Scarborough view of Knowledge Management as
Socio-Technical system. Our framework consists of: Infrastructure(technology),
Info structure (organizational structure) and Info culture (organizational
culture). This concept would lead practitioners get clear understand aspect
contribute to Knowledge Management System success as Socio-Technical System.
","['\nSetiawan Assegaff\n', '\nAb Razak Che Hussin\n']","6 pages, 3 pigures; International Journal of Computer Science Issues
  (IJCSI)Vol 9. Issue 5 2012",,http://arxiv.org/abs/1212.0387v1,cs.OH,['cs.OH'],,,[]
"BigFoot: Analysis, monitoring, tracking and sharing of bio-medical
  features of human appendages using consumer-grade home and office based
  imaging devices",http://arxiv.org/abs/1212.0992v1,2012-12-05T10:53:20Z,2012-12-05T10:53:20Z,"  Here we describe a system for personal and professional management and
analysis of bio-medical images captured using off-the-shelf, consumer-grade
imaging devices such as scanners, digital cameras, cellphones, webcams and
tablet PCs. Specifically, we describe an implementation of this system for the
analysis, monitoring and tracking of conditions and features of human feet
using a flatbed scanner as the image capture device and a custom-designed set
of algorithms and software to manage and analyze the acquired data.
","['\nSam Mavandadi\n', '\nSteve Feng\n', '\nFrank Yu\n', '\nRichard Yu\n', '\nAydogan Ozcan\n']",,,http://arxiv.org/abs/1212.0992v1,cs.OH,['cs.OH'],,,[]
"Hidden Markov Estimation of Bistatic Range From Cluttered Ultra-wideband
  Impulse Responses",http://arxiv.org/abs/1212.1080v1,2012-12-05T16:10:01Z,2012-12-05T16:10:01Z,"  Ultra-wideband (UWB) multistatic radar can be used for target detection and
tracking in buildings and rooms. Target detection and tracking relies on
accurate knowledge of the bistatic delay. Noise, measurement error, and the
problem of dense, overlapping multipath signals in the measured UWB channel
impulse response (CIR) all contribute to make bistatic delay estimation
challenging. It is often assumed that a calibration CIR, that is, a measurement
from when no person is present, is easily subtracted from a newly captured CIR.
We show this is often not the case. We propose modeling the difference between
a current set of CIRs and a set of calibration CIRs as a hidden Markov model
(HMM). Multiple experimental deployments are performed to collect CIR data and
test the performance of this model and compare its performance to existing
methods. Our experimental results show an RMSE of 2.85 ns and 2.76 ns for our
HMM-based approach, compared to a thresholding method which, if the ideal
threshold is known a priori, achieves 3.28 ns and 4.58 ns. By using the
Baum-Welch algorithm, the HMM-based estimator is shown to be very robust to
initial parameter settings. Localization performance is also improved using the
HMM-based bistatic delay estimates.
","['\nMerrick McCracken\n', '\nNeal Patwari\n']",,,http://arxiv.org/abs/1212.1080v1,cs.OH,['cs.OH'],,,[]
Foundations of scientific research (Foundations of Research Activities),http://arxiv.org/abs/1212.1651v1,2012-12-01T12:47:49Z,2012-12-01T12:47:49Z,"  During years 2008 to 2011 author gives several courses on Foundations of
Scientific Research at Computer Science Faculty of the National Aviation
University in Kiev. This text presents material to lectures of the courses. It
consists of 18 sections and some ideas of the manual can be seen from their
titles. These include: General notions about scientific research. Ontologies
and upper ontologies. Ontologies of object domains. Examples of Research
Activity. Some Notions of the Theory of Finite and Discrete Sets. Algebraic
Operations and Algebraic Structures. Elements of the Theory of Graphs and Nets.
Scientific activity on the example of Information and its investigation.
Scientific research in Artificial Intelligence. Compilers and compilation.
Objective, Concepts and History of Computer security. Methodological and
categorical apparatus of scientific research. Methodology and methods of
scientific research. Scientific idea and significance of scientific research.
Forms of scientific knowledge organization and principles of scientific
research. Theoretical study, applied study and creativity. Types of scientific
research: theoretical study, applied study. Types of scientific research: forms
of representation of material. Some sections of the text contain enough
material to lectures, but in some cases these are sketchs without references to
Foundations of Research Activities. Really this is the first version of the
manual and author plans to edit, modify and extend the version. Some reasons
impose the author to post it as e-print. . Author compiled material from many
sources and hope that it gives various points of view on Foundations of
Research Activities.
",['\nN. M. Glazunov\n'],167 pages,,http://arxiv.org/abs/1212.1651v1,cs.OH,"['cs.OH', '68-01, 68T30']",,,[]
Controlling Home Appliances Remotely through Voice Command,http://arxiv.org/abs/1212.1790v1,2012-12-08T13:38:19Z,2012-12-08T13:38:19Z,"  Controlling appliances is a main part of automation. The main object of Home
automation is to provide a wireless communication link of home appliances to
the remote user. The main objective of this work is to make such a system which
controls the home appliances remotely. This paper discusses two methods of
controlling home appliances one is via voice to text SMS and other is to use
the mobile as a remote control, this system will provide a benefit to the
elderly and disable people and also to those who are unaware of typing an SMS.
","['\nFaisal Baig\n', '\nSaira Beg\n', '\nMuhammad Fahad Khan\n']","4 pages, 4, figures, International Journal of Computer Applications",,http://dx.doi.org/10.5120/7437-0133,cs.OH,['cs.OH'],10.5120/7437-0133,,[]
Generating Strategic IS: Towards the Winning Strategy,http://arxiv.org/abs/1212.1882v1,2012-12-09T11:39:06Z,2012-12-09T11:39:06Z,"  In modern era, the role of information system in organization has been taken
many discussions. The models of information system are constantly updated.
However, most of them can not face the changing world. This paper discusses an
approach to generating of strategic information system based on features in
organization. We proposed an approach by using disadvantages in some tools of
analysis whereby the lack of analysis appear as behaviour of relation between
organisation and the world.
","['\nMaria Elfida\n', '\nMahyuddin K. M. Nasution\n']","5 pages, draft to ICOCSIM 2012",,http://arxiv.org/abs/1212.1882v1,cs.OH,['cs.OH'],,,[]
Performance Evaluation of DOA Estimation using MATLAB,http://arxiv.org/abs/1211.4442v1,2012-11-19T14:41:49Z,2012-11-19T14:41:49Z,"  This paper presents the performance analysis of directions of arrival
estimation techniques, Subspace and the Non-Subspace methods. In this paper,
exploring the analysis category of high resolution and super resolution
algorithms, presentation of description, comparison and the performance and
resolution analyses of these algorithms are made. Sensitivity to various
perturbations and the effect of parameters related to the design of the sensor
array itself such as the number of array elements and their spacing are also
investigated.
",['\nSai Suhas Balabadrapatruni\n'],,,http://arxiv.org/abs/1211.4442v1,cs.OH,['cs.OH'],,,[]
"Online Energy Generation Scheduling for Microgrids with Intermittent
  Energy Sources and Co-Generation",http://arxiv.org/abs/1211.4473v2,2012-11-19T15:59:09Z,2013-04-25T06:17:16Z,"  Microgrids represent an emerging paradigm of future electric power systems
that can utilize both distributed and centralized generations. Two recent
trends in microgrids are the integration of local renewable energy sources
(such as wind farms) and the use of co-generation (i.e., to supply both
electricity and heat). However, these trends also bring unprecedented
challenges to the design of intelligent control strategies for microgrids.
Traditional generation scheduling paradigms rely on perfect prediction of
future electricity supply and demand. They are no longer applicable to
microgrids with unpredictable renewable energy supply and with co-generation
(that needs to consider both electricity and heat demand). In this paper, we
study online algorithms for the microgrid generation scheduling problem with
intermittent renewable energy sources and co-generation, with the goal of
maximizing the cost-savings with local generation. Based on the insights from
the structure of the offline optimal solution, we propose a class of
competitive online algorithms, called CHASE (Competitive Heuristic Algorithm
for Scheduling Energy-generation), that track the offline optimal in an online
fashion. Under typical settings, we show that CHASE achieves the best
competitive ratio among all deterministic online algorithms, and the ratio is
no larger than a small constant 3.
","['\nLian Lu\n', '\nJinlong Tu\n', '\nChi-Kin Chau\n', '\nMinghua Chen\n', '\nXiaojun Lin\n']","26 pages, 13 figures. It will appear in Proc. of ACM SIGMETRICS, 2013",,http://arxiv.org/abs/1211.4473v2,cs.OH,"['cs.OH', 'C.4; F.1.2; I.2.8']",,,[]
Secured Ontology Mapping,http://arxiv.org/abs/1211.4705v1,2012-11-20T10:44:57Z,2012-11-20T10:44:57Z,"  Todays market evolution and high volatility of business requirements put an
increasing emphasis on the ability for systems to accommodate the changes
required by new organizational needs while maintaining security objectives
satisfiability. This is all the more true in case of collaboration and
interoperability between different organizations and thus between their
information systems. Ontology mapping has been used for interoperability and
several mapping systems have evolved to support the same. Usual solutions do
not take care of security. That is almost all systems do a mapping of
ontologies which are unsecured.We have developed a system for mapping secured
ontologies using graph similarity concept. Here we give no importance to the
strings that describe ontology concepts, properties etc. Because these strings
may be encrypted in the secured ontology. Instead we use the pure graphical
structure to determine mapping between various concepts of given two secured
ontologies. The paper also gives the measure of accuracy of experiment in a
tabular form in terms of precision, recall and F-measure.
","['\nManjula Shenoy. K\n', '\nK. C. Shet\n', '\nU. Dinesh Acharya\n']","arXiv admin note: substantial text overlap with arXiv:cs/0407061 by
  other authors",,http://arxiv.org/abs/1211.4705v1,cs.OH,['cs.OH'],,,[]
Memoization technique for optimizing functions with stochastic input,http://arxiv.org/abs/1211.5173v1,2012-11-21T23:50:24Z,2012-11-21T23:50:24Z,"  In this paper we present a strategy for optimization functions with
stochastic input. The main idea is to take advantage of decomposition in
combination with a look-up table. Deciding what input values should be used for
memoization is determined based on the underlying probability distribution of
input variables. Special attention is given to difficulties caused by
combinatorial explosion.
","['\nEdin H. Mulalić\n', '\nMiomir S. Stanković\n', '\nRadomir S. Stanković\n']",,,http://arxiv.org/abs/1211.5173v1,cs.OH,['cs.OH'],,,[]
"Understanding Complex Service Systems Through Different Lenses: An
  Overview",http://arxiv.org/abs/1211.5402v1,2012-11-23T01:50:46Z,2012-11-23T01:50:46Z,"  The 2011 Grand Challenge in Service conference aimed to explore, analyse and
evaluate complex service systems, utilising a case scenario of delivering on
improved perception of safety in the London Borough of Sutton, which provided a
common context to link the contributions. The key themes that emerged included
value co-creation, systems and networks, ICT and complexity, for which we
summarise the contributions. Contributions on value co-creation are based
mainly on empirical research and provide a variety of insights including the
importance of better understanding collaboration within value co-creation.
Contributions on the systems perspective, considered to arise from networks of
value co-creation, include efforts to understand the implications of the
interactions within service systems, as well as their interactions with social
systems, to co-create value. Contributions within the technological sphere,
providing ever greater connectivity between entities, focus on the creation of
new value constellations and new demand being fulfilled through hybrid
offerings of physical assets, information and people. Contributions on
complexity, arising from the value co- creation networks of technology enabled
services systems, focus on the challenges in understanding, managing and
analysing these complex service systems. The theory and applications all show
the importance of understanding service for the future.
","['\nGerard Briscoe\n', '\nKrista Keranen\n', '\nGlenn Parry\n']","12 pages, 1 figure",,http://arxiv.org/abs/1211.5402v1,cs.OH,['cs.OH'],,,[]
"Value, Variety and Viability: New Business Models for Co-Creation in
  Outcome-based Contracts",http://arxiv.org/abs/1211.5407v1,2012-11-23T02:59:08Z,2012-11-23T02:59:08Z,"  We propose that designing a manufacturer's equipment-based service value
proposition in outcome-based contracts is the design of a new business model
capable of managing threats to the firm's viability that can arise from the
contextual variety of use that customers may subject the firm's value
propositions. Furthermore, manufacturers need to understand these emerging
business models as the capability of managing both asset and service provision
to achieve use outcomes with customers, including emotional outcomes such as
customer experience. Service-Dominant Logic proposes that all ""goods are a
distribution mechanism for service provision"", upon which we propose a
value-centric approach to understanding the interactions between the asset and
service provision, and suggest a viable systems approach towards reorganising
the firm to achieve such a business model. Three case studies of B2B
equipment-based service systems were analysed to understand customers'
co-creation activities in achieving outcomes, in which we found that the
co-creation of complex multi-dimensional value could be delivered through the
different value propositions of the firm catering to different aspects
(dimensions) of the value to be co-created. The study provides a way for
managers to understand the effectiveness (rather than efficiency) of firms in
adopting emerging business models that design for value co-creation in what are
ultimately complex socio- technical systems.
","['\nIrene Ng\n', '\nGerard Briscoe\n']","26 pages, 3 figures, 1 table. arXiv admin note: text overlap with
  arXiv:1111.2651",,http://arxiv.org/abs/1211.5407v1,cs.OH,['cs.OH'],,,[]
A new class of SETI beacons that contain information (22-aug-2010),http://arxiv.org/abs/1211.6470v2,2012-11-27T22:44:02Z,2014-03-17T00:42:54Z,"  In the cm-wavelength range, an extraterrestrial electromagnetic narrow band
(sine wave) beacon is an excellent choice to get alien attention across
interstellar distances because 1) it is not strongly affected by interstellar /
interplanetary dispersion or scattering, and 2) searching for narrowband
signals is computationally efficient (scales as Ns log(Ns) where Ns = number of
voltage samples). Here we consider a special case wideband signal where two or
more delayed copies of the same signal are transmitted over the same frequency
and bandwidth, with the result that ISM dispersion and scattering cancel out
during the detection stage. Such a signal is both a good beacon (easy to find)
and carries arbitrarily large information rate (limited only by the atmospheric
transparency to about 10 GHz). The discovery process uses an autocorrelation
algorithm, and we outline a compute scheme where the beacon discovery search
can be accomplished with only 2x the processing of a conventional sine wave
search, and discuss signal to background response for sighting the beacon. Once
the beacon is discovered, the focus turns to information extraction.
Information extraction requires similar processing as for generic wideband
signal searches, but since we have already identified the beacon, the
efficiency of information extraction is negligible.
","['\nG. R. Harp\n', '\nR. F. Ackermann\n', '\nSamantha K. Blair\n', '\nJ. Arbunich\n', '\nP. R. Backus\n', '\nJ. C. Tarter\n', '\nthe ATA Team\n']","33 pages, 8 figures, 1 table",,http://arxiv.org/abs/1211.6470v2,astro-ph.IM,"['astro-ph.IM', 'cs.OH']",,,[]
A Semi-Structured Tailoring-Driven Approach for ERP Selection,http://arxiv.org/abs/1211.2445v1,2012-11-11T18:03:51Z,2012-11-11T18:03:51Z,"  It has been widely reported that selecting an inappropriate system is a major
reason for ERP implementation failures. The selection of an ERP system is
therefore critical. While the number of papers related to ERP implementation is
substantial, ERP evaluation and selection approaches have received few
attention. Motivated by the adaptation concept of the ERP systems, we propose
in this paper a semi-structured approach for ERP system selection that differs
from existing models in that it has a more holistic focus by simultaneously 1)
considering the anticipated fitness of ERP solutions after the optimal
resolution, within limited resources, of a set of the identified mismatches and
2) evaluating candidate products according to both functional and
non-functional requirements. The approach consists of an iterative selection
process model and an evaluation methodology based on 0-1 linear programming and
MACBETH technique to elaborate multi-criteria performance expressions.
","['\nAbdelilah Khaled\n', '\nMohammed Abdou Janati Idrissi\n']","10 pages, 7 figues; IJCSI International Journal of Computer Science
  Issues, Vol. 9, Issue 5, No 2, September 2012","IJCSI International Journal of Computer Science Issues, Vol. 9,
  Issue 5, No 2, September 2012",http://arxiv.org/abs/1211.2445v1,cs.OH,"['cs.OH', 'H.3.4; C.4; D.2.1; D.2.2; D.2.8; D.2.9; D.2.10; G.1.1; G.1.3; G.1.6']",,,[]
"Modified Stage-Gate: A Conceptual Model of Virtual Product Development
  Process",http://arxiv.org/abs/1210.7482v1,2012-10-28T17:50:48Z,2012-10-28T17:50:48Z,"  In today s dynamic marketplace, manufacturing companies are under strong
pressure to introduce new products for long-term survival with their
competitors. Nevertheless, every company cannot cope up progressively or
immediately with the market requirements due to knowledge dynamics being
experienced in the competitive milieu. Increased competition and reduced
product life cycles put force upon companies to develop new products faster. In
response to these pressing needs, there should be some new approach compatible
in flexible circumstances. This paper presents a solution based on the popular
Stage-Gate system, which is closely linked with virtual team approach. Virtual
teams can provide a platform to advance the knowledge-base in a company and
thus to reduce time-to-market. This article introduces conceptual product
development architecture under a virtual team umbrella. The paper describes all
the major aspects of new product development (NPD), NPD process and its
relationship with virtual teams, Stage-Gate system finally presents a modified
Stage-Gate system to cope up with the changing needs. It also provides the
guidelines for the successful implementation of virtual teams in new product
development.
","['\nNader Ale Ebrahim\n', '\nShamsuddin Ahmed\n', '\nZahari Taha\n']",24 pages,"African Journal of Marketing Management, 1(9) (2009) 211-219",http://arxiv.org/abs/1210.7482v1,cs.OH,"['cs.OH', '90B50, 90C29, 90C31, 91A35, 91B06', 'D.2.9; K.1']",,,[]
Virtual Collaborative R&D Teams in Malaysia Manufacturing SMEs,http://arxiv.org/abs/1210.7889v1,2012-10-30T02:41:51Z,2012-10-30T02:41:51Z,"  This paper presents the results of empirical research conducted during March
to September 2009. The study focused on the influence of virtual research and
development teams within Malaysian manufacturing small and medium sized
enterprises (SMEs). The specific objective of the study is better understanding
of the application of collaborative technologies in business, to find the
effective factors to assist SMEs to remain competitive in the future. The paper
stresses to find an answer for a question Is there any relationship between
company size, Internet connection facility and virtuality?. The survey data
shows SMEs are now technologically capable of performing the virtual
collaborative team, but the infrastructure usage is less. SMEs now have the
necessary technology to begin the implementation process of collaboration tools
to reduce research and development time, costs and increase productivity. So,
the manager of R and D should take the potentials of virtual teams into
account.
","['\nNader Ale Ebrahim\n', '\nShamsuddin Ahmed\n', '\nSalwa Hanim Abdul Rashid\n', '\nZahari Taha\n', '\nM. A. Wazed\n']",4 pages,"(2012). Virtual Collaborative R&D Teams in Malaysia Manufacturing
  SMEs. Advanced Materials Research, 433-440, 1653-1659",http://dx.doi.org/10.4028/www.scientific.net/AMR.433-440.1653,cs.OH,"['cs.OH', '90B50, 90C29, 90C31, 91A35, 91B06', 'D.2.9; K.1']",10.4028/www.scientific.net/AMR.433-440.1653,,[]
"Project G.N.O.S.I.S.: Geographical Network Of Synoptic Information
  System",http://arxiv.org/abs/1211.0645v1,2012-11-03T22:33:34Z,2012-11-03T22:33:34Z,"  Everybody knows how much synoptic maps are useful today. An excellent example
above all is Google Earth: its simplicity and friendly interface allows every
user to have the Earth maps ready in just one simple layout; nevertheless a
crucial dimension is missing in Google Earth: the time. This doesn't mean we
simply aim to add history to Google Earth (though it could be already a nice
goal): the main idea behind GNOSIS project is to produce applications to ""dress
up"" the Globe with a set of skin-maps representing the most various different
kind of histories like the evolution of geology, genetics, agriculture,
ethnology, linguistics, musicology, metallurgy and so forth, in time. It may be
interesting in the near future to have such a possibility to watch on the map
the positions and movements of the armies during the battles of Waterloo or
Thermopylae, the spreading of the cultivation of corn in time, the rise and
fall of Roman Empire or the diffusion of Smallpox together with the spread of a
religion, a specific dialect, the early pottery techniques or the natural
resources available to pre-Columbian civilizations on a Google-Earth-map-like,
that is to say to have at one's hand the ultimate didactic-enciclopedic tool.
To do so we foresee the use of a general-purpose intermediate/high level
programming language, possibly object-oriented such C++ or Java.
",['\nPietro Oliva\n'],"3 pages, Proposal for future project",,http://arxiv.org/abs/1211.0645v1,cs.OH,['cs.OH'],,,[]
An Evaluation of Arabic Language Learning Websites,http://arxiv.org/abs/1211.0716v1,2012-11-04T19:47:05Z,2012-11-04T19:47:05Z,"  As a result of ICT development and the increasingly growing use of the
Internet in particular, practices of language teaching and learning are about
to evolve significantly. Our study focuses on the Arabic language, and aims to
explore and evaluate Arabic language learning websites. To reach these goals,
we propose in a first step, to define an evaluation model, based on a set of
criteria for assessing the quality of websites dedicated to teaching and
learning Arabic. We subsequently apply our model on a set of Arabic sites
available on the web and give an assessment of these web sites. We finally
discuss their strengths and limitations.
","['\nHadhemi Achour\n', '\nWahiba Ben Abdessalem\n']",International Conference on Education and E-Learning Innovations,,http://arxiv.org/abs/1211.0716v1,cs.OH,"['cs.OH', '68']",,,[]
Information and Communication Technology in Combating Counterfeit Drugs,http://arxiv.org/abs/1211.1242v1,2012-11-01T15:29:05Z,2012-11-01T15:29:05Z,"  Pharma frauds are on the rise, counterfeit drugs are giving sleepless nights
to patients, pharmaceutical companies and governments. The laws prohibiting the
sales of counterfeit drugs cannot succeed without technological interventions.
Several analytical techniques and tools including spectroscopy, holograms,
barcoding, differentiated packing, radio frequency identification,
fingerprints, hyperspectral imaging etc. have been employed over the years in
combating this menace; however this challenge is becoming more sophisticated
with the evolution of the World Wide Web and online pharmacies. This paper
presents a review on the contribution of Information and Communication
Technology (ICT) as a drug counterfeit countermeasure.
",['\nHaruna Isah\n'],,"International Journal of Engineering and Technology Volume 2 No.
  9, September, 2012",http://arxiv.org/abs/1211.1242v1,cs.OH,['cs.OH'],,,[]
"From the Closed Classical Algorithmic Universe to an Open World of
  Algorithmic Constellations",http://arxiv.org/abs/1211.4547v1,2012-10-29T19:49:07Z,2012-10-29T19:49:07Z,"  In this paper we analyze methodological and philosophical implications of
algorithmic aspects of unconventional computation. At first, we describe how
the classical algorithmic universe developed and analyze why it became closed
in the conventional approach to computation. Then we explain how new models of
algorithms turned the classical closed algorithmic universe into the open world
of algorithmic constellations, allowing higher flexibility and expressive
power, supporting constructivism and creativity in mathematical modeling. As
Goedels undecidability theorems demonstrate, the closed algorithmic universe
restricts essential forms of mathematical cognition. In contrast, the open
algorithmic universe, and even more the open world of algorithmic
constellations, remove such restrictions and enable new, richer understanding
of computation.
","['\nMark Burgin\n', '\nGordana Dodig-Crnkovic\n']",,,http://arxiv.org/abs/1211.4547v1,cs.OH,['cs.OH'],,,[]
A model for quantum queue,http://arxiv.org/abs/1210.8339v1,2012-10-31T14:24:54Z,2012-10-31T14:24:54Z,"  We consider an extension of Discrete Time Markov Chain queueing model to the
quantum domain by use of Discrete Time Quantum Markov Chain. We introduce
methods for numerical analysis of such models. Using this tools we show that
quantum model behaves fundamentally differently from the classical one.
","['\nPiotr Gawron\n', '\nDariusz Kurzyk\n', '\nZbigniew Puchała\n']","14 pages, 7 figures","Int. J. Quantum Inform. 11, 1350023 (2013)",http://dx.doi.org/10.1142/S0219749913500238,quant-ph,"['quant-ph', 'cs.OH']",10.1142/S0219749913500238,,[]
Dimensions and issues of mobile agent technology,http://arxiv.org/abs/1210.4644v1,2012-10-17T06:52:04Z,2012-10-17T06:52:04Z,"  Mobile Agent is a type of software system which acts ""intelligently"" on one's
behalf with the feature of autonomy, learning ability and most importantly
mobility. Now mobile agents are gaining interest in the research community. In
this article mobile agents will be addressed as tools for mobile computing.
Mobile agents have been used in applications ranging from network management to
information management. We present mobile agent concept, characteristics,
classification, need, applications and technical constraints in the mobile
technology. We also provide a brief case study about how mobile agent is used
for information retrieval.
","['\nYashpal Singh\n', '\nKapil Gulati\n', '\nS. Niranjan\n']","11 pages, 6 figure","International Journal of Artificial Intelligence & Applications
  (IJAIA), Vol.3, No.5, 2012, 51-61",http://arxiv.org/abs/1210.4644v1,cs.OH,['cs.OH'],,,[]
A Robust Lot Sizing Problem with Ill-known Demands,http://arxiv.org/abs/1210.5386v1,2012-10-19T11:53:50Z,2012-10-19T11:53:50Z,"  The paper deals with a lot sizing problem with ill-known demands modeled by
fuzzy intervals whose membership functions are possibility distributions for
the values of the uncertain demands. Optimization criteria, in the setting of
possibility theory, that lead to choose robust production plans under fuzzy
demands are given. Some algorithms for determining optimal robust production
plans with respect to the proposed criteria, and for evaluating production
plans are provided. Some computational experiments are presented.
","['\nRomain Guillaume\n', '\nPrzemyslaw Kobylanski\n', '\nPawel Zielinski\n']",,Fuzzy Sets and Systems 206 (2012) 39-57,http://dx.doi.org/10.1016/j.fss.2012.01.015,cs.OH,['cs.OH'],10.1016/j.fss.2012.01.015,,[]
"Propuesta de sistema GeoInformático con representación de escenarios
  para auxiliar en la nueva metodología propuesta por INETER y la UNI para el
  estudio a gran escala de la vulnerabilidad y daños debido a sismos en las
  edificaciones",http://arxiv.org/abs/1210.6154v2,2012-10-23T07:41:58Z,2012-10-26T23:01:13Z,"  A GIS based software is presented which permits the estimation of seismic
vulnerability and the presentation of results in digital maps for single
houses, groups of buildings, parts of settlements or even complete towns.
Nicaragua is a country with a high seismic activity. The assessment of seismic
vulnerability requires the execution of distinct tasks, e.g. recollection of
field data, integration of data from the municipal cadastre, reprocessing or
screening to test the reliability of the data, definition of calculation of
vulnerability functions, calculation of vulnerability for single objects as
houses or buildings, calculation of mean vulnerability for certain areas as
barrios or squares. In order to reduce time and effort to be spent with several
unspecialized tools and procedures, an integrated software system was created,
the user of which has not to care about separate software tools for each part
of the process. The main advantage of the software is the combination of
Geographical Information System (GIS) with the logics that surrounds the
specific methodologies of seismic vulnerability index, index of damages and
presentation of results. The new software uses a connection with an external
centralized Enterprise Data Base which stores all the input information and
calculation results and which is automatically synchronized for the
presentation of results using GIS. The cadastral information contains data on
the constructive type of the house, dimensions, year of construction, type of
walls, roof, number of inhabitants, etc.. The system also allows to present
damage scenarios for specific seismic events with given hypocenter and
magnitude. The documentation of the software serves as a guide for students
working on object oriented software engineering by using unified modeling
language (UML) and software logic architecture (3-tiers).
","['\nFederico-Vladimir Gutierrez-Corea\nUNI\n', '\nAdolfo-Javier Urrutia-Zambrana\nUNI\n']",Published date: 02/07/2007,,http://arxiv.org/abs/1210.6154v2,cs.OH,['cs.OH'],,,"['UNI', 'UNI']"
"Designing a High Efficiency Pulse Width Modulation Step-Down DC/DC
  Converter for Mobile Phone Applications",http://arxiv.org/abs/1210.6231v1,2012-10-22T19:11:04Z,2012-10-22T19:11:04Z,"  This paper presents the design and analysis of a high efficiency, PWM
(Pulse-Width-Modulation) Buck converters for mobile phone applications. The
steady-state and average-value models for the proposed converter are developed
and simulated. A practical design approach which aims at systematizing the
procedure for the selection of the control parameters is introduced. The
switching losses are reduced by using soft switching, additionally, a simple
analog and digital form of the controller for practical realization is
provided. It is found that this controller adopts a structure similar to the
conventional PWM voltage mode controller. The proposed circuit uses a
current-mode control and a voltage-to-pulse converter for the PWM. The circuit,
fabricated using a 0.18-{\mu}m CMOS technology, reaches a peak load regulation
of 20 mV/V and line regulation of 0.5 mV/V at Current load equal 300 mA. The
used 10{\mu}H inductance and 22{\mu}F capacitor and requires clock and
Vref/Vramp input of 1,23V.
","['\nBenlafkih Abdessamad\n', '\nKrit Salah-ddine\n', '\nChafik Elidrissi Mohamed\n']","7 pages, IJCSI International Journal of Computer Science Issues, Vol.
  9, Issue 5, No 3, September 2012",,http://arxiv.org/abs/1210.6231v1,cs.OH,['cs.OH'],,,[]
Plagiarism Detection: Keeping Check on Misuse of Intellectual Property,http://arxiv.org/abs/1210.7678v1,2012-10-19T17:55:30Z,2012-10-19T17:55:30Z,"  Today, Plagiarism has become a menace. Every journal editor or conference
organizers has to deal with this problem. Simply Copying or rephrasing of text
without giving due credit to the original author has become more common. This
is considered to be an Intellectual Property Theft. We are developing a
Plagiarism Detection Tool which would deal with this problem. In this paper we
discuss the common tools available to detect plagiarism and their short comings
and the advantages of our tool over these tools.
","['\nIti Mathur\n', '\nNisheeth Joshi\n']","Proceedings of National Conference on Recent Advances in Computer
  Engineering, 2011",,http://arxiv.org/abs/1210.7678v1,cs.OH,['cs.OH'],,,[]
A Synthesis Method for Quaternary Quantum Logic Circuits,http://arxiv.org/abs/1210.8055v1,2012-10-26T12:34:05Z,2012-10-26T12:34:05Z,"  Synthesis of quaternary quantum circuits involves basic quaternary gates and
logic operations in the quaternary quantum domain. In this paper, we propose
new projection operations and quaternary logic gates for synthesizing
quaternary logic functions. We also demonstrate the realization of the proposed
gates using basic quantum quaternary operations. We then employ our synthesis
method to design of quaternary adder and some benchmark circuits. Our results
in terms of circuit cost, are better than the existing works.
","['\nSudhindu Bikash Mandal\n', '\nAmlan Chakrabarti\n', '\nSusmita Sur-Kolay\n']",10 pages,"Progress in VLSI Design and Test Lecture Notes in Computer Science
  Volume 7373, 2012, pp 270-280",http://dx.doi.org/10.1007/978-3-642-31494-0_31,cs.OH,"['cs.OH', 'quant-ph']",10.1007/978-3-642-31494-0_31,,[]
Integration of CAD and rapid manufacturing for sand casting optimisation,http://arxiv.org/abs/1210.2089v1,2012-10-07T18:32:04Z,2012-10-07T18:32:04Z,"  In order to reduce the time and costs of the products development in the sand
casting process, the SMC Colombier Fontaine company has carried out a study
based on tooling manufacturing with a new rapid prototyping process. This
evolution allowed the adequacy of the geometry used for the simulation to the
tooling employed physically in the production. This allowed a reduction of the
wall thickness to 4mm and retained reliable manufacturing process.
","['\nAlain Bernard\nIRCCyN\n', '\nJean-Charles Delplace\nCetim\n', '\nNicolas Perry\nIRCCyN\n', '\nSerge Gabriel\n']",,,http://dx.doi.org/10.1108/13552540310502220,cs.OH,['cs.OH'],10.1108/13552540310502220,,"['IRCCyN', 'Cetim', 'IRCCyN']"
Customised high-value document generation,http://arxiv.org/abs/1210.2090v1,2012-10-07T18:33:18Z,2012-10-07T18:33:18Z,"  Contributions of different experts to innovation projects improve enterprise
value, captured in documents. A subset of them is the centre of expert
constraint convergence. Their production needs to be tailored case by case.
Documents are often considered as knowledge transcription. As the base of a
structured knowledge-based information environment, this paper presents a
global approach that helps knowledge-integration tool deployment. An example,
based on process plan in aircraft manufacturing, indicates how fundamental
understanding of domain infrastructure contributes to a more coherent
architecture of knowledge-based information environments. A comparison with an
experiment in insurance services generalised the application of presented
principles.
","['\nNiek Du Preez\nGCC\n', '\nNicolas Perry\nIRCCyN\n', '\nAlexandre Candlot\nIRCCyN\n', '\nAlain Bernard\nIRCCyN\n', '\nWilhelm Uys\nGCC\n', '\nLouis Louw\nGCC\n']",,"CIRP Annals 54, 1 (2005) 123-126",http://dx.doi.org/10.1016/S0007-8506(07)60064-X,cs.OH,['cs.OH'],10.1016/S0007-8506(07)60064-X,,"['GCC', 'IRCCyN', 'IRCCyN', 'IRCCyN', 'GCC', 'GCC']"
"VCS: Value Chains Simulator, a Tool for Value Analysis of Manufacturing
  Enterprise Processes (A Value-Based Decision Support Tool)",http://arxiv.org/abs/1210.2091v1,2012-10-07T18:33:51Z,2012-10-07T18:33:51Z,"  Manufacturing enterprises are facing a competitive challenge. This paper
proposes the use of a value chain based approach to support the modelling and
simulation of manufacturing enterprise processes. The aim is to help experts to
make relevant decisions on product design and/or product manufacturing process
planning. This decision tool is based on the value chain modelling, by
considering the product requirements. In order to evaluate several performance
indicators, a simulation of various potential value chains adapted to market
demand was conducted through a Value Chains Simulator (VCS). A discrete event
simulator is used to perform the simulation of these scenarios and to evaluate
the value as a global performance criterion (balancing cost, quality, delivery
time, services, etc.). An Analytical Hierarchy Process module supports the
analysis process. The value chain model is based on activities and uses the
concepts of resource consumption, while integrating the benefiting entities
view point. A case study in the microelectronic field is carried out to
corroborate the validity of the proposed VCS.
","['\nMagali Mauchand\nUTT\n', '\nAli Siadat\nLGM2B\n', '\nNicolas Perry\nLGM2B\n', '\nAlain Bernard\nIRCCyN\n']",,Journal of Intelligent Manufacturing (2010) 700x-700x,http://dx.doi.org/10.1007/s10845-010-0452,cs.OH,['cs.OH'],10.1007/s10845-010-0452,,"['UTT', 'LGM2B', 'LGM2B', 'IRCCyN']"
"Minimum Component Based First-Order Inverting and Non-inverting Outputs
  of All-Pass Filter at the Same Circuit",http://arxiv.org/abs/1210.2485v1,2012-10-09T04:27:16Z,2012-10-09T04:27:16Z,"  In this paper, a new voltage-mode first order all-pass filter using minimum
active and passive components is presented. The proposed circuit employs one
fully differential second generation current conveyor (FDCCII), one grounded
capacitor, one resistor and offers the following advantages: the use of only
grounded capacitor which is attractive for integrated circuit implementation,
low active and passive sensitivities, providing inverting and non-inverting
voltage-mode all-pass responses simultaneously from the single circuit and no
requirement for component matching conditions. The theory is validated through
PSPICE simulation using TSMC 0.35micrometer CMOS process parameters.
","['\nJ. Mohan\n', '\nS. Maheshwari\n', '\nD. S. Chauhan\n']","ACEEE International Journal on Electrical and Power Engineering, 2010",,http://arxiv.org/abs/1210.2485v1,cs.OH,['cs.OH'],,,[]
"Minimum Grounded Component Based Voltage-Mode Quadrature Oscillator
  using DVCC",http://arxiv.org/abs/1210.2514v1,2012-10-09T07:34:55Z,2012-10-09T07:34:55Z,"  In this paper, a new voltage-mode quadrature oscillator using minimum number
of active and passive component is proposed. The proposed circuit employs
single modified DVCC, two grounded capacitor and two grounded resistors, which
is ideal for IC implementation. The active and passive sensitivity are no more
than unity. The proposed circuit is verified through PSPICE simulation results.
","['\nJ. Mohan\n', '\nS. Maheshwari\n', '\nD. S. Chauhan\n']","International Journal on Recent Trends in Engineering & Technology,
  vol.3, issue 4, 2010",,http://arxiv.org/abs/1210.2514v1,cs.OH,['cs.OH'],,,[]
"Performance Evaluation of Mobile U-Navigation based on GPS/WLAN
  Hybridization",http://arxiv.org/abs/1210.3091v1,2012-10-11T00:05:37Z,2012-10-11T00:05:37Z,"  This paper present our mobile u-navigation system. This approach utilizes
hybridization of wireless local area network and Global Positioning System
internal sensor which to receive signal strength from access point and the same
time retrieve Global Navigation System Satellite signal. This positioning
information will be switched based on type of environment in order to ensure
the ubiquity of positioning system. Finally we present our results to
illustrate the performance of the localization system for an indoor/ outdoor
environment set-up.
","['\nWan Mohd Yaakob Wan Bejuri\n', '\nMohd Murtadha Mohamad\n', '\nMaimunah Sapri\n', '\nMohd Adly Rosly\n']",Journal of Convergence Information Technology(JCIT),,http://arxiv.org/abs/1210.3091v1,cs.OH,['cs.OH'],,,[]
Microelectromechanical system cantilever-based frequency doublers,http://arxiv.org/abs/1210.3491v1,2012-10-12T12:27:11Z,2012-10-12T12:27:11Z,"  Microelectromechanical system (MEMS) based on-chip resonators offer great
potential for high frequency signal processing circuits like reference
oscillators and filters. This is due to their exceptional features like small
size, large frequency-quality factor product, integrability with CMOS ICs, low
power consumption, low cost batch fabrication etc. A capacitively transduced
cantilever beam resonator is one such popular MEMS resonator topology. In this
letter, the inherent square-law nonlinearity of the voltage-to-force transfer
function of a cantilever resonator's capacitive transducer has been employed
for the realization of frequency doubling effect. Using this concept, frequency
doubling of input signals of 500 kHz to 1 MHz, and 227.5 kHz to 455 kHz has
been experimentally demonstrated for two cantilever beams of length 51.75 and
76.75 micrometer respectively. The MEMS cantilevers have been fabricated with
polysilicon using the PolyMUMPs surface micromachining process, and their
testing has been performed using Laser Doppler Vibrometry. The test results
obtained are in reasonable compliance with the analytical and CoventorWare
finite-element simulation results. The high efficiency demonstrated by the
cantilever frequency doubler makes it a promising choice for signal generation
at high frequencies.
","['\nJoydeep Basu\n', '\nTarun K. Bhattacharyya\n']","The final, definitive version of this paper has been published in
  Journal of Intelligent Material Systems and Structures, 2012 by SAGE
  Publications Ltd. (http://online.sagepub.com), All rights reserved. Journal
  of Intelligent Material Systems and Structures, 2012",,http://dx.doi.org/10.1177/1045389X12461695,cs.OH,['cs.OH'],10.1177/1045389X12461695,,[]
Varactor-Based Dynamic Load Modulation of High Power Amplifiers,http://arxiv.org/abs/1210.3494v2,2012-10-12T12:28:54Z,2012-10-24T14:51:51Z,"  In this work, dynamic load modulation of high power amplifiers using a
varactor-based tunable matching network is presented. The feasibility of
dynamic tuning and efficiency enhancement of this technique is demonstrated
using a modular design approach for two existing high efficiency power
amplifiers (PA), a 7-W class-E, and a 10-W class-J power amplifier PA at 1 GHz.
For this purpose and for each of the PAs, a simple quasi-static inverse model
is developed allowing an efficiency-optimized control of the PA and the
varactor-based tunable matching network. Modulated measurements using a single
carrier WCDMA signal with 11.3 dB peak-to-average ratio (PAR) indicate about 10
to 14 percentage units improvements in the average power-added efficiency (PAE)
for the complete architecture.
","['\nAli Soltani Tehrani\n', '\nHossein Mashad Nemati\n', '\nHaiying Cao\n', '\nThomas Eriksson\n', '\nChristian Fager\n']",,,http://arxiv.org/abs/1210.3494v2,cs.OH,['cs.OH'],,,[]
Économie des biens immatériels - Economics of Intangible Goods,http://arxiv.org/abs/1210.4014v3,2012-10-15T13:00:01Z,2012-11-26T15:20:56Z,"  We introduce a new economic system suited for Intangible Goods ({\sc ig}). We
argue that such system can now be implemented in the real world using advance
technics in distributed network computing and cryptography. The specification
of the so called \net{} is presented. To Limit the number of financial
transactions, the system is forced to define its own currency, with many
benefits. The new ""cup"" currency, extended worldwide, is dedicated to {\sc ig},
available only for person-to-person trading, protected from speculation and
adapted for tax recovery with no additional computation. Those nices features
makes the \net{} a new democratic tool, fixing specific issues in {\sc ig}
trading and reviving a whole domain activity. We emphasis on the fact that all
proposed documentation, algorithm, program in any language related to this
proposal shall be open-source without any possibility to post any patent of any
sort on the system or subsystem. This new trading model should be considered as
a pure intellectual construction, like parts of Mathematics and then belongs to
nobody or everybody, like $1+1=2$. Next step will be to test, validate the
security of various implementations details, and to ask for legal rules
adaptations. The first draft paper is written in French language and posted to
arXiv.org and hal.archive-ouverte.fr . We expect to provide an English
translation before Christmas.
",['\nLaurent Fournier\n'],First draft in French...expected in English in December,,http://arxiv.org/abs/1210.4014v3,cs.OH,['cs.OH'],,,[]
A Tutorial for Creating and Publishing Open Source Lisp Software,http://arxiv.org/abs/1209.5626v1,2012-09-25T14:41:47Z,2012-09-25T14:41:47Z,"  The proliferation and accessability of the Internet have made it simple to
view, download, and publish source code. This paper gives a short tutorial on
how to create a new Common Lisp project and publish it.
",['\nRobert Smith\n'],Accepted for the International Lisp Conference 2012,,http://arxiv.org/abs/1209.5626v1,cs.OH,"['cs.OH', 'A.m; D.2.6; D.2.7; D.2.13']",,,[]
Smart Charging Technologies for Portable Electronic Devices,http://arxiv.org/abs/1209.5931v2,2012-09-25T13:37:10Z,2013-03-23T15:30:17Z,"  In this article we describe our efforts of extending demand-side control
concepts to the application in portable electronic devices, such as laptop
computers, mobile phones and tablet computers. As these devices feature
built-in energy storage (in the form of batteries) and the ability to run
complex control routines, they are ideal for the implementation of smart
charging concepts. We developed a prototype of a smart laptop charger that
controls the charging process depending on the locally measured frequency of
the electricity grid. If this technique is incorporated into millions of
devices in UK households, this will contribute significantly to the stability
of the electricity grid, help to mitigate the power production fluctuations
from renewable energy sources and avoid the high cost of building and
maintaining conventional power plants as standby reserve.
","['\nStefan Hild\n', '\nSean Leavey\n', '\nChristian Gräf\n', '\nBorja Sorazu\n']","Updated version with a new section describing a software based
  charging control of a laptop",,http://arxiv.org/abs/1209.5931v2,cs.OH,['cs.OH'],,,[]
Vulnerability Management for an Enterprise Resource Planning System,http://arxiv.org/abs/1209.6484v1,2012-09-28T11:34:27Z,2012-09-28T11:34:27Z,"  Enterprise resource planning (ERP) systems are commonly used in technical
educational institutions(TEIs). ERP systems should continue providing services
to its users irrespective of the level of failure. There could be many types of
failures in the ERP systems. There are different types of measures or
characteristics that can be defined for ERP systems to handle the levels of
failure. Here in this paper, various types of failure levels are identified
along with various characteristics which are concerned with those failures. The
relation between all these is summarized. The disruptions causing
vulnerabilities in TEIs are identified .A vulnerability management cycle has
been suggested along with many commercial and open source vulnerability
management tools. The paper also highlights the importance of resiliency in ERP
systems in TEIs.
","['\nShivani Goel\n', '\nRavi Kiran\n', '\nDeepak Garg\n']",,"International Journal of Computer Applications Foundation of
  Computer Science, Volume 53, No.4, 2012, pp. 19-22",http://dx.doi.org/10.5120/8409-2043,cs.OH,['cs.OH'],10.5120/8409-2043,,[]
A Review Paper on Microprocessor Based Controller Programming,http://arxiv.org/abs/1210.0576v1,2012-10-01T21:03:03Z,2012-10-01T21:03:03Z,"  Designing of microprocessor based controllers requires specific hardware as
well as software programming. Programming depends upon type of the software
whether operating software or application software. Programming requires
knowledge of system configuration and controller specific programming. Programs
are always in digital form so microprocessor can control directly at digital
level called Direct Digital Control (DDC).
","['\nJaswinder Singh Dilawari\n', '\nGurpreet Singh Sandhu\n']","5 pages,4 Figures","International Journal of Advanced Research in Computer Science and
  Software Engineering ,Volume 2,Issue 8,August2012",http://arxiv.org/abs/1210.0576v1,cs.OH,['cs.OH'],,,[]
PCNM: A New Platform for Cellular Networks Measurements and Optimization,http://arxiv.org/abs/1210.0510v1,2012-10-01T19:15:19Z,2012-10-01T19:15:19Z,"  In this paper, we present PCNM, a new mobile platform for cellular networks
measurements. PCNM is based on a set of techniques that tailors theoretical
calculations and simulations to the real cellular network environment. It
includes: (a) modules that measure different parameters of a base station (BS)
such as localization, cells identification, time advance information, reception
level and quality, (b) a new protocol that optimizes the task of network
measurement by monitoring a set of mobile nodes and finally (c) the ability to
extend an existing cellular network by adding new base stations. We evaluate
our genetic algorithm used to reduce the nodes mobility and optimize the
measurement extraction of N base stations using k mobile sensors (k >= 1). We
show how connecting real measurements (using mobile sensors in a collaborative
way) to theoretical and prediction methods is of high benefits for cellular
networks maintenance, extension and performances evaluation.
","['\nTayeb Lemlouma\n', '\nYoann Lefebvre\n', '\nFrédéric Cespedes\n']","IEEE International Conference on Wireless Communications, Networking
  and Mobile Computing (WiCOM 2007)",,http://dx.doi.org/10.1109/WICOM.2007.737,cs.NI,"['cs.NI', 'cs.OH', 'cs.PF', 'C.2']",10.1109/WICOM.2007.737,,[]
CrowdInside: Automatic Construction of Indoor Floorplans,http://arxiv.org/abs/1209.3794v1,2012-09-17T20:41:27Z,2012-09-17T20:41:27Z,"  The existence of a worldwide indoor floorplans database can lead to
significant growth in location-based applications, especially for indoor
environments. In this paper, we present CrowdInside: a crowdsourcing-based
system for the automatic construction of buildings floorplans. CrowdInside
leverages the smart phones sensors that are ubiquitously available with humans
who use a building to automatically and transparently construct accurate motion
traces. These accurate traces are generated based on a novel technique for
reducing the errors in the inertial motion traces by using the points of
interest in the indoor environment, such as elevators and stairs, for error
resetting. The collected traces are then processed to detect the overall
floorplan shape as well as higher level semantics such as detecting rooms and
corridors shapes along with a variety of points of interest in the environment.
Implementation of the system in two testbeds, using different Android phones,
shows that CrowdInside can detect the points of interest accurately with 0.2%
false positive rate and 1.3% false negative rate. In addition, the proposed
error resetting technique leads to more than 12 times enhancement in the median
distance error compared to the state-of-the-art. Moreover, the detailed
floorplan can be accurately estimated with a a relatively small number of
traces. This number is amortized over the number of users of the building. We
also discuss possible extensions to CrowdInside for inferring even higher level
semantics about the discovered floorplans.
","['\nMoustafa Alzantot\n', '\nMoustafa Youssef\n']",,"20th ACM SIGSPATIAL International Conference on Advances in
  Geographic Information Systems (ACM SIGSPATIAL GIS 2012)",http://arxiv.org/abs/1209.3794v1,cs.OH,['cs.OH'],,,[]
"Performance Analysis of MIMO Radar Waveform using Accelerated Particle
  Swarm Optimization Algorithm",http://arxiv.org/abs/1209.4015v1,2012-09-16T03:01:09Z,2012-09-16T03:01:09Z,"  The Accelerated Particle Swarm Optimization Algorithm is promoted to
numerically design orthogonal Discrete Frequency Waveforms and Modified
Discrete Frequency Waveforms (DFCWs) with good correlation properties for MIMO
radar. We employ Accelerated Particle Swarm Optimization algorithm (ACC_PSO),
Particles of a swarm communicate good positions, velocity and accelerations to
each other as well as dynamically adjust their own position, velocity and
acceleration derived from the best of all particles. The simulation results
show that the proposed algorithm is effective for the design of DFCWs signal
used in MIMO radar.
","['\nB. Roja Reddy\n', '\nUttara Kumari . M\n']","10 pages, 10 figures, Signal & Image Processing : An International
  Journal (SIPIJ), August 2012",,http://dx.doi.org/10.5121/sipij.2012.3416,cs.OH,['cs.OH'],10.5121/sipij.2012.3416,,[]
"AutoAmp : An Open-Source Analog Amplifier Design Tool - For Classroom
  and Lab Purposes",http://arxiv.org/abs/1209.4157v1,2012-09-19T06:14:28Z,2012-09-19T06:14:28Z,"  This correspondence presents an open-source tool AutoAmp developed at the
Indian Institute of Technology, Guwahati. It is available at
http://sourceforge.net/projects/autoamp-iitg/ This tool helps the user to
design different types of electronic amplifiers, using solid state devices, for
a given specification. It can handle several types of designs namely
common-emitter BJT amplifier (single and two-stage), operational amplifiers
(inverting and non-inverting) and power amplifier. Not only does it design the
amplifier, it also simulates the designed amplifier using SPICE simulator and
displays the performance curves. This tool is deemed to prove invaluable in
undergraduate teaching and labs. Especially in electronics-design related
laboratories, the student need not design the amplifiers which are mostly the
heart of many electronic designs.
","['\nOm Prasad Patri\n', '\nK. Sanmukh Rao\n']","presented at the Indian Conference for Academic Research by
  Undergraduate Students (ICARUS), 2010, IIT Kanpur; AutoAmp : An Open-Source
  Analog Amplifier Design Tool - For Classroom and Lab Purposes, Proceedings of
  the Indian Conference for Academic Research by Undergraduate Students
  (ICARUS), 2010",,http://arxiv.org/abs/1209.4157v1,cs.OH,['cs.OH'],,,[]
"A Connected Enterprise - Transformation through Mobility and Social
  Networks",http://arxiv.org/abs/1209.4894v1,2012-09-17T08:36:37Z,2012-09-17T08:36:37Z,"  Due to rapid changes in business dynamics, there is a growing demand to
encourage social conversations/exchanges and the ability to connect and
communicate with peers, partners, customers and other stakeholders anytime,
anywhere which drives the need of mobile-enable, the existing enterprise
applications. This paper highlights a distinct set of needs and key customer
challenges that must be considered and addressed for deployment of Social
Collaboration applications and Mobility services in enterprises. It not only
addresses the Critical Success Factors for enterprise mobility enablement but
also outlines the unique business requirements to rapidly create social
collaboration culture and the discipline of turning social data into meaningful
insights to drive business decisions in real-time. Moreover, the paper
emphasizes on developing composite offerings on social enterprise and Mobile
networks that not only offer the value proposition in terms of financially
oriented results, but also help customer to maximize return on investment
(ROI).
",['\nJitendra Maan\n'],"8 pages, 3 figures",,http://dx.doi.org/10.5121/ijmit.2012.4308,cs.OH,['cs.OH'],10.5121/ijmit.2012.4308,,[]
